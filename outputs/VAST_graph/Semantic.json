{
    "directed": false,
    "multigraph": false,
    "graph": {
        "name": "VAST_graph"
    },
    "nodes": [
        {
            "title": "Visual Exploration of Big Spatio-Temporal Urban Data: A Study of New York City Taxi Trips",
            "data": "As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.226",
            "id": "r_0",
            "s_ids": [
                "s_654",
                "s_1063",
                "s_1490",
                "s_641",
                "s_393"
            ],
            "type": "rich",
            "x": 7.27123498916626,
            "y": 4.883419036865234
        },
        {
            "title": "Towards Better Analysis of Deep Convolutional Neural Networks",
            "data": "Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598831",
            "id": "r_1",
            "s_ids": [
                "s_491",
                "s_1157",
                "s_837",
                "s_1255",
                "s_1488",
                "s_316"
            ],
            "type": "rich",
            "x": 2.0965170860290527,
            "y": 8.978882789611816
        },
        {
            "title": "Enterprise Data Analysis and Visualization: An Interview Study",
            "data": "Organizations rely on data analysts to model customer engagement, streamline operations, improve production, inform business decisions, and combat fraud. Though numerous analysis and visualization tools have been built to improve the scale and efficiency at which analysts can work, there has been little research on how analysis takes place within the social and organizational context of companies. To better understand the enterprise analysts' ecosystem, we conducted semi-structured interviews with 35 data analysts from 25 organizations across a variety of sectors, including healthcare, retail, marketing and finance. Based on our interview data, we characterize the process of industrial data analysis and document how organizational features of an enterprise impact it. We describe recurring pain points, outstanding challenges, and barriers to adoption for visual analytic tools. Finally, we discuss design implications and opportunities for visual analysis research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.219",
            "id": "r_2",
            "s_ids": [
                "s_1271",
                "s_1362",
                "s_298",
                "s_1320"
            ],
            "type": "rich",
            "x": 5.356631278991699,
            "y": 9.290544509887695
        },
        {
            "title": "Visual Traffic Jam Analysis Based on Trajectory Data",
            "data": "In this work, we present an interactive system for visual analysis of urban traffic congestion based on GPS trajectories. For these trajectories we develop strategies to extract and derive traffic jam information. After cleaning the trajectories, they are matched to a road network. Subsequently, traffic speed on each road segment is computed and traffic jam events are automatically detected. Spatially and temporally related events are concatenated in, so-called, traffic jam propagation graphs. These graphs form a high-level description of a traffic jam and its propagation in time and space. Our system provides multiple views for visually exploring and analyzing the traffic condition of a large city as a whole, on the level of propagation graphs, and on road segment level. Case studies with 24 days of taxi GPS trajectories collected in Beijing demonstrate the effectiveness of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.228",
            "id": "r_3",
            "s_ids": [
                "s_1163",
                "s_1471",
                "s_1274",
                "s_1406",
                "s_905"
            ],
            "type": "rich",
            "x": 7.39121150970459,
            "y": 4.8653717041015625
        },
        {
            "title": "Knowledge Generation Model for Visual Analytics",
            "data": "Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis. Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of visual analytic processes, which can be used for communication between researchers. At the end, our model reflects areas of research that future researchers can embark on.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346481",
            "id": "r_4",
            "s_ids": [
                "s_932",
                "s_1336",
                "s_1145",
                "s_1235",
                "s_714",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.80850076675415,
            "y": 9.468534469604492
        },
        {
            "title": "Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow",
            "data": "We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744878",
            "id": "r_5",
            "s_ids": [
                "s_1489",
                "s_1020",
                "s_708",
                "s_1478",
                "s_33",
                "s_376",
                "s_455",
                "s_1273",
                "s_152"
            ],
            "type": "rich",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "title": "SensePlace2: GeoTwitter analytics support for situational awareness",
            "data": "Geographically-grounded situational awareness (SA) is critical to crisis management and is essential in many other decision making domains that range from infectious disease monitoring, through regional planning, to political campaigning. Social media are becoming an important information input to support situational assessment (to produce awareness) in all domains. Here, we present a geovisual analytics approach to supporting SA for crisis events using one source of social media, Twitter. Specifically, we focus on leveraging explicit and implicit geographic information for tweets, on developing place-time-theme indexing schemes that support overview+detail methods and that scale analytical capabilities to relatively large tweet volumes, and on providing visual interface methods to enable understanding of place, time, and theme components of evolving situations. Our approach is user-centered, using scenario-based design methods that include formal scenarios to guide design and validate implementation as well as a systematic claims analysis to justify design choices and provide a framework for future testing. The work is informed by a structured survey of practitioners and the end product of Phase-I development is demonstrated / validated through implementation in SensePlace2, a map-based, web application initially focused on tweets but extensible to other media.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102456",
            "id": "r_6",
            "s_ids": [
                "s_546",
                "s_513",
                "s_907",
                "s_1078",
                "s_887",
                "s_998",
                "s_1236",
                "s_1343"
            ],
            "type": "rich",
            "x": 7.839114665985107,
            "y": 8.217082977294922
        },
        {
            "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models",
            "data": "While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744718",
            "id": "r_7",
            "s_ids": [
                "s_1465",
                "s_775",
                "s_823",
                "s_575"
            ],
            "type": "rich",
            "x": 2.157792329788208,
            "y": 8.928483963012695
        },
        {
            "title": "Temporal Event Sequence Simplification",
            "data": "Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.200",
            "id": "r_8",
            "s_ids": [
                "s_191",
                "s_713",
                "s_468",
                "s_1399",
                "s_219"
            ],
            "type": "rich",
            "x": 5.925091743469238,
            "y": 7.095569610595703
        },
        {
            "title": "UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization",
            "data": "Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.212",
            "id": "r_9",
            "s_ids": [
                "s_331",
                "s_137",
                "s_747",
                "s_1096"
            ],
            "type": "rich",
            "x": 7.193861961364746,
            "y": 10.908971786499023
        },
        {
            "title": "RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records",
            "data": "We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865027",
            "id": "r_10",
            "s_ids": [
                "s_1235",
                "s_451",
                "s_1064",
                "s_1421",
                "s_968",
                "s_1417",
                "s_101",
                "s_331"
            ],
            "type": "rich",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "title": "The Role of Uncertainty, Awareness, and Trust in Visual Analytics",
            "data": "Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467591",
            "id": "r_11",
            "s_ids": [
                "s_932",
                "s_1123",
                "s_1235",
                "s_714",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.74345588684082,
            "y": 8.819846153259277
        },
        {
            "title": "Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition",
            "data": "Recent advances in technology have enabled social media services to support space-time indexed data, and internet users from all over the world have created a large volume of time-stamped, geo-located data. Such spatiotemporal data has immense value for increasing situational awareness of local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing social media data, researchers have mainly focused on finding temporal trends according to volume-based importance. Hence, a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach that provides users with scalable and interactive social media data analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and YouTube. In order to find and understand abnormal events, the analyst can first extract major topics from a set of selected messages and rank them probabilistically using Latent Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks and outliers within topic time series. Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive visual analysis process.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400557",
            "id": "r_12",
            "s_ids": [
                "s_200",
                "s_746",
                "s_214",
                "s_605",
                "s_721",
                "s_952",
                "s_314"
            ],
            "type": "rich",
            "x": 7.801175117492676,
            "y": 7.876396656036377
        },
        {
            "title": "RuleMatrix: Visualizing and Understanding Classifiers with Rules",
            "data": "With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864812",
            "id": "r_13",
            "s_ids": [
                "s_1422",
                "s_131",
                "s_473"
            ],
            "type": "rich",
            "x": 3.229904890060425,
            "y": 8.69764518737793
        },
        {
            "title": "Characterizing Guidance in Visual Analytics",
            "data": "Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598468",
            "id": "r_14",
            "s_ids": [
                "s_810",
                "s_172",
                "s_1218",
                "s_1124",
                "s_1202",
                "s_512",
                "s_385"
            ],
            "type": "rich",
            "x": 4.256865501403809,
            "y": 9.337084770202637
        },
        {
            "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization",
            "data": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030418",
            "id": "r_15",
            "s_ids": [
                "s_1241",
                "s_1147",
                "s_643",
                "s_29",
                "s_229",
                "s_1042",
                "s_1465",
                "s_575"
            ],
            "type": "rich",
            "x": 2.1530587673187256,
            "y": 8.92031478881836
        },
        {
            "title": "Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis",
            "data": "Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a \u201chuman in the loop\u201d process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598495",
            "id": "r_16",
            "s_ids": [
                "s_932",
                "s_115",
                "s_811",
                "s_525",
                "s_519",
                "s_132",
                "s_1178",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.047524452209473,
            "y": 7.783985614776611
        },
        {
            "title": "MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering",
            "data": "Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468111",
            "id": "r_17",
            "s_ids": [
                "s_241",
                "s_1508",
                "s_858",
                "s_11",
                "s_1195",
                "s_527"
            ],
            "type": "rich",
            "x": 7.490762710571289,
            "y": 5.422610759735107
        },
        {
            "title": "Parallel Tag Clouds to explore and analyze faceted text corpora",
            "data": "Do court cases differ from place to place? What kind of picture do we get by looking at a country's collection of law cases? We introduce parallel tag clouds: a new way to visualize differences amongst facets of very large metadata-rich text corpora. We have pointed parallel tag clouds at a collection of over 600,000 US Circuit Court decisions spanning a period of 50 years and have discovered regional as well as linguistic differences between courts. The visualization technique combines graphical elements from parallel coordinates and traditional tag clouds to provide rich overviews of a document collection while acting as an entry point for exploration of individual texts. We augment basic parallel tag clouds with a details-in-context display and an option to visualize changes over a second facet of the data, such as time. We also address text mining challenges such as selecting the best words to visualize, and how to do so in reasonable time periods to maintain interactivity.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333443",
            "id": "r_18",
            "s_ids": [
                "s_1522",
                "s_1273",
                "s_152"
            ],
            "type": "rich",
            "x": 7.4462080001831055,
            "y": 10.18869686126709
        },
        {
            "title": "Spatio-temporal aggregation for visual analysis of movements",
            "data": "Data about movements of various objects are collected in growing amounts by means of current tracking technologies. Traditional approaches to visualization and interactive exploration of movement data cannot cope with data of such sizes. In this research paper we investigate the ways of using aggregation for visual analysis of movement data. We define aggregation methods suitable for movement data and find visualization and interaction techniques to represent results of aggregations and enable comprehensive exploration of the data. We consider two possible views of movement, traffic-oriented and trajectory-oriented. Each view requires different methods of analysis and of data aggregation. We illustrate our argument with example data resulting from tracking multiple cars in Milan and example analysis tasks from the domain of city traffic management.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677356",
            "id": "r_19",
            "s_ids": [
                "s_1195",
                "s_11"
            ],
            "type": "rich",
            "x": 7.472902297973633,
            "y": 5.329978942871094
        },
        {
            "title": "Visual Analytics Methodology for Eye Movement Studies",
            "data": "Eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces. However, the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large data and data with high variation. We have performed an extensive empirical evaluation of a broad range of visual analytics methods used in analysis of geographic movement data. The methods have been tested for the applicability to eye tracking data and the capability to extract useful knowledge about users' viewing behaviors. This allowed us to select the suitable methods and match them to possible analysis tasks they can support. The paper describes how the methods work in application to eye tracking data and provides guidelines for method selection depending on the analysis tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.276",
            "id": "r_20",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_1400",
                "s_132"
            ],
            "type": "rich",
            "x": 6.037551403045654,
            "y": 7.78853702545166
        },
        {
            "title": "Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes",
            "data": "While the primary goal of visual analytics research is to improve the quality of insights and findings, a substantial amount of research in provenance has focused on the history of changes and advances throughout the analysis process. The term, provenance, has been used in a variety of ways to describe different types of records and histories related to visualization. The existing body of provenance research has grown to a point where the consolidation of design knowledge requires cross-referencing a variety of projects and studies spanning multiple domain areas. We present an organizational framework of the different types of provenance information and purposes for why they are desired in the field of visual analytics. Our organization is intended to serve as a framework to help researchers specify types of provenance and coordinate design knowledge across projects. We also discuss the relationships between these factors and the methods used to capture provenance information. In addition, our organization can be used to guide the selection of evaluation methodology and the comparison of study outcomes in provenance research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467551",
            "id": "r_21",
            "s_ids": [
                "s_365",
                "s_1517",
                "s_860",
                "s_1170"
            ],
            "type": "rich",
            "x": 6.699019908905029,
            "y": 9.748359680175781
        },
        {
            "title": "SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations",
            "data": "The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598432",
            "id": "r_22",
            "s_ids": [
                "s_1095",
                "s_648",
                "s_1080",
                "s_1118",
                "s_89",
                "s_131",
                "s_1347"
            ],
            "type": "rich",
            "x": 5.385289669036865,
            "y": 7.21790075302124
        },
        {
            "title": "Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers",
            "data": "Performance analysis is critical in applied machine learning because it influences the models practitioners produce. Current performance analysis tools suffer from issues including obscuring important characteristics of model behavior and dissociating performance from data. In this work, we present Squares, a performance visualization for multiclass classification problems. Squares supports estimating common performance metrics while displaying instance-level distribution information necessary for helping practitioners prioritize efforts and access data. Our controlled study shows that practitioners can assess performance significantly faster and more accurately with Squares than a confusion matrix, a common performance analysis tool in machine learning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598828",
            "id": "r_23",
            "s_ids": [
                "s_1213",
                "s_1396",
                "s_253",
                "s_966",
                "s_985"
            ],
            "type": "rich",
            "x": 3.4941341876983643,
            "y": 8.608490943908691
        },
        {
            "title": "#FluxFlow: Visual Analysis of Anomalous Information Spreading on Social Media",
            "data": "We present FluxFlow, an interactive visual analysis system for revealing and analyzing anomalous information spreading in social media. Everyday, millions of messages are created, commented, and shared by people on social media websites, such as Twitter and Facebook. This provides valuable data for researchers and practitioners in many application domains, such as marketing, to inform decision-making. Distilling valuable social signals from the huge crowd's messages, however, is challenging, due to the heterogeneous and dynamic crowd behaviors. The challenge is rooted in data analysts' capability of discerning the anomalous information behaviors, such as the spreading of rumors or misinformation, from the rest that are more conventional patterns, such as popular topics and newsworthy events, in a timely fashion. FluxFlow incorporates advanced machine learning algorithms to detect anomalies, and offers a set of novel visualization designs for presenting the detected threads for deeper analysis. We evaluated FluxFlow with real datasets containing the Twitter feeds captured during significant events such as Hurricane Sandy. Through quantitative measurements of the algorithmic performance and qualitative interviews with domain experts, the results show that the back-end anomaly detection model is effective in identifying anomalous retweeting threads, and its front-end interactive visualizations are intuitive and useful for analysts to discover insights in data and comprehend the underlying analytical model.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346922",
            "id": "r_24",
            "s_ids": [
                "s_1354",
                "s_892",
                "s_439",
                "s_529",
                "s_1226",
                "s_1522"
            ],
            "type": "rich",
            "x": 7.767972469329834,
            "y": 8.18466567993164
        },
        {
            "title": "Interactive visual clustering of large collections of trajectories",
            "data": "One of the most common operations in exploration and analysis of various kinds of data is clustering, i.e. discovery and interpretation of groups of objects having similar properties and/or behaviors. In clustering, objects are often treated as points in multi-dimensional space of properties. However, structurally complex objects, such as trajectories of moving entities and other kinds of spatio-temporal data, cannot be adequately represented in this manner. Such data require sophisticated and computationally intensive clustering algorithms, which are very hard to scale effectively to large datasets not fitting in the computer main memory. We propose an approach to extracting meaningful clusters from large databases by combining clustering and classification, which are driven by a human analyst through an interactive visual interface.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332584",
            "id": "r_25",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_250",
                "s_1505",
                "s_340",
                "s_867"
            ],
            "type": "rich",
            "x": 3.618051052093506,
            "y": 5.657589912414551
        },
        {
            "title": "The What-If Tool: Interactive Probing of Machine Learning Models",
            "data": "A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934619",
            "id": "r_26",
            "s_ids": [
                "s_708",
                "s_499",
                "s_43",
                "s_152",
                "s_1273",
                "s_1478"
            ],
            "type": "rich",
            "x": 3.356947183609009,
            "y": 8.667536735534668
        },
        {
            "title": "Progressive Visual Analytics: User-Driven Visual Exploration of In-Progress Analytics",
            "data": "As datasets grow and analytic algorithms become more complex, the typical workflow of analysts launching an analytic, waiting for it to complete, inspecting the results, and then re-Iaunching the computation with adjusted parameters is not realistic for many real-world tasks. This paper presents an alternative workflow, progressive visual analytics, which enables an analyst to inspect partial results of an algorithm as they become available and interact with the algorithm to prioritize subspaces of interest. Progressive visual analytics depends on adapting analytical algorithms to produce meaningful partial results and enable analyst intervention without sacrificing computational speed. The paradigm also depends on adapting information visualization techniques to incorporate the constantly refining results without overwhelming analysts and provide interactions to support an analyst directing the analytic. The contributions of this paper include: a description of the progressive visual analytics paradigm; design goals for both the algorithms and visualizations in progressive visual analytics systems; an example progressive visual analytics system (Progressive Insights) for analyzing common patterns in a collection of event sequences; and an evaluation of Progressive Insights and the progressive visual analytics paradigm by clinical researchers analyzing electronic medical records.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346574",
            "id": "r_27",
            "s_ids": [
                "s_726",
                "s_1177",
                "s_873"
            ],
            "type": "rich",
            "x": 5.311514377593994,
            "y": 9.128715515136719
        },
        {
            "title": "OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media",
            "data": "It is important for many different applications such as government and business intelligence to analyze and explore the diffusion of public opinions on social media. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we introduce a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion of opinions among many users. A stacked tree is used to allow analysts to select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346920",
            "id": "r_28",
            "s_ids": [
                "s_1347",
                "s_316",
                "s_1265",
                "s_491",
                "s_982"
            ],
            "type": "rich",
            "x": 8.011079788208008,
            "y": 8.516590118408203
        },
        {
            "title": "Diamonds in the rough: Social media visual analytics for journalistic inquiry",
            "data": "Journalists increasingly turn to social media sources such as Facebook or Twitter to support their coverage of various news events. For large-scale events such as televised debates and speeches, the amount of content on social media can easily become overwhelming, yet still contain information that may aid and augment reporting via individual content items as well as via aggregate information from the crowd's response. In this work we present a visual analytic tool, Vox Civitas, designed to help journalists and media professionals extract news value from large-scale aggregations of social media content around broadcast events. We discuss the design of the tool, present the text analysis techniques used to enable the presentation, and provide details on the visual and interaction design. We provide an exploratory evaluation based on a user study in which journalists interacted with the system to explore and report on a dataset of over one hundred thousand twitter messages collected during the U.S. State of the Union presidential address in 2010.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652922",
            "id": "r_29",
            "s_ids": [
                "s_1318",
                "s_312",
                "s_915"
            ],
            "type": "rich",
            "x": 8.004839897155762,
            "y": 8.350887298583984
        },
        {
            "title": "TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centralities Using Taxi Trajectory Data",
            "data": "We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467771",
            "id": "r_30",
            "s_ids": [
                "s_279",
                "s_975",
                "s_1449",
                "s_763",
                "s_1322",
                "s_727"
            ],
            "type": "rich",
            "x": 7.365744590759277,
            "y": 4.923183917999268
        },
        {
            "title": "Dis-function: Learning distance functions interactively",
            "data": "The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400486",
            "id": "r_31",
            "s_ids": [
                "s_45",
                "s_272",
                "s_656",
                "s_333"
            ],
            "type": "rich",
            "x": 4.319952011108398,
            "y": 6.838166236877441
        },
        {
            "title": "Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models",
            "data": "Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864499",
            "id": "r_32",
            "s_ids": [
                "s_69",
                "s_160",
                "s_169",
                "s_474",
                "s_952"
            ],
            "type": "rich",
            "x": 3.061535596847534,
            "y": 8.740524291992188
        },
        {
            "title": "DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data",
            "data": "Temporal event sequence data is increasingly commonplace, with applications ranging from electronic medical records to financial transactions to social media activity. Previously developed techniques have focused on low-dimensional datasets (e.g., with less than 20 distinct event types). Real-world datasets are often far more complex. This paper describes DecisionFlow, a visual analysis technique designed to support the analysis of high-dimensional temporal event sequence data (e.g., thousands of event types). DecisionFlow combines a scalable and dynamic temporal event data structure with interactive multi-view visualizations and ad hoc statistical analytics. We provide a detailed review of our methods, and present the results from a 12-person user study. The study results demonstrate that DecisionFlow enables the quick and accurate completion of a range of sequence analysis tasks for datasets containing thousands of event types and millions of individual events.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346682",
            "id": "r_33",
            "s_ids": [
                "s_873",
                "s_759"
            ],
            "type": "rich",
            "x": 7.551761150360107,
            "y": 6.761460781097412
        },
        {
            "title": "Understanding Hidden Memories of Recurrent Neural Networks",
            "data": "Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585721",
            "id": "r_34",
            "s_ids": [
                "s_1422",
                "s_1352",
                "s_292",
                "s_837",
                "s_436",
                "s_1520",
                "s_131"
            ],
            "type": "rich",
            "x": 2.921347141265869,
            "y": 8.863079071044922
        },
        {
            "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations",
            "data": "Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934659",
            "id": "r_35",
            "s_ids": [
                "s_1042",
                "s_29",
                "s_526",
                "s_575"
            ],
            "type": "rich",
            "x": 2.1728076934814453,
            "y": 8.92521858215332
        },
        {
            "title": "Bring It to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis",
            "data": "Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team. Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745181",
            "id": "r_36",
            "s_ids": [
                "s_690",
                "s_1531",
                "s_1272",
                "s_1404",
                "s_1416",
                "s_271",
                "s_511",
                "s_1195",
                "s_1389",
                "s_1038"
            ],
            "type": "rich",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "title": "INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data",
            "data": "Predictive modeling techniques are increasingly being used by data scientists to understand the probability of predicted outcomes. However, for data that is high-dimensional, a critical step in predictive modeling is determining which features should be included in the models. Feature selection algorithms are often used to remove non-informative features from models. However, there are many different classes of feature selection algorithms. Deciding which one to use is problematic as the algorithmic output is often not amenable to user interpretation. This limits the ability for users to utilize their domain expertise during the modeling process. To improve on this limitation, we developed INFUSE, a novel visual analytics system designed to help analysts understand how predictive features are being ranked across feature selection algorithms, cross-validation folds, and classifiers. We demonstrate how our system can lead to important insights in a case study involving clinical researchers predicting patient outcomes from electronic medical records.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346482",
            "id": "r_37",
            "s_ids": [
                "s_467",
                "s_1177",
                "s_473"
            ],
            "type": "rich",
            "x": 3.636216878890991,
            "y": 8.414117813110352
        },
        {
            "title": "DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks",
            "data": "Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744358",
            "id": "r_38",
            "s_ids": [
                "s_1000",
                "s_1204",
                "s_506",
                "s_1081",
                "s_138",
                "s_434"
            ],
            "type": "rich",
            "x": 2.1612935066223145,
            "y": 8.921836853027344
        },
        {
            "title": "iForest: Interpreting Random Forests via Visual Analytics",
            "data": "As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864475",
            "id": "r_39",
            "s_ids": [
                "s_863",
                "s_469",
                "s_1256",
                "s_920"
            ],
            "type": "rich",
            "x": 3.3276243209838867,
            "y": 7.22866678237915
        },
        {
            "title": "Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models",
            "data": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865044",
            "id": "r_40",
            "s_ids": [
                "s_894",
                "s_808",
                "s_547",
                "s_1177",
                "s_1409",
                "s_463"
            ],
            "type": "rich",
            "x": 2.853757619857788,
            "y": 8.860846519470215
        },
        {
            "title": "Do Convolutional Neural Networks Learn Class Hierarchy?",
            "data": "Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744683",
            "id": "r_41",
            "s_ids": [
                "s_1397",
                "s_923",
                "s_953",
                "s_182",
                "s_1393"
            ],
            "type": "rich",
            "x": 2.1292340755462646,
            "y": 8.979385375976562
        },
        {
            "title": "Analyzing the Training Processes of Deep Generative Models",
            "data": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744938",
            "id": "r_42",
            "s_ids": [
                "s_491",
                "s_1157",
                "s_640",
                "s_1488",
                "s_316"
            ],
            "type": "rich",
            "x": 2.1177361011505127,
            "y": 8.989263534545898
        },
        {
            "title": "Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration",
            "data": "We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468078",
            "id": "r_43",
            "s_ids": [
                "s_1300",
                "s_910",
                "s_1395",
                "s_63"
            ],
            "type": "rich",
            "x": 5.216991424560547,
            "y": 5.118741035461426
        },
        {
            "title": "Visual Classifier Training for Text Document Retrieval",
            "data": "Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.277",
            "id": "r_44",
            "s_ids": [
                "s_65",
                "s_404",
                "s_214",
                "s_314"
            ],
            "type": "rich",
            "x": 4.155044078826904,
            "y": 8.48829174041748
        },
        {
            "title": "LeadLine: Interactive visual analysis of text data through event identification and exploration",
            "data": "Text data such as online news and microblogs bear valuable insights regarding important events and responses to such events. Events are inherently temporal, evolving over time. Existing visual text analysis systems have provided temporal views of changes based on topical themes extracted from text data. But few have associated topical themes with events that cause the changes. In this paper, we propose an interactive visual analytics system, LeadLine, to automatically identify meaningful events in news and social media data and support exploration of the events. To characterize events, LeadLine integrates topic modeling, event detection, and named entity recognition techniques to automatically extract information regarding the investigative 4 Ws: who, what, when, and where for each event. To further support analysis of the text corpora through events, LeadLine allows users to interactively examine meaningful events using the 4 Ws to develop an understanding of how and why. Through representing large-scale text corpora in the form of meaningful events, LeadLine provides a concise summary of the corpora. LeadLine also supports the construction of simple narratives through the exploration of events. To demonstrate the efficacy of LeadLine in identifying events and supporting exploration, two case studies were conducted using news and social media data.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400485",
            "id": "r_45",
            "s_ids": [
                "s_305",
                "s_1368",
                "s_1210",
                "s_233",
                "s_98"
            ],
            "type": "rich",
            "x": 7.922826290130615,
            "y": 7.827709674835205
        },
        {
            "title": "TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems",
            "data": "Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467196",
            "id": "r_46",
            "s_ids": [
                "s_892",
                "s_1418",
                "s_88",
                "s_319",
                "s_1226",
                "s_992"
            ],
            "type": "rich",
            "x": 7.491602897644043,
            "y": 8.011017799377441
        },
        {
            "title": "Visual analytics for the big data era---A comparative review of state-of-the-art commercial systems",
            "data": "Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400554",
            "id": "r_47",
            "s_ids": [
                "s_115",
                "s_1336",
                "s_547",
                "s_890",
                "s_511",
                "s_1231",
                "s_1029",
                "s_735",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.062925338745117,
            "y": 9.458945274353027
        },
        {
            "title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
            "data": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986948",
            "id": "r_48",
            "s_ids": [
                "s_142",
                "s_1089",
                "s_1042",
                "s_1465",
                "s_1472",
                "s_311"
            ],
            "type": "rich",
            "x": 4.593509197235107,
            "y": 8.633565902709961
        },
        {
            "title": "HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies",
            "data": "Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.162",
            "id": "r_49",
            "s_ids": [
                "s_305",
                "s_64",
                "s_1368",
                "s_880",
                "s_233"
            ],
            "type": "rich",
            "x": 7.198326110839844,
            "y": 10.895029067993164
        },
        {
            "title": "Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering",
            "data": "Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.260",
            "id": "r_50",
            "s_ids": [
                "s_1517",
                "s_358",
                "s_202"
            ],
            "type": "rich",
            "x": 5.474067211151123,
            "y": 9.94048023223877
        },
        {
            "title": "ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided filtering",
            "data": "The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.186",
            "id": "r_51",
            "s_ids": [
                "s_214",
                "s_746",
                "s_65",
                "s_289",
                "s_404",
                "s_1116",
                "s_1304",
                "s_314"
            ],
            "type": "rich",
            "x": 7.89878511428833,
            "y": 8.160530090332031
        },
        {
            "title": "Observation-level interaction with statistical models for visual analytics",
            "data": "In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus \u201cobservation\u201d) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102449",
            "id": "r_52",
            "s_ids": [
                "s_1517",
                "s_423",
                "s_618",
                "s_15",
                "s_56",
                "s_202"
            ],
            "type": "rich",
            "x": 5.255892276763916,
            "y": 9.593609809875488
        },
        {
            "title": "Combining automated analysis and visualization techniques for effective exploration of high-dimensional data",
            "data": "Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332628",
            "id": "r_53",
            "s_ids": [
                "s_1307",
                "s_391",
                "s_981",
                "s_3",
                "s_711",
                "s_251",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.812294006347656,
            "y": 6.913098335266113
        },
        {
            "title": "Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data",
            "data": "Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467619",
            "id": "r_54",
            "s_ids": [
                "s_327",
                "s_1274",
                "s_1054",
                "s_1333",
                "s_1262",
                "s_1163",
                "s_466",
                "s_918"
            ],
            "type": "rich",
            "x": 7.425845623016357,
            "y": 5.407773494720459
        },
        {
            "title": "A Partition-Based Framework for Building and Validating Regression Models",
            "data": "Regression models play a key role in many application domains for analyzing or predicting a quantitative dependent variable based on one or more independent variables. Automated approaches for building regression models are typically limited with respect to incorporating domain knowledge in the process of selecting input variables (also known as feature subset selection). Other limitations include the identification of local structures, transformations, and interactions between variables. The contribution of this paper is a framework for building regression models addressing these limitations. The framework combines a qualitative analysis of relationship structures by visualization and a quantification of relevance for ranking any number of features and pairs of features which may be categorical or continuous. A central aspect is the local approximation of the conditional target distribution by partitioning 1D and 2D feature domains into disjoint regions. This enables a visual investigation of local patterns and largely avoids structural assumptions for the quantitative ranking. We describe how the framework supports different tasks in model building (e.g., validation and comparison), and we present an interactive workflow for feature subset selection. A real-world case study illustrates the step-wise identification of a five-dimensional model for natural gas consumption. We also report feedback from domain experts after two months of deployment in the energy sector, indicating a significant effort reduction for building and improving regression models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.125",
            "id": "r_55",
            "s_ids": [
                "s_435",
                "s_275"
            ],
            "type": "rich",
            "x": 3.9174604415893555,
            "y": 8.11370849609375
        },
        {
            "title": "explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning",
            "data": "We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934629",
            "id": "r_56",
            "s_ids": [
                "s_1125",
                "s_407",
                "s_261",
                "s_799"
            ],
            "type": "rich",
            "x": 3.1717755794525146,
            "y": 8.746894836425781
        },
        {
            "title": "Clustervision: Visual Supervision of Unsupervised Clustering",
            "data": "Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745085",
            "id": "r_57",
            "s_ids": [
                "s_1235",
                "s_176",
                "s_21",
                "s_106",
                "s_987",
                "s_660",
                "s_1177"
            ],
            "type": "rich",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "title": "BaobabView: Interactive construction and analysis of decision trees",
            "data": "We present a system for the interactive construction and analysis of decision trees that enables domain experts to bring in domain specific knowledge. We identify different user tasks and corresponding requirements, and develop a system incorporating a tight integration of visualization, interaction and algorithmic support. Domain experts are supported in growing, pruning, optimizing and analysing decision trees. Furthermore, we present a scalable decision tree visualization optimized for exploration. We show the effectiveness of our approach by applying the methods to two use cases. The first case illustrates the advantages of interactive construction, the second case demonstrates the effectiveness of analysis of decision trees and exploration of the structure of the data.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102453",
            "id": "r_58",
            "s_ids": [
                "s_1300",
                "s_63"
            ],
            "type": "rich",
            "x": 3.710676431655884,
            "y": 7.653061866760254
        },
        {
            "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation",
            "data": "Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864500",
            "id": "r_59",
            "s_ids": [
                "s_1465",
                "s_783",
                "s_575",
                "s_1273",
                "s_152"
            ],
            "type": "rich",
            "x": 2.183356523513794,
            "y": 8.8823823928833
        },
        {
            "title": "Visualizing Mobility of Public Transportation System",
            "data": "Public transportation systems (PTSs) play an important role in modern cities, providing shared/massive transportation services that are essential for the general public. However, due to their increasing complexity, designing effective methods to visualize and explore PTS is highly challenging. Most existing techniques employ network visualization methods and focus on showing the network topology across stops while ignoring various mobility-related factors such as riding time, transfer time, waiting time, and round-the-clock patterns. This work aims to visualize and explore passenger mobility in a PTS with a family of analytical tasks based on inputs from transportation researchers. After exploring different design alternatives, we come up with an integrated solution with three visualization modules: isochrone map view for geographical information, isotime flow map view for effective temporal information comparison and manipulation, and OD-pair journey view for detailed visual analysis of mobility factors along routes between specific origin-destination pairs. The isotime flow map linearizes a flow map into a parallel isoline representation, maximizing the visualization of mobility information along the horizontal time axis while presenting clear and smooth pathways from origin to destinations. Moreover, we devise several interactive visual query methods for users to easily explore the dynamics of PTS mobility over space and time. Lastly, we also construct a PTS mobility model from millions of real passenger trajectories, and evaluate our visualization techniques with assorted case studies with the transportation researchers.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346893",
            "id": "r_60",
            "s_ids": [
                "s_1366",
                "s_1401",
                "s_222",
                "s_1160",
                "s_131"
            ],
            "type": "rich",
            "x": 7.451045989990234,
            "y": 4.985848426818848
        },
        {
            "title": "Finding Waldo: Learning about Users from their Interactions",
            "data": "Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346575",
            "id": "r_61",
            "s_ids": [
                "s_45",
                "s_778",
                "s_154",
                "s_413",
                "s_1187",
                "s_1517",
                "s_333"
            ],
            "type": "rich",
            "x": 4.95570182800293,
            "y": 10.061023712158203
        },
        {
            "title": "EvoRiver: Visual Analysis of Topic Coopetition on Social Media",
            "data": "Cooperation and competition (jointly called \u201ccoopetition\u201d) are two modes of interactions among a set of concurrent topics on social media. How do topics cooperate or compete with each other to gain public attention? Which topics tend to cooperate or compete with one another? Who plays the key role in coopetition-related interactions? We answer these intricate questions by proposing a visual analytics system that facilitates the in-depth analysis of topic coopetition on social media. We model the complex interactions among topics as a combination of carry-over, coopetition recruitment, and coopetition distraction effects. This model provides a close functional approximation of the coopetition process by depicting how different groups of influential users (i.e., \u201ctopic leaders\u201d) affect coopetition. We also design EvoRiver, a time-based visualization, that allows users to explore coopetition-related interactions and to detect dynamically evolving patterns, as well as their major causes. We test our model and demonstrate the usefulness of our system based on two Twitter data sets (social topics data and business topics data).",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346919",
            "id": "r_62",
            "s_ids": [
                "s_280",
                "s_1347",
                "s_316",
                "s_41",
                "s_337",
                "s_269"
            ],
            "type": "rich",
            "x": 7.939236164093018,
            "y": 8.651774406433105
        },
        {
            "title": "ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories",
            "data": "Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598664",
            "id": "r_63",
            "s_ids": [
                "s_1144",
                "s_1414",
                "s_1393",
                "s_1319"
            ],
            "type": "rich",
            "x": 5.471433162689209,
            "y": 7.99417781829834
        },
        {
            "title": "iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction",
            "data": "We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA). Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster structure. Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coordinates and a scatter plot. Furthermore, it significantly improves the interactivity and interpretability of LDA. LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain. By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space. Equipped with these functionalities, iVisClassifier supports users' classification tasks in an efficient way. Using several facial image data, we show how the above analysis is performed.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652443",
            "id": "r_64",
            "s_ids": [
                "s_331",
                "s_468",
                "s_1459",
                "s_1096"
            ],
            "type": "rich",
            "x": 4.266314506530762,
            "y": 6.490163803100586
        },
        {
            "title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks",
            "data": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864504",
            "id": "r_65",
            "s_ids": [
                "s_788",
                "s_116",
                "s_363",
                "s_874"
            ],
            "type": "rich",
            "x": 2.1607532501220703,
            "y": 8.918561935424805
        },
        {
            "title": "Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations",
            "data": "An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346578",
            "id": "r_66",
            "s_ids": [
                "s_435",
                "s_275",
                "s_73",
                "s_811",
                "s_512"
            ],
            "type": "rich",
            "x": 3.6921157836914062,
            "y": 7.853316783905029
        },
        {
            "title": "Visual Analysis of Topic Competition on Social Media",
            "data": "How do various topics compete for public attention when they are spreading on social media? What roles do opinion leaders play in the rise and fall of competitiveness of various topics? In this study, we propose an expanded topic competition model to characterize the competition for public attention on multiple topics promoted by various opinion leaders on social media. To allow an intuitive understanding of the estimated measures, we present a timeline visualization through a metaphoric interpretation of the results. The visual design features both topical and social aspects of the information diffusion process by compositing ThemeRiver with storyline style visualization. ThemeRiver shows the increase and decrease of competitiveness of each topic. Opinion leaders are drawn as threads that converge or diverge with regard to their roles in influencing the public agenda change over time. To validate the effectiveness of the visual analysis techniques, we report the insights gained on two collections of Tweets: the 2012 United States presidential election and the Occupy Wall Street movement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.221",
            "id": "r_67",
            "s_ids": [
                "s_1144",
                "s_1347",
                "s_744",
                "s_41",
                "s_316",
                "s_337",
                "s_131"
            ],
            "type": "rich",
            "x": 7.994541645050049,
            "y": 8.44855785369873
        },
        {
            "title": "Visual Exploration of Sparse Traffic Trajectory Data",
            "data": "In this paper, we present a visual analysis system to explore sparse traffic trajectory data recorded by transportation cells. Such data contains the movements of nearly all moving vehicles on the major roads of a city. Therefore it is very suitable for macro-traffic analysis. However, the vehicle movements are recorded only when they pass through the cells. The exact tracks between two consecutive cells are unknown. To deal with such uncertainties, we first design a local animation, showing the vehicle movements only in the vicinity of cells. Besides, we ignore the micro-behaviors of individual vehicles, and focus on the macro-traffic patterns. We apply existing trajectory aggregation techniques to the dataset, studying cell status pattern and inter-cell flow pattern. Beyond that, we propose to study the correlation between these two patterns with dynamic graph visualization techniques. It allows us to check how traffic congestion on one cell is correlated with traffic flows on neighbouring links, and with route selection in its neighbourhood. Case studies show the effectiveness of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346746",
            "id": "r_68",
            "s_ids": [
                "s_1163",
                "s_82",
                "s_1471",
                "s_1274",
                "s_131",
                "s_632",
                "s_876"
            ],
            "type": "rich",
            "x": 7.463348865509033,
            "y": 5.214798927307129
        },
        {
            "title": "Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study",
            "data": "Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744818",
            "id": "r_69",
            "s_ids": [
                "s_893",
                "s_412",
                "s_1530",
                "s_335",
                "s_811"
            ],
            "type": "rich",
            "x": 4.2764739990234375,
            "y": 8.524543762207031
        },
        {
            "title": "Visual Methods for Analyzing Probabilistic Classification Data",
            "data": "Multi-class classifiers often compute scores for the classification samples describing probabilities to belong to different classes. In order to improve the performance of such classifiers, machine learning experts need to analyze classification results for a large number of labeled samples to find possible reasons for incorrect classification. Confusion matrices are widely used for this purpose. However, they provide no information about classification scores and features computed for the samples. We propose a set of integrated visual methods for analyzing the performance of probabilistic classifiers. Our methods provide insight into different aspects of the classification results for a large number of samples. One visualization emphasizes at which probabilities these samples were classified and how these probabilities correlate with classification error in terms of false positives and false negatives. Another view emphasizes the features of these samples and ranks them by their separation power between selected true and false classifications. We demonstrate the insight gained using our technique in a benchmarking classification dataset, and show how it enables improving classification performance by interactively defining and evaluating post-classification rules.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346660",
            "id": "r_70",
            "s_ids": [
                "s_1397",
                "s_1392",
                "s_378",
                "s_1124",
                "s_870"
            ],
            "type": "rich",
            "x": 4.1527791023254395,
            "y": 8.461838722229004
        },
        {
            "title": "WireVis: Visualization of Categorical, Time-Varying Data From Financial Transactions",
            "data": "Large financial institutions such as Bank of America handle hundreds of thousands of wire transactions per day. Although most transactions are legitimate, these institutions have legal and financial obligations in discovering those that are suspicious. With the methods of fraudulent activities ever changing, searching on predefined patterns is often insufficient in detecting previously undiscovered methods. In this paper, we present a set of coordinated visualizations based on identifying specific keywords within the wire transactions. The different views used in our system depict relationships among keywords and accounts over time. Furthermore, we introduce a search-by-example technique which extracts accounts that show similar transaction patterns. In collaboration with the Anti-Money Laundering division at Bank of America, we demonstrate that using our tool, investigators are able to detect accounts and transactions that exhibit suspicious behaviors.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389009",
            "id": "r_71",
            "s_ids": [
                "s_333",
                "s_1222",
                "s_68",
                "s_233",
                "s_763",
                "s_224",
                "s_855",
                "s_1267",
                "s_402"
            ],
            "type": "rich",
            "x": 8.852354049682617,
            "y": 7.137535572052002
        },
        {
            "title": "Patterns and Sequences: Interactive Exploration of Clickstreams to Understand Common Visitor Paths",
            "data": "Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difficult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598797",
            "id": "r_72",
            "s_ids": [
                "s_1115",
                "s_160",
                "s_1074",
                "s_36",
                "s_1201",
                "s_159"
            ],
            "type": "rich",
            "x": 7.511104106903076,
            "y": 6.730006217956543
        },
        {
            "title": "Visual analysis of route diversity",
            "data": "Route suggestion is an important feature of GPS navigation systems. Recently, Microsoft T-drive has been enabled to suggest routes chosen by experienced taxi drivers for given source/destination pairs in given time periods, which often take less time than the routes calculated according to distance. However, in real environments, taxi drivers may use different routes to reach the same destination, which we call route diversity. In this paper we first propose a trajectory visualization method that examines the regions where the diversity exists and then develop several novel visualization techniques to display the high dimensional attributes and statistics associated with different routes to help users analyze diversity patterns. Our techniques have been applied to the real trajectory data of thousands of taxis and some interesting findings about route diversity have been obtained. We further demonstrate that our system can be used not only to suggest better routes for drivers but also to analyze traffic bottlenecks for transportation management.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102455",
            "id": "r_73",
            "s_ids": [
                "s_1364",
                "s_374",
                "s_917",
                "s_1283",
                "s_131",
                "s_270"
            ],
            "type": "rich",
            "x": 7.3848042488098145,
            "y": 4.858085632324219
        },
        {
            "title": "Jigsaw: Supporting Investigative Analysis through Interactive Visualization",
            "data": "Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389006",
            "id": "r_74",
            "s_ids": [
                "s_756",
                "s_657",
                "s_1115",
                "s_179"
            ],
            "type": "rich",
            "x": 6.378787994384766,
            "y": 9.850112915039062
        },
        {
            "title": "Visual Analysis and Dissemination of Scientific Literature Collections with SurVis",
            "data": "Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467757",
            "id": "r_75",
            "s_ids": [
                "s_399",
                "s_671",
                "s_132"
            ],
            "type": "rich",
            "x": 7.059408187866211,
            "y": 9.953104019165039
        },
        {
            "title": "A Visual Interface for Multivariate Temporal Data: Finding Patterns of Events across Multiple Histories",
            "data": "Finding patterns of events over time is important in searching patient histories, Web logs, news stories, and criminal activities. This paper presents PatternFinder, an integrated interface for query and result-set visualization for search and discovery of temporal patterns within multivariate and categorical data sets. We define temporal patterns as sequences of events with inter-event time spans. PatternFinder allows users to specify the attributes of events and time spans to produce powerful pattern queries that are difficult to express with other formalisms. We characterize the range of queries PatternFinder supports as users vary the specificity at which events and time spans are defined. Pattern Finder's query capabilities together with coupled ball-and-chain and tabular visualizations enable users to effectively query, explore and analyze event patterns both within and across data entities (e.g. patient histories, terrorist groups, Web logs, etc.)",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261421",
            "id": "r_76",
            "s_ids": [
                "s_237",
                "s_1292",
                "s_636",
                "s_219"
            ],
            "type": "rich",
            "x": 7.580902099609375,
            "y": 6.7360711097717285
        },
        {
            "title": "ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer",
            "data": "Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865041",
            "id": "r_77",
            "s_ids": [
                "s_1347",
                "s_1491",
                "s_791",
                "s_1277",
                "s_446",
                "s_567",
                "s_367",
                "s_1319"
            ],
            "type": "rich",
            "x": 8.508744239807129,
            "y": 5.945904731750488
        },
        {
            "title": "FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System",
            "data": "Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934668",
            "id": "r_78",
            "s_ids": [
                "s_1001",
                "s_393"
            ],
            "type": "rich",
            "x": 4.949009418487549,
            "y": 7.305002212524414
        },
        {
            "title": "Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data",
            "data": "A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864503",
            "id": "r_79",
            "s_ids": [
                "s_1058",
                "s_976",
                "s_1501",
                "s_1357",
                "s_76",
                "s_4",
                "s_1319"
            ],
            "type": "rich",
            "x": 4.8352508544921875,
            "y": 7.234743595123291
        },
        {
            "title": "SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories",
            "data": "Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as \u201cWhat were the taxi trips starting from Main Street and ending at Wall Street in the morning?\u201d or \u201cWhere are the taxis arriving at the Art Museum at noon typically coming from?\u201d, tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as \u201cMain Street\u201d, \u201cWall Street\u201d, and \u201cArt Museum\u201d. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598416",
            "id": "r_80",
            "s_ids": [
                "s_147",
                "s_748",
                "s_1437",
                "s_763",
                "s_913",
                "s_975",
                "s_1322",
                "s_1319",
                "s_1449",
                "s_679"
            ],
            "type": "rich",
            "x": 7.330615997314453,
            "y": 4.847436428070068
        },
        {
            "title": "Visual Abstraction and Exploration of Multi-class Scatterplots",
            "data": "Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346594",
            "id": "r_81",
            "s_ids": [
                "s_877",
                "s_1319",
                "s_1414",
                "s_1046",
                "s_384",
                "s_1430",
                "s_194",
                "s_699"
            ],
            "type": "rich",
            "x": 4.472042560577393,
            "y": 6.537318706512451
        },
        {
            "title": "EventThread: Visual Summarization and Stage Analysis of Event Sequence Data",
            "data": "Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745320",
            "id": "r_82",
            "s_ids": [
                "s_414",
                "s_28",
                "s_244",
                "s_873",
                "s_1288",
                "s_892"
            ],
            "type": "rich",
            "x": 7.656327247619629,
            "y": 6.817885875701904
        },
        {
            "title": "Supporting Communication and Coordination in Collaborative Sensemaking",
            "data": "When people work together to analyze a data set, they need to organize their findings, hypotheses, and evidence, share that information with their collaborators, and coordinate activities amongst team members. Sharing externalizations (recorded information such as notes) could increase awareness and assist with team communication and coordination. However, we currently know little about how to provide tool support for this sort of sharing. We explore how linked common work (LCW) can be employed within a `collaborative thinking space', to facilitate synchronous collaborative sensemaking activities in Visual Analytics (VA). Collaborative thinking spaces provide an environment for analysts to record, organize, share and connect externalizations. Our tool, CLIP, extends earlier thinking spaces by integrating LCW features that reveal relationships between collaborators' findings. We conducted a user study comparing CLIP to a baseline version without LCW. Results demonstrated that LCW significantly improved analytic outcomes at a collaborative intelligence task. Groups using CLIP were also able to more effectively coordinate their work, and held more discussion of their findings and hypotheses. LCW enabled them to maintain awareness of each other's activities and findings and link those findings to their own work, preventing disruptive oral awareness notifications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346573",
            "id": "r_83",
            "s_ids": [
                "s_1496",
                "s_1266"
            ],
            "type": "rich",
            "x": 5.53845739364624,
            "y": 10.754742622375488
        },
        {
            "title": "Sequence Synopsis: Optimize Visual Summary of Temporal Event Data",
            "data": "Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745083",
            "id": "r_84",
            "s_ids": [
                "s_436",
                "s_1144",
                "s_1393"
            ],
            "type": "rich",
            "x": 7.516151428222656,
            "y": 6.785694599151611
        },
        {
            "title": "Interactive Exploration of Implicit and Explicit Relations in Faceted Datasets",
            "data": "Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.167",
            "id": "r_85",
            "s_ids": [
                "s_1354",
                "s_1522",
                "s_173",
                "s_820"
            ],
            "type": "rich",
            "x": 5.232478141784668,
            "y": 6.284097194671631
        },
        {
            "title": "egoSlider: Visual Analysis of Egocentric Network Evolution",
            "data": "Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468151",
            "id": "r_86",
            "s_ids": [
                "s_469",
                "s_364",
                "s_1354",
                "s_1440",
                "s_460",
                "s_131"
            ],
            "type": "rich",
            "x": 5.1919989585876465,
            "y": 5.070548057556152
        },
        {
            "title": "VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning",
            "data": "While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864838",
            "id": "r_87",
            "s_ids": [
                "s_932",
                "s_832",
                "s_1038",
                "s_273"
            ],
            "type": "rich",
            "x": 3.9596190452575684,
            "y": 9.38784408569336
        },
        {
            "title": "ParallelTopics: A probabilistic approach to exploring document collections",
            "data": "Scalable and effective analysis of large text corpora remains a challenging problem as our ability to collect textual data continues to increase at an exponential rate. To help users make sense of large text corpora, we present a novel visual analytics system, Parallel-Topics, which integrates a state-of-the-art probabilistic topic model Latent Dirichlet Allocation (LDA) with interactive visualization. To describe a corpus of documents, ParallelTopics first extracts a set of semantically meaningful topics using LDA. Unlike most traditional clustering techniques in which a document is assigned to a specific cluster, the LDA model accounts for different topical aspects of each individual document. This permits effective full text analysis of larger documents that may contain multiple topics. To highlight this property of the model, ParallelTopics utilizes the parallel coordinate metaphor to present the probabilistic distribution of a document across topics. Such representation allows the users to discover single-topic vs. multi-topic documents and the relative importance of each topic to a document of interest. In addition, since most text corpora are inherently temporal, ParallelTopics also depicts the topic evolution over time. We have applied ParallelTopics to exploring and analyzing several text corpora, including the scientific proposals awarded by the National Science Foundation and the publications in the VAST community over the years. To demonstrate the efficacy of ParallelTopics, we conducted several expert evaluations, the results of which are reported in this paper.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102461",
            "id": "r_88",
            "s_ids": [
                "s_305",
                "s_1368",
                "s_333",
                "s_233"
            ],
            "type": "rich",
            "x": 7.205925464630127,
            "y": 10.93365478515625
        },
        {
            "title": "Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data",
            "data": "The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \u201csmart city\u201d as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744419",
            "id": "r_89",
            "s_ids": [
                "s_892",
                "s_470",
                "s_1306",
                "s_1226",
                "s_1024",
                "s_897"
            ],
            "type": "rich",
            "x": 7.012173652648926,
            "y": 7.094387531280518
        },
        {
            "title": "Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plots",
            "data": "Due to the uncertain nature of weather prediction, climate simulations are usually performed multiple times with different spatial resolutions. The outputs of simulations are multi-resolution spatial temporal ensembles. Each simulation run uses a unique set of values for multiple convective parameters. Distinct parameter settings from different simulation runs in different resolutions constitute a multi-resolution high-dimensional parameter space. Understanding the correlation between the different convective parameters, and establishing a connection between the parameter settings and the ensemble outputs are crucial to domain scientists. The multi-resolution high-dimensional parameter space, however, presents a unique challenge to the existing correlation visualization techniques. We present Nested Parallel Coordinates Plot (NPCP), a new type of parallel coordinates plots that enables visualization of intra-resolution and inter-resolution parameter correlations. With flexible user control, NPCP integrates superimposition, juxtaposition and explicit encodings in a single view for comparative data visualization and analysis. We develop an integrated visual analytics system to help domain scientists understand the connection between multi-resolution convective parameters and the large spatial temporal ensembles. Our system presents intricate climate ensembles with a comprehensive overview and on-demand geographic details. We demonstrate NPCP, along with the climate ensemble visualization system, based on real-world use-cases from our collaborators in computational and predictive science.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598830",
            "id": "r_90",
            "s_ids": [
                "s_788",
                "s_487",
                "s_363",
                "s_967"
            ],
            "type": "rich",
            "x": 6.290637969970703,
            "y": 6.369799613952637
        },
        {
            "title": "CiteRivers: Visual Analytics of Citation Patterns",
            "data": "The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467621",
            "id": "r_91",
            "s_ids": [
                "s_65",
                "s_1174",
                "s_404",
                "s_314"
            ],
            "type": "rich",
            "x": 7.109839916229248,
            "y": 9.867623329162598
        },
        {
            "title": "Space-Time Visual Analytics of Eye-Tracking Data for Dynamic Stimuli",
            "data": "We introduce a visual analytics method to analyze eye movement data recorded for dynamic stimuli such as video or animated graphics. The focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes can be analyzed in a static 3D representation. Shot-based, spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively. We also facilitate data drill-down: the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data. Common eye-tracking visualization techniques are extended to incorporate the spatiotemporal characteristics of the data. For example, heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization. Our visual analytics approach is assessed in a qualitative users study with expert users, which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.194",
            "id": "r_92",
            "s_ids": [
                "s_1512",
                "s_132"
            ],
            "type": "rich",
            "x": 6.077872276306152,
            "y": 7.662308216094971
        },
        {
            "title": "Serendip: Topic Model-Driven Visual Exploration of Text Corpora",
            "data": "Exploration and discovery in a large text corpus requires investigation at multiple levels of abstraction, from a zoomed-out view of the entire corpus down to close-ups of individual passages and words. At each of these levels, there is a wealth of information that can inform inquiry - from statistical models, to metadata, to the researcher's own knowledge and expertise. Joining all this information together can be a challenge, and there are issues of scale to be combatted along the way. In this paper, we describe an approach to text analysis that addresses these challenges of scale and multiple information sources, using probabilistic topic models to structure exploration through multiple levels of inquiry in a way that fosters serendipitous discovery. In implementing this approach into a tool called Serendip, we incorporate topic model data and metadata into a highly reorderable matrix to expose corpus level trends; extend encodings of tagged text to illustrate probabilistic information at a passage level; and introduce a technique for visualizing individual word rankings, along with interaction techniques and new statistical methods to create links between different levels and information types. We describe example uses from both the humanities and visualization research that illustrate the benefits of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042493",
            "id": "r_93",
            "s_ids": [
                "s_624",
                "s_634",
                "s_370",
                "s_522",
                "s_150"
            ],
            "type": "rich",
            "x": 7.1605377197265625,
            "y": 10.400833129882812
        },
        {
            "title": "From movement tracks through events to places: Extracting and characterizing significant places from mobility data",
            "data": "We propose a visual analytics procedure for analyzing movement data, i.e., recorded tracks of moving objects. It is oriented to a class of problems where it is required to determine significant places on the basis of certain types of events occurring repeatedly in movement data. The procedure consists of four major steps: (1) event extraction from trajectories; (2) event clustering and extraction of relevant places; (3) spatio-temporal aggregation of events or trajectories; (4) analysis of the aggregated data. All steps are scalable with respect to the amount of the data under analysis. We demonstrate the use of the procedure by example of two real-world problems requiring analysis at different spatial scales.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102454",
            "id": "r_94",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_592",
                "s_250",
                "s_621"
            ],
            "type": "rich",
            "x": 7.490694522857666,
            "y": 5.324251174926758
        },
        {
            "title": "An exploratory study of co-located collaborative visual analytics around a tabletop display",
            "data": "Co-located collaboration can be extremely valuable during complex visual analytics tasks. This paper presents an exploratory study of a system designed to support collaborative visual analysis tasks on a digital tabletop display. Fifteen participant pairs employed Cam-biera, a visual analytics system, to solve a problem involving 240 digital documents. Our analysis, supported by observations, system logs, questionnaires, and interview data, explores how pairs approached the problem around the table. We contribute a unique, rich understanding of how users worked together around the table and identify eight types of collaboration styles that can be used to identify how closely people work together while problem solving. We show how the closeness of teams' collaboration influenced how well they performed on the task overall. We further discuss the role of the tabletop for visual analytics tasks and derive novel design implications for future co-located collaborative tabletop problem solving systems.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652880",
            "id": "r_95",
            "s_ids": [
                "s_1066",
                "s_1048",
                "s_375",
                "s_545",
                "s_606"
            ],
            "type": "rich",
            "x": 5.419025897979736,
            "y": 10.711797714233398
        },
        {
            "title": "InterAxis: Steering Scatterplot Axes via Observation-Level Interaction",
            "data": "Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467615",
            "id": "r_96",
            "s_ids": [
                "s_745",
                "s_331",
                "s_1096",
                "s_1517"
            ],
            "type": "rich",
            "x": 4.537721157073975,
            "y": 6.836877346038818
        },
        {
            "title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics",
            "data": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585669",
            "id": "r_97",
            "s_ids": [
                "s_9",
                "s_1022",
                "s_480",
                "s_1517"
            ],
            "type": "rich",
            "x": 4.715351581573486,
            "y": 9.705424308776855
        },
        {
            "title": "Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics",
            "data": "Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745258",
            "id": "r_98",
            "s_ids": [
                "s_586",
                "s_1217",
                "s_1461",
                "s_15",
                "s_56",
                "s_202"
            ],
            "type": "rich",
            "x": 3.845111131668091,
            "y": 5.765905380249023
        },
        {
            "title": "VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications",
            "data": "Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467871",
            "id": "r_99",
            "s_ids": [
                "s_1280",
                "s_1391",
                "s_1512",
                "s_404",
                "s_314"
            ],
            "type": "rich",
            "x": 5.452412128448486,
            "y": 9.522770881652832
        },
        {
            "title": "MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation",
            "data": "We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.178",
            "id": "r_100",
            "s_ids": [
                "s_893",
                "s_350",
                "s_692",
                "s_1218",
                "s_511",
                "s_85"
            ],
            "type": "rich",
            "x": 7.492415904998779,
            "y": 5.876190662384033
        },
        {
            "title": "A framework for uncertainty-aware visual analytics",
            "data": "Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and clustering, have been coupled with visualization to help analysts understand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332611",
            "id": "r_101",
            "s_ids": [
                "s_883",
                "s_1113",
                "s_699"
            ],
            "type": "rich",
            "x": 4.722893714904785,
            "y": 7.768910884857178
        },
        {
            "title": "Finding comparable temporal categorical records: A similarity measure with an interactive visualization",
            "data": "An increasing number of temporal categorical databases are being collected: Electronic Health Records in healthcare organizations, traffic incident logs in transportation systems, or student records in universities. Finding similar records within these large databases requires effective similarity measures that capture the searcher's intent. Many similarity measures exist for numerical time series, but temporal categorical records are different. We propose a temporal categorical similarity measure, the M&amp;M (Match &amp; Mismatch) measure, which is based on the concept of aligning records by sentinel events, then matching events between the target and the compared records. The M&amp;M measure combines the time differences between pairs of events and the number of mismatches. To accom-modate customization of parameters in the M&amp;M measure and results interpretation, we implemented Similan, an interactive search and visualization tool for temporal categorical records. A usability study with 8 participants demonstrated that Similan was easy to learn and enabled them to find similar records, but users had difficulty understanding the M&amp;M measure. The usability study feedback, led to an improved version with a continuous timeline, which was tested in a pilot study with 5 participants.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332595",
            "id": "r_102",
            "s_ids": [
                "s_387",
                "s_219"
            ],
            "type": "rich",
            "x": 7.3873677253723145,
            "y": 6.681953430175781
        },
        {
            "title": "Applying Pragmatics Principles for Interaction with Visual Analytics",
            "data": "Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744684",
            "id": "r_103",
            "s_ids": [
                "s_256",
                "s_478",
                "s_1266",
                "s_667"
            ],
            "type": "rich",
            "x": 5.195247650146484,
            "y": 9.794124603271484
        },
        {
            "title": "Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning",
            "data": "Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934251",
            "id": "r_104",
            "s_ids": [
                "s_1259",
                "s_569",
                "s_699"
            ],
            "type": "rich",
            "x": 3.6037490367889404,
            "y": 5.632784843444824
        },
        {
            "title": "Supporting Iterative Cohort Construction with Visual Temporal Queries",
            "data": "Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467622",
            "id": "r_105",
            "s_ids": [
                "s_467",
                "s_1177",
                "s_759"
            ],
            "type": "rich",
            "x": 6.009003639221191,
            "y": 7.040498733520508
        },
        {
            "title": "Preserving Minority Structures in Graph Sampling",
            "data": "Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030428",
            "id": "r_106",
            "s_ids": [
                "s_1357",
                "s_328",
                "s_438",
                "s_1097",
                "s_974",
                "s_1091",
                "s_316",
                "s_1058",
                "s_286",
                "s_107"
            ],
            "type": "rich",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "title": "Visual Diagnosis of Tree Boosting Methods",
            "data": "Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744378",
            "id": "r_107",
            "s_ids": [
                "s_316",
                "s_369",
                "s_1289",
                "s_1069",
                "s_1433",
                "s_1488"
            ],
            "type": "rich",
            "x": 3.3129467964172363,
            "y": 7.173401355743408
        },
        {
            "title": "Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytics for Community Policing and Law Enforcement",
            "data": "In this paper, we present a visual analytics approach that provides decision makers with a proactive and predictive environment in order to assist them in making effective resource allocation and deployment decisions. The challenges involved with such predictive analytics processes include end-users' understanding, and the application of the underlying statistical algorithms at the right spatiotemporal granularity levels so that good prediction estimates can be established. In our approach, we provide analysts with a suite of natural scale templates and methods that enable them to focus and drill down to appropriate geospatial and temporal resolution levels. Our forecasting technique is based on the Seasonal Trend decomposition based on Loess (STL) method, which we apply in a spatiotemporal visual analytics context to provide analysts with predicted levels of future activity. We also present a novel kernel density estimation technique we have developed, in which the prediction process is influenced by the spatial correlation of recent incidents at nearby locations. We demonstrate our techniques by applying our methodology to Criminal, Traffic and Civil (CTC) incident datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346926",
            "id": "r_108",
            "s_ids": [
                "s_52",
                "s_721",
                "s_1055",
                "s_1211",
                "s_952"
            ],
            "type": "rich",
            "x": 6.677735328674316,
            "y": 6.570087909698486
        },
        {
            "title": "Visual cluster exploration of web clickstream data",
            "data": "Web clickstream data are routinely collected to study how users browse the web or use a service. It is clear that the ability to recognize and summarize user behavior patterns from such data is valuable to e-commerce companies. In this paper, we introduce a visual analytics system to explore the various user behavior patterns reflected by distinct clickstream clusters. In a practical analysis scenario, the system first presents an overview of clickstream clusters using a Self-Organizing Map with Markov chain models. Then the analyst can interactively explore the clusters through an intuitive user interface. He can either obtain summarization of a selected group of data or further refine the clustering result. We evaluated our system using two different datasets from eBay. Analysts who were working on the same data have confirmed the system's effectiveness in extracting user behavior patterns from complex datasets and enhancing their ability to reason.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400494",
            "id": "r_109",
            "s_ids": [
                "s_1237",
                "s_1033",
                "s_1088",
                "s_699"
            ],
            "type": "rich",
            "x": 7.071080684661865,
            "y": 8.232402801513672
        },
        {
            "title": "Clustering Trajectories by Relevant Parts for Air Traffic Analysis",
            "data": "Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744322",
            "id": "r_110",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_1053",
                "s_329"
            ],
            "type": "rich",
            "x": 7.483101844787598,
            "y": 5.324905872344971
        },
        {
            "title": "A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights",
            "data": "We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467613",
            "id": "r_111",
            "s_ids": [
                "s_694",
                "s_242",
                "s_855",
                "s_1194"
            ],
            "type": "rich",
            "x": 5.354404926300049,
            "y": 9.868034362792969
        },
        {
            "title": "Subspace search and visualization to make sense of alternative clusterings in high-dimensional data",
            "data": "In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400488",
            "id": "r_112",
            "s_ids": [
                "s_1307",
                "s_357",
                "s_988",
                "s_473",
                "s_511",
                "s_180",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.426377296447754,
            "y": 6.390743732452393
        },
        {
            "title": "Visual analytics for complex concepts using a human cognition model",
            "data": "As the information being visualized and the process of understanding that information both become increasingly complex, it is necessary to develop new visualization approaches that facilitate the flow of human reasoning. In this paper, we endeavor to push visualization design a step beyond current user models by discussing a modeling framework of human ldquohigher cognition.rdquo Based on this cognition model, we present design guidelines for the development of visual interfaces designed to maximize the complementary cognitive strengths of both human and computer. Some of these principles are already being reflected in the better visual analytics designs, while others have not yet been applied or fully applied. But none of the guidelines have explained the deeper rationale that the model provides. Lastly, we discuss and assess these visual analytics guidelines through the evaluation of several visualization examples.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677361",
            "id": "r_113",
            "s_ids": [
                "s_627",
                "s_233",
                "s_1047"
            ],
            "type": "rich",
            "x": 4.772635459899902,
            "y": 9.501033782958984
        },
        {
            "title": "TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis",
            "data": "Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865018",
            "id": "r_114",
            "s_ids": [
                "s_1095",
                "s_1144",
                "s_1393"
            ],
            "type": "rich",
            "x": 5.207071781158447,
            "y": 6.7137908935546875
        },
        {
            "title": "The Interactive Visualization Gap in Initial Exploratory Data Analysis",
            "data": "Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a \u201cvisualization gap\u201d during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743990",
            "id": "r_115",
            "s_ids": [
                "s_681",
                "s_1191"
            ],
            "type": "rich",
            "x": 5.040587425231934,
            "y": 9.46938419342041
        },
        {
            "title": "Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization",
            "data": "We propose a novel video visual analytics system for interactive exploration of surveillance video data. Our approach consists of providing analysts with various views of information related to moving objects in a video. To do this we first extract each object's movement path. We visualize each movement by (a) creating a single action shot image (a still image that coalesces multiple frames), (b) plotting its trajectory in a space-time cube and (c) displaying an overall timeline view of all the movements. The action shots provide a still view of the moving object while the path view presents movement properties such as speed and location. We also provide tools for spatial and temporal filtering based on regions of interest. This allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement. We incorporated this multi-part visual representation of moving objects in sViSIT, a tool to facilitate browsing through the video content by interactive querying and retrieval of data. Based on our interaction with security personnel who routinely interact with surveillance video data, we identified some of the most common tasks performed. This resulted in designing a user study to measure time-to-completion of the various tasks. These generally required searching for specific events of interest (targets) in videos. Fourteen different tasks were designed and a total of 120 min of surveillance video were recorded (indoor and outdoor locations recording movements of people and vehicles). The time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection. We demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest. Reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video visual analytics systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.168",
            "id": "r_116",
            "s_ids": [
                "s_562",
                "s_543"
            ],
            "type": "rich",
            "x": 7.483346462249756,
            "y": 6.029438018798828
        },
        {
            "title": "Explainers: Expert Explorations with Crafted Projections",
            "data": "This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.157",
            "id": "r_117",
            "s_ids": [
                "s_150"
            ],
            "type": "rich",
            "x": 4.085636138916016,
            "y": 7.812134265899658
        },
        {
            "title": "Designing Progressive and Interactive Analytics Processes for High-Dimensional Data Analysis",
            "data": "In interactive data analysis processes, the dialogue between the human and the computer is the enabling mechanism that can lead to actionable observations about the phenomena being investigated. It is of paramount importance that this dialogue is not interrupted by slow computational mechanisms that do not consider any known temporal human-computer interaction characteristics that prioritize the perceptual and cognitive capabilities of the users. In cases where the analysis involves an integrated computational method, for instance to reduce the dimensionality of the data or to perform clustering, such non-optimal processes are often likely. To remedy this, progressive computations, where results are iteratively improved, are getting increasing interest in visual analytics. In this paper, we present techniques and design considerations to incorporate progressive methods within interactive analysis processes that involve high-dimensional data. We define methodologies to facilitate processes that adhere to the perceptual characteristics of users and describe how online algorithms can be incorporated within these. A set of design recommendations and according methods to support analysts in accomplishing high-dimensional data analysis tasks are then presented. Our arguments and decisions here are informed by observations gathered over a series of analysis sessions with analysts from finance. We document observations and recommendations from this study and present evidence on how our approach contribute to the efficiency and productivity of interactive visual analysis sessions involving high-dimensional data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598470",
            "id": "r_118",
            "s_ids": [
                "s_81",
                "s_1305",
                "s_1",
                "s_378"
            ],
            "type": "rich",
            "x": 5.311036586761475,
            "y": 9.03400707244873
        },
        {
            "title": "An Analysis of Machine- and Human-Analytics in Classification",
            "data": "In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the \u201cbag of features\u201d approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598829",
            "id": "r_119",
            "s_ids": [
                "s_1371",
                "s_165",
                "s_273"
            ],
            "type": "rich",
            "x": 4.601602077484131,
            "y": 9.161087036132812
        },
        {
            "title": "Proximity-based visualization of movement trace data",
            "data": "The increasing availability of motion sensors and video cameras in living spaces has made possible the analysis of motion patterns and collective behavior in a number of situations. The visualization of this movement data, however, remains a challenge. Although maintaining the actual layout of the data space is often desirable, direct visualization of movement traces becomes cluttered and confusing as the spatial distribution of traces may be disparate and uneven. We present proximity-based visualization as a novel approach to the visualization of movement traces in an abstract space rather than the given spatial layout. This abstract space is obtained by considering proximity data, which is computed as the distance between entities and some number of important locations. These important locations can range from a single fixed point, to a moving point, several points, or even the proximities between the entities themselves. This creates a continuum of proximity spaces, ranging from the fixed absolute reference frame to completely relative reference frames. By combining these abstracted views with the concrete spatial views, we provide a way to mentally map the abstract spaces back to the real space. We demonstrate the effectiveness of this approach, and its applicability to visual analytics problems such as hazard prevention, migration patterns, and behavioral studies.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332593",
            "id": "r_120",
            "s_ids": [
                "s_1007",
                "s_1041",
                "s_883",
                "s_699"
            ],
            "type": "rich",
            "x": 7.479956150054932,
            "y": 5.427053928375244
        },
        {
            "title": "Literature Fingerprinting: A New Method for Visual Literary Analysis",
            "data": "In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389004",
            "id": "r_121",
            "s_ids": [
                "s_1038",
                "s_37"
            ],
            "type": "rich",
            "x": 7.587126731872559,
            "y": 10.27744197845459
        },
        {
            "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics",
            "data": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030334",
            "id": "r_122",
            "s_ids": [
                "s_935",
                "s_84",
                "s_243",
                "s_380",
                "s_390",
                "s_73",
                "s_921",
                "s_1107",
                "s_1037",
                "s_195"
            ],
            "type": "rich",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "title": "Visual Analytics for Mobile Eye Tracking",
            "data": "The analysis of eye tracking data often requires the annotation of areas of interest (AOIs) to derive semantic interpretations of human viewing behavior during experiments. This annotation is typically the most time-consuming step of the analysis process. Especially for data from wearable eye tracking glasses, every independently recorded video has to be annotated individually and corresponding AOIs between videos have to be identified. We provide a novel visual analytics approach to ease this annotation process by image-based, automatic clustering of eye tracking data integrated in an interactive labeling and analysis system. The annotation and analysis are tightly coupled by multiple linked views that allow for a direct interpretation of the labeled data in the context of the recorded video stimuli. The components of our analytics environment were developed with a user-centered design approach in close cooperation with an eye tracking expert. We demonstrate our approach with eye tracking data from a real experiment and compare it to an analysis of the data by manual annotation of dynamic AOIs. Furthermore, we conducted an expert user study with 6 external eye tracking researchers to collect feedback and identify analysis strategies they used while working with our application.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598695",
            "id": "r_123",
            "s_ids": [
                "s_1512",
                "s_582",
                "s_320",
                "s_132"
            ],
            "type": "rich",
            "x": 6.034650802612305,
            "y": 7.87465238571167
        },
        {
            "title": "Visual analytics decision support environment for epidemic modeling and response evaluation",
            "data": "In modeling infectious diseases, scientists are studying the mechanisms by which diseases spread, predicting the future course of the outbreak, and evaluating strategies applied to control an epidemic. While recent work has focused on accurately modeling disease spread, less work has been performed in developing interactive decision support tools for analyzing the future course of the outbreak and evaluating potential disease mitigation strategies. The absence of such tools makes it difficult for researchers, analysts and public health officials to evaluate response measures within outbreak scenarios. As such, our research focuses on the development of an interactive decision support environment in which users can explore epidemic models and their impact. This environment provides a spatiotemporal view where users can interactively utilize mitigative response measures and observe the impact of their decision over time. Our system also provides users with a linked decision history visualization and navigation tool that support the simultaneous comparison of mortality and infection rates corresponding to different response measures at different points in time.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102457",
            "id": "r_124",
            "s_ids": [
                "s_945",
                "s_721",
                "s_952"
            ],
            "type": "rich",
            "x": 6.411279678344727,
            "y": 6.9612812995910645
        },
        {
            "title": "SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance",
            "data": "Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744805",
            "id": "r_125",
            "s_ids": [
                "s_932",
                "s_832",
                "s_893",
                "s_547",
                "s_511",
                "s_1155",
                "s_1038"
            ],
            "type": "rich",
            "x": 3.595738649368286,
            "y": 5.5941081047058105
        },
        {
            "title": "ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding",
            "data": "Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744478",
            "id": "r_126",
            "s_ids": [
                "s_961",
                "s_1005",
                "s_220",
                "s_331",
                "s_1318",
                "s_1191"
            ],
            "type": "rich",
            "x": 7.521107196807861,
            "y": 10.293533325195312
        },
        {
            "title": "Visual Analytics for Multimodal Social Network Analysis: A Design Study with Social Scientists",
            "data": "Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an openended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.223",
            "id": "r_127",
            "s_ids": [
                "s_520",
                "s_1235",
                "s_452",
                "s_1260",
                "s_1191"
            ],
            "type": "rich",
            "x": 4.951333045959473,
            "y": 5.35188627243042
        },
        {
            "title": "Visual opinion analysis of customer feedback data",
            "data": "Today, online stores collect a lot of customer feedback in the form of surveys, reviews, and comments. This feedback is categorized and in some cases responded to, but in general it is underutilized - even though customer satisfaction is essential to the success of their business. In this paper, we introduce several new techniques to interactively analyze customer comments and ratings to determine the positive and negative opinions expressed by the customers. First, we introduce a new discrimination-based technique to automatically extract the terms that are the subject of the positive or negative opinion (such as price or customer service) and that are frequently commented on. Second, we derive a Reverse-Distance-Weighting method to map the attributes to the related positive and negative opinions in the text. Third, the resulting high-dimensional feature vectors are visualized in a new summary representation that provides a quick overview. We also cluster the reviews according to the similarity of the comments. Special thumbnails are used to provide insight into the composition of the clusters and their relationship. In addition, an interactive circular correlation map is provided to allow analysts to detect the relationships of the comments to other important attributes and the scores. We have applied these techniques to customer comments from real-world online stores and product reviews from web sites to identify the strength and problems of different products and services, and show the potential of our technique.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333919",
            "id": "r_128",
            "s_ids": [
                "s_37",
                "s_211",
                "s_1337",
                "s_1038",
                "s_1169",
                "s_1261",
                "s_653"
            ],
            "type": "rich",
            "x": 7.875896453857422,
            "y": 9.040666580200195
        },
        {
            "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics",
            "data": "In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called \u201cstacked generalization\u201d) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030352",
            "id": "r_129",
            "s_ids": [
                "s_612",
                "s_1092",
                "s_472",
                "s_527"
            ],
            "type": "rich",
            "x": 3.587266445159912,
            "y": 8.453961372375488
        },
        {
            "title": "TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections",
            "data": "Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598445",
            "id": "r_130",
            "s_ids": [
                "s_1369",
                "s_1441",
                "s_961",
                "s_331",
                "s_1191"
            ],
            "type": "rich",
            "x": 7.177179336547852,
            "y": 10.938121795654297
        },
        {
            "title": "DimStiller: Workflows for dimensional analysis and reduction",
            "data": "DimStiller is a system for dimensionality reduction and analysis. It frames the task of understanding and transforming input dimensions as a series of analysis steps where users transform data tables by chaining together different techniques, called operators, into pipelines of expressions. The individual operators have controls and views that are linked together based on the structure of the expression. Users interact with the operator controls to tune parameter choices, with immediate visual feedback guiding the exploration of local neighborhoods of the space of possible data tables. DimStiller also provides global guidance for navigating data-table space through expression templates called workflows, which permit re-use of common patterns of analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652392",
            "id": "r_131",
            "s_ids": [
                "s_372",
                "s_829",
                "s_1316",
                "s_1266",
                "s_1463",
                "s_1484"
            ],
            "type": "rich",
            "x": 4.042430877685547,
            "y": 7.574769496917725
        },
        {
            "title": "Collaborative synthesis of visual analytic results",
            "data": "Visual analytic tools allow analysts to generate large collections of useful analytical results. We anticipate that analysts in most real world situations will draw from these collections when working together to solve complicated problems. This indicates a need to understand how users synthesize multiple collections of results. This paper reports the results of collaborative synthesis experiments conducted with expert geographers and disease biologists. Ten participants were worked in pairs to complete a simulated real-world synthesis task using artifacts printed on cards on a large, paper-covered workspace. Experiment results indicate that groups use a number of different approaches to collaborative synthesis, and that they employ a variety of organizational metaphors to structure their information. It is further evident that establishing common ground and role assignment are critical aspects of collaborative synthesis. We conclude with a set of general design guidelines for collaborative synthesis support tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677358",
            "id": "r_132",
            "s_ids": [
                "s_907"
            ],
            "type": "rich",
            "x": 5.491489887237549,
            "y": 10.709460258483887
        },
        {
            "title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis",
            "data": "Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and effectively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a well-established hybrid second-order Markov chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performance in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and visually explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934630",
            "id": "r_133",
            "s_ids": [
                "s_791",
                "s_534",
                "s_1277",
                "s_356",
                "s_1491",
                "s_723",
                "s_567",
                "s_1347"
            ],
            "type": "rich",
            "x": 8.519206047058105,
            "y": 6.030973434448242
        },
        {
            "title": "Interactive visual comparison of multiple trees",
            "data": "Traditionally, the visual analysis of hierarchies, respectively, trees, is conducted by focusing on one given hierarchy. However, in many research areas multiple, differing hierarchies need to be analyzed simultaneously in a comparative way - in particular to highlight differences between them, which sometimes can be subtle. A prominent example is the analysis of so-called phylogenetic trees in biology, reflecting hierarchical evolutionary relationships among a set of organisms. Typically, the analysis considers multiple phylogenetic trees, either to account for statistical significance or for differences in derivation of such evolutionary hierarchies; for example, based on different input data, such as the 16S ribosomal RNA and protein sequences of highly conserved enzymes. The simultaneous analysis of a collection of such trees leads to more insight into the evolutionary process. We introduce a novel visual analytics approach for the comparison of multiple hierarchies focusing on both global and local structures. A new tree comparison score has been elaborated for the identification of interesting patterns. We developed a set of linked hierarchy views showing the results of automatic tree comparison on various levels of details. This combined approach offers detailed assessment of local and global tree similarities. The approach was developed in close cooperation with experts from the evolutionary biology domain. We apply it to a phylogenetic data set on bacterial ancestry, demonstrating its application benefit.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102439",
            "id": "r_134",
            "s_ids": [
                "s_889",
                "s_241",
                "s_1390",
                "s_511",
                "s_500",
                "s_1165"
            ],
            "type": "rich",
            "x": 3.3674137592315674,
            "y": 6.971466064453125
        },
        {
            "title": "Temporal MDS Plots for Analysis of Multivariate Data",
            "data": "Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or financial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identification of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the field of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467553",
            "id": "r_135",
            "s_ids": [
                "s_39",
                "s_884",
                "s_511",
                "s_1038"
            ],
            "type": "rich",
            "x": 6.8601837158203125,
            "y": 6.1878509521484375
        },
        {
            "title": "PEARL: An Interactive Visual Analytic Tool for Understanding Personal Emotion Style Derived from Social Media",
            "data": "Hundreds of millions of people leave digital footprints on social media (e.g., Twitter and Facebook). Such data not only disclose a person's demographics and opinions, but also reveal one's emotional style. Emotional style captures a person's patterns of emotions over time, including his overall emotional volatility and resilience. Understanding one's emotional style can provide great benefits for both individuals and businesses alike, including the support of self-reflection and delivery of individualized customer care. We present PEARL, a timeline-based visual analytic tool that allows users to interactively discover and examine a person's emotional style derived from this person's social media text. Compared to other visual text analytic systems, our work offers three unique contributions. First, it supports multi-dimensional emotion analysis from social media text to automatically detect a person's expressed emotions at different time points and summarize those emotions to reveal the person's emotional style. Second, it effectively visualizes complex, multi-dimensional emotion analysis results to create a visual emotional profile of an individual, which helps users browse and interpret one's emotional style. Third, it supports rich visual interactions that allow users to interactively explore and validate emotion analysis results. We have evaluated our work extensively through a series of studies. The results demonstrate the effectiveness of our tool both in emotion analysis from social media and in support of interactive visualization of the emotion analysis results.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042496",
            "id": "r_136",
            "s_ids": [
                "s_1354",
                "s_116",
                "s_1381",
                "s_98"
            ],
            "type": "rich",
            "x": 7.794764995574951,
            "y": 8.924461364746094
        },
        {
            "title": "Visual Analysis of Conflicting Opinions",
            "data": "Understanding the nature and dynamics of conflicting opinions is a profound and challenging issue. In this paper we address several aspects of the issue through a study of more than 3,000 Amazon customer reviews of the controversial bestseller The Da Vinci Code, including 1,738 positive and 918 negative reviews. The study is motivated by critical questions such as: what are the differences between positive and negative reviews? What is the origin of a particular opinion? How do these opinions change over time? To what extent can differentiating features be identified from unstructured text? How accurately can these features predict the category of a review? We first analyze terminology variations in these reviews in terms of syntactic, semantic, and statistic associations identified by TermWatch and use term variation patterns to depict underlying topics. We then select the most predictive terms based on log likelihood tests and demonstrate that this small set of terms classifies over 70% of the conflicting reviews correctly. This feature selection process reduces the dimensionality of the feature space from more than 20,000 dimensions to a couple of hundreds. We utilize automatically generated decision trees to facilitate the understanding of conflicting opinions in terms of these highly predictive terms. This study also uses a number of visualization and modeling tools to identify not only what positive and negative reviews have in common, but also they differ and evolve over time",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261431",
            "id": "r_137",
            "s_ids": [
                "s_161",
                "s_688",
                "s_1172",
                "s_309"
            ],
            "type": "rich",
            "x": 7.896146297454834,
            "y": 9.091241836547852
        },
        {
            "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
            "data": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030342",
            "id": "r_138",
            "s_ids": [
                "s_807",
                "s_1422",
                "s_131"
            ],
            "type": "rich",
            "x": 3.1508514881134033,
            "y": 8.802252769470215
        },
        {
            "title": "Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study",
            "data": "While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community's understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102438",
            "id": "r_139",
            "s_ids": [
                "s_738",
                "s_756"
            ],
            "type": "rich",
            "x": 5.415841102600098,
            "y": 9.31286334991455
        },
        {
            "title": "Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study",
            "data": "Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333878",
            "id": "r_140",
            "s_ids": [
                "s_738",
                "s_657",
                "s_756"
            ],
            "type": "rich",
            "x": 5.539113521575928,
            "y": 9.891656875610352
        },
        {
            "title": "Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters",
            "data": "Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865020",
            "id": "r_141",
            "s_ids": [
                "s_1357",
                "s_1142",
                "s_496",
                "s_1408",
                "s_286",
                "s_107",
                "s_1086",
                "s_908",
                "s_1319"
            ],
            "type": "rich",
            "x": 3.5461413860321045,
            "y": 5.572312831878662
        },
        {
            "title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics",
            "data": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745078",
            "id": "r_142",
            "s_ids": [
                "s_9",
                "s_27",
                "s_18",
                "s_1214",
                "s_45",
                "s_1517"
            ],
            "type": "rich",
            "x": 4.688411235809326,
            "y": 8.034207344055176
        },
        {
            "title": "LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets",
            "data": "Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-2-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-3-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis) and the variation of LTS in structures (the combination of<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-4-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-5-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744098",
            "id": "r_143",
            "s_ids": [
                "s_286",
                "s_1454",
                "s_1319",
                "s_1229",
                "s_1430",
                "s_42",
                "s_1358"
            ],
            "type": "rich",
            "x": 4.436132431030273,
            "y": 6.544544696807861
        },
        {
            "title": "Visual Analysis of MOOC Forums with iForum",
            "data": "Discussion forums of Massive Open Online Courses (MOOC) provide great opportunities for students to interact with instructional staff as well as other students. Exploration of MOOC forum data can offer valuable insights for these staff to enhance the course and prepare the next release. However, it is challenging due to the large, complicated, and heterogeneous nature of relevant datasets, which contain multiple dynamically interacting objects such as users, posts, and threads, each one including multiple attributes. In this paper, we present a design study for developing an interactive visual analytics system, called iForum, that allows for effectively discovering and understanding temporal patterns in MOOC forums. The design study was conducted with three domain experts in an iterative manner over one year, including a MOOC instructor and two official teaching assistants. iForum offers a set of novel visualization designs for presenting the three interleaving aspects of MOOC forums (i.e., posts, users, and threads) at three different scales. To demonstrate the effectiveness and usefulness of iForum, we describe a case study involving field experts, in which they use iForum to investigate real MOOC forum data for a course on JAVA programming.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598444",
            "id": "r_144",
            "s_ids": [
                "s_221",
                "s_1354",
                "s_920",
                "s_131"
            ],
            "type": "rich",
            "x": 5.726171970367432,
            "y": 10.625147819519043
        },
        {
            "title": "TopicPanorama: A Full Picture of Relevant Topics",
            "data": "We present a visual analytics approach to developing a full picture of relevant topics discussed in multiple sources such as news, blogs, or micro-blogs. The full picture consists of a number of common topics among multiple sources as well as distinctive topics. The key idea behind our approach is to jointly match the topics extracted from each source together in order to interactively and effectively analyze common and distinctive topics. We start by modeling each textual corpus as a topic graph. These graphs are then matched together with a consistent graph matching method. Next, we develop an LOD-based visualization for better understanding and analysis of the matched graph. The major feature of this visualization is that it combines a radially stacked tree visualization with a density-based graph visualization to facilitate the examination of the matched topic graph from multiple perspectives. To compensate for the deficiency of the graph matching algorithm and meet different users' needs, we allow users to interactively modify the graph matching result. We have applied our approach to various data including news, tweets, and blog data. Qualitative evaluation and a real-world case study with domain experts demonstrate the promise of our approach, especially in support of analyzing a topic-graph-based full picture at different levels of detail.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042494",
            "id": "r_145",
            "s_ids": [
                "s_316",
                "s_1069",
                "s_1098",
                "s_1027",
                "s_1199"
            ],
            "type": "rich",
            "x": 7.447577953338623,
            "y": 9.646245002746582
        },
        {
            "title": "Feedback-Driven Interactive Exploration of Large Multidimensional Data Supported by Visual Classifier",
            "data": "The extraction of relevant and meaningful information from multivariate or high-dimensional data is a challenging problem. One reason for this is that the number of possible representations, which might contain relevant information, grows exponentially with the amount of data dimensions. Also, not all views from a possibly large view space, are potentially relevant to a given analysis task or user. Focus+Context or Semantic Zoom Interfaces can help to some extent to efficiently search for interesting views or data segments, yet they show scalability problems for very large data sets. Accordingly, users are confronted with the problem of identifying interesting views, yet the manual exploration of the entire view space becomes ineffective or even infeasible. While certain quality metrics have been proposed recently to identify potentially interesting views, these often are defined in a heuristic way and do not take into account the application or user context. We introduce a framework for a feedback-driven view exploration, inspired by relevance feedback approaches used in Information Retrieval. Our basic idea is that users iteratively express their notion of interestingness when presented with candidate views. From that expression, a model representing the user's preferences, is trained and used to recommend further interesting view candidates. A decision support system monitors the exploration process and assesses the relevance-driven search process for convergence and stability. We present an instantiation of our framework for exploration of Scatter Plot Spaces based on visual features. We demonstrate the effectiveness of this implementation by a case study on two real-world datasets. We also discuss our framework in light of design alternatives and point out its usefulness for development of user- and context-dependent visual exploration systems.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042480",
            "id": "r_146",
            "s_ids": [
                "s_547",
                "s_228",
                "s_1334",
                "s_511"
            ],
            "type": "rich",
            "x": 5.032182216644287,
            "y": 8.1648588180542
        },
        {
            "title": "A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations",
            "data": "Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages \u201cinstance-level explanations\u201d, measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585720",
            "id": "r_147",
            "s_ids": [
                "s_467",
                "s_1356",
                "s_507",
                "s_677",
                "s_473"
            ],
            "type": "rich",
            "x": 3.168092966079712,
            "y": 8.754652976989746
        },
        {
            "title": "Guiding feature subset selection with an interactive visualization",
            "data": "We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of features. Usually a measure is applied to all entities (rows) of a data table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset selection are, thus, highly interdependent. Due to the difficulty in visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an analysis. Our visualization technique, SmartStripes, allows users to step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different feature and entity subsets. A user may even choose to control the iterations manually, taking into account the ranking measures, the contributions of different entity subsets, as well as the semantics of the features.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102448",
            "id": "r_148",
            "s_ids": [
                "s_1218",
                "s_895",
                "s_326",
                "s_99",
                "s_85"
            ],
            "type": "rich",
            "x": 4.166612148284912,
            "y": 8.035407066345215
        },
        {
            "title": "Improving the visual analysis of high-dimensional datasets using quality measures",
            "data": "Modern visualization methods are needed to cope with very high-dimensional data. Efficient visual analytical techniques are required to extract the information content in these data. The large number of possible projections for each method, which usually grow quadrat-ically or even exponentially with the number of dimensions, urges the necessity to employ automatic reduction techniques, automatic sorting or selecting the projections, based on their information-bearing content. Different quality measures have been successfully applied for several specified user tasks and established visualization techniques, like Scatterplots, Scatterplot Matrices or Parallel Coordinates. Many other popular visualization techniques exist, but due to the structural differences, the measures are not directly applicable to them and new approaches are needed. In this paper we propose new quality measures for three popular visualization methods: Radviz, Pixel-Oriented Displays and Table Lenses. Our experiments show that these measures efficiently guide the visual analysis task.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652433",
            "id": "r_149",
            "s_ids": [
                "s_391",
                "s_981",
                "s_112",
                "s_711",
                "s_251"
            ],
            "type": "rich",
            "x": 4.525405406951904,
            "y": 6.716344356536865
        },
        {
            "title": "Characterizing users' visual analytic activity for insight provenance",
            "data": "Insight provenance - a historical record of the process and rationale by which an insight is derived - is an essential requirement in many visual analytics applications. While work in this area has relied on either manually recorded provenance (e.g., user notes) or automatically recorded event-based insight provenance (e.g., clicks, drags, and key-presses), both approaches have fundamental limitations. Our aim is to develop a new approach that combines the benefits of both approaches while avoiding their deficiencies. Toward this goal, we characterize userspsila visual analytic activity at multiple levels of granularity. Moreover, we identify a critical level of abstraction, Actions, that can be used to represent visual analytic activity with a set of general but semantically meaningful behavior types. In turn, the action types can be used as the semantic building blocks for insight provenance. We present a catalog of common actions identified through observations of several different visual analytic systems. In addition, we define a taxonomy to categorize actions into three major classes based on their semantic intent. The concept of actions has been integrated into our labpsilas prototype visual analytic system, HARVEST, as the basis for its insight provenance capabilities.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677365",
            "id": "r_150",
            "s_ids": [
                "s_873",
                "s_98"
            ],
            "type": "rich",
            "x": 5.550726890563965,
            "y": 9.704619407653809
        },
        {
            "title": "Towards Better Bus Networks: A Visual Analytics Approach",
            "data": "Bus routes are typically updated every 3\u20135 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real-world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges, namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates, and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision-making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real-world data and received positive feedback from the experts. Index Terms-Bus route planning, spatial decision-making, urban data visual analytics",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030458",
            "id": "r_151",
            "s_ids": [
                "s_648",
                "s_665",
                "s_1249",
                "s_544",
                "s_1118",
                "s_89",
                "s_67",
                "s_1347"
            ],
            "type": "rich",
            "x": 7.309029579162598,
            "y": 4.781009197235107
        },
        {
            "title": "The Visual Causality Analyst: An Interactive Interface for Causal Reasoning",
            "data": "Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst - a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467931",
            "id": "r_152",
            "s_ids": [
                "s_948",
                "s_254"
            ],
            "type": "rich",
            "x": 4.244919300079346,
            "y": 8.151086807250977
        },
        {
            "title": "FeatureInsight: Visual support for error-driven feature ideation in text classification",
            "data": "Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347637",
            "id": "r_153",
            "s_ids": [
                "s_916",
                "s_1396",
                "s_253",
                "s_1068",
                "s_70",
                "s_872"
            ],
            "type": "rich",
            "x": 7.385095596313477,
            "y": 10.23172664642334
        },
        {
            "title": "ClusterSculptor: A Visual Analytics Tool for High-Dimensional Data",
            "data": "Cluster analysis (CA) is a powerful strategy for the exploration of high-dimensional data in the absence of a-priori hypotheses or data classification models, and the results of CA can then be used to form such models. But even though formal models and classification rules may not exist in these data exploration scenarios, domain scientists and experts generally have a vast amount of non-compiled knowledge and intuition that they can bring to bear in this effort. In CA, there are various popular mechanisms to generate the clusters, however, the results from their non- supervised deployment rarely fully agree with this expert knowledge and intuition. To this end, our paper describes a comprehensive and intuitive framework to aid scientists in the derivation of classification hierarchies in CA, using k-means as the overall clustering engine, but allowing them to tune its parameters interactively based on a non-distorted compact visual presentation of the inherent characteristics of the data in high- dimensional space. These include cluster geometry, composition, spatial relations to neighbors, and others. In essence, we provide all the tools necessary for a high-dimensional activity we call cluster sculpting, and the evolving hierarchy can then be viewed in a space-efficient radial dendrogram. We demonstrate our system in the context of the mining and classification of a large collection of millions of data items of aerosol mass spectra, but our framework readily applies to any high-dimensional CA scenario.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388999",
            "id": "r_154",
            "s_ids": [
                "s_826",
                "s_649",
                "s_254",
                "s_570",
                "s_77"
            ],
            "type": "rich",
            "x": 3.615382671356201,
            "y": 5.652681827545166
        },
        {
            "title": "Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework",
            "data": "Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745080",
            "id": "r_155",
            "s_ids": [
                "s_799",
                "s_847",
                "s_1209",
                "s_1038",
                "s_1522"
            ],
            "type": "rich",
            "x": 7.18955659866333,
            "y": 10.92719841003418
        },
        {
            "title": "An Uncertainty-Aware Approach for Exploratory Microblog Retrieval",
            "data": "Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467554",
            "id": "r_156",
            "s_ids": [
                "s_491",
                "s_316",
                "s_505",
                "s_772",
                "s_882",
                "s_1321"
            ],
            "type": "rich",
            "x": 7.730375289916992,
            "y": 8.678378105163574
        },
        {
            "title": "EventAction: Visual analytics for temporal event sequence recommendation",
            "data": "Recommender systems are being widely used to assist people in making decisions, for example, recommending films to watch or books to buy. Despite its ubiquity, the problem of presenting the recommendations of temporal event sequences has not been studied. We propose EventAction, which to our knowledge, is the first attempt at a prescriptive analytics interface designed to present and explain recommendations of temporal event sequences. EventAction provides a visual analytics approach to (1) identify similar records, (2) explore potential outcomes, (3) review recommended temporal event sequences that might help achieve the users' goals, and (4) interactively assist users as they define a personalized action plan associated with a probability of success. Following the design study framework, we designed and deployed EventAction in the context of student advising and reported on the evaluation with a student review manager and three graduate students.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883512",
            "id": "r_157",
            "s_ids": [
                "s_1173",
                "s_1399",
                "s_1034",
                "s_219"
            ],
            "type": "rich",
            "x": 7.681723117828369,
            "y": 7.083856582641602
        },
        {
            "title": "Feature-Driven Visual Analytics of Soccer Data",
            "data": "Soccer is one the most popular sports today and also very interesting from an scientific point of view. We present a system for analyzing high-frequency position-based soccer data at various levels of detail, allowing to interactively explore and analyze for movement features and game events. Our Visual Analytics method covers single-player, multi-player and event-based analytical views. Depending on the task the most promising features are semi-automatically selected, processed, and visualized. Our aim is to help soccer analysts in finding the most important and interesting events in a match. We present a flexible, modular, and expandable layer-based system allowing in-depth analysis. The integration of Visual Analytics techniques into the analysis process enables the analyst to find interesting events based on classification and allows, by a set of custom views, to communicate the found results. The feedback loop in the Visual Analytics pipeline helps to further improve the classification results. We evaluate our approach by investigating real-world soccer matches and collecting additional expert feedback. Several use cases and findings illustrate the capabilities of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042477",
            "id": "r_158",
            "s_ids": [
                "s_653",
                "s_932",
                "s_690",
                "s_511",
                "s_1038",
                "s_1279"
            ],
            "type": "rich",
            "x": 8.318733215332031,
            "y": 6.212932586669922
        },
        {
            "title": "Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics",
            "data": "Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934631",
            "id": "r_159",
            "s_ids": [
                "s_42",
                "s_183",
                "s_590",
                "s_721"
            ],
            "type": "rich",
            "x": 2.394528865814209,
            "y": 8.865325927734375
        },
        {
            "title": "Visual Progression Analysis of Event Sequence Data",
            "data": "Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864885",
            "id": "r_160",
            "s_ids": [
                "s_414",
                "s_633",
                "s_873",
                "s_1173",
                "s_1288",
                "s_892"
            ],
            "type": "rich",
            "x": 7.572399616241455,
            "y": 6.759763717651367
        },
        {
            "title": "Task-Driven Comparison of Topic Models",
            "data": "Topic modeling, a method of statistically extracting thematic content from a large collection of texts, is used for a wide variety of tasks within text analysis. Though there are a growing number of tools and techniques for exploring single models, comparisons between models are generally reduced to a small set of numerical metrics. These metrics may or may not reflect a model's performance on the analyst's intended task, and can therefore be insufficient to diagnose what causes differences between models. In this paper, we explore task-centric topic model comparison, considering how we can both provide detail for a more nuanced understanding of differences and address the wealth of tasks for which topic models are used. We derive comparison tasks from single-model uses of topic models, which predominantly fall into the categories of understanding topics, understanding similarity, and understanding change. Finally, we provide several visualization techniques that facilitate these tasks, including buddy plots, which combine color and position encodings to allow analysts to readily view changes in document similarity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467618",
            "id": "r_161",
            "s_ids": [
                "s_624",
                "s_150"
            ],
            "type": "rich",
            "x": 7.193721294403076,
            "y": 10.904555320739746
        },
        {
            "title": "Space Transformation for Understanding Group Movement",
            "data": "We suggest a methodology for analyzing movement behaviors of individuals moving in a group. Group movement is analyzed at two levels of granularity: the group as a whole and the individuals it comprises. For analyzing the relative positions and movements of the individuals with respect to the rest of the group, we apply space transformation, in which the trajectories of the individuals are converted from geographical space to an abstract 'group space'. The group space reference system is defined by both the position of the group center, which is taken as the coordinate origin, and the direction of the group's movement. Based on the individuals' positions mapped onto the group space, we can compare the behaviors of different individuals, determine their roles and/or ranks within the groups, and, possibly, understand how group movement is organized. The utility of the methodology has been evaluated by applying it to a set of real data concerning movements of wild social animals and discussing the results with experts in animal ethology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.193",
            "id": "r_162",
            "s_ids": [
                "s_11",
                "s_1195",
                "s_1197",
                "s_232",
                "s_1233"
            ],
            "type": "rich",
            "x": 7.453543663024902,
            "y": 5.439891338348389
        },
        {
            "title": "Multi-Model Semantic Interaction for Text Analytics",
            "data": "Semantic interaction offers an intuitive communication mechanism between human users and complex statistical models. By shielding the users from manipulating model parameters, they focus instead on directly manipulating the spatialization, thus remaining in their cognitive zone. However, this technique is not inherently scalable past hundreds of text documents. To remedy this, we present the concept of multi-model semantic interaction, where semantic interactions can be used to steer multiple models at multiple levels of data scale, enabling users to tackle larger data problems. We also present an updated visualization pipeline model for generalized multi-model semantic interaction. To demonstrate multi-model semantic interaction, we introduce StarSPIRE, a visual text analytics prototype that transforms user interactions on documents into both small-scale display layout updates as well as large-scale relevancy-based document selection.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042492",
            "id": "r_163",
            "s_ids": [
                "s_96",
                "s_202",
                "s_15",
                "s_56"
            ],
            "type": "rich",
            "x": 6.742397785186768,
            "y": 10.764819145202637
        },
        {
            "title": "Us vs. Them: Understanding Social Dynamics in Wikipedia with Revert Graph Visualizations",
            "data": "Wikipedia is a wiki-based encyclopedia that has become one of the most popular collaborative on-line knowledge systems. As in any large collaborative system, as Wikipedia has grown, conflicts and coordination costs have increased dramatically. Visual analytic tools provide a mechanism for addressing these issues by enabling users to more quickly and effectively make sense of the status of a collaborative environment. In this paper we describe a model for identifying patterns of conflicts in Wikipedia articles. The model relies on users' editing history and the relationships between user edits, especially revisions that void previous edits, known as \"reverts\". Based on this model, we constructed Revert Graph, a tool that visualizes the overall conflict patterns between groups of users. It enables visual analysis of opinion groups and rapid interactive exploration of those relationships via detail drill- downs. We present user patterns and case studies that show the effectiveness of these techniques, and discuss how they could generalize to other systems.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389010",
            "id": "r_164",
            "s_ids": [
                "s_127",
                "s_764",
                "s_1127",
                "s_1099"
            ],
            "type": "rich",
            "x": 7.373030662536621,
            "y": 9.61923599243164
        },
        {
            "title": "Situ: Identifying and Explaining Suspicious Behavior in Networks",
            "data": "Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865029",
            "id": "r_165",
            "s_ids": [
                "s_447",
                "s_365",
                "s_583",
                "s_1429",
                "s_1533",
                "s_419",
                "s_325",
                "s_830"
            ],
            "type": "rich",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        },
        {
            "title": "TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text",
            "data": "We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467531",
            "id": "r_166",
            "s_ids": [
                "s_1221",
                "s_14",
                "s_829"
            ],
            "type": "rich",
            "x": 7.942614555358887,
            "y": 7.702369689941406
        },
        {
            "title": "Inter-active learning of ad-hoc classifiers for video visual analytics",
            "data": "Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users' trust in the classifier trained. We approach the challenges of ad-hoc training by inter-active learning, which extends active learning by integrating human experts' background knowledge to greater extent. In contrast to active learning, not only does inter-active learning include the users' expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of inter-active learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400492",
            "id": "r_167",
            "s_ids": [
                "s_937",
                "s_121",
                "s_914",
                "s_132",
                "s_875"
            ],
            "type": "rich",
            "x": 4.178314208984375,
            "y": 8.581392288208008
        },
        {
            "title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
            "data": "Traffic light detection is crucial for environment perception and decision-making in autonomous driving. State-of-the-art detectors are built upon deep Convolutional Neural Networks (CNNs) and have exhibited promising performance. However, one looming concern with CNN based detectors is how to thoroughly evaluate the performance of accuracy and robustness before they can be deployed to autonomous vehicles. In this work, we propose a visual analytics system, VATLD, equipped with a disentangled representation learning and semantic adversarial learning, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. The disentangled representation learning extracts data semantics to augment human cognition with human-friendly visual summarization, and the semantic adversarial learning efficiently exposes interpretable robustness risks and enables minimal human interaction for actionable insights. We also demonstrate the effectiveness of various performance improvement strategies derived from actionable insights with our visual analytics system, VATLD, and illustrate some practical implications for safety-critical applications in autonomous driving.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030350",
            "id": "r_168",
            "s_ids": [
                "s_116",
                "s_611",
                "s_962",
                "s_235",
                "s_1111",
                "s_1485",
                "s_1393"
            ],
            "type": "rich",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management",
            "data": "Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934655",
            "id": "r_169",
            "s_ids": [
                "s_1357",
                "s_1146",
                "s_1039",
                "s_1523",
                "s_1242",
                "s_107",
                "s_377",
                "s_908",
                "s_1319"
            ],
            "type": "rich",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "title": "Interactive Correction of Mislabeled Training Data",
            "data": "In this paper, we develop a visual analysis method for interactively improving the quality of labeled data, which is essential to the success of supervised and semi-supervised learning. The quality improvement is achieved through the use of user-selected trusted items. We employ a bi-level optimization model to accurately match the labels of the trusted items and to minimize the training loss. Based on this model, a scalable data correction algorithm is developed to handle tens of thousands of labeled data efficiently. The selection of the trusted items is facilitated by an incremental tSNE with improved computational efficiency and layout stability to ensure a smooth transition between different levels. We evaluated our method on real-world datasets through quantitative evaluation and case studies, and the results were generally favorable.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986943",
            "id": "r_170",
            "s_ids": [
                "s_386",
                "s_299",
                "s_286",
                "s_1433",
                "s_1065",
                "s_316"
            ],
            "type": "rich",
            "x": 4.549491882324219,
            "y": 8.062954902648926
        },
        {
            "title": "The Anchoring Effect in Decision-Making with Visual Analytics",
            "data": "Anchoring effect is the tendency to focus too heavily on one piece of information when making decisions. In this paper, we present a novel, systematic study and resulting analyses that investigate the effects of anchoring effect on human decision-making using visual analytic systems. Visual analytics interfaces typically contain multiple views that present various aspects of information such as spatial, temporal, and categorical. These views are designed to present complex, heterogeneous data in accessible forms that aid decision-making. However, human decision-making is often hindered by the use of heuristics, or cognitive biases, such as anchoring effect. Anchoring effect can be triggered by the order in which information is presented or the magnitude of information presented. Through carefully designed laboratory experiments, we present evidence of anchoring effect in analysis with visual analytics interfaces when users are primed by representation of different pieces of information. We also describe detailed analyses of users' interaction logs which reveal the impact of anchoring bias on the visual representation preferred and paths of analysis. We discuss implications for future research to possibly detect and alleviate anchoring bias.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585665",
            "id": "r_171",
            "s_ids": [
                "s_1077",
                "s_1087",
                "s_343",
                "s_1518",
                "s_1296",
                "s_305"
            ],
            "type": "rich",
            "x": 4.635231018066406,
            "y": 9.437541007995605
        },
        {
            "title": "Capturing and supporting the analysis process",
            "data": "Visual analytics tools provide powerful visual representations in order to support the sense-making process. In this process, analysts typically iterate through sequences of steps many times, varying parameters each time. Few visual analytics tools support this process well, nor do they provide support for visualizing and understanding the analysis process itself. To help analysts understand, explore, reference, and reuse their analysis process, we present a visual analytics system named CzSaw (See-Saw) that provides an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them (in a manner inspired by Jigsaw). Conventional history navigation tools range from basic undo and redo to branching timelines of user actions. In CzSaw's approach to this, first, user interactions are translated into a script language that drives the underlying scripting-driven propagation system. The latter allows analysts to edit analysis steps, and ultimately to program them. Second, on this base, we build both a history view showing progress and alternative paths, and a dependency graph showing the underlying logic of the analysis and dependency relations among the results of each step. These tools result in a visual model of the sense-making process, providing a way for analysts to visualize their analysis process, to reinterpret the problem, explore alternative paths, extract analysis patterns from existing history, and reuse them with other related analyses.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333020",
            "id": "r_172",
            "s_ids": [
                "s_1056",
                "s_1185",
                "s_117",
                "s_1387",
                "s_563",
                "s_551",
                "s_1108",
                "s_1419"
            ],
            "type": "rich",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "title": "Semantic Image Browser: Bridging Information Visualization with Automated Intelligent Image Analysis",
            "data": "Browsing and retrieving images from large image collections are becoming common and important activities. Semantic image analysis techniques, which automatically detect high level semantic contents of images for annotation, are promising solutions toward this problem. However, few efforts have been made to convey the annotation results to users in an intuitive manner to enable effective image browsing and retrieval. There is also a lack of methods to monitor and evaluate the automatic image analysis algorithms due to the high dimensional nature of image data, features, and contents. In this paper, we propose a novel, scalable semantic image browser by applying existing information visualization techniques to semantic image analysis. This browser not only allows users to effectively browse and search in large image databases according to the semantic content of images, but also allows analysts to evaluate their annotation process through interactive visual exploration. The major visualization components of this browser are multi-dimensional scaling (MDS) based image layout, the value and relation (VaR) display that allows effective high dimensional visualization without dimension reduction, and a rich set of interaction tools such as search by sample images and content relationship detection. Our preliminary user study showed that the browser was easy to use and understand, and effective in supporting image browsing and retrieval tasks",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261425",
            "id": "r_173",
            "s_ids": [
                "s_763",
                "s_629",
                "s_922",
                "s_24",
                "s_1503",
                "s_233",
                "s_970"
            ],
            "type": "rich",
            "x": 5.232822895050049,
            "y": 8.288137435913086
        },
        {
            "title": "GPGPU Linear Complexity t-SNE Optimization",
            "data": "In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934307",
            "id": "r_174",
            "s_ids": [
                "s_1000",
                "s_201",
                "s_445",
                "s_1204",
                "s_203",
                "s_1081",
                "s_138",
                "s_434"
            ],
            "type": "rich",
            "x": 4.3963398933410645,
            "y": 6.363718032836914
        },
        {
            "title": "An Interactive Method to Improve Crowdsourced Annotations",
            "data": "In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864843",
            "id": "r_175",
            "s_ids": [
                "s_316",
                "s_718",
                "s_804",
                "s_1344",
                "s_1181"
            ],
            "type": "rich",
            "x": 4.297730922698975,
            "y": 8.582598686218262
        },
        {
            "title": "AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings",
            "data": "Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users' complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user's drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users' nonlinear domain knowledge; 2) the underlying model that translates users' input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users' complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598446",
            "id": "r_176",
            "s_ids": [
                "s_1235",
                "s_745",
                "s_9",
                "s_331",
                "s_1096",
                "s_1517"
            ],
            "type": "rich",
            "x": 4.489716053009033,
            "y": 6.9701619148254395
        },
        {
            "title": "Visual analytics methods for categoric spatio-temporal data",
            "data": "We focus on visual analysis of space- and time-referenced categorical data, which describe possible states of spatial (geographical) objects or locations and their changes over time. The analysis of these data is difficult as there are only limited possibilities to analyze the three aspects (location, time and category) simultaneously. We present a new approach which interactively combines (a) visualization of categorical changes over time; (b) various spatial data displays; (c) computational techniques for task-oriented selection of time steps. They provide an expressive visualization with regard to either the overall evolution over time or unusual changes. We apply our approach on two use cases demonstrating its usefulness for a wide variety of tasks. We analyze data from movement tracking and meteorologic areas. Using our approach, expected events could be detected and new insights were gained.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400553",
            "id": "r_177",
            "s_ids": [
                "s_241",
                "s_889",
                "s_11",
                "s_1195",
                "s_1456"
            ],
            "type": "rich",
            "x": 7.034478664398193,
            "y": 6.645401477813721
        },
        {
            "title": "Visual market sector analysis for financial time series data",
            "data": "The massive amount of financial time series data that originates from the stock market generates large amounts of complex data of high interest. However, adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare. In this paper, we present two techniques and applications that enable the user to interactively analyze large amounts of time series data in real-time in order to get insight into the development of assets, market sectors, countries, and the financial market as a whole. The first technique allows users to quickly analyze combinations of single assets, market sectors as well as countries, compare them to each other, and to visually discover the periods of time where market sectors and countries get into turbulence. The second application clusters a selection of large amounts of financial time series data according to their similarity, and analyzes the distribution of the assets among market sectors. This allows users to identify the characteristic graphs which are representative for the development of a particular market sector, and also to identify the assets which behave considerably differently compared to other assets in the same sector. Both applications allow the user to perform investigative exploration techniques and interactive visual analysis in real-time.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652530",
            "id": "r_178",
            "s_ids": [
                "s_999",
                "s_971",
                "s_535",
                "s_1038"
            ],
            "type": "rich",
            "x": 8.669734954833984,
            "y": 7.024385452270508
        },
        {
            "title": "Flow-based scatterplots for sensitivity analysis",
            "data": "Visualization of multi-dimensional data is challenging due to the number of complex correlations that may be present in the data but that are difficult to be visually identified. One of the main causes for this problem is the inherent loss of information that occurs when high-dimensional data is projected into 2D or 3D. Although 2D scatterplots are ubiquitous due to their simplicity and familiarity, there are not a lot of variations on their basic metaphor. In this paper, we present a new way of visualizing multidimensional data using scatterplots. We extend 2D scatterplots using sensitivity coefficients to highlight local variation of one variable with respect to another. When applied to a scatterplot, these sensitivities can be understood as velocities, and the resulting visualization resembles a flow field. We also present a number of operations, based on flow-field analysis, that help users navigate, select and cluster points in an efficient manner. We show the flexibility and generality of this approach using a number of multidimensional data sets across different domains.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652460",
            "id": "r_179",
            "s_ids": [
                "s_1113",
                "s_883",
                "s_699"
            ],
            "type": "rich",
            "x": 4.475854873657227,
            "y": 6.6913743019104
        },
        {
            "title": "Design Considerations for Collaborative Visual Analytics",
            "data": "Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389011",
            "id": "r_180",
            "s_ids": [
                "s_1320",
                "s_661"
            ],
            "type": "rich",
            "x": 5.535860061645508,
            "y": 10.637393951416016
        },
        {
            "title": "Stories in GeoTime",
            "data": "A story is a powerful abstraction used by intelligence analysts to conceptualize threats and understand patterns as part of the analytical process. This paper demonstrates a system that detects geo-temporal patterns and integrates story narration to increase analytic sense-making cohesion in GeoTime. The GeoTime geo-temporal event visualization tool was augmented with a story system that uses narratives, hypertext linked visualizations, visual annotations, and pattern detection to create an environment for analytic exploration and communication, thereby assisting the analyst in identifying, extracting, arranging and presenting stories within the data The story system lets analysts operate at the story level with higher-level abstractions of data, such as behaviors and events, while staying connected to the evidence. The story system was developed and evaluated in collaboration with analysts.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388992",
            "id": "r_181",
            "s_ids": [
                "s_928",
                "s_1220",
                "s_284",
                "s_796"
            ],
            "type": "rich",
            "x": 7.934601306915283,
            "y": 7.798727989196777
        },
        {
            "title": "BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence",
            "data": "The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864814",
            "id": "r_182",
            "s_ids": [
                "s_1045",
                "s_1427",
                "s_146",
                "s_359",
                "s_774",
                "s_1507",
                "s_348"
            ],
            "type": "rich",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "title": "VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model",
            "data": "Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598497",
            "id": "r_183",
            "s_ids": [
                "s_1001",
                "s_393"
            ],
            "type": "rich",
            "x": 4.862977504730225,
            "y": 7.319921970367432
        },
        {
            "title": "VAET: A Visual Analytics Approach for E-Transactions Time-Series",
            "data": "Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346913",
            "id": "r_184",
            "s_ids": [
                "s_1329",
                "s_1319",
                "s_315",
                "s_766",
                "s_1137",
                "s_763"
            ],
            "type": "rich",
            "x": 8.78813648223877,
            "y": 7.098605155944824
        },
        {
            "title": "Visual Analytics for Model Selection in Time Series Analysis",
            "data": "Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.222",
            "id": "r_185",
            "s_ids": [
                "s_78",
                "s_489",
                "s_1442",
                "s_427",
                "s_1124",
                "s_345"
            ],
            "type": "rich",
            "x": 3.5563883781433105,
            "y": 8.345845222473145
        },
        {
            "title": "DropoutSeer: Visualizing learning patterns in Massive Open Online Courses for dropout reasoning and prediction",
            "data": "Aiming at massive participation and open access education, Massive Open Online Courses (MOOCs) have attracted millions of learners over the past few years. However, the high dropout rate of learners is considered to be one of the most crucial factors that may hinder the development of MOOCs. To tackle this problem, statistical models have been developed to predict dropout behavior based on learner activity logs. Although predictive models can foresee the dropout behavior, it is still difficult for users to understand the reasons behind the predicted results and further design interventions to prevent dropout. In addition, with a better understanding of dropout, researchers in the area of predictive modeling in turn can improve the models. In this paper, we introduce DropoutSeer, a visual analytics system which not only helps instructors and education experts understand the reasons for dropout, but also allows researchers to identify crucial features which can further improve the performance of the models. Both the heterogeneous data extracted from three different kinds of learner activity logs (i.e., clickstream, forum posts and assignment records) and the predicted results are visualized in the proposed system. Case studies and expert interviews have been conducted to demonstrate the usefulness and effectiveness of DropoutSeer.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883517",
            "id": "r_186",
            "s_ids": [
                "s_436",
                "s_267",
                "s_1373",
                "s_1524",
                "s_734",
                "s_131"
            ],
            "type": "rich",
            "x": 3.8067502975463867,
            "y": 8.83480167388916
        },
        {
            "title": "The Data Context Map: Fusing Data and Attributes into a Unified Display",
            "data": "Numerous methods have been described that allow the visualization of the data matrix. But all suffer from a common problem - observing the data points in the context of the attributes is either impossible or inaccurate. We describe a method that allows these types of comprehensive layouts. We achieve it by combining two similarity matrices typically used in isolation - the matrix encoding the similarity of the attributes and the matrix encoding the similarity of the data points. This combined matrix yields two of the four submatrices needed for a full multi-dimensional scaling type layout. The remaining two submatrices are obtained by creating a fused similarity matrix - one that measures the similarity of the data points with respect to the attributes, and vice versa. The resulting layout places the data objects in direct context of the attributes and hence we call it the data context map. It allows users to simultaneously appreciate (1) the similarity of data objects, (2) the similarity of attributes in the specific scope of the collection of data objects, and (3) the relationships of data objects with attributes and vice versa. The contextual layout also allows data regions to be segmented and labeled based on the locations of the attributes. This enables, for example, the map's application in selection tasks where users seek to identify one or more data objects that best fit a certain configuration of factors, using the map to visually balance the tradeoffs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467552",
            "id": "r_187",
            "s_ids": [
                "s_1462",
                "s_254"
            ],
            "type": "rich",
            "x": 4.3357110023498535,
            "y": 6.458837509155273
        },
        {
            "title": "Analyzing the Noise Robustness of Deep Neural Networks",
            "data": "Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802509",
            "id": "r_188",
            "s_ids": [
                "s_491",
                "s_316",
                "s_926",
                "s_640",
                "s_1488"
            ],
            "type": "rich",
            "x": 2.165336847305298,
            "y": 8.943147659301758
        },
        {
            "title": "PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes",
            "data": "In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players' passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team's employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team's passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030359",
            "id": "r_189",
            "s_ids": [
                "s_1491",
                "s_791",
                "s_446",
                "s_1277",
                "s_367",
                "s_567",
                "s_1319",
                "s_1347"
            ],
            "type": "rich",
            "x": 8.564358711242676,
            "y": 5.9735212326049805
        },
        {
            "title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
            "data": "Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934547",
            "id": "r_190",
            "s_ids": [
                "s_1116",
                "s_603",
                "s_227",
                "s_1335",
                "s_644",
                "s_461",
                "s_1409"
            ],
            "type": "rich",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "title": "RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis",
            "data": "We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865043",
            "id": "r_191",
            "s_ids": [
                "s_198",
                "s_1308",
                "s_571",
                "s_155",
                "s_844",
                "s_133",
                "s_63"
            ],
            "type": "rich",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "title": "Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs",
            "data": "During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745279",
            "id": "r_192",
            "s_ids": [
                "s_1354",
                "s_293",
                "s_1066",
                "s_173",
                "s_255"
            ],
            "type": "rich",
            "x": 5.639580249786377,
            "y": 10.001466751098633
        },
        {
            "title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods",
            "data": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598544",
            "id": "r_193",
            "s_ids": [
                "s_1356",
                "s_185",
                "s_686",
                "s_996",
                "s_888",
                "s_684",
                "s_742"
            ],
            "type": "rich",
            "x": 4.675631999969482,
            "y": 9.515111923217773
        },
        {
            "title": "How locus of control influences compatibility with visualization style",
            "data": "Existing research suggests that individual personality differences are correlated with a user's speed and accuracy in solving problems with different types of complex visualization systems. In this paper, we extend this research by isolating factors in personality traits as well as in the visualizations that could have contributed to the observed correlation. We focus on a personality trait known as \u201clocus of control,\u201d which represents a person's tendency to see themselves as controlled by or in control of external events. To isolate variables of the visualization design, we control extraneous factors such as color, interaction, and labeling, and specifically focus on the overall layout style of the visualizations. We conduct a user study with four visualizations that gradually shift from an indentation metaphor to a containment metaphor and compare the participants' speed, accuracy, and preference with their locus of control. Our findings demonstrate that there is indeed a correlation between the two: participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor, while those with an external locus of control perform well with such visualizations. We discuss a possible explanation for this relationship based in cognitive psychology and propose that these results can be used to better understand how people use visualizations and how to adapt visual analytics design to an individual user's needs.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102445",
            "id": "r_194",
            "s_ids": [
                "s_855",
                "s_865",
                "s_501",
                "s_410",
                "s_233",
                "s_333"
            ],
            "type": "rich",
            "x": 4.692488193511963,
            "y": 9.84524154663086
        },
        {
            "title": "Perception-based visual quality measures",
            "data": "In recent years diverse quality measures to support the exploration of high-dimensional data sets have been proposed. Such measures can be very useful to rank and select information-bearing projections of very high dimensional data, when the visual exploration of all possible projections becomes unfeasible. But even though a ranking of the low dimensional projections may support the user in the visual exploration task, different measures deliver different distances between the views that do not necessarily match the expectations of human perception. As an alternative solution, we propose a perception-based approach that, similar to the existing measures, can be used to select information bearing projections of the data. Specifically, we construct a perceptual embedding for the different projections based on the data from a psychophysics study and multi-dimensional scaling. This embedding together with a ranking function is then used to estimate the value of the projections for a specific user task in a perceptual sense.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102437",
            "id": "r_195",
            "s_ids": [
                "s_391",
                "s_981",
                "s_251"
            ],
            "type": "rich",
            "x": 4.503381729125977,
            "y": 6.770071983337402
        },
        {
            "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes",
            "data": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934267",
            "id": "r_196",
            "s_ids": [
                "s_1422",
                "s_1144",
                "s_807",
                "s_131",
                "s_1393"
            ],
            "type": "rich",
            "x": 3.0583817958831787,
            "y": 8.788422584533691
        },
        {
            "title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution",
            "data": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864769",
            "id": "r_197",
            "s_ids": [
                "s_799",
                "s_1209",
                "s_1279",
                "s_1038",
                "s_1522"
            ],
            "type": "rich",
            "x": 6.841172218322754,
            "y": 10.838187217712402
        },
        {
            "title": "VAiRoma: A Visual Analytics System for Making Sense of Places, Times, and Events in Roman History",
            "data": "Learning and gaining knowledge of Roman history is an area of interest for students and citizens at large. This is an example of a subject with great sweep (with many interrelated sub-topics over, in this case, a 3,000 year history) that is hard to grasp by any individual and, in its full detail, is not available as a coherent story. In this paper, we propose a visual analytics approach to construct a data driven view of Roman history based on a large collection of Wikipedia articles. Extracting and enabling the discovery of useful knowledge on events, places, times, and their connections from large amounts of textual data has always been a challenging task. To this aim, we introduce VAiRoma, a visual analytics system that couples state-of-the-art text analysis methods with an intuitive visual interface to help users make sense of events, places, times, and more importantly, the relationships between them. VAiRoma goes beyond textual content exploration, as it permits users to compare, make connections, and externalize the findings all within the visual interface. As a result, VAiRoma allows users to learn and create new knowledge regarding Roman history in an informed way. We evaluated VAiRoma with 16 participants through a user study, with the task being to learn about roman piazzas through finding relevant articles and new relationships. Our study results showed that the VAiRoma system enables the participants to find more relevant articles and connections compared to Web searches and literature search conducted in a roman library. Subjective feedback on VAiRoma was also very positive. In addition, we ran two case studies that demonstrate how VAiRoma can be used for deeper analysis, permitting the rapid discovery and analysis of a small number of key documents even when the original collection contains hundreds of thousands of documents.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467971",
            "id": "r_198",
            "s_ids": [
                "s_1077",
                "s_305",
                "s_394",
                "s_192",
                "s_233"
            ],
            "type": "rich",
            "x": 7.275294780731201,
            "y": 10.136569023132324
        },
        {
            "title": "Analyst's Workspace: An embodied sensemaking environment for large, high-resolution displays",
            "data": "Distributed cognition and embodiment provide compelling models for how humans think and interact with the environment. Our examination of the use of large, high-resolution displays from an embodied perspective has lead directly to the development of a new sensemaking environment called Analyst's Workspace (AW). AW leverages the embodied resources made more accessible through the physical nature of the display to create a spatial workspace. By combining spatial layout of documents and other artifacts with an entity-centric, explorative investigative approach, AW aims to allow the analyst to externalize elements of the sensemaking process as a part of the investigation, integrated into the visual representations of the data itself. In this paper, we describe the various capabilities of AW and discuss the key principles and concepts underlying its design, emphasizing unique design principles for designing visual analytic tools for large, high-resolution displays.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400559",
            "id": "r_199",
            "s_ids": [
                "s_1492",
                "s_202"
            ],
            "type": "rich",
            "x": 5.537674427032471,
            "y": 10.173970222473145
        },
        {
            "title": "Towards the Personal Equation of Interaction: The impact of personality factors on visual analytics interface interaction",
            "data": "These current studies explored the impact of individual differences in personality factors on interface interaction and learning performance behaviors in both an interactive visualization and a menu-driven web table in two studies. Participants were administered 3 psychometric measures designed to assess Locus of Control, Extraversion, and Neuroticism. Participants were then asked to complete multiple procedural learning tasks in each interface. Results demonstrated that all three measures predicted completion times. Additionally, results analyses demonstrated personality factors also predicted the number of insights participants reported while completing the tasks in each interface. We discuss how these findings advance our ongoing research in the Personal Equation of Interaction.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5653587",
            "id": "r_200",
            "s_ids": [
                "s_627",
                "s_1047"
            ],
            "type": "rich",
            "x": 4.850905895233154,
            "y": 10.085546493530273
        },
        {
            "title": "Understanding text corpora with multiple facets",
            "data": "Text visualization becomes an increasingly more important research topic as the need to understand massive-scale textual information is proven to be imperative for many people and businesses. However, it is still very challenging to design effective visual metaphors to represent large corpora of text due to the unstructured and high-dimensional nature of text. In this paper, we propose a data model that can be used to represent most of the text corpora. Such a data model contains four basic types of facets: time, category, content (unstructured), and structured facet. To understand the corpus with such a data model, we develop a hybrid visualization by combining the trend graph with tag-clouds. We encode the four types of data facets with four separate visual dimensions. To help people discover evolutionary and correlation patterns, we also develop several visual interaction methods that allow people to interactively analyze text by one or more facets. Finally, we present two case studies to demonstrate the effectiveness of our solution in support of multi-faceted visual analysis of text corpora.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652931",
            "id": "r_201",
            "s_ids": [
                "s_32",
                "s_882",
                "s_316",
                "s_1119",
                "s_139",
                "s_98"
            ],
            "type": "rich",
            "x": 7.562387943267822,
            "y": 10.414213180541992
        },
        {
            "title": "Monitoring Network Traffic with Radial Traffic Analyzer",
            "data": "Extensive spread of malicious code on the Internet and also within intranets has risen the user's concern about what kind of data is transferred between her or his computer and other hosts on the network. Visual analysis of this kind of information is a challenging task, due to the complexity and volume of the data type considered, and requires special design of appropriate visualization techniques. In this paper, we present a scalable visualization toolkit for analyzing network activity of computer hosts on a network. The visualization combines network packet volume and type distribution information with geographic information, enabling the analyst to use geographic distortion techniques such as the HistoMap technique to become aware of the traffic components in the course of the analysis. The presented analysis tool is especially useful to compare important network load characteristics in a geographically aware display, to relate communication partners, and to identify the type of network traffic occurring. The results of the analysis are helpful in understanding typical network communication activities, and in anticipating potential performance bottlenecks or problems. It is suited for both off-line analysis of historic data, and via animation for on-line monitoring of packet-based network traffic in real time",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261438",
            "id": "r_202",
            "s_ids": [
                "s_1038",
                "s_444",
                "s_3",
                "s_511"
            ],
            "type": "rich",
            "x": 6.86965274810791,
            "y": 7.68827486038208
        },
        {
            "title": "TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees",
            "data": "Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745158",
            "id": "r_203",
            "s_ids": [
                "s_435",
                "s_382",
                "s_1484",
                "s_275"
            ],
            "type": "rich",
            "x": 3.3038711547851562,
            "y": 7.186951160430908
        },
        {
            "title": "VarifocalReader -- In-Depth Visual Analysis of Large Text Documents",
            "data": "Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview-detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus-context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346677",
            "id": "r_204",
            "s_ids": [
                "s_404",
                "s_1391",
                "s_1304",
                "s_843",
                "s_314"
            ],
            "type": "rich",
            "x": 7.425528049468994,
            "y": 10.315485000610352
        },
        {
            "title": "A correlative analysis process in a visual analytics environment",
            "data": "Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson's product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400491",
            "id": "r_205",
            "s_ids": [
                "s_52",
                "s_721",
                "s_1191",
                "s_605",
                "s_952",
                "s_949"
            ],
            "type": "rich",
            "x": 6.880544662475586,
            "y": 6.19067907333374
        },
        {
            "title": "Visual sentiment analysis on twitter data streams",
            "data": "Twitter currently receives about 190 million tweets (small text-based Web posts) a day, in which people share their comments regarding a wide range of topics. A large number of tweets include opinions about products and services. However, with Twitter being a relatively new phenomenon, these tweets are underutilized as a source for evaluating customer sentiment. To explore high-volume twitter data, we introduce three novel time-based visual sentiment analysis techniques: (1) topic-based sentiment analysis that extracts, maps, and measures customer opinions; (2) stream analysis that identifies interesting tweets based on their density, negativity, and influence characteristics; and (3) pixel cell-based sentiment calendars and high density geo maps that visualize large volumes of data in a single view. We applied these techniques to a variety of twitter data, (e.g., movies, amusement parks, and hotels) to show their distribution and patterns, and to identify influential opinions.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102472",
            "id": "r_206",
            "s_ids": [
                "s_211",
                "s_1337",
                "s_1531",
                "s_1169",
                "s_1038",
                "s_1261",
                "s_989"
            ],
            "type": "rich",
            "x": 8.00954818725586,
            "y": 8.506704330444336
        },
        {
            "title": "Session Viewer: Visual Exploratory Analysis of Web Session Logs",
            "data": "Large-scale session log analysis typically includes statistical methods and detailed log examinations. While both methods have merits, statistical methods can miss previously unknown sub- populations in the data and detailed analyses may have selection biases. We therefore built Session Viewer, a visualization tool to facilitate and bridge between statistical and detailed analyses. Taking a multiple-coordinated view approach, Session Viewer shows multiple session populations at the Aggregate, Multiple, and Detail data levels to support different analysis styles. To bridge between the statistical and the detailed analysis levels, Session Viewer provides fluid traversal between data levels and side-by-side comparison at all data levels. We describe an analysis of a large-scale web usage study to demonstrate the use of Session Viewer, where we quantified the importance of grouping sessions based on task type.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389008",
            "id": "r_207",
            "s_ids": [
                "s_509",
                "s_642",
                "s_800",
                "s_829"
            ],
            "type": "rich",
            "x": 5.484500408172607,
            "y": 8.928224563598633
        },
        {
            "title": "Beyond Usability: Evaluation Aspects of Visual Analytic Environments",
            "data": "A new field of research, visual analytics, has been introduced. This has been defined as \"the science of analytical reasoning facilitated by interactive visual interfaces\" (Thomas and Cook, 2005). Visual analytic environments, therefore, support analytical reasoning using visual representations and interactions, with data representations and transformation capabilities, to support production, presentation, and dissemination. As researchers begin to develop visual analytic environments, it is advantageous to develop metrics and methodologies to help researchers measure the progress of their work and understand the impact their work has on the users who work in such environments. This paper presents five areas or aspects of visual analytic environments that should be considered as metrics and methodologies for evaluation are developed. Evaluation aspects need to include usability, but it is necessary to go beyond basic usability. The areas of situation awareness, collaboration, interaction, creativity, and utility are proposed as the five evaluation areas for initial consideration. The steps that need to be undertaken to develop systematic evaluation methodologies and metrics for visual analytic environments are outlined",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261416",
            "id": "r_208",
            "s_ids": [
                "s_1244"
            ],
            "type": "rich",
            "x": 5.1464033126831055,
            "y": 9.692605018615723
        },
        {
            "title": "Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses",
            "data": "In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of \u201cTropical Cyclone Karl\u201d, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745178",
            "id": "r_209",
            "s_ids": [
                "s_321",
                "s_909",
                "s_342",
                "s_584",
                "s_103",
                "s_1443"
            ],
            "type": "rich",
            "x": 3.5826504230499268,
            "y": 5.609038829803467
        },
        {
            "title": "BiSet: Semantic Edge Bundling with Biclusters for Sensemaking",
            "data": "Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships, such as between lists of entities, require repetitious manual selection and significant mental aggregation in cluttered visualizations to find coordinated relationships. In this paper, we present BiSet, a visual analytics technique to support interactive exploration of coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset. Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer task-oriented semantic insights about potentially coordinated activities. We make bundles as first class objects and add a new layer, \u201cin-between\u201d, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467813",
            "id": "r_210",
            "s_ids": [
                "s_102",
                "s_700",
                "s_202",
                "s_1461"
            ],
            "type": "rich",
            "x": 4.947014808654785,
            "y": 5.668623447418213
        },
        {
            "title": "Visual cluster analysis of trajectory data with interactive Kohonen Maps",
            "data": "Visual-interactive cluster analysis provides valuable tools for effectively analyzing large and complex data sets. Due to desirable properties and an inherent predisposition for visualization, the Kohonen Feature Map (or self-organizing map, or SOM) algorithm is among the most popular and widely used visual clustering techniques. However, the unsupervised nature of the algorithm may be disadvantageous in certain applications. Depending on initialization and data characteristics, cluster maps (cluster layouts) may emerge that do not comply with user preferences, expectations, or the application context. Considering SOM-based analysis of trajectory data, we propose a comprehensive visual-interactive monitoring and control framework extending the basic SOM algorithm. The framework implements the general Visual Analytics idea to effectively combine automatic data analysis with human expert supervision. It provides simple, yet effective facilities for visually monitoring and interactively controlling the trajectory clustering process at arbitrary levels of detail. The approach allows the user to leverage existing domain knowledge and user preferences, arriving at improved cluster maps. We apply the framework on a trajectory clustering problem, demonstrating its potential in combining both unsupervised (machine) and supervised (human expert) processing, in producing appropriate cluster results.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677350",
            "id": "r_211",
            "s_ids": [
                "s_511",
                "s_893",
                "s_728",
                "s_85"
            ],
            "type": "rich",
            "x": 3.6725172996520996,
            "y": 5.735623359680176
        },
        {
            "title": "D-Dupe: An Interactive Tool for Entity Resolution in Social Networks",
            "data": "Visualizing and analyzing social networks is a challenging problem that has been receiving growing attention. An important first step, before analysis can begin, is ensuring that the data is accurate. A common data quality problem is that the data may inadvertently contain several distinct references to the same underlying entity; the process of reconciling these references is called entity-resolution. D-Dupe is an interactive tool that combines data mining algorithms for entity resolution with a task-specific network visualization. Users cope with complexity of cleaning large networks by focusing on a small subnetwork containing a potential duplicate pair. The subnetwork highlights relationships in the social network, making the common relationships easy to visually identify. D-Dupe users resolve ambiguities either by merging nodes or by marking them distinct. The entity resolution process is iterative: as pairs of nodes are resolved, additional duplicates may be revealed; therefore, resolution decisions are often chained together. We give examples of how users can flexibly apply sequences of actions to produce a high quality entity resolution result. We illustrate and evaluate the benefits of D-Dupe on three bibliographic collections. Two of the datasets had already been cleaned, and therefore should not have contained duplicates; despite this fact, many duplicates were rapidly identified using D-Dupe's unique combination of entity resolution algorithms within a task-specific visual interface",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261429",
            "id": "r_212",
            "s_ids": [
                "s_1083",
                "s_885",
                "s_145",
                "s_219"
            ],
            "type": "rich",
            "x": 4.7123332023620605,
            "y": 5.83228063583374
        },
        {
            "title": "FairSight: Visual Analytics for Fairness in Decision Making",
            "data": "Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions \u2013 understanding, measuring, diagnosing and mitigating biases \u2013 that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934262",
            "id": "r_213",
            "s_ids": [
                "s_1413",
                "s_1226"
            ],
            "type": "rich",
            "x": 4.636303901672363,
            "y": 8.653554916381836
        },
        {
            "title": "VIGOR: Interactive Visual Exploration of Graph Query Results",
            "data": "Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744898",
            "id": "r_214",
            "s_ids": [
                "s_680",
                "s_1042",
                "s_1517",
                "s_1338",
                "s_572",
                "s_587",
                "s_1072",
                "s_311"
            ],
            "type": "rich",
            "x": 4.909725189208984,
            "y": 5.396003723144531
        },
        {
            "title": "EVA: Visual Analytics to Identify Fraudulent Events",
            "data": "Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744758",
            "id": "r_215",
            "s_ids": [
                "s_19",
                "s_172",
                "s_1124",
                "s_405",
                "s_1150",
                "s_49",
                "s_1031"
            ],
            "type": "rich",
            "x": 8.856771469116211,
            "y": 7.150928497314453
        },
        {
            "title": "A Visual Analytics Approach for Understanding Reasons behind Snowballing and Comeback in MOBA Games",
            "data": "To design a successful Multiplayer Online Battle Arena (MOBA) game, the ratio of snowballing and comeback occurrences to all matches played must be maintained at a certain level to ensure its fairness and engagement. Although it is easy to identify these two types of occurrences, game developers often find it difficult to determine their causes and triggers with so many game design choices and game parameters involved. In addition, the huge amounts of MOBA game data are often heterogeneous, multi-dimensional and highly dynamic in terms of space and time, which poses special challenges for analysts. In this paper, we present a visual analytics system to help game designers find key events and game parameters resulting in snowballing or comeback occurrences in MOBA game data. We follow a user-centered design process developing the system with game analysts and testing with real data of a trial version MOBA game from NetEase Inc. We apply novel visualization techniques in conjunction with well-established ones to depict the evolution of players' positions, status and the occurrences of events. Our system can reveal players' strategies and performance throughout a single match and suggest patterns, e.g., specific player' actions and game events, that have led to the final occurrences. We further demonstrate a workflow of leveraging human analyzed patterns to improve the scalability and generality of match data analysis. Finally, we validate the usability of our system by proving the identified patterns are representative in snowballing or comeback matches in a one-month-long MOBA tournament dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598415",
            "id": "r_216",
            "s_ids": [
                "s_492",
                "s_1332",
                "s_986",
                "s_651",
                "s_930",
                "s_131",
                "s_1470"
            ],
            "type": "rich",
            "x": 8.538335800170898,
            "y": 6.002535820007324
        },
        {
            "title": "D-Map: Visual Analysis of Ego-centric Information Diffusion Patterns in Social Media",
            "data": "Popular social media platforms could rapidly propagate vital information over social networks among a significant number of people. In this work we present D-Map (Diffusion Map), a novel visualization method to support exploration and analysis of social behaviors during such information diffusion and propagation on typical social media through a map metaphor. In D-Map, users who participated in reposting (i.e., resending a message initially posted by others) one central user's posts (i.e., a series of original tweets) are collected and mapped to a hexagonal grid based on their behavior similarities and in chronological order of the repostings. With additional interaction and linking, D-Map is capable of providing visual portraits of the influential users and describing their social behaviors. A comprehensive visual analysis system is developed to support interactive exploration with D-Map. We evaluate our work with real world social media data and find interesting patterns among users. Key players, important information diffusion paths, and interactions among social communities can be identified.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883510",
            "id": "r_217",
            "s_ids": [
                "s_327",
                "s_373",
                "s_1054",
                "s_1262",
                "s_1274",
                "s_892",
                "s_1060"
            ],
            "type": "rich",
            "x": 7.819021224975586,
            "y": 8.561983108520508
        },
        {
            "title": "Integrating Predictive Analytics and Social Media",
            "data": "A key analytical task across many domains is model building and exploration for predictive analysis. Data is collected, parsed and analyzed for relationships, and features are selected and mapped to estimate the response of a system under exploration. As social media data has grown more abundant, data can be captured that may potentially represent behavioral patterns in society. In turn, this unstructured social media data can be parsed and integrated as a key factor for predictive intelligence. In this paper, we present a framework for the development of predictive models utilizing social media data. We combine feature selection mechanisms, similarity comparisons and model cross-validation through a variety of interactive visualizations to support analysts in model building and prediction. In order to explore how predictions might be performed in such a framework, we present results from a user study focusing on social media data as a predictor for movie box-office success.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042495",
            "id": "r_218",
            "s_ids": [
                "s_804",
                "s_1116",
                "s_746",
                "s_854",
                "s_404",
                "s_314",
                "s_721"
            ],
            "type": "rich",
            "x": 7.719987869262695,
            "y": 8.917404174804688
        },
        {
            "title": "Baseball4D: A Tool for Baseball Game Reconstruction & Visualization",
            "data": "While many sports use statistics and video to analyze and improve game play, baseball has led the charge throughout its history. With the advent of new technologies that allow all players and the ball to be tracked across the entire field, it is now possible to bring this understanding to another level. From discrete positions across time, we present techniques to reconstruct entire baseball games and visually explore each play. This provides opportunities to not only derive new metrics for the game, but also allow us to investigate existing measures with targeted visualizations. In addition, our techniques allow users to filter on demand so specific situations can be analyzed both in general and according to those situations. We show that gameplay can be accurately reconstructed from the raw position data and discuss how visualization and statistical methods can combine to better inform baseball analyses.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042478",
            "id": "r_219",
            "s_ids": [
                "s_1011",
                "s_704",
                "s_1490",
                "s_393"
            ],
            "type": "rich",
            "x": 8.519011497497559,
            "y": 6.024631023406982
        },
        {
            "title": "finVis: Applied visual analytics for personal financial planning",
            "data": "FinVis is a visual analytics tool that allows the non-expert casual user to interpret the return, risk and correlation aspects of financial data and make personal finance decisions. This interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes. FinVis allows for exploration of inter-temporal data to analyze outcomes of short-term or long-term investment decisions. FinVis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification. Because this software is accessible by non-expert users, decision-makers from the general population can benefit greatly from using FinVis in practical applications. We quantify the value of FinVis using experimental economics methods and find that subjects using the FinVis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information. We also find that FinVis engages the user, which results in greater exploration of the dataset and increased learning as compared to a tabular display. Further, participants using FinVis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333920",
            "id": "r_220",
            "s_ids": [
                "s_206",
                "s_749",
                "s_952"
            ],
            "type": "rich",
            "x": 8.839740753173828,
            "y": 7.2049055099487305
        },
        {
            "title": "A Utility-Aware Visual Approach for Anonymizing Multi-Attribute Tabular Data",
            "data": "Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745139",
            "id": "r_221",
            "s_ids": [
                "s_281",
                "s_812",
                "s_1319",
                "s_454",
                "s_1050",
                "s_1494",
                "s_699"
            ],
            "type": "rich",
            "x": 6.123578071594238,
            "y": 8.758930206298828
        },
        {
            "title": "Visually Exploring Transportation Schedules",
            "data": "Public transportation schedules are designed by agencies to optimize service quality under multiple constraints. However, real service usually deviates from the plan. Therefore, transportation analysts need to identify, compare and explain both eventual and systemic performance issues that must be addressed so that better timetables can be created. The purely statistical tools commonly used by analysts pose many difficulties due to the large number of attributes at tripand station-level for planned and real service. Also challenging is the need for models at multiple scales to search for patterns at different times and stations, since analysts do not know exactly where or when relevant patterns might emerge and need to compute statistical summaries for multiple attributes at different granularities. To aid in this analysis, we worked in close collaboration with a transportation expert to design TR-EX, a visual exploration tool developed to identify, inspect and compare spatio-temporal patterns for planned and real transportation service. TR-EX combines two new visual encodings inspired by Marey's Train Schedule: Trips Explorer for trip-level analysis of frequency, deviation and speed; and Stops Explorer for station-level study of delay, wait time, reliability and performance deficiencies such as bunching. To tackle overplotting and to provide a robust representation for a large numbers of trips and stops at multiple scales, the system supports variable kernel bandwidths to achieve the level of detail required by users for different tasks. We justify our design decisions based on specific analysis needs of transportation analysts. We provide anecdotal evidence of the efficacy of TR-EX through a series of case studies that explore NYC subway service, which illustrate how TR-EX can be used to confirm hypotheses and derive new insights through visual exploration.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467592",
            "id": "r_222",
            "s_ids": [
                "s_1278",
                "s_556",
                "s_393",
                "s_641"
            ],
            "type": "rich",
            "x": 7.280351638793945,
            "y": 4.832824230194092
        },
        {
            "title": "An Affordance-Based Framework for Human Computation and Human-Computer Collaboration",
            "data": "Visual Analytics is \u201cthe science of analytical reasoning facilitated by visual interactive interfaces\u201d [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.195",
            "id": "r_223",
            "s_ids": [
                "s_865",
                "s_333"
            ],
            "type": "rich",
            "x": 5.2440009117126465,
            "y": 10.282380104064941
        },
        {
            "title": "Click2Annotate: Automated Insight Externalization with rich semantics",
            "data": "Insight Externalization (IE) refers to the process of capturing and recording the semantics of insights in decision making and problem solving. To reduce human effort, Automated Insight Externalization (AIE) is desired. Most existing IE approaches achieve automation by capturing events (e.g., clicks and key presses) or actions (e.g., panning and zooming). In this paper, we propose a novel AIE approach named Click2Annotate. It allows semi-automatic insight annotation that captures low-level analytics task results (e.g., clusters and outliers), which have higher semantic richness and abstraction levels than actions and events. Click2Annotate has two significant benefits. First, it reduces human effort required in IE and generates annotations easy to understand. Second, the rich semantic information encoded in the annotations enables various insight management activities, such as insight browsing and insight retrieval. We present a formal user study that proved this first benefit. We also illustrate the second benefit by presenting the novel insight management activities we developed based on Click2Annotate, namely scented insight browsing and faceted insight search.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652885",
            "id": "r_224",
            "s_ids": [
                "s_1065",
                "s_1137",
                "s_763"
            ],
            "type": "rich",
            "x": 6.1070556640625,
            "y": 9.703055381774902
        },
        {
            "title": "Entity-based collaboration tools for intelligence analysis",
            "data": "Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677362",
            "id": "r_225",
            "s_ids": [
                "s_613",
                "s_819",
                "s_886"
            ],
            "type": "rich",
            "x": 5.510870933532715,
            "y": 10.6893892288208
        },
        {
            "title": "Interactive Visual Synthesis of Analytic Knowledge",
            "data": "A visual investigation involves both the examination of existing information and the synthesis of new analytic knowledge. This is a progressive process in which newly synthesized knowledge becomes the foundation for future discovery. In this paper, we present a novel system supporting interactive, progressive synthesis of analytic knowledge. Here we use the term \"analytic knowledge\" to refer to concepts that a user derives from existing data along with the evidence supporting such concepts. Unlike existing visual analytic-tools, which typically support only exploration of existing information, our system offers two unique features. First, we support user-system cooperative visual synthesis of analytic knowledge from existing data. Specifically, users can visually define new concepts by annotating existing information, and refine partially formed concepts by linking additional evidence or manipulating related concepts. In response to user actions, our system can automatically manage the evolving corpus of synthesized knowledge and its corresponding evidence. Second, we support progressive visual analysis of synthesized knowledge. This feature allows analysts to visually explore both existing knowledge and synthesized knowledge, dynamically incorporating earlier analytic conclusions into the ensuing discovery process. We have applied our system to two complex but very different analytic applications. Our preliminary evaluation shows the promise of our work",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261430",
            "id": "r_226",
            "s_ids": [
                "s_873",
                "s_98",
                "s_74"
            ],
            "type": "rich",
            "x": 4.942386150360107,
            "y": 9.645306587219238
        },
        {
            "title": "Pixnostics: Towards Measuring the Value of Visualization",
            "data": "During the last two decades a wide variety of advanced methods for the visual exploration of large data sets have been proposed. For most of these techniques user interaction has become a crucial element, since there are many situations in which a user or an analyst has to select the right parameter settings from among many or select a subset of the available attribute space for the visualization process, in order to construct valuable visualizations that provide insight, into the data and reveal interesting patterns. The right choice of input parameters is often essential, since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions. In this paper we propose a novel method for automatically determining meaningful parameter- and attribute settings based on the information content of the resulting visualizations. Our technique called Pixnostics, in analogy to Scagnostics (Wilkinson et al., 2005), automatically analyses pixel images resulting from diverse parameter mappings and ranks them according to the potential value for the user. This allows a more effective and more efficient visual data analysis process, since the attribute/parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data. Real world applications are provided to show the benefit of the proposed approach",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261423",
            "id": "r_227",
            "s_ids": [
                "s_3",
                "s_344",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.4891438484191895,
            "y": 7.050303936004639
        },
        {
            "title": "Boba: Authoring and Visualizing Multiverse Analyses",
            "data": "Multiverse analysis is an approach to data analysis in which all \u201creasonable\u201d analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028985",
            "id": "r_228",
            "s_ids": [
                "s_1230",
                "s_779",
                "s_61",
                "s_1320"
            ],
            "type": "rich",
            "x": 4.1113362312316895,
            "y": 8.209181785583496
        },
        {
            "title": "EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection",
            "data": "Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802454",
            "id": "r_229",
            "s_ids": [
                "s_492",
                "s_1282",
                "s_1436",
                "s_86",
                "s_1514",
                "s_1470"
            ],
            "type": "rich",
            "x": 4.743052959442139,
            "y": 5.725571155548096
        },
        {
            "title": "A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Roads",
            "data": "Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset (&amp;gt; 30GB) show that our system performs well for on-demand transport assessment and reasoning.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042486",
            "id": "r_230",
            "s_ids": [
                "s_679",
                "s_1319",
                "s_1138",
                "s_975",
                "s_1076",
                "s_554",
                "s_1257",
                "s_269",
                "s_1044"
            ],
            "type": "rich",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "title": "Network-based visual analysis of tabular data",
            "data": "Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102440",
            "id": "r_231",
            "s_ids": [
                "s_1115",
                "s_1072",
                "s_756"
            ],
            "type": "rich",
            "x": 5.071720600128174,
            "y": 5.358020782470703
        },
        {
            "title": "Toward a Multi-Analyst, Collaborative Framework for Visual Analytics",
            "data": "We describe a framework for the display of complex, multidimensional data, designed to facilitate exploration, analysis, and collaboration among multiple analysts. This framework aims to support human collaboration by making it easier to share representations, to translate from one point of view to another, to explain arguments, to update conclusions when underlying assumptions change, and to justify or account for decisions or actions. Multidimensional visualization techniques are used with interactive, context-sensitive, and tunable graphs. Visual representations are flexibly generated using a knowledge representation scheme based on annotated logic; this enables not only tracking and fusing different viewpoints, but also unpacking them. Fusing representations supports the creation of multidimensional meta-displays as well as the translation or mapping from one point of view to another. At the same time, analysts also need to be able to unpack one another's complex chains of reasoning, especially if they have reached different conclusions, and to determine the implications, if any, when underlying assumptions or evidence turn out to be false. The framework enables us to support a variety of scenarios as well as to systematically generate and test experimental hypotheses about the impact of different kinds of visual representations upon interactive collaboration by teams of distributed analysts",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261439",
            "id": "r_232",
            "s_ids": [
                "s_919",
                "s_254",
                "s_177",
                "s_805",
                "s_324",
                "s_750"
            ],
            "type": "rich",
            "x": 5.165153503417969,
            "y": 9.47240924835205
        },
        {
            "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics",
            "data": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030461",
            "id": "r_233",
            "s_ids": [
                "s_153",
                "s_788",
                "s_363",
                "s_1379",
                "s_50",
                "s_896"
            ],
            "type": "rich",
            "x": 2.099738597869873,
            "y": 8.981844902038574
        },
        {
            "title": "A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications",
            "data": "Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865026",
            "id": "r_234",
            "s_ids": [
                "s_1329",
                "s_705",
                "s_254"
            ],
            "type": "rich",
            "x": 7.053226470947266,
            "y": 7.47855806350708
        },
        {
            "title": "VIS Author Profiles: Interactive Descriptions of Publication Records Combining Text and Visualization",
            "data": "Publication records and collaboration networks are important for assessing the expertise and experience of researchers. Existing digital libraries show the raw publication lists in author profiles, whereas visualization techniques focus on specific subproblems. Instead, we look at publication records from various perspectives mixing low-level publication data with high-level abstractions and background information. This work presents VIS Author Profiles, a novel approach to generate integrated textual and visual descriptions to highlight patterns in publication records. We leverage template-based natural language generation to summarize notable publication statistics, evolution of research topics, and collaboration relationships. Seamlessly integrated visualizations augment the textual description and are interactively connected with each other and the text. The underlying publication data and detailed explanations of the analysis are available on demand. We compare our approach to existing systems by taking into account information needs of users and demonstrate its usefulness in two realistic application examples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865022",
            "id": "r_235",
            "s_ids": [
                "s_16",
                "s_399"
            ],
            "type": "rich",
            "x": 7.126319885253906,
            "y": 9.943053245544434
        },
        {
            "title": "SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data",
            "data": "Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744738",
            "id": "r_236",
            "s_ids": [
                "s_863",
                "s_469",
                "s_920",
                "s_359",
                "s_122",
                "s_1153",
                "s_1256",
                "s_131"
            ],
            "type": "rich",
            "x": 4.907010555267334,
            "y": 8.000067710876465
        },
        {
            "title": "VAICo: Visual Analysis for Image Comparison",
            "data": "Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.213",
            "id": "r_237",
            "s_ids": [
                "s_607",
                "s_531",
                "s_927"
            ],
            "type": "rich",
            "x": 4.579288482666016,
            "y": 6.015818119049072
        },
        {
            "title": "Semantics of Directly Manipulating Spatializations",
            "data": "When high-dimensional data is visualized in a 2D plane by using parametric projection algorithms, users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures. However, few users are well-versed in the algorithms behind the visualizations, making parameter tweaking more of a guessing game than a series of decisive interactions. Translating user interactions into algorithmic input is a key component of Visual to Parametric Interaction (V2PI) [13]. Instead of adjusting parameters, users directly move data points on the screen, which then updates the underlying statistical model. However, we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved. Users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual. However, in current V2PI interactions, these points are not explicitly identified when directly manipulating the moved points. We design a richer set of interactions that makes this context more explicit, and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into V2PI.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.188",
            "id": "r_238",
            "s_ids": [
                "s_123",
                "s_96",
                "s_618",
                "s_15",
                "s_202",
                "s_56"
            ],
            "type": "rich",
            "x": 4.294029712677002,
            "y": 6.903424263000488
        },
        {
            "title": "Urbane: A 3D framework to support data driven decision making in urban development",
            "data": "Architects working with developers and city planners typically rely on experience, precedent and data analyzed in isolation when making decisions that impact the character of a city. These decisions are critical in enabling vibrant, sustainable environments but must also negotiate a range of complex political and social forces. This requires those shaping the built environment to balance maximizing the value of a new development with its impact on the character of a neighborhood. As a result architects are focused on two issues throughout the decision making process: a) what defines the character of a neighborhood? and b) how will a new development change its neighborhood? In the first, character can be influenced by a variety of factors and understanding the interplay between diverse data sets is crucial; including safety, transportation access, school quality and access to entertainment. In the second, the impact of a new development is measured, for example, by how it impacts the view from the buildings that surround it. In this paper, we work in collaboration with architects to design Urbane, a 3-dimensional multi-resolution framework that enables a data-driven approach for decision making in the design of new urban development. This is accomplished by integrating multiple data layers and impact analysis techniques facilitating architects to explore and assess the effect of these attributes on the character and value of a neighborhood. Several of these data layers, as well as impact analysis, involve working in 3-dimensions and operating in real time. Efficient computation and visualization is accomplished through the use of techniques from computer graphics. We demonstrate the effectiveness of Urbane through a case study of development in Manhattan depicting how a data-driven understanding of the value and impact of speculative buildings can benefit the design-development process between architects, planners and developers.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347636",
            "id": "r_239",
            "s_ids": [
                "s_654",
                "s_931",
                "s_30",
                "s_1490",
                "s_767",
                "s_1104",
                "s_574",
                "s_393"
            ],
            "type": "rich",
            "x": 5.759721755981445,
            "y": 8.101409912109375
        },
        {
            "title": "Two-stage framework for visualization of clustered high dimensional data",
            "data": "In this paper, we discuss dimension reduction methods for 2D visualization of high dimensional clustered data. We propose a two-stage framework for visualizing such data based on dimension reduction methods. In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria. The resulting optimal reduced dimension depends on the optimization criteria and is often larger than 2. In the second stage, the dimension is further reduced to 2 for visualization purposes by another dimension reduction method such as principal component analysis. The role of the second-stage is to minimize the loss of information due to reducing the dimension all the way to 2. Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332629",
            "id": "r_240",
            "s_ids": [
                "s_331",
                "s_1128",
                "s_1096"
            ],
            "type": "rich",
            "x": 4.323966979980469,
            "y": 6.500546455383301
        },
        {
            "title": "Visual Analytics on Mobile Devices for Emergency Response",
            "data": "Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education, business, law enforcement, protective services, medical services, scientific discovery, and homeland security. In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our system provides visual analytics with locational scene data within a simple interface tailored to mobile device capabilities. In particular, we focus on processing and displaying sensor network data for first responders. To verify our system, we have used simulated data of The Station nightclub fire evacuation.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388994",
            "id": "r_241",
            "s_ids": [
                "s_346",
                "s_605",
                "s_1359",
                "s_952",
                "s_1212"
            ],
            "type": "rich",
            "x": 6.643281936645508,
            "y": 7.454643249511719
        },
        {
            "title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations",
            "data": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598543",
            "id": "r_242",
            "s_ids": [
                "s_1354",
                "s_293",
                "s_1175",
                "s_173",
                "s_255"
            ],
            "type": "rich",
            "x": 6.848206043243408,
            "y": 9.837905883789062
        },
        {
            "title": "Cupid: Cluster-Based Exploration of Geometry Generators with Parallel Coordinates and Radial Trees",
            "data": "Geometry generators are commonly used in video games and evaluation systems for computer vision to create geometric shapes such as terrains, vegetation or airplanes. The parameters of the generator are often sampled automatically which can lead to many similar or unwanted geometric shapes. In this paper, we propose a novel visual exploration approach that combines the abstract parameter space of the geometry generator with the resulting 3D shapes in a composite visualization. Similar geometric shapes are first grouped using hierarchical clustering and then nested within an illustrative parallel coordinates visualization. This helps the user to study the sensitivity of the generator with respect to its parameter space and to identify invalid parameter settings. Starting from a compact overview representation, the user can iteratively drill-down into local shape differences by clicking on the respective clusters. Additionally, a linked radial tree gives an overview of the cluster hierarchy and enables the user to manually split or merge clusters. We evaluate our approach by exploring the parameter space of a cup generator and provide feedback from domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346626",
            "id": "r_243",
            "s_ids": [
                "s_740",
                "s_946",
                "s_531",
                "s_290"
            ],
            "type": "rich",
            "x": 4.04945182800293,
            "y": 6.402584075927734
        },
        {
            "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines",
            "data": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030361",
            "id": "r_244",
            "s_ids": [
                "s_87",
                "s_849",
                "s_1284",
                "s_473",
                "s_641",
                "s_393"
            ],
            "type": "rich",
            "x": 3.2641701698303223,
            "y": 8.662834167480469
        },
        {
            "title": "GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms",
            "data": "Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865021",
            "id": "r_245",
            "s_ids": [
                "s_281",
                "s_1319",
                "s_812",
                "s_352",
                "s_454",
                "s_1050",
                "s_189",
                "s_699"
            ],
            "type": "rich",
            "x": 6.170205593109131,
            "y": 8.641339302062988
        },
        {
            "title": "A Visual Analytics Approach to Multiscale Exploration of Environmental Time Series",
            "data": "We present a Visual Analytics approach that addresses the detection of interesting patterns in numerical time series, specifically from environmental sciences. Crucial for the detection of interesting temporal patterns are the time scale and the starting points one is looking at. Our approach makes no assumption about time scale and starting position of temporal patterns and consists of three main steps: an algorithm to compute statistical values for all possible time scales and starting positions of intervals, visual identification of potentially interesting patterns in a matrix visualization, and interactive exploration of detected patterns. We demonstrate the utility of this approach in two scientific scenarios and explain how it allowed scientists to gain new insight into the dynamics of environmental systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.191",
            "id": "r_246",
            "s_ids": [
                "s_344",
                "s_140",
                "s_1131",
                "s_1207",
                "s_1166"
            ],
            "type": "rich",
            "x": 6.895112991333008,
            "y": 6.188204288482666
        },
        {
            "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics",
            "data": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585498",
            "id": "r_247",
            "s_ids": [
                "s_481",
                "s_645",
                "s_345",
                "s_188",
                "s_1124",
                "s_489"
            ],
            "type": "rich",
            "x": 4.098504543304443,
            "y": 9.517640113830566
        },
        {
            "title": "Towards Interactive, Intelligent, and Integrated Multimedia Analytics",
            "data": "The size and importance of visual multimedia collections grew rapidly over the last years, creating a need for sophisticated multimedia analytics systems enabling large-scale, interactive, and insightful analysis. These systems need to integrate the human's natural expertise in analyzing multimedia with the machine's ability to process large-scale data. The paper starts off with a comprehensive overview of representation, learning, and interaction techniques from both the human's and the machine's point of view. To this end, hundreds of references from the related disciplines (visual analytics, information visualization, computer vision, multimedia information retrieval) have been surveyed. Based on the survey, a novel general multimedia analytics model is synthesized. In the model, the need for semantic navigation of the collection is emphasized and multimedia analytics tasks are placed on the exploration-search axis. The axis is composed of both exploration and search in a certain proportion which changes as the analyst progresses towards insight. Categorization is proposed as a suitable umbrella task realizing the exploration-search axis in the model. Finally, the pragmatic gap, defined as the difference between the tight machine categorization model and the flexible human categorization model is identified as a crucial multimedia analytics topic.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042476",
            "id": "r_248",
            "s_ids": [
                "s_1110",
                "s_1206"
            ],
            "type": "rich",
            "x": 5.1939921379089355,
            "y": 8.387750625610352
        },
        {
            "title": "VAST 2008 Challenge: Introducing mini-challenges",
            "data": "Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The VAST 2008 Challenge is the third year that such a competition was held in conjunction with the IEEE Visual Analytics Science and Technology (VAST) symposium. The authors restructured the contest format used in 2006 and 2007 to reduce the barriers to participation and offered four mini-challenges and a Grand Challenge. Mini Challenge participants were to use visual analytic tools to explore one of four heterogeneous data collections to analyze specific activities of a fictitious, controversial movement. Questions asked in the Grand Challenge required the participants to synthesize data from all four data sets. In this paper we give a brief overview of the data sets, the tasks, the participation, the judging, and the results.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677383",
            "id": "r_249",
            "s_ids": [
                "s_1205",
                "s_1399",
                "s_595",
                "s_354",
                "s_1244",
                "s_231"
            ],
            "type": "rich",
            "x": 6.814907550811768,
            "y": 8.75832462310791
        },
        {
            "title": "Visual Causality Analysis of Event Sequence Data",
            "data": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030465",
            "id": "r_250",
            "s_ids": [
                "s_633",
                "s_414",
                "s_1323",
                "s_132",
                "s_873",
                "s_892"
            ],
            "type": "rich",
            "x": 7.734045028686523,
            "y": 7.463884353637695
        },
        {
            "title": "Evaluation of Sampling Methods for Scatterplots",
            "data": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030432",
            "id": "r_251",
            "s_ids": [
                "s_381",
                "s_386",
                "s_286",
                "s_1223",
                "s_316"
            ],
            "type": "rich",
            "x": 4.388597011566162,
            "y": 6.54218053817749
        },
        {
            "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction",
            "data": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028889",
            "id": "r_252",
            "s_ids": [
                "s_1259",
                "s_1250",
                "s_553",
                "s_517",
                "s_1386",
                "s_699"
            ],
            "type": "rich",
            "x": 4.579966068267822,
            "y": 6.905692100524902
        },
        {
            "title": "AirVis: Visual Analytics of Air Pollution Propagation",
            "data": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934670",
            "id": "r_253",
            "s_ids": [
                "s_1249",
                "s_648",
                "s_110",
                "s_336",
                "s_7",
                "s_1118",
                "s_89",
                "s_1347"
            ],
            "type": "rich",
            "x": 6.819993019104004,
            "y": 6.766763210296631
        },
        {
            "title": "KnowledgePearls: Provenance-Based Visualization Retrieval",
            "data": "Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865024",
            "id": "r_254",
            "s_ids": [
                "s_674",
                "s_73",
                "s_275",
                "s_1447",
                "s_512"
            ],
            "type": "rich",
            "x": 6.838124752044678,
            "y": 9.806111335754395
        },
        {
            "title": "EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data",
            "data": "The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864825",
            "id": "r_255",
            "s_ids": [
                "s_28",
                "s_1004",
                "s_199",
                "s_651",
                "s_892"
            ],
            "type": "rich",
            "x": 6.7766804695129395,
            "y": 7.190415382385254
        },
        {
            "title": "Blockwise Human Brain Network Visual Comparison Using NodeTrix Representation",
            "data": "Visually comparing human brain networks from multiple population groups serves as an important task in the field of brain connectomics. The commonly used brain network representation, consisting of nodes and edges, may not be able to reveal the most compelling network differences when the reconstructed networks are dense and homogeneous. In this paper, we leveraged the block information on the Region Of Interest (ROI) based brain networks and studied the problem of blockwise brain network visual comparison. An integrated visual analytics framework was proposed. In the first stage, a two-level ROI block hierarchy was detected by optimizing the anatomical structure and the predictive comparison performance simultaneously. In the second stage, the NodeTrix representation was adopted and customized to visualize the brain network with block information. We conducted controlled user experiments and case studies to evaluate our proposed solution. Results indicated that our visual analytics method outperformed the commonly used node-link graph and adjacency matrix design in the blockwise network comparison tasks. We have shown compelling findings from two real-world brain network data sets, which are consistent with the prior connectomics studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598472",
            "id": "r_256",
            "s_ids": [
                "s_432",
                "s_32",
                "s_1082",
                "s_401",
                "s_301",
                "s_1085"
            ],
            "type": "rich",
            "x": 5.197463035583496,
            "y": 5.419427871704102
        },
        {
            "title": "Visual Causality Analysis Made Practical",
            "data": "Deriving the exact casual model that governs the relations between variables in a multidimensional dataset is difficult in practice. It is because causal inference algorithms by themselves typically cannot encode an adequate amount of domain knowledge to break all ties. Visual analytic approaches are considered a feasible alternative to fully automated methods. However, their application in real-world scenarios can be tedious. This paper focuses on these practical aspects of visual causality analysis. The most imperative of these aspects is posed by Simpson' Paradox. It implies the existence of multiple causal models differing in both structure and parameter depending on how the data is subdivided. We propose a comprehensive interface that engages human experts in identifying these subdivisions and allowing them to establish the corresponding causal models via a rich set of interactive facilities. Other features of our interface include: (1) a new causal network visualization that emphasizes the flow of causal dependencies, (2) a model scoring mechanism with visual hints for interactive model refinement, and (3) flexible approaches for handling heterogeneous data. Various real-world data examples are given.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585647",
            "id": "r_257",
            "s_ids": [
                "s_948",
                "s_254"
            ],
            "type": "rich",
            "x": 4.277530670166016,
            "y": 8.225810050964355
        },
        {
            "title": "SpiralView: Towards Security Policies Assessment through Visual Correlation of Network Resources with Evolution of Alarms",
            "data": "This article presents SpiralView, a visualization tool for helping system administrators to assess network policies. The tool is meant to be a complementary support to the routine activity of network monitoring, enabling a retrospective view on the alarms generated during and extended period of time. The tool permits to reason about how alarms distribute over time and how they correlate with network resources (e.g., users, IPs, applications, etc.), supporting the analysts in understanding how the network evolves and thus in devising new security policies for the future. The spiral visualization plots alarms in time, and, coupled with interactive bar charts and a users/applications graph view, is used to present network data and perform queries. The user is able to segment the data in meaningful subsets, zoom on specific related information, and inspect for relationships between alarms, users, and applications. In designing the visualizations and their interaction, and through tests with security experts, several ameliorations over the standard techniques have been provided.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389007",
            "id": "r_258",
            "s_ids": [
                "s_473",
                "s_5",
                "s_1009"
            ],
            "type": "rich",
            "x": 6.889247417449951,
            "y": 7.69767951965332
        },
        {
            "title": "Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts",
            "data": "While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.224",
            "id": "r_259",
            "s_ids": [
                "s_738",
                "s_756"
            ],
            "type": "rich",
            "x": 5.200143337249756,
            "y": 9.58105754852295
        },
        {
            "title": "Comparative visual analysis of vector field ensembles",
            "data": "We present a new visual analysis approach to support the comparative exploration of 2D vector-valued ensemble fields. Our approach enables the user to quickly identify the most similar groups of ensemble members, as well as the locations where the variation among the members is high. We further provide means to visualize the main features of the potentially multimodal directional distributions at user-selected locations. For this purpose, directional data is modelled using mixtures of probability density functions (pdfs), which allows us to characterize and classify complex distributions with relatively few parameters. The resulting mixture models are used to determine the degree of similarity between ensemble members, and to construct glyphs showing the direction, spread, and strength of the principal modes of the directional distributions. We also propose several similarity measures, based on which we compute pairwise member similarities in the spatial domain and form clusters of similar members. The hierarchical clustering is shown using dendrograms and similarity matrices, which can be used to select particular members and visualize their variations. A user interface providing multiple linked views enables the simultaneous visualization of aggregated global and detailed local variations, as well as the selection of members for a detailed comparison.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347634",
            "id": "r_260",
            "s_ids": [
                "s_1103",
                "s_1258",
                "s_290",
                "s_103"
            ],
            "type": "rich",
            "x": 4.286999702453613,
            "y": 6.186981201171875
        },
        {
            "title": "Doccurate: A Curation-Based Approach for Clinical Text Visualization",
            "data": "Before seeing a patient, physicians seek to obtain an overview of the patient's medical history. Text plays a major role in this activity since it represents the bulk of the clinical documentation, but reviewing it quickly becomes onerous when patient charts grow too large. Text visualization methods have been widely explored to manage this large scale through visual summaries that rely on information retrieval algorithms to structure text and make it amenable to visualization. However, the integration with such automated approaches comes with a number of limitations, including significant error rates and the need for healthcare providers to fine-tune algorithms without expert knowledge of their inner mechanics. In addition, several of these approaches obscure or substitute the original clinical text and therefore fail to leverage qualitative and rhetorical flavours of the clinical notes. These drawbacks have limited the adoption of text visualization and other summarization technologies in clinical practice. In this work we present Doccurate, a novel system embodying a curation-based approach for the visualization of large clinical text datasets. Our approach offers automation auditing and customizability to physicians while also preserving and extensively linking to the original text. We discuss findings of a formal qualitative evaluation conducted with 6 domain experts, shedding light onto physicians' information needs, perceived strengths and limitations of automated tools, and the importance of customization while balancing efficiency. We also present use case scenarios to showcase Doccurate's envisioned usage in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864905",
            "id": "r_261",
            "s_ids": [
                "s_720",
                "s_1476",
                "s_752",
                "s_173"
            ],
            "type": "rich",
            "x": 7.287152290344238,
            "y": 10.204761505126953
        },
        {
            "title": "Exploring Evolving Media Discourse Through Event Cueing",
            "data": "Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467991",
            "id": "r_262",
            "s_ids": [
                "s_804",
                "s_1510",
                "s_861",
                "s_1325",
                "s_1301",
                "s_1251",
                "s_1412",
                "s_1059",
                "s_721"
            ],
            "type": "rich",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "title": "SensePath: Understanding the Sensemaking Process Through Analytic Provenance",
            "data": "Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467611",
            "id": "r_263",
            "s_ids": [
                "s_57",
                "s_638",
                "s_729",
                "s_593",
                "s_75",
                "s_1424"
            ],
            "type": "rich",
            "x": 5.667359352111816,
            "y": 10.38341236114502
        },
        {
            "title": "The User Puzzle---Explaining the Interaction with Visual Analytics Systems",
            "data": "Visual analytics emphasizes the interplay between visualization, analytical procedures performed by computers and human perceptual and cognitive activities. Human reasoning is an important element in this context. There are several theories in psychology and HCI explaining open-ended and exploratory reasoning. Five of these theories (sensemaking theories, gestalt theories, distributed cognition, graph comprehension theories and skill-rule-knowledge models) are described in this paper. We discuss their relevance for visual analytics. In order to do this more systematically, we developed a schema of categories relevant for visual analytics research and evaluation. All these theories have strengths but also weaknesses in explaining interaction with visual analytics systems. A possibility to overcome the weaknesses would be to combine two or more of these theories.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.273",
            "id": "r_264",
            "s_ids": [
                "s_1150",
                "s_1134",
                "s_265"
            ],
            "type": "rich",
            "x": 4.754398345947266,
            "y": 9.436184883117676
        },
        {
            "title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
            "data": "Time-series data is widely studied in various scenarios, like weather forecast, stock market, customer behavior analysis. To comprehensively learn about the dynamic environments, it is necessary to comprehend features from multiple data sources. This paper proposes a novel visual analysis approach for detecting and analyzing concept drifts from multi-sourced time-series. We propose a visual detection scheme for discovering concept drifts from multiple sourced time-series based on prediction models. We design a drift level index to depict the dynamics, and a consistency judgment model to justify whether the concept drifts from various sources are consistent. Our integrated visual interface, ConceptExplorer, facilitates visual exploration, extraction, understanding, and comparison of concepts and concept drifts from multi-source time-series data. We conduct three case studies and expert interviews to verify the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00006",
            "id": "r_265",
            "s_ids": [
                "s_1444",
                "s_1319",
                "s_286",
                "s_1380",
                "s_263",
                "s_1140",
                "s_67",
                "s_511"
            ],
            "type": "rich",
            "x": 6.856532096862793,
            "y": 6.264466762542725
        },
        {
            "title": "VizCept: Supporting synchronous collaboration for constructing visualizations in intelligence analysis",
            "data": "In this paper, we present a new web-based visual analytics system, VizCept, which is designed to support fluid, collaborative analysis of large textual intelligence datasets. The main approach of the design is to combine individual workspace and shared visualization in an integrated environment. Collaborating analysts will be able to identify concepts and relationships from the dataset based on keyword searches in their own workspace and collaborate visually with other analysts using visualization tools such as a concept map view and a timeline view. The system allows analysts to parallelize the work by dividing initial sets of concepts, investigating them on their own workspace, and then integrating individual findings automatically on shared visualizations with support for interaction and personal graph layout in real time, in order to develop a unified plot. We highlight several design considerations that promote communication and analytic performance in small team synchronous collaboration. We report the result of a pair of case study applications including collaboration and communication methods, analysis strategies, and user behaviors under a competition setting in the same location at the same time. The results of these demonstrate the tool's effectiveness for synchronous collaborative construction and use of visualizations in intelligence data analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652932",
            "id": "r_266",
            "s_ids": [
                "s_1423",
                "s_780",
                "s_1511",
                "s_1492",
                "s_1302",
                "s_202"
            ],
            "type": "rich",
            "x": 5.334270477294922,
            "y": 10.654705047607422
        },
        {
            "title": "What's being said near \"Martha\"? Exploring name entities in literary text collections",
            "data": "A common task in literary analysis is to study characters in a novel or collection. Automatic entity extraction, text analysis and effective user interfaces facilitate character analysis. Using our interface, called POSvis, the scholar uses word clouds and self-organizing graphs to review vocabulary, to filter by part of speech, and to explore the network of characters located near characters under review. Further, visualizations show word usages within an analysis window (i.e. a book chapter), which can be compared with a reference window (i.e. the whole book). We describe the interface and report on an early case study with a humanities scholar.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333248",
            "id": "r_267",
            "s_ids": [
                "s_977",
                "s_1527",
                "s_1399",
                "s_236"
            ],
            "type": "rich",
            "x": 7.462177753448486,
            "y": 10.412565231323242
        },
        {
            "title": "Narratives: A visualization to track narrative events as they develop",
            "data": "Analyzing unstructured text streams can be challenging. One popular approach is to isolate specific themes in the text, and to visualize the connections between them. Some existing systems, like ThemeRiver, provide a temporal view of changes in themes; other systems, like In-Spire, use clustering techniques to help an analyst identify the themes at a single point in time. Narratives combines both of these techniques; it uses a temporal axis to visualize ways that concepts have changed over time, and introduces several methods to explore how those concepts relate to each other. Narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time. Users can relate articles through time by examining the topical keywords that summarize a specific news event. By tracking the attention to a news article in the form of references in social media (such as weblogs), a user discovers both important events and measures the social relevance of these stories.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677364",
            "id": "r_268",
            "s_ids": [
                "s_1048",
                "s_576",
                "s_1349",
                "s_1246"
            ],
            "type": "rich",
            "x": 7.937456130981445,
            "y": 7.830206394195557
        },
        {
            "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics",
            "data": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030410",
            "id": "r_269",
            "s_ids": [
                "s_1366",
                "s_868",
                "s_418",
                "s_591",
                "s_286",
                "s_81",
                "s_1319"
            ],
            "type": "rich",
            "x": 7.166633605957031,
            "y": 4.956838130950928
        },
        {
            "title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations",
            "data": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030387",
            "id": "r_270",
            "s_ids": [
                "s_762",
                "s_602",
                "s_829"
            ],
            "type": "rich",
            "x": 7.126302242279053,
            "y": 9.960545539855957
        },
        {
            "title": "SIRIUS: Dual, Symmetric, Interactive Dimension Reductions",
            "data": "Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865047",
            "id": "r_271",
            "s_ids": [
                "s_898",
                "s_586",
                "s_798",
                "s_56",
                "s_15",
                "s_202"
            ],
            "type": "rich",
            "x": 4.801039218902588,
            "y": 6.734798431396484
        },
        {
            "title": "Supporting the Visual Analysis of Dynamic Networks by Clustering associated Temporal Attributes",
            "data": "The visual analysis of dynamic networks is a challenging task. In this paper, we introduce a new approach supporting the discovery of substructures sharing a similar trend over time by combining computation, visualization and interaction. With existing techniques, their discovery would be a tedious endeavor because of the number of nodes, edges as well as time points to be compared. First, on the basis of the supergraph, we therefore group nodes and edges according to their associated attributes that are changing over time. Second, the supergraph is visualized to provide an overview of the groups of nodes and edges with similar behavior over time in terms of their associated attributes. Third, we provide specific interactions to explore and refine the temporal clustering, allowing the user to further steer the analysis of the dynamic network. We demonstrate our approach by the visual analysis of a large wireless mesh network.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.198",
            "id": "r_272",
            "s_ids": [
                "s_1162",
                "s_757",
                "s_683",
                "s_790"
            ],
            "type": "rich",
            "x": 5.256996154785156,
            "y": 5.111698627471924
        },
        {
            "title": "The semantics of sketch: Flexibility in visual query systems for time series data",
            "data": "Sketching allows analysts to specify complex and free-form patterns of interest. Visual query systems can make use of sketches to locate these patterns of interest in large datasets. However, sketching is ambiguous: the same drawing could represent a multitude of potential queries. In this work, we investigate these ambiguities as they apply to visual query systems for time series data. We define a class of \u201cinvariants\u201d - the properties of a time series that the analyst wishes to ignore when performing a sketch-based query. We present the results of a crowd-sourced study, showing that these invariants are key components of how people rate the strength of match between sketch and target. We adapt a number of algorithms for time series matching to support invariants in sketches. Lastly, we present a web-deployed prototype sketch-based visual query system that relies on these invariants. We apply the prototype to data from finance, the digital humanities, and political science.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883519",
            "id": "r_273",
            "s_ids": [
                "s_1275",
                "s_150"
            ],
            "type": "rich",
            "x": 6.427948951721191,
            "y": 6.106138706207275
        },
        {
            "title": "Supporting visual exploration for multiple users in large display environments",
            "data": "We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883506",
            "id": "r_274",
            "s_ids": [
                "s_440",
                "s_502",
                "s_1191",
                "s_543"
            ],
            "type": "rich",
            "x": 5.138099670410156,
            "y": 10.092191696166992
        },
        {
            "title": "Intelligent Visual Analytics Queries",
            "data": "Visualizations of large multi-dimensional data sets, occurring in scientific and commercial applications, often reveal interesting local patterns. Analysts want to identify the causes and impacts of these interesting areas, and they also want to search for similar patterns occurring elsewhere in the data set. In this paper we introduce the Intelligent Visual Analytics Query (IVQuery) concept that combines visual interaction with automated analytical methods to support analysts in discovering the special properties and relations of identified patterns. The idea of IVQuery is to interactively select focus areas in the visualization. Then, according to the characteristics of the selected areas, such as the data dimensions and records, IVQuery employs analytical methods to identify the relationships to other portions of the data set. Finally, IVQuery generates visual representations for analysts to view and refine the results. IVQuery has been applied successfully to different real-world data sets, such as data warehouse performance, product sales, and sever performance analysis, and demonstrates the benefits of this technique over traditional filtering and zooming techniques. The visual analytics query technique can be used with many different types of visual representation. In this paper we show how to use IVQuery with parallel coordinates, visual maps, and scatter plots.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389001",
            "id": "r_275",
            "s_ids": [
                "s_211",
                "s_1169",
                "s_1038",
                "s_828",
                "s_3"
            ],
            "type": "rich",
            "x": 5.132715702056885,
            "y": 6.853967666625977
        },
        {
            "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures",
            "data": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934261",
            "id": "r_276",
            "s_ids": [
                "s_604",
                "s_1177",
                "s_333",
                "s_894"
            ],
            "type": "rich",
            "x": 2.2512009143829346,
            "y": 8.85483455657959
        },
        {
            "title": "Visualizing Dimension Coverage to Support Exploratory Analysis",
            "data": "Data analysis involves constantly formulating and testing new hypotheses and questions about data. When dealing with a new dataset, especially one with many dimensions, it can be cumbersome for the analyst to clearly remember which aspects of the data have been investigated (i.e., visually examined for patterns, trends, outliers etc.) and which combinations have not. Yet this information is critical to help the analyst formulate new questions that they have not already answered. We observe that for tabular data, questions are typically comprised of varying combinations of data dimensions (e.g., what are the trends of Sales and Profit for different Regions?). We propose representing analysis history from the angle of dimension coverage (i.e., which data dimensions have been investigated and in which combinations). We use scented widgets to incorporate dimension coverage of the analysts' past work into interaction widgets of a visualization tool. We demonstrate how this approach can assist analysts with the question formation process. Our approach extends the concept of scented widgets to reveal aspects of one's own analysis history, and offers a different perspective on one's past work than typical visualization history tools. Results of our empirical study showed that participants with access to embedded dimension coverage information relied on this information when formulating questions, asked more questions about the data, generated more top-level findings, and showed greater breadth of their analysis without sacrificing depth.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598466",
            "id": "r_277",
            "s_ids": [
                "s_1377",
                "s_1266",
                "s_1496"
            ],
            "type": "rich",
            "x": 5.452300548553467,
            "y": 8.776118278503418
        },
        {
            "title": "MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data",
            "data": "Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468292",
            "id": "r_278",
            "s_ids": [
                "s_210",
                "s_1191",
                "s_1182"
            ],
            "type": "rich",
            "x": 7.460609436035156,
            "y": 5.697226047515869
        },
        {
            "title": "Visual Analysis of Public Utility Service Problems in a Metropolis",
            "data": "Issues about city utility services reported by citizens can provide unprecedented insights into the various aspects of such services. Analysis of these issues can improve living quality through evidence-based decision making. However, these issues are complex, because of the involvement of spatial and temporal components, in addition to having multi-dimensional and multivariate natures. Consequently, exploring utility service problems and creating visual representations are difficult. To analyze these issues, we propose a visual analytics process based on the main tasks of utility service management. We also propose an aggregate method that transforms numerous issues into legible events and provide visualizations for events. In addition, we provide a set of tools and interaction techniques to explore such issues. Our approach enables administrators to make more informed decisions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346898",
            "id": "r_279",
            "s_ids": [
                "s_918",
                "s_646",
                "s_530",
                "s_1317",
                "s_475",
                "s_215",
                "s_181",
                "s_1274"
            ],
            "type": "rich",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "title": "A Five-Level Design Framework for Bicluster Visualizations",
            "data": "Analysts often need to explore and identify coordinated relationships (e.g., four people who visited the same five cities on the same set of days) within some large datasets for sensemaking. Biclusters provide a potential solution to ease this process, because each computed bicluster bundles individual relationships into coordinated sets. By understanding such computed, structural, relations within biclusters, analysts can leverage their domain knowledge and intuition to determine the importance and relevance of the extracted relationships for making hypotheses. However, due to the lack of systematic design guidelines, it is still a challenge to design effective and usable visualizations of biclusters to enhance their perceptibility and interactivity for exploring coordinated relationships. In this paper, we present a five-level design framework for bicluster visualizations, with a survey of the state-of-the-art design considerations and applications that are related or that can be applied to bicluster visualizations. We summarize pros and cons of these design options to support user tasks at each of the five-level relationships. Finally, we discuss future research challenges for bicluster visualizations and their incorporation into visual analytics tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346665",
            "id": "r_280",
            "s_ids": [
                "s_102",
                "s_202",
                "s_1461"
            ],
            "type": "rich",
            "x": 4.843799114227295,
            "y": 5.712470054626465
        },
        {
            "title": "DocuCompass: Effective exploration of document landscapes",
            "data": "The creation of interactive visualization to analyze text documents has gained an impressive momentum in recent years. This is not surprising in the light of massive and still increasing amounts of available digitized texts. Websites, social media, news wire, and digital libraries are just few examples of the diverse text sources whose visual analysis and exploration offers new opportunities to effectively mine and manage the information and knowledge hidden within them. A popular visualization method for large text collections is to represent each document by a glyph in 2D space. These landscapes can be the result of optimizing pairwise distances in 2D to represent document similarities, or they are provided directly as meta data, such as geo-locations. For well-defined information needs, suitable interaction methods are available for these spatializations. However, free exploration and navigation on a level of abstraction between a labeled document spatialization and reading single documents is largely unsupported. As a result, vital foraging steps for task-tailored actions, such as selecting subgroups of documents for detailed inspection, or subsequent sense-making steps are hampered. To fill in this gap, we propose DocuCompass, a focus+context approach based on the lens metaphor. It comprises multiple methods to characterize local groups of documents, and to efficiently guide exploration based on users' requirements. DocuCompass thus allows for effective interactive exploration of document landscapes without disrupting the mental map of users by changing the layout itself. We discuss the suitability of multiple navigation and characterization methods for different spatializations and texts. Finally, we provide insights generated through user feedback and discuss the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883507",
            "id": "r_281",
            "s_ids": [
                "s_65",
                "s_1391",
                "s_1174",
                "s_404",
                "s_314"
            ],
            "type": "rich",
            "x": 7.267745494842529,
            "y": 10.18940544128418
        },
        {
            "title": "Helping users recall their reasoning process",
            "data": "The final product of an analyst's investigation using a visualization is often a report of the discovered knowledge, as well as the methods employed and reasoning behind the discovery. We believe that analysts may have difficulty keeping track of their knowledge discovery process and will require tools to assist in accurately recovering their reasoning. We first report on a study examining analysts' recall of their strategies and methods, demonstrating their lack of memory of the path of knowledge discovery. We then explore whether a tool visualizing the steps of the visual analysis can aid users in recalling their reasoning process. The results of our second study indicate that visualizations of interaction logs can serve as an effective memory aid, allowing analysts to recall additional details of their strategies and decisions.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5653598",
            "id": "r_282",
            "s_ids": [
                "s_0",
                "s_1294",
                "s_305",
                "s_1314",
                "s_333"
            ],
            "type": "rich",
            "x": 5.596868991851807,
            "y": 9.598109245300293
        },
        {
            "title": "Discovering bits of place histories from people's activity traces",
            "data": "Events that happened in the past are important for understanding the ongoing processes, predicting future developments, and making informed decisions. Significant and/or interesting events tend to attract many people. Some people leave traces of their attendance in the form of computer-processable data, such as records in the databases of mobile phone operators or photos on photo sharing web sites. We developed a suite of visual analytics methods for reconstructing past events from these activity traces. Our tools combine geocomputations, interactive geovisualizations and statistical methods to enable integrated analysis of the spatial, temporal, and thematic components of the data, including numeric attributes and texts. We demonstrate the utility of our approach on two large real data sets, mobile phone calls in Milano during 9 days and flickr photos made on British Isles during 5 years.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652478",
            "id": "r_283",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_479",
                "s_488",
                "s_409"
            ],
            "type": "rich",
            "x": 7.473939895629883,
            "y": 7.139910697937012
        },
        {
            "title": "Model-driven Visual Analytics",
            "data": "We describe a visual analytics (VA) infrastructure, rooted on techniques in machine learning and logic-based deductive reasoning that will assist analysts to make sense of large, complex data sets by facilitating the generation and validation of models representing relationships in the data. We use logic programming (LP) as the underlying computing machinery to encode the relations as rules and facts and compute with them. A unique aspect of our approach is that the LP rules are automatically learned, using Inductive Logic Programming, from examples of data that the analyst deems interesting when viewing the data in the high-dimensional visualization interface. Using this system, analysts will be able to construct models of arbitrary relationships in the data, explore the data for scenarios that fit the model, refine the model if necessary, and query the model to automatically analyze incoming (future) data exhibiting the encoded relationships. In other words it will support both model-driven data exploration, as well as data-driven model evolution. More importantly, by basing the construction of models on techniques from machine learning and logic-based deduction, the VA process will be both flexible in terms of modeling arbitrary, user-driven relationships in the data as well as readily scale across different data domains.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677352",
            "id": "r_284",
            "s_ids": [
                "s_938",
                "s_294",
                "s_805",
                "s_254"
            ],
            "type": "rich",
            "x": 3.451568126678467,
            "y": 8.536810874938965
        },
        {
            "title": "Topology Density Map for Urban Data Visualization and Analysis",
            "data": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030469",
            "id": "r_285",
            "s_ids": [
                "s_540",
                "s_617",
                "s_1366",
                "s_736",
                "s_131"
            ],
            "type": "rich",
            "x": 4.558706283569336,
            "y": 6.38838005065918
        },
        {
            "title": "Understanding the Role of Alternatives in Data Analysis Practices",
            "data": "Data workers are people who perform data analysis activities as a part of their daily work but do not formally identify as data scientists. They come from various domains and often need to explore diverse sets of hypotheses and theories, a variety of data sources, algorithms, methods, tools, and visual designs. Taken together, we call these alternatives. To better understand and characterize the role of alternatives in their analyses, we conducted semi-structured interviews with 12 data workers with different types of expertise. We conducted four types of analyses to understand 1) why data workers explore alternatives; 2) the different notions of alternatives and how they fit into the sensemaking process; 3) the high-level processes around alternatives; and 4) their strategies to generate, explore, and manage those alternatives. We find that participants' diverse levels of domain and computational expertise, experience with different tools, and collaboration within their broader context play an important role in how they explore these alternatives. These findings call out the need for more attention towards a deeper understanding of alternatives and the need for better tools to facilitate the exploration, interpretation, and management of alternatives. Drawing upon these analyses and findings, we present a framework based on participants' 1) degree of attention, 2) abstraction level, and 3) analytic processes. We show how this framework can help understand how data workers consider such alternatives in their analyses and how tool designers might create tools to better support them.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934593",
            "id": "r_286",
            "s_ids": [
                "s_824",
                "s_550",
                "s_538"
            ],
            "type": "rich",
            "x": 5.635706424713135,
            "y": 10.295689582824707
        },
        {
            "title": "Magnostics: Image-Based Search of Interesting Matrix Views for Guided Network Exploration",
            "data": "In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or Magnostics), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. Magnostics can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors-27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS-with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for Magnostics and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598467",
            "id": "r_287",
            "s_ids": [
                "s_547",
                "s_1168",
                "s_558",
                "s_598",
                "s_821",
                "s_90",
                "s_511"
            ],
            "type": "rich",
            "x": 4.52726936340332,
            "y": 6.233945369720459
        },
        {
            "title": "NameClarifier: A Visual Analytics System for Author Name Disambiguation",
            "data": "In this paper, we present a novel visual analytics system called NameClarifier to interactively disambiguate author names in publications by keeping humans in the loop. Specifically, NameClarifier quantifies and visualizes the similarities between ambiguous names and those that have been confirmed in digital libraries. The similarities are calculated using three key factors, namely, co-authorships, publication venues, and temporal information. Our system estimates all possible allocations, and then provides visual cues to users to help them validate every ambiguous case. By looping users in the disambiguation process, our system can achieve more reliable results than general data mining models for highly ambiguous cases. In addition, once an ambiguous case is resolved, the result is instantly added back to our system and serves as additional cues for all the remaining unidentified names. In this way, we open up the black box in traditional disambiguation processes, and help intuitively and comprehensively explain why the corresponding classifications should hold. We conducted two use cases and an expert review to demonstrate the effectiveness of NameClarifier.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598465",
            "id": "r_288",
            "s_ids": [
                "s_1474",
                "s_1052",
                "s_1093",
                "s_469",
                "s_131",
                "s_920"
            ],
            "type": "rich",
            "x": 7.16457462310791,
            "y": 9.706252098083496
        },
        {
            "title": "Interactive Visual Profiling of Musicians",
            "data": "Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project, priority (1) the definition of various measures to determine the similarity of musicians' attributes, and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467620",
            "id": "r_289",
            "s_ids": [
                "s_317",
                "s_338",
                "s_859"
            ],
            "type": "rich",
            "x": 6.975711345672607,
            "y": 9.864171981811523
        },
        {
            "title": "VisOHC: Designing Visual Analytics for Online Health Communities",
            "data": "Through online health communities (OHCs), patients and caregivers exchange their illness experiences and strategies for overcoming the illness, and provide emotional support. To facilitate healthy and lively conversations in these communities, their members should be continuously monitored and nurtured by OHC administrators. The main challenge of OHC administrators' tasks lies in understanding the diverse dimensions of conversation threads that lead to productive discussions in their communities. In this paper, we present a design study in which three domain expert groups participated, an OHC researcher and two OHC administrators of online health communities, which was conducted to find with a visual analytic solution. Through our design study, we characterized the domain goals of OHC administrators and derived tasks to achieve these goals. As a result of this study, we propose a system called VisOHC, which visualizes individual OHC conversation threads as collapsed boxes-a visual metaphor of conversation threads. In addition, we augmented the posters' reply authorship network with marks and/or beams to show conversation dynamics within threads. We also developed unique measures tailored to the characteristics of OHCs, which can be encoded for thread visualizations at the users' requests. Our observation of the two administrators while using VisOHC showed that it supports their tasks and reveals interesting insights into online health communities. Finally, we share our methodological lessons on probing visual designs together with domain experts by allowing them to freely encode measurements into visual variables.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467555",
            "id": "r_290",
            "s_ids": [
                "s_1235",
                "s_170",
                "s_1028",
                "s_331",
                "s_845",
                "s_1260"
            ],
            "type": "rich",
            "x": 5.768592834472656,
            "y": 10.660682678222656
        },
        {
            "title": "LoyalTracker: Visualizing Loyalty Dynamics in Search Engines",
            "data": "The huge amount of user log data collected by search engine providers creates new opportunities to understand user loyalty and defection behavior at an unprecedented scale. However, this also poses a great challenge to analyze the behavior and glean insights into the complex, large data. In this paper, we introduce LoyalTracker, a visual analytics system to track user loyalty and switching behavior towards multiple search engines from the vast amount of user log data. We propose a new interactive visualization technique (flow view) based on a flow metaphor, which conveys a proper visual summary of the dynamics of user loyalty of thousands of users over time. Two other visualization techniques, a density map and a word cloud, are integrated to enable analysts to gain further insights into the patterns identified by the flow view. Case studies and the interview with domain experts are conducted to demonstrate the usefulness of our technique in understanding user loyalty and switching behavior in search engines.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346912",
            "id": "r_291",
            "s_ids": [
                "s_1418",
                "s_1347",
                "s_316",
                "s_285",
                "s_131"
            ],
            "type": "rich",
            "x": 7.652134895324707,
            "y": 8.588372230529785
        },
        {
            "title": "An Extensible Framework for Provenance in Human Terrain Visual Analytics",
            "data": "We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.132",
            "id": "r_292",
            "s_ids": [
                "s_1532",
                "s_1411",
                "s_514",
                "s_638",
                "s_1126",
                "s_57",
                "s_1040",
                "s_593",
                "s_134"
            ],
            "type": "rich",
            "x": 6.659133434295654,
            "y": 9.591086387634277
        },
        {
            "title": "Mixed-initiative visual analytics using task-driven recommendations",
            "data": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347625",
            "id": "r_293",
            "s_ids": [
                "s_684",
                "s_888",
                "s_186",
                "s_1483",
                "s_1018",
                "s_787",
                "s_1517"
            ],
            "type": "rich",
            "x": 5.411427021026611,
            "y": 9.972384452819824
        },
        {
            "title": "iLAMP: Exploring high-dimensional spacing through backward multidimensional projection",
            "data": "Ever improving computing power and technological advances are greatly augmenting data collection and scientific observation. This has directly contributed to increased data complexity and dimensionality, motivating research of exploration techniques for multidimensional data. Consequently, a recent influx of work dedicated to techniques and tools that aid in understanding multidimensional datasets can be observed in many research fields, including biology, engineering, physics and scientific computing. While the effectiveness of existing techniques to analyze the structure and relationships of multidimensional data varies greatly, few techniques provide flexible mechanisms to simultaneously visualize and actively explore high-dimensional spaces. In this paper, we present an inverse linear affine multidimensional projection, coined iLAMP, that enables a novel interactive exploration technique for multidimensional data. iLAMP operates in reverse to traditional projection methods by mapping low-dimensional information into a high-dimensional space. This allows users to extrapolate instances of a multidimensional dataset while exploring a projection of the data to the planar domain. We present experimental results that validate iLAMP, measuring the quality and coherence of the extrapolated data; as well as demonstrate the utility of iLAMP to hypothesize the unexplored regions of a high-dimensional space.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400489",
            "id": "r_294",
            "s_ids": [
                "s_51",
                "s_383",
                "s_411",
                "s_262",
                "s_360",
                "s_1135"
            ],
            "type": "rich",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "title": "SAVE: Sensor anomaly visualization engine",
            "data": "Diagnosing a large-scale sensor network is a crucial but challenging task. Particular challenges include the resource and bandwidth constraints on sensor nodes, the spatiotemporally dynamic network behaviors, and the lack of accurate models to understand such behaviors in a hostile environment. In this paper, we present the Sensor Anomaly Visualization Engine (SAVE), a system that fully leverages the power of both visualization and anomaly detection analytics to guide the user to quickly and accurately diagnose sensor network failures and faults. SAVE combines customized visualizations over separate sensor data facets as multiple coordinated views. Temporal expansion model, correlation graph and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a case study with real-world sensor network system and administrators, we demonstrate that SAVE is able to help better locate the system problem and further identify the root cause of major sensor network failure scenarios.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102458",
            "id": "r_295",
            "s_ids": [
                "s_32",
                "s_55",
                "s_23",
                "s_719",
                "s_1263",
                "s_1114"
            ],
            "type": "rich",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "title": "Visual analysis of graphs with multiple connected components",
            "data": "In this paper, we present a system for the interactive visualization and exploration of graphs with many weakly connected components. The visualization of large graphs has recently received much research attention. However, specific systems for visual analysis of graph data sets consisting of many components are rare. In our approach, we rely on graph clustering using an extensive set of topology descriptors. Specifically, we use the self-organizing-map algorithm in conjunction with a user-adaptable combination of graph features for clustering of graphs. It offers insight into the overall structure of the data set. The clustering output is presented in a grid containing clusters of the connected components of the input graph. Interactive feature selection and task-tailored data views allow the exploration of the whole graph space. The system provides also tools for assessment and display of cluster quality. We demonstrate the usefulness of our system by application to a shareholder network analysis problem based on a large real-world data set. While so far our approach is applied to weighted directed graphs only, it can be used for various graph types.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333893",
            "id": "r_296",
            "s_ids": [
                "s_241",
                "s_40",
                "s_511"
            ],
            "type": "rich",
            "x": 4.9069132804870605,
            "y": 5.314910411834717
        },
        {
            "title": "Guided analysis of hurricane trends using statistical processes integrated with interactive parallel coordinates",
            "data": "This paper demonstrates the promise of augmenting interactive multivariate representations with information from statistical processes in the domain of weather data analysis. Statistical regression, correlation analysis, and descriptive statistical calculations are integrated via graphical indicators into an enhanced parallel coordinates system, called the Multidimensional Data eXplorer (MDX). These statistical indicators, which highlight significant associations in the data, are complemented with interactive visual analysis capabilities. The resulting system allows a smooth, interactive, and highly visual workflow. The system's utility is demonstrated with an extensive hurricane climate study that was conducted by a hurricane expert. In the study, the expert used a new data set of environmental weather data, composed of 28 independent variables, to predict annual hurricane activity. MDX shows the Atlantic Meridional Mode increases the explained variance of hurricane seasonal activity by 7-15% and removes less significant variables used in earlier studies. The findings and feedback from the expert (1) validate the utility of the data set for hurricane prediction, and (2) indicate that the integration of statistical processes with interactive parallel coordinates, as implemented in MDX, addresses both deficiencies in traditional weather data analysis and exhibits some of the expected benefits of visual data analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332586",
            "id": "r_297",
            "s_ids": [
                "s_583",
                "s_10",
                "s_1499",
                "s_773"
            ],
            "type": "rich",
            "x": 6.37584924697876,
            "y": 6.424874305725098
        },
        {
            "title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics",
            "data": "User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934609",
            "id": "r_298",
            "s_ids": [
                "s_6",
                "s_564",
                "s_327",
                "s_11",
                "s_1195",
                "s_1014",
                "s_81"
            ],
            "type": "rich",
            "x": 5.912964820861816,
            "y": 9.671320915222168
        },
        {
            "title": "Visual Analytics for Complex Engineering Systems: Hybrid Visual Steering of Simulation Ensembles",
            "data": "In this paper we propose a novel approach to hybrid visual steering of simulation ensembles. A simulation ensemble is a collection of simulation runs of the same simulation model using different sets of control parameters. Complex engineering systems have very large parameter spaces so a na\u00efve sampling can result in prohibitively large simulation ensembles. Interactive steering of simulation ensembles provides the means to select relevant points in a multi-dimensional parameter space (design of experiment). Interactive steering efficiently reduces the number of simulation runs needed by coupling simulation and visualization and allowing a user to request new simulations on the fly. As system complexity grows, a pure interactive solution is not always sufficient. The new approach of hybrid steering combines interactive visual steering with automatic optimization. Hybrid steering allows a domain expert to interactively (in a visualization) select data points in an iterative manner, approximate the values in a continuous region of the simulation space (by regression) and automatically find the \u201cbest\u201d points in this continuous region based on the specified constraints and objectives (by optimization). We argue that with the full spectrum of optimization options, the steering process can be improved substantially. We describe an integrated system consisting of a simulation, a visualization, and an optimization component. We also describe typical tasks and propose an interactive analysis workflow for complex engineering systems. We demonstrate our approach on a case study from automotive industry, the optimization of a hydraulic circuit in a high pressure common rail Diesel injection system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346744",
            "id": "r_299",
            "s_ids": [
                "s_840",
                "s_990",
                "s_709",
                "s_1312",
                "s_817",
                "s_378",
                "s_2"
            ],
            "type": "rich",
            "x": 3.272869348526001,
            "y": 7.7402143478393555
        },
        {
            "title": "Transformation of an Uncertain Video Search Pipeline to a Sketch-Based Visual Analytics Loop",
            "data": "Traditional sketch-based image or video search systems rely on machine learning concepts as their core technology. However, in many applications, machine learning alone is impractical since videos may not be semantically annotated sufficiently, there may be a lack of suitable training data, and the search requirements of the user may frequently change for different tasks. In this work, we develop a visual analytics systems that overcomes the shortcomings of the traditional approach. We make use of a sketch-based interface to enable users to specify search requirement in a flexible manner without depending on semantic annotation. We employ active machine learning to train different analytical models for different types of search requirements. We use visualization to facilitate knowledge discovery at the different stages of visual analytics. This includes visualizing the parameter space of the trained model, visualizing the search space to support interactive browsing, visualizing candidature search results to support rapid interaction for active learning while minimizing watching videos, and visualizing aggregated information of the search results. We demonstrate the system for searching spatiotemporal attributes from sports video to identify key instances of the team and player performance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.207",
            "id": "r_300",
            "s_ids": [
                "s_1434",
                "s_825",
                "s_785",
                "s_295",
                "s_17",
                "s_118",
                "s_273"
            ],
            "type": "rich",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media",
            "data": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585638",
            "id": "r_301",
            "s_ids": [
                "s_327",
                "s_373",
                "s_323",
                "s_1274",
                "s_1262",
                "s_257"
            ],
            "type": "rich",
            "x": 7.854940891265869,
            "y": 8.153303146362305
        },
        {
            "title": "SenseMap: Supporting browser-based online sensemaking through analytic provenance",
            "data": "Sensemaking is described as the process in which people collect, organize and create representations of information, all centered around some problem they need to understand. People often get lost when solving complicated tasks using big datasets over long periods of exploration and analysis. They may forget what they have done, are unaware of where they are in the context of the overall task, and are unsure where to continue. In this paper, we introduce a tool, SenseMap, to address these issues in the context of browser-based online sensemaking. We conducted a semi-structured interview with nine participants to explore their behaviors in online sensemaking with existing browser functionality. A simplified sensemaking model based on Pirolli and Card's model is derived to better represent the behaviors we found: users iteratively collect information sources relevant to the task, curate them in a way that makes sense, and finally communicate their findings to others. SenseMap automatically captures provenance of user sensemaking actions and provides multi-linked views to visualize the collected information and enable users to curate and communicate their findings. To explore how SenseMap is used, we conducted a user study in a naturalistic work setting with five participants completing the same sensemaking task related to their daily work activities. All participants found the visual representation and interaction of the tool intuitive to use. Three of them engaged with the tool and produced successful outcomes. It helped them to organize information sources, to quickly find and navigate to the sources they wanted, and to effectively communicate their findings.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883515",
            "id": "r_302",
            "s_ids": [
                "s_57",
                "s_638",
                "s_1365",
                "s_869",
                "s_1450",
                "s_593"
            ],
            "type": "rich",
            "x": 5.82072114944458,
            "y": 10.373441696166992
        },
        {
            "title": "How ideas flow across multiple social groups",
            "data": "Tracking how correlated ideas flow within and across multiple social groups facilitates the understanding of the transfer of information, opinions, and thoughts on social media. In this paper, we present IdeaFlow, a visual analytics system for analyzing the lead-lag changes within and across pre-defined social groups regarding a specific set of correlated ideas, each of which is described by a set of words. To model idea flows accurately, we develop a random-walk-based correlation model and integrate it with Bayesian conditional cointegration and a tensor-based technique. To convey complex lead-lag relationships over time, IdeaFlow combines the strengths of a bubble tree, a flow map, and a timeline. In particular, we develop a Voronoi-treemap-based bubble tree to help users get an overview of a set of ideas quickly. A correlated-clustering-based layout algorithm is used to simultaneously generate multiple flow maps with less ambiguity. We also introduce a focus+context timeline to explore huge amounts of temporal data at different levels of time granularity. Quantitative evaluation and case studies demonstrate the accuracy and effectiveness of IdeaFlow.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883511",
            "id": "r_303",
            "s_ids": [
                "s_1069",
                "s_316",
                "s_1065",
                "s_41",
                "s_1515",
                "s_763",
                "s_1199"
            ],
            "type": "rich",
            "x": 7.48647928237915,
            "y": 7.571491241455078
        },
        {
            "title": "Model space visualization for multivariate linear trend discovery",
            "data": "Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333431",
            "id": "r_304",
            "s_ids": [
                "s_1384",
                "s_970",
                "s_797"
            ],
            "type": "rich",
            "x": 4.824171543121338,
            "y": 6.756379127502441
        },
        {
            "title": "A Visual Analytics Approach for Exploratory Causal Analysis: Exploration, Validation, and Applications",
            "data": "Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028957",
            "id": "r_305",
            "s_ids": [
                "s_1491",
                "s_1173",
                "s_1347"
            ],
            "type": "rich",
            "x": 4.371328353881836,
            "y": 8.479657173156738
        },
        {
            "title": "TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data and Unstructured Text",
            "data": "We describe TextTile, a data visualization tool for investigation of datasets and questions that require seamless and flexible analysis of structured data and unstructured text. TextTile is based on real-world data analysis problems gathered through our interaction with a number of domain experts and provides a general purpose solution to such problems. The system integrates a set of operations that can interchangeably be applied to the structured as well as to unstructured text part of the data to generate useful data summaries. Such summaries are then organized in visual tiles in a grid layout to allow their analysis and comparison. We validate TextTile with task analysis, use cases and a user study showing the system can be easily learned and proficiently used to carry out nontrivial tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598447",
            "id": "r_306",
            "s_ids": [
                "s_959",
                "s_600",
                "s_473"
            ],
            "type": "rich",
            "x": 7.566127777099609,
            "y": 10.470183372497559
        },
        {
            "title": "Identifying Redundancy and Exposing Provenance in Crowdsourced Data Analysis",
            "data": "We present a system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights. We take advantage of the fact that, for many types of data, independent crowd workers can readily perform basic analysis tasks like examining views and generating explanations for trends and patterns. However, workers operating in parallel can often generate redundant explanations. Moreover, because workers have different competencies and domain knowledge, some responses are likely to be more plausible than others. To efficiently utilize the crowd's work, analysts must be able to quickly identify and consolidate redundant responses and determine which explanations are the most plausible. In this paper, we demonstrate several crowd-assisted techniques to help analysts make better use of crowdsourced explanations: (1) We explore crowd-assisted strategies that utilize multiple workers to detect redundant explanations. We introduce color clustering with representative selection-a strategy in which multiple workers cluster explanations and we automatically select the most-representative result-and show that it generates clusterings that are as good as those produced by experts. (2) We capture explanation provenance by introducing highlighting tasks and capturing workers' browsing behavior via an embedded web browser, and refine that provenance information via source-review tasks. We expose this information in an explanation-management interface that allows analysts to interactively filter and sort responses, select the most plausible explanations, and decide which to explore further.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.164",
            "id": "r_307",
            "s_ids": [
                "s_351",
                "s_1120",
                "s_1475",
                "s_973",
                "s_661"
            ],
            "type": "rich",
            "x": 5.919469833374023,
            "y": 10.14952278137207
        },
        {
            "title": "Do What I Mean, Not What I Say! Design Considerations for Supporting Intent and Context in Analytical Conversation",
            "data": "Natural language can be a useful modality for creating and interacting with visualizations but users often have unrealistic expectations about the intelligence of natural language systems. The gulf between user expectations and system capabilities may lead to a disappointing user experience. So - if we want to engineer a natural language system, what are the requirements around system intelligence? This work takes a retrospective look at how we answered this question in the design of Ask Data, a natural language interaction feature for Tableau. We examine two factors contributing to perceived system intelligence: the system's ability to understand the analytic intent behind an input utterance and the ability to interpret an utterance contextually (i.e. taking into account the current visualization state and recent actions). Our aim was to understand the ways in which a system would need to support these two aspects of intelligence to enable a positive user experience. We first describe a pre-design Wizard of Oz study that offered insight into this question and narrowed the space of designs under consideration. We then reflect on the impact of this study on system development, examining how design implications from the study played out in practice. Our work contributes insights for the design of natural language interaction in visual analytics as well as a reflection on the value of pre-design empirical studies in the development of visual analytic systems.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986918",
            "id": "r_308",
            "s_ids": [
                "s_1266",
                "s_478"
            ],
            "type": "rich",
            "x": 5.122883319854736,
            "y": 10.096379280090332
        },
        {
            "title": "ScatterBlogs: Geo-spatial document analysis",
            "data": "We presented Scatterblogs, a system for microblog analysis that seamlessly integrates search backend and visual frontend. It provides powerful, automatic algorithms for detecting spatio-temporal `anomalies' within blog entries as well as corresponding visual representations and interaction facilities for inspecting anomalies or exploiting them in further analytic steps. Apart from that, we consider the system's combinatoric facilities for building complex hypotheses from temporal, spatial, and content-related aspects an important feature. This was the key for creating a cross-checked analysis for MC1.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102488",
            "id": "r_309",
            "s_ids": [
                "s_214",
                "s_746",
                "s_1304",
                "s_404",
                "s_289",
                "s_39",
                "s_314"
            ],
            "type": "rich",
            "x": 7.568024635314941,
            "y": 7.852721691131592
        },
        {
            "title": "A visual analytics process for maritime resource allocation and risk assessment",
            "data": "In this paper, we present our collaborative work with the U.S. Coast Guard's Ninth District and Atlantic Area Commands where we developed a visual analytics system to analyze historic response operations and assess the potential risks in the maritime environment associated with the hypothetical allocation of Coast Guard resources. The system includes linked views and interactive displays that enable the analysis of trends, patterns and anomalies among the U.S. Coast Guard search and rescue (SAR) operations and their associated sorties. Our system allows users to determine the potential change in risks associated with closing certain stations in terms of response time, potential lives and property lost and provides optimal direction as to the nearest available station. We provide maritime risk assessment tools that allow analysts to explore Coast Guard coverage for SAR operations and identify regions of high risk. The system also enables a thorough assessment of all SAR operations conducted by each Coast Guard station in the Great Lakes region. Our system demonstrates the effectiveness of visual analytics in analyzing risk within the maritime domain and is currently being used by analysts at the Coast Guard Atlantic Area.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102460",
            "id": "r_310",
            "s_ids": [
                "s_52",
                "s_721",
                "s_303",
                "s_952"
            ],
            "type": "rich",
            "x": 6.568792343139648,
            "y": 7.300233840942383
        },
        {
            "title": "Applied visual analytics for economic decision-making",
            "data": "This paper introduces the application of visual analytics techniques as a novel approach for improving economic decision making. Particularly, we focus on two known problems where subjectspsila behavior consistently deviates from the optimal, the Winnerpsilas and Loserpsilas Curse. According to economists, subjects fail to recognize the profit-maximizing decision strategy in both the Winnerpsilas and Loserpsilas curse because they are unable to properly consider all the available information. As such, we have created a visual analytics tool to aid subjects in decision making under the Acquiring a Company framework common in many economic experiments. We demonstrate the added value of visual analytics in the decision making process through a series of user studies comparing standard visualization methods with interactive visual analytics techniques. Our work presents not only a basis for development and evaluation of economic visual analytic research, but also empirical evidence demonstrating the added value of applying visual analytics to general decision making tasks.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677363",
            "id": "r_311",
            "s_ids": [
                "s_749",
                "s_721",
                "s_952"
            ],
            "type": "rich",
            "x": 4.569857120513916,
            "y": 9.084524154663086
        },
        {
            "title": "Exploring Large-Scale Video News via Interactive Visualization",
            "data": "In this paper, we have developed a novel visualization framework to enable more effective visual analysis of large-scale news videos, where keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness measurement to help audiences rind news stories of interest at first glance. A computational approach is also developed to quantify the interestingness measurement of video clips. Our experimental results have shown that our techniques for intelligent news video analysis have the capacity to enable more effective visualization of large-scale news videos. Our news video visualization system is very useful for security applications and for general audiences to quickly find news topics of interest from among many channels",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261433",
            "id": "r_312",
            "s_ids": [
                "s_1503",
                "s_629",
                "s_763",
                "s_233",
                "s_608"
            ],
            "type": "rich",
            "x": 5.916130065917969,
            "y": 8.278640747070312
        },
        {
            "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes",
            "data": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028888",
            "id": "r_313",
            "s_ids": [
                "s_42",
                "s_784",
                "s_1180",
                "s_731",
                "s_721"
            ],
            "type": "rich",
            "x": 2.2220568656921387,
            "y": 8.885293960571289
        },
        {
            "title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems",
            "data": "Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers and ensuring the efficient operation of the systems. To this end, a variety of automated techniques have been developed to identify anomalies in cloud computing. These techniques are usually adopted to track the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of cloud computing data, the effectiveness of these automated methods is affected. Thus, substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing systems. A novel unsupervised anomaly detection algorithm is developed to identify anomalies based on the specific temporal patterns of the given metrics data (e.g., the periodic pattern). Rich visualization and interaction designs are used to help understand the anomalies in the spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934613",
            "id": "r_314",
            "s_ids": [
                "s_28",
                "s_651",
                "s_1252",
                "s_524",
                "s_722",
                "s_1426",
                "s_585",
                "s_362",
                "s_131"
            ],
            "type": "rich",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "title": "Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification",
            "data": "Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864526",
            "id": "r_315",
            "s_ids": [
                "s_1310",
                "s_100",
                "s_469"
            ],
            "type": "rich",
            "x": 5.50734806060791,
            "y": 10.76229476928711
        },
        {
            "title": "Analyzing High-dimensional Multivariate Network Links with Integrated Anomaly Detection, Highlighting and Exploration",
            "data": "This paper focuses on the integration of a family of visual analytics techniques for analyzing high-dimensional, multivariate network data that features spatial and temporal information, network connections, and a variety of other categorical and numerical data types. Such data types are commonly encountered in transportation, shipping, and logistics industries. Due to the scale and complexity of the data, it is essential to integrate techniques for data analysis, visualization, and exploration. We present new visual representations, Petal and Thread, to effectively present many-to-many network data including multi-attribute vectors. In addition, we deploy an information-theoretic model for anomaly detection across varying dimensions, displaying highlighted anomalies in a visually consistent manner, as well as supporting a managed process of exploration. Lastly, we evaluate the proposed methodology through data exploration and an empirical study.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042484",
            "id": "r_316",
            "s_ids": [
                "s_1239",
                "s_945",
                "s_406",
                "s_933",
                "s_200",
                "s_52",
                "s_605",
                "s_273",
                "s_952"
            ],
            "type": "rich",
            "x": 5.301107883453369,
            "y": 6.747445583343506
        },
        {
            "title": "The Deshredder: A visual analytic approach to reconstructing shredded documents",
            "data": "Reconstruction of shredded documents remains a significant challenge. Creating a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries' attempts to gain access to information. Existing approaches to reconstructing shredded documents adopt either a predominantly manual (e.g., crowd-sourcing) or a near automatic approach. We describe Deshredder, a visual analytic approach that scales well and effectively incorporates user input to direct the reconstruction process. Deshredder represents shredded pieces as time series and uses nearest neighbor matching techniques that enable matching both the contours of shredded pieces as well as the content of shreds themselves. More importantly, Deshred-der's interface support visual analytics through user interaction with similarity matrices as well as higher level assembly through more complex stitching functions. We identify a functional task taxonomy leading to design considerations for constructing deshredding solutions, and describe how Deshredder applies to problems from the DARPA Shredder Challenge through expert evaluations.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400560",
            "id": "r_317",
            "s_ids": [
                "s_539",
                "s_619",
                "s_1461"
            ],
            "type": "rich",
            "x": 7.316325664520264,
            "y": 10.24779987335205
        },
        {
            "title": "Two-stage framework for a topology-based projection and visualization of classified document collections",
            "data": "During the last decades, electronic textual information has become the world's largest and most important information source. Daily newspapers, books, scientific and governmental publications, blogs and private messages have grown into a wellspring of endless information and knowledge. Since neither existing nor new information can be read in its entirety, we rely increasingly on computers to extract and visualize meaningful or interesting topics and documents from this huge information reservoir. In this paper, we extend, improve and combine existing individual approaches into an overall framework that supports topologi-cal analysis of high dimensional document point clouds given by the well-known tf-idf document-term weighting method. We show that traditional distance-based approaches fail in very high dimensional spaces, and we describe an improved two-stage method for topology-based projections from the original high dimensional information space to both two dimensional (2-D) and three dimensional (3-D) visualizations. To demonstrate the accuracy and usability of this framework, we compare it to methods introduced recently and apply it to complex document and patent collections.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652940",
            "id": "r_318",
            "s_ids": [
                "s_456",
                "s_859",
                "s_1148",
                "s_1315",
                "s_404",
                "s_314",
                "s_960"
            ],
            "type": "rich",
            "x": 7.229828834533691,
            "y": 10.067737579345703
        },
        {
            "title": "DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data",
            "data": "Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389013",
            "id": "r_319",
            "s_ids": [
                "s_1191",
                "s_756",
                "s_911"
            ],
            "type": "rich",
            "x": 5.081255912780762,
            "y": 6.8517327308654785
        },
        {
            "title": "Exploratory Visualization of Multivariate Data with Variable Quality",
            "data": "Real-world data is known to be imperfect, suffering from various forms of defects such as sensor variability, estimation errors, uncertainty, human errors in data entry, and gaps in data gathering. Analysis conducted on variable quality data can lead to inaccurate or incorrect results. An effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content, but also its quality attributes. While some research has been conducted on visualizing uncertainty in spatio-temporal data and univariate data, little work has been reported on extending this capability into multivariate data visualization. In this paper we describe our approach to the problem of visually exploring multivariate data with variable quality. As a foundation, we propose a general approach to defining quality measures for tabular data, in which data may experience quality problems at three granularities: individual data values, complete records, and specific dimensions. We then present two approaches to visual mapping of quality information into display space. In particular, one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions. The other solution is to superimpose the quality information within the data visualizations using additional visual variables. We also report on user studies conducted to assess alternate mappings of quality attributes to visual variables for the second method. In addition, we describe case studies that expose some of the advantages and disadvantages of these two approaches",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261424",
            "id": "r_320",
            "s_ids": [
                "s_141",
                "s_143",
                "s_970",
                "s_797"
            ],
            "type": "rich",
            "x": 4.6266655921936035,
            "y": 7.6206278800964355
        },
        {
            "title": "PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models",
            "data": "PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes-each with its own temporally evolving prevalence and co-occurrence of phenotypes-without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745118",
            "id": "r_321",
            "s_ids": [
                "s_293",
                "s_835",
                "s_274",
                "s_173",
                "s_255",
                "s_939",
                "s_752"
            ],
            "type": "rich",
            "x": 5.8732194900512695,
            "y": 6.8065009117126465
        },
        {
            "title": "PhenoBlocks: Phenotype Comparison Visualizations",
            "data": "The differential diagnosis of hereditary disorders is a challenging task for clinicians due to the heterogeneity of phenotypes that can be observed in patients. Existing clinical tools are often text-based and do not emphasize consistency, completeness, or granularity of phenotype reporting. This can impede clinical diagnosis and limit their utility to genetics researchers. Herein, we present PhenoBlocks, a novel visual analytics tool that supports the comparison of phenotypes between patients, or between a patient and the hallmark features of a disorder. An informal evaluation of PhenoBlocks with expert clinicians suggested that the visualization effectively guides the process of differential diagnosis and could reinforce the importance of complete, granular phenotypic reporting.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467733",
            "id": "r_322",
            "s_ids": [
                "s_293",
                "s_689",
                "s_173",
                "s_1175",
                "s_255",
                "s_696",
                "s_752"
            ],
            "type": "rich",
            "x": 5.901120662689209,
            "y": 6.837912082672119
        },
        {
            "title": "Visual Analytics for Comparison of Ocean Model Output with Reference Data: Detecting and Analyzing Geophysical Processes Using Clustering Ensembles",
            "data": "Researchers assess the quality of an ocean model by comparing its output to that of a previous model version or to observations. One objective of the comparison is to detect and to analyze differences and similarities between both data sets regarding geophysical processes, such as particular ocean currents. This task involves the analysis of thousands or hundreds of thousands of geographically referenced temporal profiles in the data. To cope with the amount of data, modelers combine aggregation of temporal profiles to single statistical values with visual comparison. Although this strategy is based on experience and a well-grounded body of expert knowledge, our discussions with domain experts have shown that it has two limitations: (1) using a single statistical measure results in a rather limited scope of the comparison and in significant loss of information, and (2) the decisions modelers have to make in the process may lead to important aspects being overlooked.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346751",
            "id": "r_323",
            "s_ids": [
                "s_140",
                "s_344",
                "s_1215",
                "s_1166"
            ],
            "type": "rich",
            "x": 6.401253700256348,
            "y": 6.265356063842773
        },
        {
            "title": "Diagnosing Concept Drift with Visual Analytics",
            "data": "Concept drift is a phenomenon in which the distribution of a data stream changes over time in unforeseen ways, causing prediction models built on historical data to become inaccurate. While a variety of automated methods have been developed to identify when concept drift occurs, there is limited support for analysts who need to understand and correct their models when drift is detected. In this paper, we present a visual analytics method, DriftVis, to support model builders and analysts in the identification and correction of concept drift in streaming data. DriftVis combines a distribution-based drift detection method with a streaming scatterplot to support the analysis of drift caused by the distribution changes of data streams and to explore the impact of these changes on the model\u2019s accuracy. A quantitative experiment and two case studies on weather prediction and text classification have been conducted to demonstrate our proposed tool and illustrate how visual analytics can be used to support the detection, examination, and correction of concept drift.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00007",
            "id": "r_324",
            "s_ids": [
                "s_125",
                "s_837",
                "s_491",
                "s_804",
                "s_640",
                "s_721",
                "s_316"
            ],
            "type": "rich",
            "x": 6.844028949737549,
            "y": 6.4676408767700195
        },
        {
            "title": "Visual analytic roadblocks for novice investigators",
            "data": "We have observed increasing interest in visual analytics tools and their applications in investigative analysis. Despite the growing interest and substantial studies regarding the topic, understanding the major roadblocks of using such tools from novice users' perspectives is still limited. Therefore, we attempted to identify such \u201cvisual analytic roadblocks\u201d for novice users in an investigative analysis scenario. To achieve this goal, we reviewed the existing models, theories, and frameworks that could explain the cognitive processes of human-visualization interaction in investigative analysis. Then, we conducted a qualitative experiment with six novice participants, using a slightly modified version of pair analytics, and analyzed the results through the open-coding method. As a result, we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and theories. We also provided design suggestions to overcome these roadblocks.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102435",
            "id": "r_325",
            "s_ids": [
                "s_1235",
                "s_1047",
                "s_1260"
            ],
            "type": "rich",
            "x": 5.336162090301514,
            "y": 9.967113494873047
        },
        {
            "title": "Visual exploration of classification models for risk assessment",
            "data": "In risk assessment applications well informed decisions are made based on huge amounts of multi-dimensional data. In many domains not only the risk of a wrong decision, but in particular the trade-off between the costs of possible decisions are of utmost importance. In this paper we describe a framework tightly integrating interactive visual exploration with machine learning to support the decision making process. The proposed approach uses a series of interactive 2D visualizations of numeric and ordinal data combined with visualization of classification models. These series of visual elements are further linked to the classifier's performance visualized using an interactive performance curve. An interactive decision point on the performance curve allows the decision maker to steer the classification model and instantly identify the critical, cost changing data elements, in the various linked visualizations. The critical data elements are represented as images in order to trigger associations related to the knowledge of the expert. In this context the data visualization and classification results are not only linked together, but are also linked back to the classification model. Such a visual analytics framework allows the user to interactively explore the costs of his decisions for different settings of the model and accordingly use the most suitable classification model and make more informed and reliable decisions. A case study on data from the Forensic Psychiatry domain reveals the usefulness of the suggested approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652398",
            "id": "r_326",
            "s_ids": [
                "s_1035",
                "s_1206"
            ],
            "type": "rich",
            "x": 4.454586505889893,
            "y": 8.281006813049316
        },
        {
            "title": "What do Constraint Programming Users Want to See? Exploring the Role of Visualisation in Profiling of Models and Search",
            "data": "Constraint programming allows difficult combinatorial problems to be modelled declaratively and solved automatically. Advances in solver technologies over recent years have allowed the successful use of constraint programming in many application areas. However, when a particular solver's search for a solution takes too long, the complexity of the constraint program execution hinders the programmer's ability to profile that search and understand how it relates to their model. Therefore, effective tools to support such profiling and allow users of constraint programming technologies to refine their model or experiment with different search parameters are essential. This paper details the first user-centred design process for visual profiling tools in this domain. We report on: our insights and opportunities identified through an on-line questionnaire and a creativity workshop with domain experts carried out to elicit requirements for analytical and visual profiling techniques; our designs and functional prototypes realising such techniques; and case studies demonstrating how these techniques shed light on the behaviour of the solvers in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598545",
            "id": "r_327",
            "s_ids": [
                "s_84",
                "s_1298",
                "s_195",
                "s_1339",
                "s_984",
                "s_276"
            ],
            "type": "rich",
            "x": 3.7070932388305664,
            "y": 7.860748291015625
        },
        {
            "title": "LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design",
            "data": "State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468011",
            "id": "r_328",
            "s_ids": [
                "s_532",
                "s_92",
                "s_1486",
                "s_60",
                "s_531",
                "s_275"
            ],
            "type": "rich",
            "x": 3.3469347953796387,
            "y": 7.586907386779785
        },
        {
            "title": "Visual Reconciliation of Alternative Similarity Spaces in Climate Modeling",
            "data": "Visual data analysis often requires grouping of data objects based on their similarity. In many application domains researchers use algorithms and techniques like clustering and multidimensional scaling to extract groupings from data. While extracting these groups using a single similarity criteria is relatively straightforward, comparing alternative criteria poses additional challenges. In this paper we define visual reconciliation as the problem of reconciling multiple alternative similarity spaces through visualization and interaction. We derive this problem from our work on model comparison in climate science where climate modelers are faced with the challenge of making sense of alternative ways to describe their models: one through the output they generate, another through the large set of properties that describe them. Ideally, they want to understand whether groups of models with similar spatio-temporal behaviors share similar sets of criteria or, conversely, whether similar criteria lead to similar behaviors. We propose a visual analytics solution based on linked views, that addresses this problem by allowing the user to dynamically create, modify and observe the interaction among groupings, thereby making the potential explanations apparent. We present case studies that demonstrate the usefulness of our technique in the area of climate science.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346755",
            "id": "r_329",
            "s_ids": [
                "s_1063",
                "s_1356",
                "s_594",
                "s_1100",
                "s_1171",
                "s_207",
                "s_1451",
                "s_473",
                "s_393"
            ],
            "type": "rich",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "title": "Visual Analytics for Spatial Clustering: Using a Heuristic Approach for Guided Exploration",
            "data": "We propose a novel approach of distance-based spatial clustering and contribute a heuristic computation of input parameters for guiding users in the search of interesting cluster constellations. We thereby combine computational geometry with interactive visualization into one coherent framework. Our approach entails displaying the results of the heuristics to users, as shown in Figure 1, providing a setting from which to start the exploration and data analysis. Addition interaction capabilities are available containing visual feedback for exploring further clustering options and is able to cope with noise in the data. We evaluate, and show the benefits of our approach on a sophisticated artificial dataset and demonstrate its usefulness on real-world data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.224",
            "id": "r_330",
            "s_ids": [
                "s_111",
                "s_637",
                "s_1355",
                "s_954",
                "s_196"
            ],
            "type": "rich",
            "x": 3.7874088287353516,
            "y": 6.028797149658203
        },
        {
            "title": "The Impact of Physical Navigation on Spatial Organization for Sensemaking",
            "data": "Spatial organization has been proposed as a compelling approach to externalizing the sensemaking process. However, there are two ways in which space can be provided to the user: by creating a physical workspace that the user can interact with directly, such as can be provided by a large, high-resolution display, or through the use of a virtual workspace that the user navigates using virtual navigation techniques such as zoom and pan. In this study we explicitly examined the use of spatial sensemaking techniques within these two environments. The results demonstrate that these two approaches to providing sensemaking space are not equivalent, and that the greater embodiment afforded by the physical workspace changes how the space is perceived and used, leading to increased externalization of the sensemaking process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.205",
            "id": "r_331",
            "s_ids": [
                "s_1492",
                "s_202"
            ],
            "type": "rich",
            "x": 5.718267917633057,
            "y": 10.30970573425293
        },
        {
            "title": "Scatter/Gather Clustering: Flexibly Incorporating User Feedback to Steer Clustering Results",
            "data": "Significant effort has been devoted to designing clustering algorithms that are responsive to user feedback or that incorporate prior domain knowledge in the form of constraints. However, users desire more expressive forms of interaction to influence clustering outcomes. In our experiences working with diverse application scientists, we have identified an interaction style scatter/gather clustering that helps users iteratively restructure clustering results to meet their expectations. As the names indicate, scatter and gather are dual primitives that describe whether clusters in a current segmentation should be broken up further or, alternatively, brought back together. By combining scatter and gather operations in a single step, we support very expressive dynamic restructurings of data. Scatter/gather clustering is implemented using a nonlinear optimization framework that achieves both locality of clusters and satisfaction of user-supplied constraints. We illustrate the use of our scatter/gather clustering approach in a visual analytic application to study baffle shapes in the bat biosonar (ears and nose) system. We demonstrate how domain experts are adept at supplying scatter/gather constraints, and how our framework incorporates these constraints effectively without requiring numerous instance-level constraints.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.258",
            "id": "r_332",
            "s_ids": [
                "s_1438",
                "s_776",
                "s_672",
                "s_355",
                "s_803",
                "s_1461"
            ],
            "type": "rich",
            "x": 3.6070477962493896,
            "y": 5.6530327796936035
        },
        {
            "title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
            "data": "In this work, we propose a generic visual analytics framework to support tactic analysis based on data collected from racquet sports (such as tennis and badminton). The proposed approach models each rally in a game as a sequence of hits (i.e., events) until one athlete scores a point. Each hit can be described with a set of attributes, such as the positions of the ball and the techniques used to hit the ball (such as drive and volley in tennis). Thus, the mentioned sequence of hits can be viewed as a multivariate event sequence. By detecting and analyzing the multivariate subsequences that frequently occur in the rallies (namely, tactical patterns), athletes can gain insights into the playing styles adopted by their opponents, and therefore help them identify systematic weaknesses of the opponents and develop counter strategies in matches. To support such analysis effectively, we propose a steerable multivariate sequential pattern mining algorithm with adjustable weights over event attributes, such that the domain expert can obtain frequent tactical patterns according to the attributes specified by himself. We also propose a re-configurable glyph design to help users simultaneously analyze multiple attributes of the hits. The framework further supports comparative analysis of the tactical patterns, e.g., for different athletes or the same athlete playing under different conditions. By applying the framework on two datasets collected in tennis and badminton matches, we demonstrate that the system is generic and effective for tactic analysis in sports and can help identify signature techniques used by individual athletes. Finally, we discuss the strengths and limitations of the proposed approach based on the feedback from the domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00009",
            "id": "r_333",
            "s_ids": [
                "s_1019",
                "s_245",
                "s_48",
                "s_230",
                "s_1347"
            ],
            "type": "rich",
            "x": 8.534769058227539,
            "y": 5.998741626739502
        },
        {
            "title": "Using Visualizations to Monitor Changes and Harvest Insights from a Global-Scale Logging Infrastructure at Twitter",
            "data": "Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042487",
            "id": "r_334",
            "s_ids": [
                "s_387",
                "s_1269"
            ],
            "type": "rich",
            "x": 7.255488872528076,
            "y": 7.872509956359863
        },
        {
            "title": "An Insight- and Task-based Methodology for Evaluating Spatiotemporal Visual Analytics",
            "data": "We present a method for evaluating visualizations using both tasks and exploration, and demonstrate this method in a study of spatiotemporal network designs for a visual analytics system. The method is well suited for studying visual analytics applications in which users perform both targeted data searches and analyses of broader patterns. In such applications, an effective visualization design is one that helps users complete tasks accurately and efficiently, and supports hypothesis generation during open-ended exploration. To evaluate both of these aims in a single study, we developed an approach called layered insight- and task-based evaluation (LITE) that interposes several prompts for observations about the data model between sequences of predefined search tasks. We demonstrate the evaluation method in a user study of four network visualizations for spatiotemporal data in a visual analytics application. Results include findings that might have been difficult to obtain in a single experiment using a different methodology. For example, with one dataset we studied, we found that on average participants were faster on search tasks using a force-directed layout than using our other designs; at the same time, participants found this design least helpful in understanding the data. Our contributions include a novel evaluation method that combines well-defined tasks with exploration and observation, an evaluation of network visualization designs for spatiotemporal visual analytics, and guidelines for using this evaluation method.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042482",
            "id": "r_335",
            "s_ids": [
                "s_242",
                "s_694",
                "s_855",
                "s_1194"
            ],
            "type": "rich",
            "x": 5.218771457672119,
            "y": 9.095818519592285
        },
        {
            "title": "Connecting the dots in visual analysis",
            "data": "During visual analysis, users must often connect insights discovered at various points of time. This process is often called ldquoconnecting the dots.rdquo When analysts interactively explore complex datasets over multiple sessions, they may uncover a large number of findings. As a result, it is often difficult for them to recall the past insights, views and concepts that are most relevant to their current line of inquiry. This challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others. In this paper, we describe a context-based retrieval algorithm to identify notes, views and concepts from users' past analyses that are most relevant to a view or a note based on their line of inquiry. We then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm. We have implemented this recommendation feature in HARVEST, a Web based visual analytic system. We evaluate the related notes recommendation feature of HARVEST through a case study and discuss the implications of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333023",
            "id": "r_336",
            "s_ids": [
                "s_754",
                "s_873",
                "s_1525"
            ],
            "type": "rich",
            "x": 6.752778053283691,
            "y": 10.02120304107666
        },
        {
            "title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
            "data": "Pathogen outbreaks (i.e., outbreaks of bacteria and viruses) in hospitals can cause high mortality rates and increase costs for hospitals significantly. An outbreak is generally noticed when the number of infected patients rises above an endemic level or the usual prevalence of a pathogen in a defined population. Reconstructing transmission pathways back to the source of an outbreak - the patient zero or index patient - requires the analysis of microbiological data and patient contacts. This is often manually completed by infection control experts. We present a novel visual analytics approach to support the analysis of transmission pathways, patient contacts, the progression of the outbreak, and patient timelines during hospitalization. Infection control experts applied our solution to a real outbreak of Klebsiella pneumoniae in a large German hospital. Using our system, our experts were able to scale the analysis of transmission pathways to longer time intervals (i.e., several years of data instead of days) and across a larger number of wards. Also, the system is able to reduce the analysis time from days to hours. In our final study, feedback from twenty-five experts from seven German hospitals provides evidence that our solution brings significant benefits for analyzing outbreaks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030437",
            "id": "r_337",
            "s_ids": [
                "s_239",
                "s_341",
                "s_950",
                "s_80",
                "s_420",
                "s_1460",
                "s_208",
                "s_424",
                "s_1420",
                "s_426",
                "s_725",
                "s_1090",
                "s_241"
            ],
            "type": "rich",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos",
            "data": "Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934656",
            "id": "r_338",
            "s_ids": [
                "s_339",
                "s_1188",
                "s_781",
                "s_1153",
                "s_492",
                "s_1517",
                "s_131"
            ],
            "type": "rich",
            "x": 5.177468776702881,
            "y": 9.842877388000488
        },
        {
            "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference",
            "data": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934595",
            "id": "r_339",
            "s_ids": [
                "s_808",
                "s_894",
                "s_1116",
                "s_1409",
                "s_463"
            ],
            "type": "rich",
            "x": 3.0237932205200195,
            "y": 8.854640007019043
        },
        {
            "title": "A Visual Analytics Framework for Spatiotemporal Trade Network Analysis",
            "data": "Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864844",
            "id": "r_340",
            "s_ids": [
                "s_1325",
                "s_804",
                "s_652",
                "s_1510",
                "s_854",
                "s_71",
                "s_721"
            ],
            "type": "rich",
            "x": 8.675040245056152,
            "y": 7.296395301818848
        },
        {
            "title": "BiDots: Visual Exploration of Weighted Biclusters",
            "data": "Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity. This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744458",
            "id": "r_341",
            "s_ids": [
                "s_1354",
                "s_102",
                "s_482",
                "s_761"
            ],
            "type": "rich",
            "x": 4.838989734649658,
            "y": 5.702586650848389
        },
        {
            "title": "PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations",
            "data": "Cross-sectional phenotype studies are used by genetics researchers to better understand how phenotypes vary across patients with genetic diseases, both within and between cohorts. Analyses within cohorts identify patterns between phenotypes and patients (e.g., co-occurrence) and isolate special cases (e.g., potential outliers). Comparing the variation of phenotypes between two cohorts can help distinguish how different factors affect disease manifestation (e.g., causal genes, age of onset, etc.). PhenoStacks is a novel visual analytics tool that supports the exploration of phenotype variation within and between cross-sectional patient cohorts. By leveraging the semantic hierarchy of the Human Phenotype Ontology, phenotypes are presented in context, can be grouped and clustered, and are summarized via overviews to support the exploration of phenotype distributions. The design of PhenoStacks was motivated by formative interviews with genetics researchers: we distil high-level tasks, present an algorithm for simplifying ontology topologies for visualization, and report the results of a deployment evaluation with four expert genetics researchers. The results suggest that PhenoStacks can help identify phenotype patterns, investigate data quality issues, and inform data collection design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598469",
            "id": "r_342",
            "s_ids": [
                "s_293",
                "s_1458",
                "s_173",
                "s_255",
                "s_752",
                "s_939"
            ],
            "type": "rich",
            "x": 5.903914451599121,
            "y": 6.849249362945557
        },
        {
            "title": "TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data",
            "data": "Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.206",
            "id": "r_343",
            "s_ids": [
                "s_345",
                "s_427",
                "s_489",
                "s_1397",
                "s_1124"
            ],
            "type": "rich",
            "x": 6.666491985321045,
            "y": 7.1162872314453125
        },
        {
            "title": "Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data",
            "data": "Contingency tables summarize the relations between categorical variables and arise in both scientific and business domains. Asymmetrically large two-way contingency tables pose a problem for common visualization methods. The Contingency Wheel has been recently proposed as an interactive visual method to explore and analyze such tables. However, the scalability and readability of this method are limited when dealing with large and dense tables. In this paper we present Contingency Wheel++, new visual analytics methods that overcome these major shortcomings: (1) regarding automated methods, a measure of association based on Pearson's residuals alleviates the bias of the raw residuals originally used, (2) regarding visualization methods, a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible, and (3) regarding the interactive exploration environment, a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated views. We illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large categorical data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2012.254",
            "id": "r_344",
            "s_ids": [
                "s_1397",
                "s_489",
                "s_1124",
                "s_531"
            ],
            "type": "rich",
            "x": 5.193445205688477,
            "y": 6.149930477142334
        },
        {
            "title": "Visual social network analytics for relationship discovery in the enterprise",
            "data": "As people continue to author and share increasing amounts of information in social media, the opportunity to leverage such information for relationship discovery tasks increases. In this paper, we describe a set of systems that mine, aggregate, and infer a social graph from social media inside an enterprise, resulting in over 73 million relationships between 450,000 people. We then describe SaNDVis, a novel visual analytics tool that supports people-centric tasks like expertise location, team building, and team coordination in the enterprise. We also provide details of a 12-month-long, large-scale deployment to almost 1,800 users from which we extract dominant use cases from log and interview data. By integrating social position, evidence, and facets into SaNDVis, we demonstrate how users can use a visual analytics tool to reflect on existing relationships as well as build new relationships in an enterprise setting.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102443",
            "id": "r_345",
            "s_ids": [
                "s_1177",
                "s_568",
                "s_437",
                "s_465",
                "s_306"
            ],
            "type": "rich",
            "x": 7.6279706954956055,
            "y": 8.709637641906738
        },
        {
            "title": "Iterative integration of visual insights during patent search and analysis",
            "data": "Patents are an important economic factor in todays globalized markets. Therefore, the analysis of patent information has become an inevitable task for a variety of interest groups. The retrieval of relevant patent information is an integral part of almost every patent analysis scenario. Unfortunately, the complexity of patent material inhibits a straightforward retrieval of all relevant patent documents and leads to iterative, time-consuming approaches in practice. With `PatViz', a new system for interactive analysis of patent information has been developed to leverage iterative query refinement. PatViz supports users in building complex queries visually and in exploring patent result sets interactively. Thereby, the visual query module introduces an abstraction layer that provides uniform access to different retrieval systems and relieves users of the burden to learn different complex query languages. By establishing an integrated environment it allows for interactive reintegration of insights gained from visual result set exploration into the visual query representation. We expect that the approach we have taken is also suitable to improve iterative query refinement in other Visual Analytics systems.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333564",
            "id": "r_346",
            "s_ids": [
                "s_404",
                "s_214",
                "s_1446",
                "s_314"
            ],
            "type": "rich",
            "x": 6.595191478729248,
            "y": 9.53964900970459
        },
        {
            "title": "Time Tree: Exploring Time Changing Hierarchies",
            "data": "Intelligence analysis often involves the task of gathering information about an organization. Knowledge about individuals in an organization and their relationships, often represented as a hierarchical organization chart, is crucial for understanding the organization. However, it is difficult for intelligence analysts to follow all individuals in an organization. Existing hierarchy visualizations have largely focused on the visualization of fixed structures and can not effectively depict the evolution of a hierarchy over time. We introduce TimeTree, a novel visualization tool designed to enable exploration of a changing hierarchy. TimeTree enables analysts to navigate the history of an organization, identify events associated with a specific entity (visualized on a TimeSlider), and explore an aggregate view of an individual's career path (a CareerTree). We demonstrate the utility of TimeTree by investigating a set of scenarios developed by an expert intelligence analyst. The scenarios are evaluated using a real dataset composed of eighteen thousand career events from more than eight thousand individuals. Insights gained from this analysis are presented",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261450",
            "id": "r_347",
            "s_ids": [
                "s_819",
                "s_127",
                "s_1127",
                "s_1432",
                "s_886"
            ],
            "type": "rich",
            "x": 5.738348960876465,
            "y": 8.632399559020996
        },
        {
            "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
            "data": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030370",
            "id": "r_348",
            "s_ids": [
                "s_1254",
                "s_1295",
                "s_1153",
                "s_425",
                "s_287",
                "s_131",
                "s_733"
            ],
            "type": "rich",
            "x": 8.882956504821777,
            "y": 7.1464362144470215
        },
        {
            "title": "MotionRugs: Visualizing Collective Trends in Space and Time",
            "data": "Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865049",
            "id": "r_349",
            "s_ids": [
                "s_1516",
                "s_39",
                "s_1374",
                "s_264",
                "s_1038"
            ],
            "type": "rich",
            "x": 7.466686725616455,
            "y": 5.47048282623291
        },
        {
            "title": "ConTour: Data-Driven Exploration of Multi-Relational Datasets for Drug Discovery",
            "data": "Large scale data analysis is nowadays a crucial part of drug discovery. Biologists and chemists need to quickly explore and evaluate potentially effective yet safe compounds based on many datasets that are in relationship with each other. However, there is a lack of tools that support them in these processes. To remedy this, we developed ConTour, an interactive visual analytics technique that enables the exploration of these complex, multi-relational datasets. At its core ConTour lists all items of each dataset in a column. Relationships between the columns are revealed through interaction: selecting one or multiple items in one column highlights and re-sorts the items in other columns. Filters based on relationships enable drilling down into the large data space. To identify interesting items in the first place, ConTour employs advanced sorting strategies, including strategies based on connectivity strength and uniqueness, as well as sorting based on item attributes. ConTour also introduces interactive nesting of columns, a powerful method to show the related items of a child column for each item in the parent column. Within the columns, ConTour shows rich attribute data about the items as well as information about the connection strengths to other datasets. Finally, ConTour provides a number of detail views, which can show items from multiple datasets and their associated data at the same time. We demonstrate the utility of our system in case studies conducted with a team of chemical biologists, who investigate the effects of chemical compounds on cells and need to understand the underlying mechanisms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346752",
            "id": "r_350",
            "s_ids": [
                "s_484",
                "s_205",
                "s_512",
                "s_894",
                "s_238",
                "s_1409",
                "s_1425"
            ],
            "type": "rich",
            "x": 5.349819660186768,
            "y": 6.387389659881592
        },
        {
            "title": "Vismate: Interactive Visual Analysis of Station-Based Observation Data on Climate Changes",
            "data": "We present a new approach to visualizing the climate data of multi-dimensional, time-series, and geo-related characteristics. Our approach integrates three new highly interrelated visualization techniques, and uses the same input data types as in the traditional model-based analysis methods. As the main visualization view, Global Radial Map is used to identify the overall state of climate changes and provide users with a compact and intuitive view for analyzing spatial and temporal patterns at the same time. Other two visualization techniques, providing complementary views, are specialized in analysing time trend and detecting abnormal cases, which are two important analysis tasks in any climate change study. Case studies and expert reviews have been conducted, through which the effectiveness and scalability of the proposed approach has been confirmed.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042489",
            "id": "r_351",
            "s_ids": [
                "s_408",
                "s_313",
                "s_187"
            ],
            "type": "rich",
            "x": 6.323979377746582,
            "y": 6.447656631469727
        },
        {
            "title": "Poster: Dynamic time transformation for interpreting clusters of trajectories with space-time cube",
            "data": "We propose a set of techniques that support visual interpretation of trajectory clusters by transforming absolute time references into relative positions within temporal cycles or with respect to the starting and/or ending times of the trajectories. We demonstrate the work of the approach on a real data set about individual movement over one year.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5653580",
            "id": "r_352",
            "s_ids": [
                "s_1195",
                "s_11"
            ],
            "type": "rich",
            "x": 7.5169243812561035,
            "y": 5.298519611358643
        },
        {
            "title": "Analysis of community-contributed space- and time-referenced data (example of flickr and panoramio photos)",
            "data": "Space- and time-referenced data published on the Web by general people can be viewed in a dual way: as independent spatio-temporal events and as trajectories of people in the geographical space. These two views suppose different approaches to the analysis, which can yield different kinds of valuable knowledge about places and about people. We define possible types of analysis tasks related to the two views of the data and present several analysis methods appropriate for these tasks. The methods are suited to large amounts of the data.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333472",
            "id": "r_353",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_637",
                "s_1372",
                "s_1038"
            ],
            "type": "rich",
            "x": 6.054115295410156,
            "y": 6.42355489730835
        },
        {
            "title": "Multidimensional visual analysis using cross-filtered views",
            "data": "Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677370",
            "id": "r_354",
            "s_ids": [
                "s_309"
            ],
            "type": "rich",
            "x": 5.042435169219971,
            "y": 6.32241678237915
        },
        {
            "title": "Evaluating the relationship between user interaction and financial visual analysis",
            "data": "It has been widely accepted that interactive visualization techniques enable users to more effectively form hypotheses and identify areas for more detailed investigation. There have been numerous empirical user studies testing the effectiveness of specific visual analytical tools. However, there has been limited effort in connecting a userpsilas interaction with his reasoning for the purpose of extracting the relationship between the two. In this paper, we present an approach for capturing and analyzing user interactions in a financial visual analytical tool and describe an exploratory user study that examines these interaction strategies. To achieve this goal, we created two visual tools to analyze raw interaction data captured during the user session. The results of this study demonstrate one possible strategy for understanding the relationship between interaction and reasoning both operationally and strategically.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677360",
            "id": "r_355",
            "s_ids": [
                "s_596",
                "s_305",
                "s_0",
                "s_1294",
                "s_333",
                "s_233"
            ],
            "type": "rich",
            "x": 5.157791614532471,
            "y": 9.96960735321045
        },
        {
            "title": "Visual Analytics for Temporal Hypergraph Model Exploration",
            "data": "Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute Hyper-Matrix, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030408",
            "id": "r_356",
            "s_ids": [
                "s_842",
                "s_1509",
                "s_400",
                "s_248",
                "s_1038",
                "s_1206"
            ],
            "type": "rich",
            "x": 4.8054656982421875,
            "y": 5.500706195831299
        },
        {
            "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections",
            "data": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934654",
            "id": "r_357",
            "s_ids": [
                "s_799",
                "s_1431",
                "s_1522",
                "s_1038",
                "s_1279"
            ],
            "type": "rich",
            "x": 6.9785237312316895,
            "y": 10.85755729675293
        },
        {
            "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots",
            "data": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934208",
            "id": "r_358",
            "s_ids": [
                "s_428",
                "s_1414",
                "s_1357",
                "s_697",
                "s_561",
                "s_662",
                "s_1319"
            ],
            "type": "rich",
            "x": 4.4847564697265625,
            "y": 6.689899921417236
        },
        {
            "title": "Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities",
            "data": "Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864901",
            "id": "r_359",
            "s_ids": [
                "s_321",
                "s_1443",
                "s_584",
                "s_103"
            ],
            "type": "rich",
            "x": 6.343745231628418,
            "y": 6.369995594024658
        },
        {
            "title": "MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration",
            "data": "Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864886",
            "id": "r_360",
            "s_ids": [
                "s_1310",
                "s_1115",
                "s_715",
                "s_100"
            ],
            "type": "rich",
            "x": 7.579283237457275,
            "y": 6.7240729331970215
        },
        {
            "title": "Understanding the Relationship Between Interactive Optimisation and Visual Analytics in the Context of Prostate Brachytherapy",
            "data": "The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose \u201cblack-box\u201d solver. In practice, however, many problems cannot be solved completely automatically, but require a \u201chuman-in-the-loop\u201d to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient's tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744418",
            "id": "r_361",
            "s_ids": [
                "s_789",
                "s_195",
                "s_1247",
                "s_162",
                "s_12"
            ],
            "type": "rich",
            "x": 3.569378137588501,
            "y": 7.797091007232666
        },
        {
            "title": "Supporting Awareness through Collaborative Brushing and Linking of Tabular Data",
            "data": "Maintaining an awareness of collaborators' actions is critical during collaborative work, including during collaborative visualization activities. Particularly when collaborators are located at a distance, it is important to know what everyone is working on in order to avoid duplication of effort, share relevant results in a timely manner and build upon each other's results. Can a person's brushing actions provide an indication of their queries and interests in a data set? Can these actions be revealed to a collaborator without substantially disrupting their own independent work? We designed a study to answer these questions in the context of distributed collaborative visualization of tabular data. Participants in our study worked independently to answer questions about a tabular data set, while simultaneously viewing brushing actions of a fictitious collaborator, shown directly within a shared workspace. We compared three methods of presenting the collaborator's actions: brushing &amp;amp; linking (i.e. highlighting exactly what the collaborator would see), selection (i.e. showing only a selected item), and persistent selection (i.e. showing only selected items but having them persist for some time). Our results demonstrated that persistent selection enabled some awareness of the collaborator's activities while causing minimal interference with independent work. Other techniques were less effective at providing awareness, and brushing &amp;amp; linking caused substantial interference. These findings suggest promise for the idea of exploiting natural brushing actions to provide awareness in collaborative work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.197",
            "id": "r_362",
            "s_ids": [
                "s_741",
                "s_1266",
                "s_167"
            ],
            "type": "rich",
            "x": 5.484528541564941,
            "y": 10.792652130126953
        },
        {
            "title": "Visual analysis and coding of data-rich user behavior",
            "data": "Investigating user behavior involves abstracting low-level events to higher-level concepts. This requires an analyst to study individual user activities, assign codes which categorize behavior, and develop a consistent classification scheme. To better support this reasoning process of an analyst, we suggest a novel visual analytics approach which integrates rich user data including transcripts, videos, eye movement data, and interaction logs. Word-sized visualizations embedded into a tabular representation provide a space-efficient and detailed overview of user activities. An analyst assigns codes, grouped into code categories, as part of an interactive process. Filtering and searching helps to select specific activities and focus an analysis. A comparison visualization summarizes results of coding and reveals relationships between codes. Editing features support efficient assignment, refinement, and aggregation of codes. We demonstrate the practical applicability and usefulness of our approach in a case study and describe expert feedback.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883520",
            "id": "r_363",
            "s_ids": [
                "s_1280",
                "s_399",
                "s_458",
                "s_314",
                "s_132"
            ],
            "type": "rich",
            "x": 5.900495529174805,
            "y": 9.673620223999023
        },
        {
            "title": "Wavelet-based visualization of time-varying data on graphs",
            "data": "Visualizing time-varying data defined on the nodes of a graph is a challenging problem that has been faced with different approaches. Although techniques based on aggregation, topology, and topic modeling have proven their usefulness, the visual analysis of smooth and/or abrupt data variations as well as the evolution of such variations over time are aspects not properly tackled by existing methods. In this work we propose a novel visualization methodology that relies on graph wavelet theory and stacked graph metaphor to enable the visual analysis of time-varying data defined on the nodes of a graph. The proposed method is able to identify regions where data presents abrupt and mild spacial and/or temporal variation while still been able to show how such changes evolve over time, making the identification of events an easier task. The usefulness of our approach is shown through a set of results using synthetic as well as a real data set involving taxi trips in downtown Manhattan. The methodology was able to reveal interesting phenomena and events such as the identification of specific locations with abrupt variation in the number of taxi pickups.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347624",
            "id": "r_364",
            "s_ids": [
                "s_1183",
                "s_1495",
                "s_1363",
                "s_393",
                "s_360"
            ],
            "type": "rich",
            "x": 6.9307403564453125,
            "y": 6.105090141296387
        },
        {
            "title": "NewsLab: Exploratory Broadcast News Video Analysis",
            "data": "In this paper, we introduce NewsLab, an exploratory visualization approach for the analysis of large scale broadcast news video collections containing many thousands of news stories over extended periods of time. A river metaphor is used to depict the thematic changes of the news over time. An interactive lens metaphor allows the playback of fine-grained video segments selected through the river overview. Multi-resolution navigation is supported via a hierarchical time structure as well as a hierarchical theme structure. Themes can be explored hierarchically according to their thematic structure, or in an unstructured fashion using various ranking criteria. A rich set of interactions such as filtering, drill-down/roll-up navigation, history animation, and keyword based search are also provided. Our case studies show how this set of tools can be used to find emerging topics in the news, compare different broadcasters, or mine the news for topics of interest.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389005",
            "id": "r_365",
            "s_ids": [
                "s_1222",
                "s_737",
                "s_763",
                "s_233"
            ],
            "type": "rich",
            "x": 7.862253189086914,
            "y": 7.706037521362305
        },
        {
            "title": "NetLens: Iterative Exploration of Content-Actor Network Data",
            "data": "Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261426",
            "id": "r_366",
            "s_ids": [
                "s_485",
                "s_1399",
                "s_253",
                "s_1285"
            ],
            "type": "rich",
            "x": 5.051174640655518,
            "y": 5.1694254875183105
        },
        {
            "title": "VAST 2006 Contest - A Tale of Alderwood",
            "data": "Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The first visual analytics science and technology (VAST) contest was held in conjunction with the 2006 IEEE VAST Symposium. The competition entailed the identification of possible political shenanigans in the fictitious town of Alderwood. A synthetic data set was made available as well as tasks. We summarize how we prepared and advertised the contest, developed some initial metrics for evaluation, and selected the winners. The winners were invited to participate at an additional live competition at the symposium to provide them with feedback from senior analysts",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261420",
            "id": "r_367",
            "s_ids": [
                "s_1205",
                "s_934",
                "s_595",
                "s_1399",
                "s_1244",
                "s_231"
            ],
            "type": "rich",
            "x": 6.743419170379639,
            "y": 8.804743766784668
        },
        {
            "title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering",
            "data": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030347",
            "id": "r_368",
            "s_ids": [
                "s_291",
                "s_1473",
                "s_90",
                "s_1399",
                "s_1183"
            ],
            "type": "rich",
            "x": 3.5988824367523193,
            "y": 5.591180801391602
        },
        {
            "title": "Run Watchers: Automatic Simulation-Based Decision Support in Flood Management",
            "data": "In this paper, we introduce a simulation-based approach to design protection plans for flood events. Existing solutions require a lot of computation time for an exhaustive search, or demand for a time-consuming expert supervision and steering. We present a faster alternative based on the automated control of multiple parallel simulation runs. Run Watchers are dedicated system components authorized to monitor simulation runs, terminate them, and start new runs originating from existing ones according to domain-specific rules. This approach allows for a more efficient traversal of the search space and overall performance improvements due to a re-use of simulated states and early termination of failed runs. In the course of search, Run Watchers generate large and complex decision trees. We visualize the entire set of decisions made by Run Watchers using interactive, clustered timelines. In addition, we present visualizations to explain the resulting response plans. Run Watchers automatically generate storyboards to convey plan details and to justify the underlying decisions, including those which leave particular buildings unprotected. We evaluate our solution with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346930",
            "id": "r_369",
            "s_ids": [
                "s_144",
                "s_1388",
                "s_79",
                "s_578",
                "s_25",
                "s_1466",
                "s_531"
            ],
            "type": "rich",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "title": "SilkViser: A Visual Explorer of Blockchain-based Cryptocurrency Transaction Data",
            "data": "Many blockchain-based cryptocurrencies provide users with online blockchain explorers for viewing online transaction data. However, traditional blockchain explorers mostly present transaction information in textual and tabular forms. Such forms make understanding cryptocurrency transaction mechanisms difficult for novice users (NUsers). They are also insufficiently informative for experienced users (EUsers) to recognize advanced transaction information. This study introduces a new online cryptocurrency transaction data viewing tool called SilkViser. Guided by detailed scenario and requirement analyses, we create a series of appreciating visualization designs, such as paper ledger-inspired block and blockchain visualizations and ancient copper coin-inspired transaction visualizations, to help users understand cryptocurrency transaction mechanisms and recognize advanced transaction information. We also provide a set of lightweight interactions to facilitate easy and free data exploration. Moreover, a controlled user study is conducted to quantitatively evaluate the usability and effectiveness of SilkViser. Results indicate that SilkViser can satisfy the requirements of NUsers and EUsers. Our visualization designs can compensate for the inexperience of NUsers in data viewing and attract potential users to participate in cryptocurrency transactions.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00014",
            "id": "r_370",
            "s_ids": [
                "s_878",
                "s_1299",
                "s_422",
                "s_1357",
                "s_107",
                "s_1142",
                "s_955"
            ],
            "type": "rich",
            "x": 8.83371639251709,
            "y": 7.131134986877441
        },
        {
            "title": "AlVis: Situation awareness in the surveillance of road tunnels",
            "data": "In the surveillance of road tunnels, video data plays an important role for a detailed inspection and as an input to systems for an automated detection of incidents. In disaster scenarios like major accidents, however, the increased amount of detected incidents may lead to situations where human operators lose a sense of the overall meaning of that data, a problem commonly known as a lack of situation awareness. The primary contribution of this paper is a design study of AlVis, a system designed to increase situation awareness in the surveillance of road tunnels. The design of AlVis is based on a simplified tunnel model which enables an overview of the spatiotemporal development of scenarios in real-time. The visualization explicitly represents the present state, the history, and predictions of potential future developments. Concepts for situation-sensitive prioritization of information ensure scalability from normal operation to major disaster scenarios. The visualization enables an intuitive access to live and historic video for any point in time and space. We illustrate AlVis by means of a scenario and report qualitative feedback by tunnel experts and operators. This feedback suggests that AlVis is suitable to save time in recognizing dangerous situations and helps to maintain an overview in complex disaster scenarios.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400556",
            "id": "r_371",
            "s_ids": [
                "s_275",
                "s_1311",
                "s_91"
            ],
            "type": "rich",
            "x": 6.642140865325928,
            "y": 7.3651442527771
        },
        {
            "title": "LAHVA: Linked Animal-Human Health Visual Analytics",
            "data": "Coordinated animal-human health monitoring can provide an early warning system with fewer false alarms for naturally occurring disease outbreaks, as well as biological, chemical and environmental incidents. This monitoring requires the integration and analysis of multi-field, multi-scale and multi-source data sets. In order to better understand these data sets, models and measurements at different resolutions must be analyzed. To facilitate these investigations, we have created an application to provide a visual analytics framework for analyzing both human emergency room data and veterinary hospital data. Our integrated visual analytic tool links temporally varying geospatial visualization of animal and human patient health information with advanced statistical analysis of these multi-source data. Various statistical analysis techniques have been applied in conjunction with a spatio-temporal viewing window. Such an application provides researchers with the ability to visually search the data for clusters in both a statistical model view and a spatio-temporal view. Our interface provides a factor specification/filtering component to allow exploration of causal factors and spread patterns. In this paper, we will discuss the application of our linked animal-human visual analytics (LAHVA) tool to two specific case studies. The first case study is the effect of seasonal influenza and its correlation with different companion animals (e.g., cats, dogs) syndromes. Here we use data from the Indiana Network for Patient Care (INPC) and Banfield Pet Hospitals in an attempt to determine if there are correlations between respiratory syndromes representing the onset of seasonal influenza in humans and general respiratory syndromes in cats and dogs. Our second case study examines the effect of the release of industrial wastewater in a community through companion animal surveillance.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388993",
            "id": "r_372",
            "s_ids": [
                "s_721",
                "s_464",
                "s_605",
                "s_1101",
                "s_580",
                "s_952",
                "s_54",
                "s_1435",
                "s_1141",
                "s_1130"
            ],
            "type": "rich",
            "x": 6.365241527557373,
            "y": 6.8800506591796875
        },
        {
            "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs",
            "data": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030443",
            "id": "r_373",
            "s_ids": [
                "s_604",
                "s_441",
                "s_27",
                "s_65",
                "s_559",
                "s_1293",
                "s_150",
                "s_1517",
                "s_333"
            ],
            "type": "rich",
            "x": 5.659013748168945,
            "y": 8.697281837463379
        },
        {
            "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter Optimization of Deep Neural Networks",
            "data": "To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030380",
            "id": "r_374",
            "s_ids": [
                "s_703",
                "s_1264",
                "s_1159",
                "s_331"
            ],
            "type": "rich",
            "x": 3.6053433418273926,
            "y": 7.882735729217529
        },
        {
            "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling",
            "data": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934266",
            "id": "r_375",
            "s_ids": [
                "s_389",
                "s_1394",
                "s_1032",
                "s_952"
            ],
            "type": "rich",
            "x": 7.635458946228027,
            "y": 8.185929298400879
        },
        {
            "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media",
            "data": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934263",
            "id": "r_376",
            "s_ids": [
                "s_373",
                "s_318",
                "s_327",
                "s_1274"
            ],
            "type": "rich",
            "x": 7.80152702331543,
            "y": 8.48024845123291
        },
        {
            "title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics",
            "data": "Tennis players and coaches of all proficiency levels seek to understand and improve their play. Summary statistics alone are inadequate to provide the insights players need to improve their games. Spatio-temporal data capturing player and ball movements is likely to provide the actionable insights needed to identify player strengths, weaknesses, and strategies. To fully utilize this spatio-temporal data, we need to integrate it with domain-relevant context meta-data. In this paper, we propose CourtTime, a novel approach to perform data-driven visual analysis of individual tennis matches. Our visual approach introduces a novel visual metaphor, namely 1\u2013D Space-Time Charts that enable the analysis of single points at a glance based on small multiples. We also employ user-driven sorting and clustering techniques and a layout technique that aligns the last few shots in a point to facilitate shot pattern discovery. We discuss the usefulness of CourtTime via an extensive case study and report on feedback from an amateur tennis player and three tennis coaches.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934243",
            "id": "r_377",
            "s_ids": [
                "s_119",
                "s_39",
                "s_1402",
                "s_763"
            ],
            "type": "rich",
            "x": 8.476827621459961,
            "y": 6.0509443283081055
        },
        {
            "title": "Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization",
            "data": "Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744843",
            "id": "r_378",
            "s_ids": [
                "s_31",
                "s_1361",
                "s_1517",
                "s_100"
            ],
            "type": "rich",
            "x": 5.053277969360352,
            "y": 5.204998970031738
        },
        {
            "title": "A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections",
            "data": "Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598479",
            "id": "r_379",
            "s_ids": [
                "s_1329",
                "s_1385",
                "s_254"
            ],
            "type": "rich",
            "x": 4.2564167976379395,
            "y": 7.324117660522461
        },
        {
            "title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems",
            "data": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598460",
            "id": "r_380",
            "s_ids": [
                "s_865",
                "s_480",
                "s_1517",
                "s_684"
            ],
            "type": "rich",
            "x": 4.893017292022705,
            "y": 9.907453536987305
        },
        {
            "title": "DIA2: Web-based Cyberinfrastructure for Visual Analysis of Funding Portfolios",
            "data": "We present a design study of the Deep Insights Anywhere, Anytime (DIA2) platform, a web-based visual analytics system that allows program managers and academic staff at the U.S. National Science Foundation to search, view, and analyze their research funding portfolio. The goal of this system is to facilitate users' understanding of both past and currently active research awards in order to make more informed decisions of their future funding. This user group is characterized by high domain expertise yet not necessarily high literacy in visualization and visual analytics-they are essentially casual experts-and thus require careful visual and information design, including adhering to user experience standards, providing a self-instructive interface, and progressively refining visualizations to minimize complexity. We discuss the challenges of designing a system for casual experts and highlight how we addressed this issue by modeling the organizational structure and workflows of the NSF within our system. We discuss each stage of the design process, starting with formative interviews, prototypes, and finally live deployments and evaluation with stakeholders.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346747",
            "id": "r_381",
            "s_ids": [
                "s_1367",
                "s_1191",
                "s_288",
                "s_330",
                "s_673",
                "s_1176",
                "s_1487",
                "s_1006"
            ],
            "type": "rich",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "title": "Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations",
            "data": "For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.219",
            "id": "r_382",
            "s_ids": [
                "s_918",
                "s_1383",
                "s_1521",
                "s_701",
                "s_646"
            ],
            "type": "rich",
            "x": 5.673964023590088,
            "y": 6.457597255706787
        },
        {
            "title": "Origraph: Interactive Network Wrangling",
            "data": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986909",
            "id": "r_383",
            "s_ids": [
                "s_958",
                "s_1192",
                "s_1428",
                "s_205"
            ],
            "type": "rich",
            "x": 5.120394229888916,
            "y": 5.120257377624512
        },
        {
            "title": "Pointwise local pattern exploration for sensitivity analysis",
            "data": "Sensitivity analysis is a powerful method for discovering the significant factors that contribute to targets and understanding the interaction between variables in multivariate datasets. A number of sensitivity analysis methods fall into the class of local analysis, in which the sensitivity is defined as the partial derivatives of a target variable with respect to a group of independent variables. Incorporating sensitivity analysis in visual analytic tools is essential for multivariate phenomena analysis. However, most current multivariate visualization techniques do not allow users to explore local patterns individually for understanding the sensitivity from a pointwise view. In this paper, we present a novel pointwise local pattern exploration system for visual sensitivity analysis. Using this system, analysts are able to explore local patterns and the sensitivity at individual data points, which reveals the relationships between a focal point and its neighbors. During exploration, users are able to interactively change the derivative coefficients to perform sensitivity analysis based on different requirements as well as their domain knowledge. Each local pattern is assigned an outlier factor, so that users can quickly identify anomalous local patterns that do not conform with the global pattern. Users can also compare the local pattern with the global pattern both visually and statistically. Finally, the local pattern is integrated into the original attribute space using color mapping and jittering, which reveals the distribution of the partial derivatives. Case studies with real datasets are used to investigate the effectiveness of the visualizations and interactions.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102450",
            "id": "r_384",
            "s_ids": [
                "s_1384",
                "s_970",
                "s_797",
                "s_1479"
            ],
            "type": "rich",
            "x": 4.514427661895752,
            "y": 6.832135200500488
        },
        {
            "title": "Orion: A system for modeling, transformation and visualization of multidimensional heterogeneous networks",
            "data": "The study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people, institutions and artifacts. Despite numerous advances in algorithms and visualization techniques for understanding such social networks, the process of constructing network models and performing exploratory analysis remains difficult and time-consuming. In this paper we present Orion, a system for interactive modeling, transformation and visualization of network data. Orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types. Orion maps these user interactions to statements in a declarative workflow language that incorporates both relational operators (e.g., selection, aggregation and joins) and network analytics (e.g., centrality measures). We demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities, academic collaboration and distributed software development.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102441",
            "id": "r_385",
            "s_ids": [
                "s_1320",
                "s_1177"
            ],
            "type": "rich",
            "x": 5.075143814086914,
            "y": 5.1812639236450195
        },
        {
            "title": "Analysis Guided Visual Exploration of Multivariate Data",
            "data": "Visualization systems traditionally focus on graphical representation of information. They tend not to provide integrated analytical services that could aid users in tackling complex knowledge discovery tasks. Users' exploration in such environments is usually impeded due to several problems: 1) valuable information is hard to discover when too much data is visualized on the screen; 2) Users have to manage and organize their discoveries off line, because no systematic discovery management mechanism exists; 3) their discoveries based on visual exploration alone may lack accuracy; 4) and they have no convenient access to the important knowledge learned by other users. To tackle these problems, it has been recognized that analytical tools must be introduced into visualization systems. In this paper, we present a novel analysis-guided exploration system, called the nugget management system (NMS). It leverages the collaborative effort of human comprehensibility and machine computations to facilitate users' visual exploration processes. Specifically, NMS first extracts the valuable information (nuggets) hidden in datasets based on the interests of users. Given that similar nuggets may be re-discovered by different users, NMS consolidates the nugget candidate set by clustering based on their semantic similarity. To solve the problem of inaccurate discoveries, localized data mining techniques are applied to refine the nuggets to best represent the captured patterns in datasets. Lastly, the resulting well-organized nugget pool is used to guide users' exploration. To evaluate the effectiveness of NMS, we integrated NMS into Xmd- vTool, a freeware multivariate visualization system. User studies were performed to compare the users' efficiency and accuracy in finishing tasks on real datasets, with and without the help of NMS. Our user studies confirmed the effectiveness of NMS.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389000",
            "id": "r_386",
            "s_ids": [
                "s_1179",
                "s_797",
                "s_970"
            ],
            "type": "rich",
            "x": 5.75692892074585,
            "y": 8.83036994934082
        },
        {
            "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation",
            "data": "Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934661",
            "id": "r_387",
            "s_ids": [
                "s_873",
                "s_113",
                "s_631",
                "s_541",
                "s_459"
            ],
            "type": "rich",
            "x": 7.561800003051758,
            "y": 6.763139724731445
        },
        {
            "title": "NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation",
            "data": "Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters, which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We performed two case studies, and discovered multiple new parameter configurations, which can trigger high cell polarization results in the original simulation model. We evaluated our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934591",
            "id": "r_388",
            "s_ids": [
                "s_129",
                "s_678",
                "s_1132",
                "s_363",
                "s_1351"
            ],
            "type": "rich",
            "x": 3.0904009342193604,
            "y": 7.828068256378174
        },
        {
            "title": "VIBR: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle",
            "data": "Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864826",
            "id": "r_389",
            "s_ids": [
                "s_1219",
                "s_1144",
                "s_1002",
                "s_1393"
            ],
            "type": "rich",
            "x": 4.913002967834473,
            "y": 5.747988224029541
        },
        {
            "title": "Interactive Visual Alignment of Medieval Text Versions",
            "data": "Textual criticism consists of the identification and analysis of variant readings among different versions of a text. Being a relatively simple task for modern languages, the collation of medieval text traditions ranges from the complex to the virtually impossible depending on the degree of instability of textual transmission. We present a visual analytics environment that supports computationally aligning such complex textual differences typical of orally inflected medieval poetry. For the purpose of analyzing alignment, we provide interactive visualizations for different text hierarchy levels, specifically, a meso reading view to support investigating repetition and variance at the line level across text segments. In addition to outlining important aspects of our interdisciplinary collaboration, we emphasize the utility of the proposed system by various usage scenarios in medieval French literature.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585505",
            "id": "r_390",
            "s_ids": [
                "s_317",
                "s_1061"
            ],
            "type": "rich",
            "x": 7.528163909912109,
            "y": 10.42373275756836
        },
        {
            "title": "DemographicVis: Analyzing demographic information based on user generated content",
            "data": "The wide-spread of social media provides unprecedented sources of written language that can be used to model and infer online demographics. In this paper, we introduce a novel visual text analytics system, DemographicVis, to aid interactive analysis of such demographic information based on user-generated content. Our approach connects categorical data (demographic information) with textual data, allowing users to understand the characteristics of different demographic groups in a transparent and exploratory manner. The modeling and visualization are based on ground truth demographic information collected via a survey conducted on Reddit.com. Detailed user information is taken into our modeling process that connects the demographic groups with features that best describe the distinguishing characteristics of each group. Features including topical and linguistic are generated from the user-generated contents. Such features are then analyzed and ranked based on their ability to predict the users' demographic information. To enable interactive demographic analysis, we introduce a web-based visual interface that presents the relationship of the demographic groups, their topic interests, as well as the predictive power of various features. We present multiple case studies to showcase the utility of our visual analytics approach in exploring and understanding the interests of different demographic groups. We also report results from a comparative evaluation, showing that the DemographicVis is quantitatively superior or competitive and subjectively preferred when compared to a commercial text analysis tool.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347631",
            "id": "r_391",
            "s_ids": [
                "s_305",
                "s_1077",
                "s_304",
                "s_331",
                "s_1368",
                "s_233"
            ],
            "type": "rich",
            "x": 7.529202938079834,
            "y": 9.4118013381958
        },
        {
            "title": "Integrating predictive analytics into a spatiotemporal epidemic simulation",
            "data": "The Epidemic Simulation System (EpiSimS) is a scalable, complex modeling tool for analyzing disease within the United States. Due to its high input dimensionality, time requirements, and resource constraints, simulating over the entire parameter space is unfeasible. One solution is to take a granular sampling of the input space and use simpler predictive models (emulators) in between. The quality of the implemented emulator depends on many factors: robustness, sophistication, configuration, and suitability to the input data. Visual analytics can be leveraged to provide guidance and understanding of these things to the user. In this paper, we have implemented a novel interface and workflow for emulator building and use. We introduce a workflow to build emulators, make predictions, and then analyze the results. Our prediction process first predicts temporal time series, and uses these to derive predicted spatial densities. Integrated into the EpiSimS framework, we target users who are non-experts at statistical modeling. This approach allows for a high level of analysis into the state of the built emulators and their resultant predictions. We present our workflow, models, the associated system, and evaluate the overall utility with feedback from EpiSimS scientists.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347626",
            "id": "r_392",
            "s_ids": [
                "s_352",
                "s_528",
                "s_213",
                "s_699"
            ],
            "type": "rich",
            "x": 3.117333173751831,
            "y": 7.8499674797058105
        },
        {
            "title": "YMCA - Your Mesh Comparison Application",
            "data": "Polygonal meshes can be created in several different ways. In this paper we focus on the reconstruction of meshes from point clouds, which are sets of points in 3D. Several algorithms that tackle this task already exist, but they have different benefits and drawbacks, which leads to a large number of possible reconstruction results (i.e., meshes). The evaluation of those techniques requires extensive comparisons between different meshes which is up to now done by either placing images of rendered meshes side-by-side, or by encoding differences by heat maps. A major drawback of both approaches is that they do not scale well with the number of meshes. This paper introduces a new comparative visual analysis technique for 3D meshes which enables the simultaneous comparison of several meshes and allows for the interactive exploration of their differences. Our approach gives an overview of the differences of the input meshes in a 2D view. By selecting certain areas of interest, the user can switch to a 3D representation and explore the spatial differences in detail. To inspect local variations, we provide a magic lens tool in 3D. The location and size of the lens provide further information on the variations of the reconstructions in the selected area. With our comparative visualization approach, differences between several mesh reconstruction algorithms can be easily localized and inspected.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042491",
            "id": "r_393",
            "s_ids": [
                "s_607",
                "s_687",
                "s_1493",
                "s_978",
                "s_531",
                "s_927"
            ],
            "type": "rich",
            "x": 4.083239555358887,
            "y": 6.6136088371276855
        },
        {
            "title": "Supporting effective common ground construction in Asynchronous Collaborative Visual Analytics",
            "data": "Asynchronous Collaborative Visual Analytics (ACVA) leverages group sensemaking by releasing the constraints on when, where, and who works collaboratively. A significant task to be addressed before ACVA can reach its full potential is effective common ground construction, namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them. This is challenging due to the lack of instant communication and scale of collaboration in ACVA. We propose a novel visual analytics approach that automatically gathers, organizes, and summarizes insights to form common ground with reduced human effort. The rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review, explore, and compare insights in detail. A working prototype of the approach has been implemented. We have conducted a case study and a user study to demonstrate its effectiveness.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102447",
            "id": "r_394",
            "s_ids": [
                "s_1065",
                "s_1023",
                "s_1137",
                "s_763",
                "s_975"
            ],
            "type": "rich",
            "x": 5.630010604858398,
            "y": 10.661463737487793
        },
        {
            "title": "Multidimensional data dissection using attribute relationship graphs",
            "data": "Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652520",
            "id": "r_395",
            "s_ids": [
                "s_309"
            ],
            "type": "rich",
            "x": 5.023892879486084,
            "y": 6.279308795928955
        },
        {
            "title": "Multivariate visual explanation for high dimensional datasets",
            "data": "Understanding multivariate relationships is an important task in multivariate data analysis. Unfortunately, existing multivariate visualization systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions. We present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization techniques. The result is an efficient workflow for multivariate analysis model construction, interactive dimension reduction, and multivariate knowledge discovery leveraging both automatic multivariate analysis and interactive multivariate data visual exploration. Case studies and a formal user study with a real dataset illustrate the effectiveness of this approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677368",
            "id": "r_396",
            "s_ids": [
                "s_1137",
                "s_816",
                "s_626",
                "s_763",
                "s_834"
            ],
            "type": "rich",
            "x": 4.889342308044434,
            "y": 6.623239517211914
        },
        {
            "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design",
            "data": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030337",
            "id": "r_397",
            "s_ids": [
                "s_1004",
                "s_1057",
                "s_1153",
                "s_131",
                "s_1470"
            ],
            "type": "rich",
            "x": 5.183478832244873,
            "y": 10.278133392333984
        },
        {
            "title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness",
            "data": "Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934614",
            "id": "r_398",
            "s_ids": [
                "s_442",
                "s_743",
                "s_1394",
                "s_942",
                "s_952"
            ],
            "type": "rich",
            "x": 4.136512279510498,
            "y": 8.611287117004395
        },
        {
            "title": "Visual Analysis of Higher-Order Conjunctive Relationships in Multidimensional Data Using a Hypergraph Query System",
            "data": "Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.220",
            "id": "r_399",
            "s_ids": [
                "s_1121",
                "s_309"
            ],
            "type": "rich",
            "x": 5.138487339019775,
            "y": 6.1423020362854
        },
        {
            "title": "FDive: Learning Relevance Models Using Pattern-based Similarity Measures",
            "data": "The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models, assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986940",
            "id": "r_400",
            "s_ids": [
                "s_490",
                "s_119",
                "s_801",
                "s_511",
                "s_1409",
                "s_547"
            ],
            "type": "rich",
            "x": 4.621323108673096,
            "y": 8.148672103881836
        },
        {
            "title": "Visually and statistically guided imputation of missing values in univariate seasonal time series",
            "data": "Missing values are a problem in many real world applications, for example failing sensor measurements. For further analysis these missing values need to be imputed. Thus, imputation of such missing values is important in a wide range of applications. We propose a visually and statistically guided imputation approach, that allows applying different imputation techniques to estimate the missing values as well as evaluating and fine tuning the imputation by visual guidance. In our approach we include additional visual information about uncertainty and employ the cyclic structure of time inherent in the data. Including this cyclic structure enables visually judging the adequateness of the estimated values with respect to the uncertainty/error boundaries and according to the patterns of the neighbouring time points in linear and cyclic (e.g., the months of the year) time.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347672",
            "id": "r_401",
            "s_ids": [
                "s_78",
                "s_1442",
                "s_172",
                "s_1124",
                "s_489",
                "s_345",
                "s_427"
            ],
            "type": "rich",
            "x": 4.46194314956665,
            "y": 7.570594787597656
        },
        {
            "title": "Watch this: A taxonomy for dynamic data visualization",
            "data": "Visualizations embody design choices about data access, data transformation, visual representation, and interaction. To interpret a static visualization, a person must identify the correspondences between the visual representation and the underlying data. These correspondences become moving targets when a visualization is dynamic. Dynamics may be introduced in a visualization at any point in the analysis and visualization process. For example, the data itself may be streaming, shifting subsets may be selected, visual representations may be animated, and interaction may modify presentation. In this paper, we focus on the impact of dynamic data. We present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations. Visualization techniques are organized into categories at various levels of abstraction. The salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices. Examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability (and thus utility) of visualizations. The taxonomy presented provides a reference point for further exploration of dynamic data visualization techniques.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400552",
            "id": "r_402",
            "s_ids": [
                "s_1225",
                "s_1457",
                "s_309"
            ],
            "type": "rich",
            "x": 5.028372764587402,
            "y": 9.135009765625
        },
        {
            "title": "NetClinic: Interactive visualization to enhance automated fault diagnosis in enterprise networks",
            "data": "Diagnosing faults in an operational computer network is a frustrating, time-consuming exercise. Despite advances, automatic diagnostic tools are far from perfect: they occasionally miss the true culprit and are mostly only good at narrowing down the search to a few potential culprits. This uncertainty and the inability to extract useful sense from tool output renders most tools not usable to administrators. To bridge this gap, we present NetClinic, a visual analytics system that couples interactive visualization with an automated diagnostic tool for enterprise networks. It enables administrators to verify the output of the automatic analysis at different levels of detail and to move seamlessly across levels while retaining appropriate context. A qualitative user study shows that NetClinic users can accurately identify the culprit, even when it is not present in the suggestions made by the automated component. We also find that supporting a variety of sensemaking strategies is a key to the success of systems that enhance automated diagnosis.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652910",
            "id": "r_403",
            "s_ids": [
                "s_1115",
                "s_253",
                "s_108",
                "s_555"
            ],
            "type": "rich",
            "x": 6.996976375579834,
            "y": 7.708852291107178
        },
        {
            "title": "Understanding syndromic hotspots - a visual analytics approach",
            "data": "When analyzing syndromic surveillance data, health care officials look for areas with unusually high cases of syndromes. Unfortunately, many outbreaks are difficult to detect because their signal is obscured by the statistical noise. Consequently, many detection algorithms have a high false positive rate. While many false alerts can be easily filtered by trained epidemiologists, others require health officials to drill down into the data, analyzing specific segments of the population and historical trends over time and space. Furthermore, the ability to accurately recognize meaningful patterns in the data becomes more challenging as these data sources increase in volume and complexity. To facilitate more accurate and efficient event detection, we have created a visual analytics tool that provides analysts with linked geo-spatiotemporal and statistical analytic views. We model syndromic hotspots by applying a kernel density estimation on the population sample. When an analyst selects a syndromic hotspot, temporal statistical graphs of the hotspot are created. Similarly, regions in the statistical plots may be selected to generate geospatial features specific to the current time period. Demographic filtering can then be combined to determine if certain populations are more affected than others. These tools allow analysts to perform real-time hypothesis testing and evaluation.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677354",
            "id": "r_404",
            "s_ids": [
                "s_721",
                "s_206",
                "s_1452",
                "s_1268",
                "s_936",
                "s_1435",
                "s_54",
                "s_1141",
                "s_1224",
                "s_952"
            ],
            "type": "rich",
            "x": 6.5760884284973145,
            "y": 6.957261085510254
        },
        {
            "title": "Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval",
            "data": "In this paper, we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge visualization. To relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports, a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently. Our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization techniques on semantic video retrieval systems. Our techniques on intelligent news video analysis and knowledge discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections. In addition, news video visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge discovery.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389003",
            "id": "r_405",
            "s_ids": [
                "s_1503",
                "s_629",
                "s_763",
                "s_233",
                "s_608"
            ],
            "type": "rich",
            "x": 5.910371780395508,
            "y": 8.259980201721191
        },
        {
            "title": "FemaRepViz: Automatic Extraction and Geo-Temporal Visualization of FEMA National Situation Updates",
            "data": "An architecture for visualizing information extracted from text documents is proposed. In conformance with this architecture, a toolkit, FemaRepViz, has been implemented to extract and visualize temporal, geospatial, and summarized information from FEMA national update reports. Preliminary tests have shown satisfactory accuracy for FEMARepViz. A central component of the architecture is an entity extractor that extracts named entities like person names, location names, temporal references, etc. FEMARepViz is based on FactXtractor, an entity-extractor that works on text documents. The information extracted using FactXtractor is processed using GeoTagger, a geographical name disambiguation tool based on a novel clustering-based disambiguation algorithm. To extract relationships among entities, we propose a machine-learning based algorithm that uses a novel stripped dependency tree kernel. We illustrate and evaluate the usefulness of our system on the FEMA National Situation Updates. Daily reports are fetched by FEMARepViz from the FEMA website, segmented into coherent sections and each section is classified into one of several known incident types. We use concept Vista, Google maps and Google earth to visualize the events extracted from the text reports and allow the user to interactively filter the topics, locations, and time-periods of interest to create a visual analytics toolkit that is useful for rapid analysis of events reported in a large set of text documents.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388991",
            "id": "r_406",
            "s_ids": [
                "s_1109",
                "s_998"
            ],
            "type": "rich",
            "x": 7.200150966644287,
            "y": 9.710108757019043
        },
        {
            "title": "Interactive Visualization and Analysis of Network and Sensor Data on Mobile Devices",
            "data": "Mobile devices are rapidly gaining popularity due to their small size and their wide range of functionality. With the constant improvement in wireless network access, they are an attractive option not only for day to day use. but also for in-field analytics by first responders in widespread areas. However, their limited processing, display, graphics and power resources pose a major challenge in developing effective applications. Nevertheless, they are vital for rapid decision making in emergencies when combined with appropriate analysis tools. In this paper, we present an efficient, interactive visual analytic system using a PDA to visualize network information from Purdue's Ross-Ade Stadium during football games as an example of in-held data analytics combined with text and video analysis. With our system, we can monitor the distribution of attendees with mobile devices throughout the stadium through their access of information and association/disassociation from wireless access points, enabling the detection of crowd movement and event activity. Through correlative visualization and analysis of synchronized video (instant replay video) and text information (play statistics) with the network activity, we can provide insightful information to network monitoring personnel, safety personnel and analysts. This work provides a demonstration and testbed for mobile sensor analytics that will help to improve network performance and provide safety personnel with information for better emergency planning and guidance",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261434",
            "id": "r_407",
            "s_ids": [
                "s_533",
                "s_993",
                "s_605",
                "s_952",
                "s_1094",
                "s_891",
                "s_806"
            ],
            "type": "rich",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality",
            "data": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations-causal graphs and Hasse diagrams-with and without an associated textual narrative. Finally, we describe Causeworks, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate Causeworks through interviews with experts who used the system for understanding complex events.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030358",
            "id": "r_408",
            "s_ids": [
                "s_557",
                "s_758",
                "s_104",
                "s_1220",
                "s_1382",
                "s_1461",
                "s_1191"
            ],
            "type": "rich",
            "x": 7.877171039581299,
            "y": 7.68709135055542
        },
        {
            "title": "Visual cohort comparison for spatial single-cell omics-data",
            "data": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030336",
            "id": "r_409",
            "s_ids": [
                "s_1287",
                "s_1348",
                "s_246",
                "s_471",
                "s_836",
                "s_1081",
                "s_1204"
            ],
            "type": "rich",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "title": "Analysis of Flight Variability: a Systematic Approach",
            "data": "In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864811",
            "id": "r_410",
            "s_ids": [
                "s_11",
                "s_1195",
                "s_329",
                "s_166"
            ],
            "type": "rich",
            "x": 7.486071586608887,
            "y": 5.270229339599609
        },
        {
            "title": "SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space",
            "data": "High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPad&lt;sup&gt;ND&lt;/sup&gt;, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPad&lt;sup&gt;ND&lt;/sup&gt; offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.190",
            "id": "r_411",
            "s_ids": [
                "s_765",
                "s_397",
                "s_254"
            ],
            "type": "rich",
            "x": 4.627654552459717,
            "y": 7.064582347869873
        },
        {
            "title": "CrystalBall: A Visual Analytic System for Future Event Discovery and Analysis from Social Media Data",
            "data": "Social media data bear valuable insights regarding events that occur around the world. Events are inherently temporal and spatial. Existing visual text analysis systems have focused on detecting and analyzing past and ongoing events. Few have leveraged social media information to look for events that may occur in the future. In this paper, we present an interactive visual analytic system, CrystalBall, that automatically identifies and ranks future events from Twitter streams. CrystalBall integrates new methods to discover events with interactive visualizations that permit sensemaking of the identified future events. Our computational methods integrate seven different measures to identify and characterize future events, leveraging information regarding time, location, social networks, and the informativeness of the messages. A visual interface is tightly coupled with the computational methods to present a concise summary of the possible future events. A novel connection graph and glyphs are designed to visualize the characteristics of the future events. To demonstrate the efficacy of CrystalBall in identifying future events and supporting interactive analysis, we present multiple case studies and validation studies on analyzing events derived from Twitter data.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585658",
            "id": "r_412",
            "s_ids": [
                "s_1077",
                "s_1087",
                "s_1326",
                "s_233",
                "s_305"
            ],
            "type": "rich",
            "x": 7.89020299911499,
            "y": 7.764756679534912
        },
        {
            "title": "A Visual Analytics System for Optimizing Communications in Massively Parallel Applications",
            "data": "Current and future supercomputers have tens of thousands of compute nodes interconnected with high-dimensional networks and complex network topologies for improved performance. Application developers are required to write scalable parallel programs in order to achieve high throughput on these machines. Application performance is largely determined by efficient inter-process communication. A common way to analyze and optimize performance is through profiling parallel codes to identify communication bottlenecks. However, understanding gigabytes of profiled at a is not a trivial task. In this paper, we present a visual analytics system for identifying the scalability bottlenecks and improving the communication efficiency of massively parallel applications. Visualization methods used in this system are designed to comprehend large-scale and varied communication patterns on thousands of nodes in complex networks such as the 5D torus and the dragonfly. We also present efficient rerouting and remapping algorithms that can be coupled with our interactive visual analytics design for performance optimization. We demonstrate the utility of our system with several case studies using three benchmark applications on two leading supercomputers. The mapping suggestion from our system led to 38% improvement in hop-bytes for Mini AMR application on 4,096 MPI processes.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585646",
            "id": "r_413",
            "s_ids": [
                "s_1259",
                "s_1106",
                "s_1015",
                "s_1276",
                "s_666",
                "s_699"
            ],
            "type": "rich",
            "x": 4.80540657043457,
            "y": 6.956305503845215
        },
        {
            "title": "Interactive visual steering of hierarchical simulation ensembles",
            "data": "Multi-level simulation models, i.e., models where different components are simulated using sub-models of varying levels of complexity, belong to the current state-of-the-art in simulation. The existing analysis practice for multi-level simulation results is to manually compare results from different levels of complexity, amounting to a very tedious and error-prone, trial-and-error exploration process. In this paper, we introduce hierarchical visual steering, a new approach to the exploration and design of complex systems. Hierarchical visual steering makes it possible to explore and analyze hierarchical simulation ensembles at different levels of complexity. At each level, we deal with a dynamic simulation ensemble - the ensemble grows during the exploration process. There is at least one such ensemble per simulation level, resulting in a collection of dynamic ensembles, analyzed simultaneously. The key challenge is to map the multi-dimensional parameter space of one ensemble to the multi-dimensional parameter space of another ensemble (from another level). In order to support the interactive visual analysis of such complex data we propose a novel approach to interactive and semi-automatic parameter space segmentation and comparison. The approach combines a novel interaction technique and automatic, computational methods - clustering, concave hull computation, and concave polygon overlapping - to support the analysts in the cross-ensemble parameter space mapping. In addition to the novel parameter space segmentation we also deploy coordinated multiple views with standard plots. We describe the abstract analysis tasks, identified during a case study, i.e., the design of a variable valve actuation system of a car engine. The study is conducted in cooperation with experts from the automotive industry. Very positive feedback indicates the usefulness and efficiency of the newly proposed approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347635",
            "id": "r_414",
            "s_ids": [
                "s_709",
                "s_840",
                "s_990",
                "s_1312",
                "s_378"
            ],
            "type": "rich",
            "x": 3.281541585922241,
            "y": 7.755031108856201
        },
        {
            "title": "Four considerations for supporting visual analysis in display ecologies",
            "data": "The current proliferation of large displays and mobile devices presents a number of exciting opportunities for visual analytics and information visualization. The display ecology enables multiple displays to function in concert within a broader technological environment to accomplish visual analysis tasks. Based on a comprehensive survey of multi-display systems from a variety of fields, we propose four key considerations for visual analysis in display ecologies: 1) Display Composition, 2) Information Coordination/Transfer, 3) Information Connection, and 4) Display Membership. Different aspects of display ecologies stemming from these design considerations will enable users to transform and empower multiple displays as a display ecology for visual analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347628",
            "id": "r_415",
            "s_ids": [
                "s_1423",
                "s_202",
                "s_852",
                "s_1170"
            ],
            "type": "rich",
            "x": 5.052469253540039,
            "y": 9.884970664978027
        },
        {
            "title": "BoundarySeer: Visual Analysis of 2D Boundary Changes",
            "data": "Boundary changes exist ubiquitously in our daily life. From the Antarctic ozone hole to the land desertification, and from the territory of a country to the area within one-hour reach from a downtown location, boundaries change over time. With a large number of time-varying boundaries recorded, people often need to analyze the changes, detect their similarities or differences, and find out spatial and temporal patterns of the evolution for various applications. In this paper, we present a comprehensive visual analytics system, BoundarySeer, to help users gain insight into the changes of boundaries. Our system consists of four major viewers: 1) a global viewer to show boundary groups based on their similarity and the distribution of boundary attributes such as smoothness and perimeter; 2) a region viewer to display the regions encircled by the boundaries and how they are affected by boundary changes; 3) a trend viewer to reveal the temporal patterns in the boundary evolution and potential spatio-temporal correlations; 4) a directional change viewer to encode movements of boundary segments in different directions. Quantitative analyses of boundaries (e.g., similarity measurement and adaptive clustering) and intuitive visualizations (e.g., density map and ThemeRiver) are integrated into these viewers, which enable users to explore boundary changes from different aspects and at different scales. Case studies with two real-world datasets have been carried out to demonstrate the effectiveness of our system.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042490",
            "id": "r_416",
            "s_ids": [
                "s_1003",
                "s_1010",
                "s_131",
                "s_1319",
                "s_531",
                "s_270"
            ],
            "type": "rich",
            "x": 6.644397735595703,
            "y": 6.411413192749023
        },
        {
            "title": "SpRay: A visual analytics approach for gene expression data",
            "data": "We present a new application, SpRay, designed for the visual exploration of gene expression data. It is based on an extension and adaption of parallel coordinates to support the visual exploration of large and high-dimensional datasets. In particular, we investigate the visual analysis of gene expression data as generated by micro-array experiments; We combine refined visual exploration with statistical methods to a visual analytics approach that proved to be particularly successful in this application domain. We will demonstrate the usefulness on several multidimensional gene expression datasets from different bioinformatics applications.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333911",
            "id": "r_417",
            "s_ids": [
                "s_415",
                "s_1102",
                "s_1327",
                "s_349"
            ],
            "type": "rich",
            "x": 4.756052494049072,
            "y": 6.701610088348389
        },
        {
            "title": "Have Green - A Visual Analytics Framework for Large Semantic Graphs",
            "data": "A semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology. In intelligence analysis, investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends, patterns, and insights. However, as new information continues to arrive from a multitude of sources, the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses. We introduce a powerful visual analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs. The paper describes the overall framework design, presents major development accomplishments to date, and discusses future directions of a new visual analytics system known as Have Green",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261432",
            "id": "r_418",
            "s_ids": [
                "s_421",
                "s_197",
                "s_980",
                "s_1025",
                "s_1403"
            ],
            "type": "rich",
            "x": 4.890290260314941,
            "y": 5.3867316246032715
        },
        {
            "title": "Avian Flu Case Study with nSpace and GeoTime",
            "data": "GeoTime and nSpace are new analysis tools that provide innovative visual analytic capabilities. This paper uses an epidemiology analysis scenario to illustrate and discuss these new investigative methods and techniques. In addition, this case study is an exploration and demonstration of the analytical synergy achieved by combining GeoTime's geo-temporal analysis capabilities, with the rapid information triage, scanning and sense-making provided by nSpace. A fictional analyst works through the scenario from the initial brainstorming through to a final collaboration and report. With the efficient knowledge acquisition and insights into large amounts of documents, there is more time for the analyst to reason about the problem and imagine ways to mitigate threats. The use of both nSpace and GeoTime initiated a synergistic exchange of ideas, where hypotheses generated in either software tool could be cross-referenced, refuted, and supported by the other tool",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261427",
            "id": "r_419",
            "s_ids": [
                "s_668",
                "s_429",
                "s_956",
                "s_1330",
                "s_284",
                "s_796"
            ],
            "type": "rich",
            "x": 6.7051167488098145,
            "y": 8.124053955078125
        },
        {
            "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams",
            "data": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030411",
            "id": "r_420",
            "s_ids": [
                "s_327",
                "s_11",
                "s_1195",
                "s_408",
                "s_1274"
            ],
            "type": "rich",
            "x": 4.7486796379089355,
            "y": 5.923239707946777
        },
        {
            "title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories",
            "data": "Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934275",
            "id": "r_421",
            "s_ids": [
                "s_1143",
                "s_1529",
                "s_436",
                "s_1153",
                "s_1012",
                "s_739",
                "s_664",
                "s_131"
            ],
            "type": "rich",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "title": "Enhancing Web-based Analytics Applications through Provenance",
            "data": "Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865039",
            "id": "r_422",
            "s_ids": [
                "s_1350",
                "s_560",
                "s_102",
                "s_704"
            ],
            "type": "rich",
            "x": 5.957756519317627,
            "y": 9.685969352722168
        },
        {
            "title": "How Do Ancestral Traits Shape Family Trees Over Generations?",
            "data": "Whether and how does the structure of family trees differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, previous work mostly focuses on parent-child scenarios due to the lack of proper tools to handle the complexity of extending the research to multi-generational processes. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744080",
            "id": "r_423",
            "s_ids": [
                "s_221",
                "s_947",
                "s_920",
                "s_1354",
                "s_131"
            ],
            "type": "rich",
            "x": 3.2904086112976074,
            "y": 7.0784125328063965
        },
        {
            "title": "The Spinel Explorer - Interactive Visual Analysis of Spinel Group Minerals",
            "data": "Geologists usually deal with rocks that are up to several thousand million years old. They try to reconstruct the tectonic settings where these rocks were formed and the history of events that affected them through the geological time. The spinel group minerals provide useful information regarding the geological environment in which the host rocks were formed. They constitute excellent indicators of geological environments (tectonic settings) and are of invaluable help in the search for mineral deposits of economic interest. The current workflow requires the scientists to work with different applications to analyze spine data. They do use specific diagrams, but these are usually not interactive. The current workflow hinders domain experts to fully exploit the potentials of tediously and expensively collected data. In this paper, we introduce the Spinel Explorer-an interactive visual analysis application for spinel group minerals. The design of the Spinel Explorer and of the newly introduced interactions is a result of a careful study of geologists' tasks. The Spinel Explorer includes most of the diagrams commonly used for analyzing spinel group minerals, including 2D binary plots, ternary plots, and 3D Spinel prism plots. Besides specific plots, conventional information visualization views are also integrated in the Spinel Explorer. All views are interactive and linked. The Spinel Explorer supports conventional statistics commonly used in spinel minerals exploration. The statistics views and different data derivation techniques are fully integrated in the system. Besides the Spinel Explorer as newly proposed interactive exploration system, we also describe the identified analysis tasks, and propose a new workflow. We evaluate the Spinel Explorer using real-life data from two locations in Argentina: the Frontal Cordillera in Central Andes and Patagonia. We describe the new findings of the geologists which would have been much more difficult to achieve using the current workflow only. Very positive feedback from geologists confirms the usefulness of the Spinel Explorer.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346754",
            "id": "r_424",
            "s_ids": [
                "s_622",
                "s_732",
                "s_282",
                "s_175",
                "s_994",
                "s_531",
                "s_840"
            ],
            "type": "rich",
            "x": 5.589276313781738,
            "y": 6.46633243560791
        },
        {
            "title": "iConViz: Interactive Visual Exploration of the Default Contagion Risk of Networked-Guarantee Loans",
            "data": "Groups of enterprises can serve as guarantees for one another and form complex networks when obtaining loans from commercial banks. During economic slowdowns, corporate default may spread like a virus and lead to large-scale defaults or even systemic financial crises. To help financial regulatory authorities and banks manage the risk associated with networked loans, we identified the default contagion risk, a pivotal issue in developing preventive measures, and established iConViz, an interactive visual analysis tool that facilitates the closed-loop analysis process. A novel financial metric, the contagion effect, was formulated to quantify the infectious consequences of guarantee chains in this type of network. Based on this metric, we designed and implemented a series of novel and coordinated views that address the analysis of financial problems. Experts evaluated the system using real-world financial data. The proposed approach grants practitioners the ability to avoid previous ad hoc analysis methodologies and extend coverage of the conventional Capital Accord to the banking industry.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00013",
            "id": "r_425",
            "s_ids": [
                "s_1026",
                "s_266",
                "s_1291",
                "s_755",
                "s_918"
            ],
            "type": "rich",
            "x": 8.89720630645752,
            "y": 7.1770339012146
        },
        {
            "title": "VIANA: Visual Interactive Annotation of Argumentation",
            "data": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986917",
            "id": "r_426",
            "s_ids": [
                "s_1209",
                "s_847",
                "s_1431",
                "s_799"
            ],
            "type": "rich",
            "x": 7.249513626098633,
            "y": 10.305989265441895
        },
        {
            "title": "PorosityAnalyzer: Visual Analysis and Evaluation of Segmentation Pipelines to Determine the Porosity in Fiber-Reinforced Polymers",
            "data": "In this paper we present PorosityAnalyzer, a novel tool for detailed evaluation and visual analysis of pore segmentation pipelines to determine the porosity in fiber-reinforced polymers (FRPs). The presented tool consists of two modules: the computation module and the analysis module. The computation module enables a convenient setup and execution of distributed off-line-computations on industrial 3D X-ray computed tomography datasets. It allows the user to assemble individual segmentation pipelines in the form of single pipeline steps, and to specify the parameter ranges as well as the sampling of the parameter-space of each pipeline segment. The result of a single segmentation run consists of the input parameters, the calculated 3D binary-segmentation mask, the resulting porosity value, and other derived results (e.g., segmentation pipeline run-time). The analysis module presents the data at different levels of detail by drill-down filtering in order to determine accurate and robust segmentation pipelines. Overview visualizations allow to initially compare and evaluate the segmentation pipelines. With a scatter plot matrix (SPLOM), the segmentation pipelines are examined in more detail based on their input and output parameters. Individual segmentation-pipeline runs are selected in the SPLOM and visually examined and compared in 2D slice views and 3D renderings by using aggregated segmentation masks and statistical contour renderings. PorosityAnalyzer has been thoroughly evaluated with the help of twelve domain experts. Two case studies demonstrate the applicability of our proposed concepts and visualization techniques, and show that our tool helps domain experts to gain new insights and improve their workflow efficiency.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883516",
            "id": "r_427",
            "s_ids": [
                "s_599",
                "s_398",
                "s_531",
                "s_857",
                "s_20"
            ],
            "type": "rich",
            "x": 5.075896739959717,
            "y": 6.747625827789307
        },
        {
            "title": "SocialBrands: Visual analysis of public perceptions of brands on social media",
            "data": "Public perceptions of a brand is critical to its performance. While social media has demonstrated a huge potential to shape public perceptions of brands, existing tools are not intuitive and explanatory for domain users to use as they fail to provide a comprehensive analysis framework for perceptions of brands. In this paper, we present SocialBrands, a novel visual analysis tool for brand managers to understand public perceptions of brands on social media. Social-Brands leverages brand personality framework in marketing literature and social computing approaches to compute the personality of brands from three driving factors (user imagery, employee imagery, and official announcement) on social media, and construct an evidence network explaining the association between brand personality and driving factors. These computational results are then integrated with new interactive visualizations to help brand managers understand personality traits and their driving factors. We demonstrate the usefulness and effectiveness of SocialBrands through a series of user studies with brand managers in an enterprise context. Design lessons are also derived from our studies.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883513",
            "id": "r_428",
            "s_ids": [
                "s_487",
                "s_1481",
                "s_116",
                "s_135",
                "s_864",
                "s_363"
            ],
            "type": "rich",
            "x": 7.857691764831543,
            "y": 8.961512565612793
        },
        {
            "title": "TimeStitch: Interactive multi-focus cohort discovery and comparison",
            "data": "Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347682",
            "id": "r_429",
            "s_ids": [
                "s_300",
                "s_782",
                "s_1465",
                "s_1500",
                "s_311"
            ],
            "type": "rich",
            "x": 7.04141902923584,
            "y": 6.825443267822266
        },
        {
            "title": "Interactive decision making using dissimilarity to visually represented prototypes",
            "data": "To make informed decisions, an expert has to reason with multi-dimensional, heterogeneous data and analysis results of these. Items in such datasets are typically represented by features. However, as argued in cognitive science, features do not yield an optimal space for human reasoning. In fact, humans tend to organize complex information in terms of prototypes or known cases rather than in absolute terms. When confronted with unknown data items, humans assess them in terms of similarity to these prototypical elements. Interestingly, an analogues similarity-to-prototype approach, where prototypes are taken from the data, has been successfully applied in machine learning. Combining such a machine learning approach with human prototypical reasoning in a Visual Analytics context requires to integrate similarity-based classification with interactive visualizations. To that end, the data prototypes should be visually represented to trigger direct associations to cases familiar to the domain experts. In this paper, we propose a set of highly interactive visualizations to explore data and classification results in terms of dissimilarities to visually represented prototypes. We argue that this approach not only supports human reasoning processes, but is also suitable to enhance understanding of heterogeneous data. The proposed framework is applied to a risk assessment case study in Forensic Psychiatry.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102451",
            "id": "r_430",
            "s_ids": [
                "s_1035",
                "s_506",
                "s_1206"
            ],
            "type": "rich",
            "x": 4.324527740478516,
            "y": 8.049986839294434
        },
        {
            "title": "G-PARE: A visual analytic tool for comparative analysis of uncertain graphs",
            "data": "There are a growing number of machine learning algorithms which operate on graphs. Example applications for these algorithms include predicting which customers will recommend products to their friends in a viral marketing campaign using a customer network, predicting the topics of publications in a citation network, or predicting the political affiliations of people in a social network. It is important for an analyst to have tools to help compare the output of these machine learning algorithms. In this work, we present G-PARE, a visual analytic tool for comparing two uncertain graphs, where each uncertain graph is produced by a machine learning algorithm which outputs probabilities over node labels. G-PARE provides several different views which allow users to obtain a global overview of the algorithms output, as well as focused views that show subsets of nodes of interest. By providing an adaptive exploration environment, G-PARE guides the users to places in the graph where two algorithms predictions agree and places where they disagree. This enables the user to follow cascades of misclassifications by comparing the algorithms outcome with the ground truth. After describing the features of G-PARE, we illustrate its utility through several use cases based on networks from different domains.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102442",
            "id": "r_431",
            "s_ids": [
                "s_1196",
                "s_225",
                "s_771",
                "s_145",
                "s_151"
            ],
            "type": "rich",
            "x": 3.6647329330444336,
            "y": 7.409862041473389
        },
        {
            "title": "A visual analytics approach to model learning",
            "data": "The process of learning models from raw data typically requires a substantial amount of user input during the model initialization phase. We present an assistive visualization system which greatly reduces the load on the users and makes the process of model initialization and refinement more efficient, problem-driven, and engaging. Utilizing a sequence segmentation task with a Hidden Markov Model as an example, we assign each token in the sequence a feature vector based on its various properties within the sequence. These vectors are then clustered according to similarity, generating a layout of the individual tokens in form of a node link diagram where the length of the links is determined by the feature vector similarity. Users may then tune the weights of the feature vector components to improve the segmentation, which is visualized as a better separation of the clusters. Also, as individual clusters represent different classes, the user can now work at the cluster level to define token classes, instead of labelling one entry at time. Inconsistent entries visually identify themselves by locating at the periphery of clusters, and the user then helps refine the model by resolving these inconsistencies. Our system therefore makes efficient use of the knowledge of its users, only requesting user assistance for non-trivial data items. It so allows users to visually analyse data at a higher, more abstract level, improving scalability.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652484",
            "id": "r_432",
            "s_ids": [
                "s_938",
                "s_805",
                "s_254"
            ],
            "type": "rich",
            "x": 3.7341740131378174,
            "y": 5.902337551116943
        },
        {
            "title": "Visual evaluation of text features for document summarization and analysis",
            "data": "Thanks to the Web-related and other advanced technologies, textual information is increasingly being stored in digital form and posted online. Automatic methods to analyze such textual information are becoming inevitable. Many of those methods are based on quantitative text features. Analysts face the challenge to choose the most appropriate features for their tasks. This requires effective approaches for evaluation and feature-engineering.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677359",
            "id": "r_433",
            "s_ids": [
                "s_37",
                "s_637",
                "s_1038",
                "s_548",
                "s_951"
            ],
            "type": "rich",
            "x": 7.499957084655762,
            "y": 10.237372398376465
        },
        {
            "title": "Visual Analysis of Controversy in User-generated Encyclopedias",
            "data": "Wikipedia is a large and rapidly growing Web-based collaborative authoring environment, where anyone on the Internet can create, modify, and delete pages about encyclopedic topics. A remarkable property of some Wikipedia pages is that they are written by up to thousands of authors who may have contradicting opinions. In this paper we show that a visual analysis of the \"who revises whom\"- network gives deep insight into controversies. We propose a set of analysis and visualization techniques that reveal the dominant authors of a page, the roles they play, and the alters they confront. Thereby we provide tools to understand how Wikipedia authors collaborate in the presence of controversy.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389012",
            "id": "r_434",
            "s_ids": [
                "s_264",
                "s_114"
            ],
            "type": "rich",
            "x": 7.301630973815918,
            "y": 9.870648384094238
        },
        {
            "title": "Balancing Interactive Data Management of Massive Data with Situational Awareness through Smart Aggregation",
            "data": "Designing a visualization system capable of processing, managing, and presenting massive data sets while maximizing the user's situational awareness (SA) is a challenging, but important, research question in visual analytics. Traditional data management and interactive retrieval approaches have often focused on solving the data overload problem at the expense of the user's SA. This paper discusses various data management strategies and the strengths and limitations of each approach in providing the user with SA. A new data management strategy, coined Smart Aggregation, is presented as a powerful approach to overcome the challenges of both massive data sets and maintaining SA. By combining automatic data aggregation with user-defined controls on what, how, and when data should be aggregated, we present a visualization system that can handle massive amounts of data while affording the user with the best possible SA. This approach ensures that a system is always usable in terms of both system resources and human perceptual resources. We have implemented our Smart Aggregation approach in a visual analytics system called VIAssist (Visual Assistant for Information Assurance Analysis) to facilitate exploration, discovery, and SA in the domain of Information Assurance.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388998",
            "id": "r_435",
            "s_ids": [
                "s_136",
                "s_447"
            ],
            "type": "rich",
            "x": 5.447331428527832,
            "y": 8.479851722717285
        },
        {
            "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis",
            "data": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030376",
            "id": "r_436",
            "s_ids": [
                "s_226",
                "s_1198",
                "s_112"
            ],
            "type": "rich",
            "x": 5.147592544555664,
            "y": 9.267566680908203
        },
        {
            "title": "Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets",
            "data": "Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (AoI) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the AoIs may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial AoIs changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865042",
            "id": "r_437",
            "s_ids": [
                "s_850",
                "s_223",
                "s_1341",
                "s_1186",
                "s_1149"
            ],
            "type": "rich",
            "x": 6.459039211273193,
            "y": 7.370312690734863
        },
        {
            "title": "Genotet: An Interactive Web-based Visual Exploration Framework to Support Validation of Gene Regulatory Networks",
            "data": "Elucidation of transcriptional regulatory networks (TRNs) is a fundamental goal in biology, and one of the most important components of TRNs are transcription factors (TFs), proteins that specifically bind to gene promoter and enhancer regions to alter target gene expression patterns. Advances in genomic technologies as well as advances in computational biology have led to multiple large regulatory network models (directed networks) each with a large corpus of supporting data and gene-annotation. There are multiple possible biological motivations for exploring large regulatory network models, including: validating TF-target gene relationships, figuring out co-regulation patterns, and exploring the coordination of cell processes in response to changes in cell state or environment. Here we focus on queries aimed at validating regulatory network models, and on coordinating visualization of primary data and directed weighted gene regulatory networks. The large size of both the network models and the primary data can make such coordinated queries cumbersome with existing tools and, in particular, inhibits the sharing of results between collaborators. In this work, we develop and demonstrate a web-based framework for coordinating visualization and exploration of expression data (RNA-seq, microarray), network models and gene-binding data (ChIP-seq). Using specialized data structures and multiple coordinated views, we design an efficient querying model to support interactive analysis of the data. Finally, we show the effectiveness of our framework through case studies for the mouse immune system (a dataset focused on a subset of key cellular functions) and a model bacteria (a small genome with high data-completeness).",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346753",
            "id": "r_438",
            "s_ids": [
                "s_1001",
                "s_30",
                "s_1036",
                "s_669",
                "s_483",
                "s_497",
                "s_249",
                "s_814",
                "s_393"
            ],
            "type": "rich",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "title": "Using Interactive Visual Reasoning to Support Sense-Making: Implications for Design",
            "data": "This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.211",
            "id": "r_439",
            "s_ids": [
                "s_521",
                "s_75",
                "s_593",
                "s_1376",
                "s_361"
            ],
            "type": "rich",
            "x": 5.564159870147705,
            "y": 10.273418426513672
        },
        {
            "title": "SMAP: A Joint Dimensionality Reduction Scheme for Secure Multi-Party Visualization",
            "data": "Nowadays, as data becomes increasingly complex and distributed, data analyses often involve several related datasets that are stored on different servers and probably owned by different stakeholders. While there is an emerging need to provide these stakeholders with a full picture of their data under a global context, conventional visual analytical methods, such as dimensionality reduction, could expose data privacy when multi-party datasets are fused into a single site to build point-level relationships. In this paper, we reformulate the conventional t-SNE method from the single-site mode into a secure distributed infrastructure. We present a secure multi-party scheme for joint t-SNE computation, which can minimize the risk of data leakage. Aggregated visualization can be optionally employed to hide disclosure of point-level relationships. We build a prototype system based on our method, SMAP, to support the organization, computation, and exploration of secure joint embedding. We demonstrate the effectiveness of our approach with three case studies, one of which is based on the deployment of our system in real-world applications.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00015",
            "id": "r_440",
            "s_ids": [
                "s_286",
                "s_510",
                "s_259",
                "s_1319",
                "s_1065",
                "s_257",
                "s_1329",
                "s_511"
            ],
            "type": "rich",
            "x": 6.134147644042969,
            "y": 8.68932056427002
        },
        {
            "title": "Pattern Trails: Visual Analysis of Pattern Transitions in Subspaces",
            "data": "Subspace analysis methods have gained interest for identifying patterns in subspaces of high-dimensional data. Existing techniques allow to visualize and compare patterns in subspaces. However, many subspace analysis methods produce an abundant amount of patterns, which often remain redundant and are difficult to relate. Creating effective layouts for comparison of subspace patterns remains challenging. We introduce Pattern Trails, a novel approach for visually ordering and comparing subspace patterns. Central to our approach is the notion of pattern transitions as an interpretable structure imposed to order and compare patterns between subspaces. The basic idea is to visualize projections of subspaces side-by-side, and indicate changes between adjacent patterns in the subspaces by a linked representation, hence introducing pattern transitions. Our contributions comprise a systematization for how pairs of subspace patterns can be compared, and how changes can be interpreted in terms of pattern transitions. We also contribute a technique for visual subspace analysis based on a data-driven similarity measure between subspace representations. This measure is useful to order the patterns, and interactively group subspaces to reduce redundancy. We demonstrate the usefulness of our approach by application to several use cases, indicating that data can be meaningfully ordered and interpreted in terms of pattern transitions.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585613",
            "id": "r_441",
            "s_ids": [
                "s_39",
                "s_558",
                "s_547",
                "s_1038",
                "s_511"
            ],
            "type": "rich",
            "x": 4.404964447021484,
            "y": 6.315691947937012
        },
        {
            "title": "Real-time aggregation of Wikipedia data for visual analytics",
            "data": "Wikipedia has been built to gather encyclopedic knowledge using a collaborative social process that has proved its effectiveness. However, the workload required for raising the quality and increasing the coverage of Wikipedia is exhausting the community. Based on several participatory design sessions with active Wikipedia contributors (a.k.a. Wikipedians), we have collected a set of measures related to Wikipedia activity that, if available and visualized effectively, could spare a lot of monitoring time to these Wikipedians, allowing them to focus on quality and coverage of Wikipedia instead of spending their time navigating heavily to track vandals and copyright infringements. However, most of these measures cannot be computed on the fly using the available Wikipedia API. Therefore, we have designed an open architecture called WikiReactive to compute incrementally and maintain several aggregated measures on the French Wikipedia. This aggregated data is available as a Web Service and can be used to overlay information on Wikipedia articles through Wikipedia Skins or for new services for Wikipedians or people studying Wikipedia. This article describes the architecture, its performance and some of its uses.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652896",
            "id": "r_442",
            "s_ids": [
                "s_550",
                "s_173",
                "s_90"
            ],
            "type": "rich",
            "x": 7.298385143280029,
            "y": 9.924187660217285
        },
        {
            "title": "Geo-historical context support for information foraging and sensemaking: Conceptual model, implementation, and assessment",
            "data": "Information foraging and sensemaking with heterogeneous information are context-dependent activities. Thus visual analytics tools to support these activities must incorporate context. But, context is a difficult concept to define, model, and represent. Creating and representing context in support of visually-enabled reasoning about complex problems with complex information is a complementary but different challenge than that addressed in context-aware computing. In the latter, the goal is automated adaptation of the system to meet user needs for applications such as mobile location-based services where information about the location, the user, and the user goals filters what gets presented on a small mobile device. In contrast, for visual analytics-enabled information foraging and sensemaking, the user is likely to take an active role in foraging for the contextual information needed to support sensemaking in relation to some multifaceted problem. In this paper, we address the challenges of constructing and representing context within visual interfaces that support analytical reasoning in crisis management and humanitarian relief. The challenges stem from the diverse forms of information that can provide context and difficulty in defining and operationalizing context itself. Here, we pay particular attention to document foraging to support construction of the geographic and historical context within which monitoring and sensemaking can be carried out. Specifically, we present the concept of geo-historical context (GHC) and outline an empirical assessment of both the concept and its implementation in the Context Discovery Application, a web-based tool that supports document foraging and sensemaking.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652895",
            "id": "r_443",
            "s_ids": [
                "s_1117",
                "s_546"
            ],
            "type": "rich",
            "x": 5.850448131561279,
            "y": 10.297879219055176
        },
        {
            "title": "VAST 2007 Contest - Blue Iguanodon",
            "data": "Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The second visual analytics science and technology (VAST) contest was held in conjunction with the 2007 IEEE VAST symposium. In this contest participants were to use visual analytic tools to explore a large heterogeneous data collection to construct a scenario and find evidence buried in the data of illegal and terrorist activities that were occurring. A synthetic data set was made available as well as tasks. In this paper we describe some of the advances we have made from the first competition held in 2006.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389032",
            "id": "r_444",
            "s_ids": [
                "s_1205",
                "s_1399",
                "s_595",
                "s_934",
                "s_1244",
                "s_231"
            ],
            "type": "rich",
            "x": 6.723597526550293,
            "y": 8.76470947265625
        },
        {
            "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
            "data": "The collection and visual analysis of large-scale data from complex systems, such as electronic health records or clickstream data, has become increasingly common across a wide range of industries. This type of retrospective visual analysis, however, is prone to a variety of selection bias effects, especially for high-dimensional data where only a subset of dimensions is visualized at any given time. The risk of selection bias is even higher when analysts dynamically apply filters or perform grouping operations during ad hoc analyses. These bias effects threaten the validity and generalizability of insights discovered during visual analysis as the basis for decision making. Past work has focused on bias transparency, helping users understand when selection bias may have occurred. However, countering the effects of selection bias via bias mitigation is typically left for the user to accomplish as a separate process. Dynamic reweighting (DR) is a novel computational approach to selection bias mitigation that helps users craft bias-corrected visualizations. This paper describes the DR workflow, introduces key DR visualization designs, and presents statistical methods that support the DR process. Use cases from the medical domain, as well as findings from domain expert user interviews, are also reported.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030455",
            "id": "r_445",
            "s_ids": [
                "s_459",
                "s_113",
                "s_495",
                "s_873"
            ],
            "type": "rich",
            "x": 4.845474720001221,
            "y": 8.684896469116211
        },
        {
            "title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios",
            "data": "Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present s Portfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system's effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934660",
            "id": "r_446",
            "s_ids": [
                "s_1045",
                "s_62",
                "s_1248",
                "s_724",
                "s_97",
                "s_957",
                "s_131"
            ],
            "type": "rich",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries",
            "data": "The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Nowadays, the coordination between the muscles to generate simple movements is still not well understood, hindering the knowledge of how to best treat patients with this type of peripheral nerve injury. To acquire enough information for medical data analysis, physicians conduct motion analysis assessments with patients to produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Without the ability to integrate, compare, and compute multiple data sources in one platform, physicians can only compute simple statistical values to describe patient's behavior vaguely, which limits the possibility to answer clinical questions and generate hypotheses for research. To address this challenge, we have developed Motion Browser, an interactive visual analytics system which provides an efficient framework to extract and compare muscle activity patterns from the patient's limbs and coordinated views to help users analyze muscle signals, motion data, and video information to address different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies showing physicians can utilize the information displayed to understand how individuals coordinate their muscles to initiate appropriate treatment and generate new hypotheses for future research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934280",
            "id": "r_447",
            "s_ids": [
                "s_1219",
                "s_360",
                "s_1133",
                "s_204",
                "s_1238",
                "s_393"
            ],
            "type": "rich",
            "x": 5.91933536529541,
            "y": 6.946452617645264
        },
        {
            "title": "VEEVVIE: Visual Explorer for Empirical Visualization, VR and Interaction Experiments",
            "data": "Empirical, hypothesis-driven, experimentation is at the heart of the scientific discovery process and has become commonplace in human-factors related fields. To enable the integration of visual analytics in such experiments, we introduce VEEVVIE, the Visual Explorer for Empirical Visualization, VR and Interaction Experiments. VEEVVIE is comprised of a back-end ontology which can model several experimental designs encountered in these fields. This formalization allows VEEVVIE to capture experimental data in a query-able form and makes it accessible through a front-end interface. This front-end offers several multi-dimensional visualization widgets with built-in filtering and highlighting functionality. VEEVVIE is also expandable to support custom experimental measurements and data types through a plug-in visualization widget architecture. We demonstrate VEEVVIE through several case studies of visual analysis, performed on the design and data collected during an experiment on the scalability of high-resolution, immersive, tiled-display walls.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467954",
            "id": "r_448",
            "s_ids": [
                "s_818",
                "s_149",
                "s_750"
            ],
            "type": "rich",
            "x": 4.349888324737549,
            "y": 7.443033218383789
        },
        {
            "title": "Decision Exploration Lab: A Visual Analytics Solution for Decision Management",
            "data": "We present a visual analytics solution designed to address prevalent issues in the area of Operational Decision Management (ODM). In ODM, which has its roots in Artificial Intelligence (Expert Systems) and Management Science, it is increasingly important to align business decisions with business goals. In our work, we consider decision models (executable models of the business domain) as ontologies that describe the business domain, and production rules that describe the business logic of decisions to be made over this ontology. Executing a decision model produces an accumulation of decisions made over time for individual cases. We are interested, first, to get insight in the decision logic and the accumulated facts by themselves. Secondly and more importantly, we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model, and the reality as captured by the executed decisions. We illustrate the motivation, added value for visual analytics, and our proposed solution and tooling through a business case from the car insurance industry.",
            "url": "http://dx.doi.org/10.1109/TVCG.2013.146",
            "id": "r_449",
            "s_ids": [
                "s_1455",
                "s_995",
                "s_1253",
                "s_1084"
            ],
            "type": "rich",
            "x": 4.68471097946167,
            "y": 8.889886856079102
        },
        {
            "title": "Visual Abstraction of Geographical Point Data with Spatial Autocorrelations",
            "data": "Scatterplots are always employed to visualize geographical point datasets, which often suffer from an overdraw problem due to the increase of data sizes. A variety of sampling strategies have been proposed to reduce overdraw and visual clutter with the spatial densities of points taken into account. However, informative attributes associated with the points also play significant roles in the exploration of geographical datasets. In this paper, we propose an attribute-based abstraction method to simplify the cluttered visualization of large-scale geographical points. Spatial autocorrelations are utilized to measure the attribute relationships of points in local areas, and a novel attribute-based sampling model is designed to generate a subset of points to preserve both density and attribute characteristics of original geographical points. A set of visual designs and user-friendly interactions are implemented, enabling users to capture the spatial distribution of geographical points and get deeper insights into the attribute features across local areas. Case studies and quantitative comparisons based on the real-world datasets further demonstrate the effectiveness of our method in the abstraction and exploration of large-scale geographical point datasets.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00011",
            "id": "r_450",
            "s_ids": [
                "s_1058",
                "s_1122",
                "s_38",
                "s_614",
                "s_716",
                "s_1464",
                "s_66",
                "s_1357",
                "s_1319"
            ],
            "type": "rich",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "title": "A Visual Analytics Framework for Contrastive Network Analysis",
            "data": "A common network analysis task is comparison of two networks to identify unique characteristics in one network with respect to the other. For example, when comparing protein interaction networks derived from normal and cancer tissues, one essential task is to discover protein-protein interactions unique to cancer tissues. However, this task is challenging when the networks contain complex structural (and semantic) relations. To address this problem, we design ContraNA, a visual analytics framework leveraging both the power of machine learning for uncovering unique characteristics in networks and also the effectiveness of visualization for understanding such uniqueness. The basis of ContraNA is cNRL, which integrates two machine learning schemes, network representation learning (NRL) and contrastive learning (CL), to generate a low-dimensional embedding that reveals the uniqueness of one network when compared to another. ContraNA provides an interactive visualization interface to help analyze the uniqueness by relating embedding results and network structures as well as explaining the learned features by cNRL. We demonstrate the usefulness of ContraNA with two case studies using real-world datasets. We also evaluate ContraNA through a controlled user study with 12 participants on network comparison tasks. The results show that participants were able to both effectively identify unique characteristics from complex networks and interpret the results obtained from cNRL.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00010",
            "id": "r_451",
            "s_ids": [
                "s_1259",
                "s_1354",
                "s_482",
                "s_699"
            ],
            "type": "rich",
            "x": 5.193649768829346,
            "y": 5.368021011352539
        },
        {
            "title": "The Effect of Semantic Interaction on Foraging in Text Analysis",
            "data": "Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1) a synthesis model that aids in document layout, and (2) a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword search foraging (KSF), \u201csemantic interaction foraging\u201d (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword search.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802424",
            "id": "r_452",
            "s_ids": [
                "s_586",
                "s_96",
                "s_898",
                "s_15",
                "s_202"
            ],
            "type": "rich",
            "x": 5.932607650756836,
            "y": 10.339508056640625
        },
        {
            "title": "Shape Grammar Extraction for Efficient Query-by-Sketch Pattern Matching in Long Time Series",
            "data": "Long time-series, involving thousands or even millions of time steps, are common in many application domains but remain very difficult to explore interactively. Often the analytical task in such data is to identify specific patterns, but this is a very complex and computationally difficult problem and so focusing the search in order to only identify interesting patterns is a common solution. We propose an efficient method for exploring user-sketched patterns, incorporating the domain expert's knowledge, in time series data through a shape grammar based approach. The shape grammar is extracted from the time series by considering the data as a combination of basic elementary shapes positioned across different amplitudes. We represent these basic shapes using a ratio value, perform binning on ratio values and apply a symbolic approximation. Our proposed method for pattern matching is amplitude-, scale- and translation-invariant and, since the pattern search and pattern constraint relaxation happen at the symbolic level, is very efficient permitting its use in a real-time/online system. We demonstrate the effectiveness of our method in a case study on stock market data although it is applicable to any numeric time series data.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883518",
            "id": "r_453",
            "s_ids": [
                "s_850",
                "s_223",
                "s_1149",
                "s_1186"
            ],
            "type": "rich",
            "x": 6.717698574066162,
            "y": 6.0533623695373535
        },
        {
            "title": "Supporting activity recognition by visual analytics",
            "data": "Recognizing activities has become increasingly relevant in many application domains, such as security or ambient assisted living. To handle different scenarios, the underlying automated algorithms are configured using multiple input parameters. However, the influence and interplay of these parameters is often not clear, making exhaustive evaluations necessary. On this account, we propose a visual analytics approach to supporting users in understanding the complex relationships among parameters, recognized activities, and associated accuracies. First, representative parameter settings are determined. Then, the respective output is computed and statistically analyzed to assess parameters' influence in general. Finally, visualizing the parameter settings along with the activities provides overview and allows to investigate the computed results in detail. Coordinated interaction helps to explore dependencies, compare different settings, and examine individual activities. By integrating automated, visual, and interactive means users can select parameter values that meet desired quality criteria. We demonstrate the application of our solution in a use case with realistic complexity, involving a study of human protagonists in daily living with respect to hundreds of parameter settings.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347629",
            "id": "r_454",
            "s_ids": [
                "s_1193",
                "s_537",
                "s_620",
                "s_963",
                "s_757",
                "s_78",
                "s_1397",
                "s_1124"
            ],
            "type": "rich",
            "x": 3.764721155166626,
            "y": 7.994372844696045
        },
        {
            "title": "A two-stage framework for designing visual analytics system in organizational environments",
            "data": "A perennially interesting research topic in the field of visual analytics is how to effectively develop systems that support organizational users' decision-making and reasoning processes. The problem is, however, most domain analytical practices generally vary from organization to organization. This leads to diverse designs of visual analytics systems in incorporating domain analytical processes, making it difficult to generalize the success from one domain to another. Exacerbating this problem is the dearth of general models of analytical workflows available to enable such timely and effective designs. To alleviate these problems, we present a two-stage framework for informing the design of a visual analytics system. This design framework builds upon and extends current practices pertaining to analytical workflow and focuses, in particular, on incorporating both general domain analysis processes as well as individual's analytical activities. We illustrate both stages and their design components through examples, and hope this framework will be useful for designing future visual analytics systems. We validate the soundness of our framework with two visual analytics systems, namely Entity Workspace [8] and PatViz [37].",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102463",
            "id": "r_455",
            "s_ids": [
                "s_1368",
                "s_305",
                "s_658",
                "s_613",
                "s_233"
            ],
            "type": "rich",
            "x": 5.303890228271484,
            "y": 9.486879348754883
        },
        {
            "title": "A closer look at note taking in the co-located collaborative visual analytics process",
            "data": "This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652879",
            "id": "r_456",
            "s_ids": [
                "s_1496",
                "s_1377",
                "s_1266"
            ],
            "type": "rich",
            "x": 5.519694805145264,
            "y": 10.768251419067383
        },
        {
            "title": "Interactive visual analysis of multiobjective optimizations",
            "data": "Optimization problems are typically addressed by purely automatic approaches. For multi-objective problems, however, a single best solution often does not exist. In this case, it is necessary to analyze trade-offs between many conflicting goals within a given application context. This poster describes an approach that tightly integrates automatic algorithms for multi-objective optimization and interactive multivariate visualizations. Ad-hoc selections support a flexible definition of input data for subsequent algorithms. These algorithms in turn represent their result as derived data attributes that can be assigned to visualizations or be used as a basis for further selections (e.g., to constrain the result set). This enables a guided search that still involves the knowledge of domain experts. We describe our approach in the context of multi-run simulation data from the application domain of car engine design.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5651694",
            "id": "r_457",
            "s_ids": [
                "s_1189",
                "s_275"
            ],
            "type": "rich",
            "x": 3.634395122528076,
            "y": 7.800271511077881
        },
        {
            "title": "Jigsaw meets Blue Iguanodon - The VAST 2007 Contest",
            "data": "This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw's use and how the different views helped us to uncover key parts of the underlying plot.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389034",
            "id": "r_458",
            "s_ids": [
                "s_657",
                "s_1115",
                "s_753",
                "s_1073",
                "s_756"
            ],
            "type": "rich",
            "x": 6.486181735992432,
            "y": 9.125225067138672
        },
        {
            "title": "Scentindex: Conceptually Reorganizing Subject Indexes for Reading",
            "data": "A great deal of analytical work is done in the context of reading, in digesting the semantics of the material, the identification of important entities, and capturing the relationship between entities. Visual analytic environments, therefore, must encompass reading tools that enable the rapid digestion of large amount of reading material. Other than plain text search, subject indexes, and basic highlighting, tools are needed for rapid foraging of text. In this paper, we describe a technique that presents an enhanced subject index for a book by conceptually reorganizing it to suit particular expressed user information needs. Users first enter information needs via keywords describing the concepts they are trying to retrieve and comprehend. Then our system, called ScentIndex, computes what index entries are conceptually related and reorganizes and displays these index entries on a single page. We also provide a number of navigational cues to help users peruse over this list of index entries and find relevant passages quickly. Compared to regular reading of a paper book, our study showed that users are more efficient and more accurate in finding, comparing, and comprehending material in our system",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261418",
            "id": "r_459",
            "s_ids": [
                "s_764",
                "s_1477",
                "s_307",
                "s_819"
            ],
            "type": "rich",
            "x": 7.2240824699401855,
            "y": 10.312528610229492
        },
        {
            "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics",
            "data": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030453",
            "id": "r_460",
            "s_ids": [
                "s_991",
                "s_699"
            ],
            "type": "rich",
            "x": 5.2668681144714355,
            "y": 8.542102813720703
        },
        {
            "title": "II-20: Intelligent and pragmatic analytic categorization of image collections",
            "data": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030383",
            "id": "r_461",
            "s_ids": [
                "s_1110",
                "s_1206",
                "s_63"
            ],
            "type": "rich",
            "x": 4.909894943237305,
            "y": 8.428279876708984
        },
        {
            "title": "An Examination of Grouping and Spatial Organization Tasks for High-Dimensional Data Exploration",
            "data": "How do analysts think about grouping and spatial operations? This overarching research question incorporates a number of points for investigation, including understanding how analysts begin to explore a dataset, the types of grouping/spatial structures created and the operations performed on them, the relationship between grouping and spatial structures, the decisions analysts make when exploring individual observations, and the role of external information. This work contributes the design and results of such a study, in which a group of participants are asked to organize the data contained within an unfamiliar quantitative dataset. We identify several overarching approaches taken by participants to design their organizational space, discuss the interactions performed by the participants, and propose design recommendations to improve the usability of future high-dimensional data exploration tools that make use of grouping (clustering) and spatial (dimension reduction) operations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028890",
            "id": "r_462",
            "s_ids": [
                "s_586",
                "s_202"
            ],
            "type": "rich",
            "x": 5.553137302398682,
            "y": 8.72069263458252
        },
        {
            "title": "A Natural-language-based Visual Query Approach of Uncertain Human Trajectories",
            "data": "Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934671",
            "id": "r_463",
            "s_ids": [
                "s_523",
                "s_975",
                "s_1319",
                "s_706",
                "s_44",
                "s_1513",
                "s_794",
                "s_997",
                "s_67"
            ],
            "type": "rich",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "title": "Galex: Exploring the Evolution and Intersection of Disciplines",
            "data": "Revealing the evolution of science and the intersections among its sub-fields is extremely important to understand the characteristics of disciplines, discover new topics, and predict the future. The current work focuses on either building the skeleton of science, lacking interaction, detailed exploration and interpretation or on the lower topic level, missing high-level macro-perspective. To fill this gap, we design and implement Galaxy Evolution Explorer (Galex), a hierarchical visual analysis system, in combination with advanced text mining technologies, that could help analysts to comprehend the evolution and intersection of one discipline rapidly. We divide Galex into three progressively fine-grained levels: discipline, area, and institution levels. The combination of interactions enables analysts to explore an arbitrary piece of history and an arbitrary part of the knowledge space of one discipline. Using a flexible spotlight component, analysts could freely select and quickly understand an exploration region. A tree metaphor allows analysts to perceive the expansion, decline, and intersection of topics intuitively. A synchronous spotlight interaction aids in comparing research contents among institutions easily. Three cases demonstrate the effectiveness of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934667",
            "id": "r_464",
            "s_ids": [
                "s_702",
                "s_1227",
                "s_1067",
                "s_918"
            ],
            "type": "rich",
            "x": 6.394692420959473,
            "y": 10.036843299865723
        },
        {
            "title": "Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data",
            "data": "The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias\u2013when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort \u201cdrift\u201d, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934209",
            "id": "r_465",
            "s_ids": [
                "s_459",
                "s_631",
                "s_113",
                "s_541",
                "s_873"
            ],
            "type": "rich",
            "x": 5.713511943817139,
            "y": 7.0280022621154785
        },
        {
            "title": "VASA: Interactive Computational Steering of Large Asynchronous Simulation Pipelines for Societal Infrastructure",
            "data": "We present VASA, a visual analytics platform consisting of a desktop application, a component model, and a suite of distributed simulation components for modeling the impact of societal threats such as weather, food contamination, and traffic on critical infrastructure such as supply chains, road networks, and power grids. Each component encapsulates a high-fidelity simulation model that together form an asynchronous simulation pipeline: a system of systems of individual simulations with a common data and parameter exchange format. At the heart of VASA is the Workbench, a visual analytics application providing three distinct features: (1) low-fidelity approximations of the distributed simulation components using local simulation proxies to enable analysts to interactively configure a simulation run; (2) computational steering mechanisms to manage the execution of individual simulation components; and (3) spatiotemporal and interactive methods to explore the combined results of a simulation run. We showcase the utility of the platform using examples involving supply chains during a hurricane as well as food contamination in a fast food restaurant chain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2014.2346911",
            "id": "r_466",
            "s_ids": [
                "s_1239",
                "s_1032",
                "s_1164",
                "s_945",
                "s_1368",
                "s_448",
                "s_1191",
                "s_659",
                "s_1158",
                "s_1075",
                "s_929",
                "s_862",
                "s_233",
                "s_952"
            ],
            "type": "rich",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "title": "TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling",
            "data": "Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or \u201ctargets\u201d rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986922",
            "id": "r_467",
            "s_ids": [
                "s_745",
                "s_710",
                "s_1154",
                "s_1517",
                "s_1096"
            ],
            "type": "rich",
            "x": 7.219537734985352,
            "y": 10.913814544677734
        },
        {
            "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach",
            "data": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802486",
            "id": "r_468",
            "s_ids": [
                "s_1105",
                "s_547",
                "s_983",
                "s_278",
                "s_536",
                "s_157",
                "s_1139",
                "s_597",
                "s_1038"
            ],
            "type": "rich",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "title": "VUSphere: Visual Analysis of Video Utilization in Online Distance Education",
            "data": "Online Distance Education (ODE) provides massive course videos of various specialties for students across the country to learn professional knowledge anytime and anywhere. Analyzing the utilization of these videos from user log data can help academics better understand the learning process of students, evaluate the quality of service provided by regional learning centers, and improve the quality of program curriculum in the future. However, due to the lack of comparable indicators, it is a great challenge to discover the utilization patterns of massive videos and analyze the learning process of large-scale student population from learning log data. In this paper, we introduce a visual analytics system, called VUSphere, to explore the video utilization from multiple perspectives with two proposed indicators. This system offers three coordinated views: a spherical layout overview to depict the overall utilization distribution of videos, courses, and students; a detailed statistics view with four panels to present video utilization statistics of each element from multiple perspectives; and a comparison view to examine the differences in individual elements. Based on the real dataset from our ODE school, several patterns related to video utilization and enrollment are found in the case study with our domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802383",
            "id": "r_469",
            "s_ids": [
                "s_508",
                "s_733",
                "s_287"
            ],
            "type": "rich",
            "x": 5.9608869552612305,
            "y": 8.278266906738281
        },
        {
            "title": "C2A: Crowd consensus analytics for virtual colonoscopy",
            "data": "We present a medical crowdsourcing visual analytics platform called C<sup>2</sup>A to visualize, classify and filter crowdsourced clinical data. More specifically, C<sup>2</sup>A is used to build consensus on a clinical diagnosis by visualizing crowd responses and filtering out anomalous activity. Crowdsourcing medical applications have recently shown promise where the non-expert users (the crowd) were able to achieve accuracy similar to the medical experts. This has the potential to reduce interpretation/reading time and possibly improve accuracy by building a consensus on the findings beforehand and letting the medical experts make the final diagnosis. In this paper, we focus on a virtual colonoscopy (VC) application with the clinical technicians as our target users, and the radiologists acting as consultants and classifying segments as benign or malignant. In particular, C<sup>2</sup>A is used to analyze and explore crowd responses on video segments, created from fly-throughs in the virtual colon. C<sup>2</sup>A provides several interactive visualization components to build crowd consensus on video segments, to detect anomalies in the crowd data and in the VC video segments, and finally, to improve the non-expert user's work quality and performance by A/B testing for the optimal crowdsourcing platform and application-specific parameters. Case studies and domain experts feedback demonstrate the effectiveness of our framework in improving workers' output quality, the potential to reduce the radiologists' interpretation time, and hence, the potential to improve the traditional clinical workflow by marking the majority of the video segments as benign based on the crowd consensus.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883508",
            "id": "r_470",
            "s_ids": [
                "s_247",
                "s_777",
                "s_601",
                "s_750"
            ],
            "type": "rich",
            "x": 5.832940101623535,
            "y": 7.073251247406006
        },
        {
            "title": "Visual Analytics for fraud detection and monitoring",
            "data": "One of the primary concerns of financial institutions is to guarantee security and legitimacy in their services. Being able to detect and avoid fraudulent schemes also enhances the credibility of these institutions. Currently, fraud detection approaches still lack Visual Analytics techniques. We propose a Visual Analytics process that tackles the main challenges in the area of fraud detection.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347678",
            "id": "r_471",
            "s_ids": [
                "s_19",
                "s_172",
                "s_1124",
                "s_49",
                "s_1031"
            ],
            "type": "rich",
            "x": 8.887590408325195,
            "y": 7.185838222503662
        },
        {
            "title": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs",
            "data": "Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042485",
            "id": "r_472",
            "s_ids": [
                "s_1156",
                "s_871",
                "s_889",
                "s_1165",
                "s_241"
            ],
            "type": "rich",
            "x": 5.561883926391602,
            "y": 6.028954029083252
        },
        {
            "title": "Exploring the impact of emotion on visual judgement",
            "data": "Existing research suggests that individual personality differences can influence performance with visualizations. In addition to stable traits such as locus of control, research in psychology has found that temporary changes in affect (emotion) can significantly impact individual performance on cognitive tasks. We examine the relationship between fundamental visual judgement tasks and affect through a crowdsourced user study that combines affective-priming techniques from psychology with longstanding graphical perception experiments. Our results suggest that affective-priming can significantly influence accuracy in visual judgements, and that some chart types may be more affected than others.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400540",
            "id": "r_473",
            "s_ids": [
                "s_120",
                "s_333",
                "s_403"
            ],
            "type": "rich",
            "x": 4.682358264923096,
            "y": 9.893468856811523
        },
        {
            "title": "A generic model for the integration of interactive visualization and statistical computing using R",
            "data": "This poster describes general concepts of integrating the statistical computation package R into a coordinated multiple views framework. The integration is based on a cyclic analysis workflow. In this model, interactive selections are a key aspect to trigger and control computations in R. Dynamic updates of data columns are a generic mechanism to transfer computational results back to the interactive visualization. Further aspects include the integration of the R console and an R object browser as views in our system. We illustrate our approach by means of an interactive modeling process.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400537",
            "id": "r_474",
            "s_ids": [
                "s_290",
                "s_693",
                "s_1442",
                "s_275"
            ],
            "type": "rich",
            "x": 3.554858684539795,
            "y": 7.912291526794434
        },
        {
            "title": "MassVis: Visual analysis of protein complexes using mass spectrometry",
            "data": "Protein complexes are formed when two or more proteins non-covalently interact to form a larger three dimensional structure with specific biological function. Understanding the composition of such complexes is vital to understanding cell biology at the molecular level. MassVis is a visual analysis tool designed to assist the interpretation of data from a new workflow for detecting the composition of such protein complexes in biological samples. The data generated by the laboratory workflow naturally lends itself to a scatter plot visualization. However, characteristics of this data give rise to some unique aspects not typical of a standard scatter plot. We are able to take the output from tandem mass spectrometry and render the data in such a way that it mimics more traditional two-dimensional gel techniques and at the same time reveals the correlated behavior indicative of protein complexes. By computationally measuring these correlated patterns in the data, membership in putative complexes can be inferred. User interactions are provided to support both an interactive discovery mode as well as an unsupervised clustering of likely complexes. The specific analysis tasks led us to design a unique arrangement of item selection and coordinated detail views in order to simultaneously view different aspects of the selected item.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333895",
            "id": "r_475",
            "s_ids": [
                "s_602",
                "s_899"
            ],
            "type": "rich",
            "x": 5.504063129425049,
            "y": 6.4216389656066895
        },
        {
            "title": "VAST contest dataset use in education",
            "data": "The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333245",
            "id": "r_476",
            "s_ids": [
                "s_231",
                "s_202",
                "s_1517",
                "s_1244",
                "s_635",
                "s_297",
                "s_1403"
            ],
            "type": "rich",
            "x": 6.7198567390441895,
            "y": 8.789078712463379
        },
        {
            "title": "Visual Analysis of Historic Hotel Visitation Patterns",
            "data": "Understanding the space and time characteristics of human interaction in complex social networks is a critical component of visual tools for intelligence analysis, consumer behavior analysis, and human geography. Visual identification and comparison of patterns of recurring events is an essential feature of such tools. In this paper, we describe a tool for exploring hotel visitation patterns in and around Rebersburg, Pennsylvania from 1898-1900. The tool uses a wrapping spreadsheet technique, called reruns, to display cyclic patterns of geographic events in multiple overlapping natural and artificial calendars. Implemented as an improvise visualization, the tool is in active development through a iterative process of data collection, hypothesis, design, discovery, and evaluation in close collaboration with historical geographers. Several discoveries have inspired ongoing data collection and plans to expand exploration to include historic weather records and railroad schedules. Distributed online evaluations of usability and usefulness have resulted in numerous feature and design recommendations",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261428",
            "id": "r_477",
            "s_ids": [
                "s_309",
                "s_577",
                "s_907",
                "s_8",
                "s_717",
                "s_546"
            ],
            "type": "rich",
            "x": 7.1402506828308105,
            "y": 7.083461284637451
        },
        {
            "title": "Visual Neural Decomposition to Explain Multivariate Data Sets",
            "data": "Investigating relationships between variables in multi-dimensional data sets is a common task for data analysts and engineers. More specifically, it is often valuable to understand which ranges of which input variables lead to particular values of a given target variable. Unfortunately, with an increasing number of independent variables, this process may become cumbersome and time-consuming due to the many possible combinations that have to be explored. In this paper, we propose a novel approach to visualize correlations between input variables and a target output variable that scales to hundreds of variables. We developed a visual model based on neural networks that can be explored in a guided way to help analysts find and understand such correlations. First, we train a neural network to predict the target from the input variables. Then, we visualize the inner workings of the resulting model to help understand relations within the data set. We further introduce a new regularization term for the backpropagation algorithm that encourages the neural network to learn representations that are easier to interpret visually. We apply our method to artificial and real-world data sets to show its utility.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030420",
            "id": "r_478",
            "s_ids": [
                "s_1313",
                "s_851",
                "s_404",
                "s_314"
            ],
            "type": "rich",
            "x": 4.120519161224365,
            "y": 7.768464088439941
        },
        {
            "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs",
            "data": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030398",
            "id": "r_479",
            "s_ids": [
                "s_1374",
                "s_407",
                "s_39",
                "s_1038",
                "s_511"
            ],
            "type": "rich",
            "x": 5.088621616363525,
            "y": 5.253386974334717
        },
        {
            "title": "InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming",
            "data": "Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864887",
            "id": "r_480",
            "s_ids": [
                "s_174",
                "s_260",
                "s_651",
                "s_1354",
                "s_47",
                "s_939"
            ],
            "type": "rich",
            "x": 7.414912700653076,
            "y": 10.379096031188965
        },
        {
            "title": "Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores",
            "data": "This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744686",
            "id": "r_481",
            "s_ids": [
                "s_1131",
                "s_802",
                "s_344",
                "s_112"
            ],
            "type": "rich",
            "x": 6.390726089477539,
            "y": 6.075316429138184
        },
        {
            "title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment",
            "data": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599378",
            "id": "r_482",
            "s_ids": [
                "s_1310",
                "s_1003",
                "s_1010",
                "s_131"
            ],
            "type": "rich",
            "x": 3.848210334777832,
            "y": 7.332578182220459
        },
        {
            "title": "DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection",
            "data": "Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with a data structuring and exploration scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assistance widgets to interactively detect the relations of multi-dimensional data.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883514",
            "id": "r_483",
            "s_ids": [
                "s_1164",
                "s_1319",
                "s_1129",
                "s_912",
                "s_315",
                "s_952"
            ],
            "type": "rich",
            "x": 5.058387756347656,
            "y": 6.625837802886963
        },
        {
            "title": "EgoNetCloud: Event-based egocentric dynamic network visualization",
            "data": "Event-based egocentric dynamic networks are an important class of networks widely seen in many domains. In this paper, we present a visual analytics approach for these networks by combining data-driven network simplifications with a novel visualization design - EgoNetCloud. In particular, an integrated data processing pipeline is proposed to prune, compress and filter the networks into smaller but salient abstractions. To accommodate the simplified network into the visual design, we introduce a constrained graph layout algorithm on the dynamic network. Through a real-life case study as well as conversations with the domain expert, we demonstrate the effectiveness of the EgoNetCloud design and system in completing analysis tasks on event-based dynamic networks. The user study comparing EgoNetCloud with a working system on academic search confirms the effectiveness and convenience of our visual analytics based approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347632",
            "id": "r_484",
            "s_ids": [
                "s_301",
                "s_353",
                "s_32",
                "s_1286",
                "s_1502",
                "s_881"
            ],
            "type": "rich",
            "x": 5.147846698760986,
            "y": 5.0413899421691895
        },
        {
            "title": "Collaborative visual analysis with RCloud",
            "data": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347627",
            "id": "r_485",
            "s_ids": [
                "s_1178",
                "s_1071",
                "s_809",
                "s_379"
            ],
            "type": "rich",
            "x": 5.817330360412598,
            "y": 10.671513557434082
        },
        {
            "title": "An adaptive parameter space-filling algorithm for highly interactive cluster exploration",
            "data": "For a user to perceive continuous interactive response time in a visualization tool, the rule of thumb is that it must process, deliver, and display rendered results for any given interaction in under 100 milliseconds. In many visualization systems, successive interactions trigger independent queries and caching of results. Consequently, computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions, precluding visual benefits such as motion parallax. In this paper, we describe a heuristic prefetching technique to improve the interactive response time of KMeans clustering in dynamic query visualizations of multidimensional data. We address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results, and characterizing these similarities within a parameter space of interaction. We focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time series data. Using calculation of nearest neighbors of interaction points in parameter space, we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result. The method adapts to user interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space. A performance study on Mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand, exact-range clustering with LRU caching. We also present initial evidence that approximate, temporary clustering results are sufficiently accurate (compared to exact results) to convey useful cluster structure during rapid and protracted interaction.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400493",
            "id": "r_486",
            "s_ids": [
                "s_943",
                "s_309"
            ],
            "type": "rich",
            "x": 3.781320095062256,
            "y": 6.027749538421631
        },
        {
            "title": "Obvious: A meta-toolkit to encapsulate information visualization toolkits - One toolkit to bind them all",
            "data": "This article describes \u201cObvious\u201d: a meta-toolkit that abstracts and encapsulates information visualization toolkits implemented in the Java language. It intends to unify their use and postpone the choice of which concrete toolkit(s) to use later-on in the development of visual analytics applications. We also report on the lessons we have learned when wrapping popular toolkits with Obvious, namely Prefuse, the InfoVis Toolkit, partly Improvise, JUNG and other data management libraries. We show several examples on the uses of Obvious, how the different toolkits can be combined, for instance sharing their data models. We also show how Weka and Rapid-Miner, two popular machine-learning toolkits, have been wrapped with Obvious and can be used directly with all the other wrapped toolkits. We expect Obvious to start a co-evolution process: Obvious is meant to evolve when more components of Information Visualization systems will become consensual. It is also designed to help information visualization systems adhere to the best practices to provide a higher level of interoperability and leverage the domain of visual analytics.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102446",
            "id": "r_487",
            "s_ids": [
                "s_90",
                "s_395",
                "s_995",
                "s_1126"
            ],
            "type": "rich",
            "x": 5.43972110748291,
            "y": 8.626875877380371
        },
        {
            "title": "A multi-level middle-out cross-zooming approach for large graph analytics",
            "data": "This paper presents a working graph analytics model that embraces the strengths of the traditional top-down and bottom-up approaches with a resilient crossover concept to exploit the vast middle-ground information overlooked by the two extreme analytical approaches. Our graph analytics model is co-developed by users and researchers, who carefully studied the functional requirements that reflect the critical thinking and interaction pattern of a real-life intelligence analyst. To evaluate the model, we implement a system prototype, known as GreenHornet, which allows our analysts to test the theory in practice, identify the technological and usage-related gaps in the model, and then adapt the new technology in their work space. The paper describes the implementation of GreenHornet and compares its strengths and weaknesses against the other prevailing models and tools.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333880",
            "id": "r_488",
            "s_ids": [
                "s_421",
                "s_1025",
                "s_684",
                "s_368",
                "s_980",
                "s_231"
            ],
            "type": "rich",
            "x": 5.126974582672119,
            "y": 5.073885440826416
        },
        {
            "title": "Configurable Spaces: Temporal analysis in diagrammatic contexts",
            "data": "Social network graphs, concept maps, and process charts are examples of diagrammatic representations employed by intelligence analysts to understand complex systems. Unfortunately, these 2D representations currently do not easily convey the flow, sequence, tempo and other important dynamic behaviors within these systems. In this paper we present Configurable Spaces, a novel analytical method for visualizing patterns of activity over time in complex diagrammatically- represented systems. Configurable Spaces extends GeoTime's X, Y, T coordinate workspace space for temporal analysis to any arbitrary diagrammatic work space by replacing a geographic map with a diagram. This paper traces progress from concept to prototype, and discusses how diagrams can be created, transformed and leveraged for analysis, including generating diagrams from knowledge bases, visualizing temporal concept maps, and the use of linked diagrams for exploring complex, multi-dimensional, sequences of events. An evaluation of the prototype by the National Institute of Standards and Technology showed intelligence analysts believed they were able to attain an increased level of insight, were able to explore data more efficiently, and that Configurable Spaces would help them work faster.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677355",
            "id": "r_489",
            "s_ids": [
                "s_1220",
                "s_928",
                "s_284",
                "s_796"
            ],
            "type": "rich",
            "x": 6.237151622772217,
            "y": 7.669819355010986
        },
        {
            "title": "Interactive Wormhole Detection in Large Scale Wireless Networks",
            "data": "Wormhole attacks in wireless networks can severely deteriorate the network performance and compromise the security through spoiling the routing protocols and weakening the security enhancements. This paper develops an approach, interactive visualization of wormholes (IVoW), to monitor and detect such attacks in large scale wireless networks in real time. We characterize the topology features of a network under wormhole attacks through the node position changes and visualize the information at dynamically adjusted scales. We integrate an automatic detection algorithm with appropriate user interactions to handle complicated scenarios that include a large number of moving nodes and multiple worm-hole attackers. Various visual forms have been adopted to assist the understanding and analysis of the reconstructed network topology and improve the detection accuracy. Extended simulation has demonstrated that the proposed approach can effectively locate the fake neighbor connections without introducing many false alarms. IVoW does not require the wireless nodes to be equipped with any special hardware, thus avoiding any additional cost. The proposed approach demonstrates that interactive visualization can be successfully combined with network security mechanisms to greatly improve the intrusion detection capabilities",
            "url": "http://dx.doi.org/10.1109/VAST.2006.261435",
            "id": "r_490",
            "s_ids": [
                "s_371",
                "s_403"
            ],
            "type": "rich",
            "x": 6.884183406829834,
            "y": 7.541064262390137
        },
        {
            "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection",
            "data": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030430",
            "id": "r_491",
            "s_ids": [
                "s_218",
                "s_841",
                "s_778"
            ],
            "type": "rich",
            "x": 5.984103202819824,
            "y": 9.63005256652832
        },
        {
            "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems",
            "data": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030364",
            "id": "r_492",
            "s_ids": [
                "s_789",
                "s_195",
                "s_984",
                "s_73",
                "s_1247"
            ],
            "type": "rich",
            "x": 3.6478431224823,
            "y": 7.894081115722656
        },
        {
            "title": "Exploranative Code Quality Documents",
            "data": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934669",
            "id": "r_493",
            "s_ids": [
                "s_1184",
                "s_16",
                "s_399",
                "s_132"
            ],
            "type": "rich",
            "x": 7.38595724105835,
            "y": 10.323249816894531
        },
        {
            "title": "An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments",
            "data": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865025",
            "id": "r_494",
            "s_ids": [
                "s_273",
                "s_1075",
                "s_1448",
                "s_1407"
            ],
            "type": "rich",
            "x": 4.857062816619873,
            "y": 9.41756534576416
        },
        {
            "title": "Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities",
            "data": "The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865023",
            "id": "r_495",
            "s_ids": [
                "s_1008",
                "s_959",
                "s_433",
                "s_1375",
                "s_473"
            ],
            "type": "rich",
            "x": 6.941458702087402,
            "y": 8.855831146240234
        },
        {
            "title": "Visual Analytics for Development and Evaluation of Order Selection Criteria for Autoregressive Processes",
            "data": "Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator-combined with specialized responsive visualizations-allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467612",
            "id": "r_496",
            "s_ids": [
                "s_1021",
                "s_1270",
                "s_391",
                "s_130",
                "s_251"
            ],
            "type": "rich",
            "x": 3.6500320434570312,
            "y": 7.781797885894775
        },
        {
            "title": "Visual analysis of route choice behaviour based on GPS trajectories",
            "data": "There are often multiple routes between regions. Many factors potentially affect driver's route choice, such as expected time cost, length etc. In this work, we present a visual analysis system to explore driver's route choice behaviour based on taxi GPS trajectory data. With interactive trajectory filtering, the system constructs feasible routes between regions of interest. Using a rank-based visualization, the attributes of multiple routes are explored and compared. Based on a statistical model, the system supports to verify trajectory-related factors' impact on route choice behaviour. The effectiveness of the system is demonstrated by applying to real trajectory dataset.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347679",
            "id": "r_497",
            "s_ids": [
                "s_1471",
                "s_449",
                "s_82",
                "s_1262",
                "s_1274"
            ],
            "type": "rich",
            "x": 7.39639139175415,
            "y": 4.817005157470703
        },
        {
            "title": "Comparing different levels of interaction constraints for deriving visual problem isomorphs",
            "data": "Interaction and manual manipulation have been shown in the cognitive science literature to play a critical role in problem solving. Given different types of interactions or constraints on interactions, a problem can appear to have different degrees of difficulty. While this relationship between interaction and problem solving has been well studied in the cognitive science literatures, the visual analytics community has yet to exploit this understanding for analytical problem solving. In this paper, we hypothesize that constraints on interactions and constraints encoded in visual representations can lead to strategies of varying effectiveness during problem solving. To test our hypothesis, we conducted a user study in which participants were given different levels of interaction constraints when solving a simple math game called Number Scrabble. Number Scrabble is known to have an optimal visual problem isomorph, and the goal of this study is to learn if and how the participants could derive the isomorph and to analyze the strategies that the participants utilize in solving the problem. Our results indicate that constraints on interactions do affect problem solving, and that while the optimal visual isomorph is difficult to derive, certain interaction constraints can lead to a higher chance of deriving the isomorph.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5653599",
            "id": "r_498",
            "s_ids": [
                "s_305",
                "s_855",
                "s_120",
                "s_596",
                "s_431",
                "s_233",
                "s_1368",
                "s_333"
            ],
            "type": "rich",
            "x": 5.070459365844727,
            "y": 10.236028671264648
        },
        {
            "title": "A visual analytics system for radio frequency fingerprinting-based localization",
            "data": "Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user's current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332596",
            "id": "r_499",
            "s_ids": [
                "s_1297",
                "s_712",
                "s_756",
                "s_171"
            ],
            "type": "rich",
            "x": 6.5693864822387695,
            "y": 7.4537224769592285
        },
        {
            "title": "A 3D treemap approach for analyzing the classificatory distribution in patent portfolios",
            "data": "Due to the complexity of the patent domain and the huge amount of data, advanced interactive visual techniques are needed to support the analysis of large patent collections and portfolios. In this paper we present a new approach for visualizing the classificatory distribution of patent collections among the International Patent Classification (IPC) - todaypsilas most important internationally agreed patent classification system with about 70.000 categories. Our approach is based on an interactive three-dimensional treemap overlaid with adjacency edge bundles.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677380",
            "id": "r_500",
            "s_ids": [
                "s_1446",
                "s_214",
                "s_314"
            ],
            "type": "rich",
            "x": 4.979891777038574,
            "y": 6.067989826202393
        },
        {
            "title": "The Scalable Reasoning System: Lightweight visualization for distributed analytics",
            "data": "A central challenge in visual analytics is the creation of accessible, widely distributable analysis applications that bring the benefits of visual discovery to as broad a user base as possible. Moreover, to support the role of visualization in the knowledge creation process, it is advantageous to allow users to describe the reasoning strategies they employ while interacting with analytic environments. We introduce an application suite called the scalable reasoning system (SRS), which provides Web-based and mobile interfaces for visual analysis. The service-oriented analytic framework that underlies SRS provides a platform for deploying pervasive visual analytic environments across an enterprise. SRS represents a ldquolightweightrdquo approach to visual analytics whereby thin client analytic applications can be rapidly deployed in a platform-agnostic fashion. Client applications support multiple coordinated views while giving analysts the ability to record evidence, assumptions, hypotheses and other reasoning artifacts. We describe the capabilities of SRS in the context of a real-world deployment at a regional law enforcement organization.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677366",
            "id": "r_501",
            "s_ids": [
                "s_268",
                "s_1018",
                "s_252",
                "s_322",
                "s_480",
                "s_924",
                "s_792",
                "s_163",
                "s_518"
            ],
            "type": "rich",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models",
            "data": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030449",
            "id": "r_502",
            "s_ids": [
                "s_430",
                "s_1345",
                "s_486",
                "s_131",
                "s_273"
            ],
            "type": "rich",
            "x": 4.543193817138672,
            "y": 9.233100891113281
        },
        {
            "title": "GUIRO: User-Guided Matrix Reordering",
            "data": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934300",
            "id": "r_503",
            "s_ids": [
                "s_547",
                "s_511",
                "s_1409"
            ],
            "type": "rich",
            "x": 3.8788437843322754,
            "y": 7.368292331695557
        },
        {
            "title": "ICE: An Interactive Configuration Explorer for High Dimensional Categorical Parameter Spaces",
            "data": "There are many applications where users seek to explore the impact of the settings of several categorical variables with respect to one dependent numerical variable. For example, a computer systems analyst might want to study how the type of file system or storage device affects system performance. A usual choice is the method of Parallel Sets designed to visualize multivariate categorical variables, However, we found that the magnitude of the parameter impacts on the numerical variable cannot be easily observed here. We also attempted a dimension reduction approach based on Multiple Correspondence Analysis but found that the SVD-generated 2D layout resulted in a loss of information. We hence propose a novel approach, the Interactive Configuration Explorer (ICE), which directly addresses the need of analysts to learn how the dependent numerical variable is affected by the parameter settings given multiple optimization objectives. No information is lost as ICE shows the complete distribution and statistics of the dependent variable in context with each categorical variable. Analysts can interactively filter the variables to optimize for certain goals such as achieving a system with maximum performance, low variance, etc. Our system was developed in tight collaboration with a group of systems performance researchers and its final effectiveness was evaluated with expert interviews, a comparative user study, and two case studies.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986923",
            "id": "r_504",
            "s_ids": [
                "s_240",
                "s_1378",
                "s_1453",
                "s_1151",
                "s_254"
            ],
            "type": "rich",
            "x": 3.9564976692199707,
            "y": 7.766167163848877
        },
        {
            "title": "Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts",
            "data": "Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of ego-networks one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802415",
            "id": "r_505",
            "s_ids": [
                "s_1310",
                "s_469",
                "s_100"
            ],
            "type": "rich",
            "x": 5.187515735626221,
            "y": 5.082503795623779
        },
        {
            "title": "CRICTO: Supporting Sensemaking through Crowdsourced Information Schematization",
            "data": "We present CRICTO, a new crowdsourcing visual analytics environment for making sense of and analyzing text data, whereby multiple crowdworkers are able to parallelize the simple information schematization tasks of relating and connecting entities across documents. The diverse links from these schematization tasks are then automatically combined and the system visualizes them based on the semantic types of the linkages. CRICTO also includes several tools that allow analysts to interactively explore and refine crowdworkers' results to better support their own sensemaking processes. We evaluated CRICTO's techniques and analysis workflow with deployments of CRICTO using Amazon Mechanical Turk and a user study that assess the effect of crowdsourced schematization in sensemaking tasks. The results of our evaluation show that CRICTO's crowdsourcing approaches and workflow help analysts explore diverse aspects of datasets, and uncover more accurate hidden stories embedded in the text datasets.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585484",
            "id": "r_506",
            "s_ids": [
                "s_1423",
                "s_1016",
                "s_827",
                "s_1492"
            ],
            "type": "rich",
            "x": 6.9483442306518555,
            "y": 10.12969970703125
        },
        {
            "title": "An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging",
            "data": "For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042481",
            "id": "r_507",
            "s_ids": [
                "s_1243",
                "s_698",
                "s_128",
                "s_1062",
                "s_840",
                "s_628",
                "s_1480"
            ],
            "type": "rich",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "title": "Smart super views---A knowledge-assisted interface for medical visualization",
            "data": "Due to the ever growing volume of acquired data and information, users have to be constantly aware of the methods for their exploration and for interaction. Of these, not each might be applicable to the data at hand or might reveal the desired result. Owing to this, innovations may be used inappropriately and users may become skeptical. In this paper we propose a knowledge-assisted interface for medical visualization, which reduces the necessary effort to use new visualization methods, by providing only the most relevant ones in a smart way. Consequently, we are able to expand such a system with innovations without the users to worry about when, where, and especially how they may or should use them. We present an application of our system in the medical domain and give qualitative feedback from domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400555",
            "id": "r_508",
            "s_ids": [
                "s_1439",
                "s_581",
                "s_822",
                "s_124",
                "s_158",
                "s_927",
                "s_1232",
                "s_531"
            ],
            "type": "rich",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "title": "Matrix-based visual correlation analysis on large timeseries data",
            "data": "In recent years, the quantity of time series data generated in a wide variety of domains grown consistently. Thus, it is difficult for analysts to process and understand this overwhelming amount of data. In the specific case of time series data another problem arises: time series can be highly interrelated. This problem becomes even more challenging when a set of parameters influences the progression of a time series. However, while most visual analysis techniques support the analysis of short time periods, e.g. one day or one week, they fail to visualize large-scale time series, ranging over one year or more. In our approach we present a time series matrix visualization that tackles this problem. Its primary advantages are that it scales to a large number of time series with different start and end points and allows for the visual comparison / correlation analysis of a set of influencing factors. To evaluate our approach, we applied our technique to a real-world data set, showing the impact of local weather conditions on the efficiency of photovoltaic power plants.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400549",
            "id": "r_509",
            "s_ids": [
                "s_547",
                "s_326",
                "s_511",
                "s_1038",
                "s_85"
            ],
            "type": "rich",
            "x": 6.901823997497559,
            "y": 6.214321136474609
        },
        {
            "title": "EmailTime: Visual analytics of emails",
            "data": "Although the discovery and analysis of communication patterns in large and complex email datasets are difficult tasks, they can be a valuable source of information. This paper presents EmailTime's capabilities through several examples. EmailTime is a visual analysis of email correspondence patterns over the course of time that interactively portrays personal and interpersonal networks using the correspondence in the email dataset. We suggest that integrating both statistics and visualizations in order to display information about the email datasets may simplify its evaluation.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652968",
            "id": "r_510",
            "s_ids": [
                "s_443",
                "s_848",
                "s_1108"
            ],
            "type": "rich",
            "x": 7.692496299743652,
            "y": 8.206583023071289
        },
        {
            "title": "Visual readability analysis: How to make your writings easier to read",
            "data": "We present a tool that is specifically designed to support a writer in revising a draft-version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we therefore discuss a semi-automatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. The user can choose different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case-studies are presented that show the wide range of applicability of our tool.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652926",
            "id": "r_511",
            "s_ids": [
                "s_37",
                "s_676",
                "s_1336",
                "s_1038"
            ],
            "type": "rich",
            "x": 7.403539180755615,
            "y": 10.271008491516113
        },
        {
            "title": "VAST 2009 challenge: An insider threat",
            "data": "The 4&lt;sup&gt;th&lt;/sup&gt; VAST Challenge centered on a cyber analytics scenario and offered three mini-challenges with datasets of badge and network traffic data, a social network including geospatial information, and security video. Teams could also enter the Grand challenge which combined all three datasets. In this paper, we summarize the dataset, the overall scenario and the questions asked in the challenges. We describe the judging process and new infrastructure developed to manage the submissions and compute accuracy measures in the social network mini challenge. We received 49 entries from 30 teams, and gave 23 different awards to a total of 16 teams.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5334454",
            "id": "r_512",
            "s_ids": [
                "s_1205",
                "s_1244",
                "s_231",
                "s_1399"
            ],
            "type": "rich",
            "x": 6.889862060546875,
            "y": 8.712366104125977
        },
        {
            "title": "Innovative filtering techniques and customized analytics tools",
            "data": "The VAST 2009 Challenge consisted of three heterogeneous synthetic data sets organized into separate mini-challenges with minimal correspondence information. The challenge task was the identification of a suspected data theft from cyber and real-world traces. The grand challenge required integrating the findings from the mini challenges into a plausible, consistent scenario. A mixture of linked, customized tools based on queryable models and rapid prototyping as well as generic analysis tools (developed in-house) helped us correctly solve all of the mini challenges. A collaborative analytic process was employed to reconstruct the scenario and to propose the correct steps for the reliable identification of the criminal organization based on activity traces of its members.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5334300",
            "id": "r_513",
            "s_ids": [
                "s_214",
                "s_1102",
                "s_831",
                "s_937",
                "s_751",
                "s_914",
                "s_1304",
                "s_404"
            ],
            "type": "rich",
            "x": 6.911941051483154,
            "y": 8.79002857208252
        },
        {
            "title": "Professional analysts using a large, high-resolution display",
            "data": "Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5332485",
            "id": "r_514",
            "s_ids": [
                "s_1517",
                "s_1492",
                "s_1030",
                "s_202"
            ],
            "type": "rich",
            "x": 5.355887413024902,
            "y": 9.362960815429688
        },
        {
            "title": "Visual Analytics Approach to User-Controlled Evacuation Scheduling",
            "data": "Application of the ideas of visual analytics is a promising approach to supporting decision making, in particular, where the problems have geographic (or spatial) and temporal aspects. Visual analytics may be especially helpful in time-critical applications, which pose hard challenges to decision support. We have designed a suite of tools to support transportation-planning tasks such as emergency evacuation of people from a disaster- affected area. The suite combines a tool for automated scheduling based on a genetic algorithm with visual analytics techniques allowing the user to evaluate tool results and direct its work. A transportation schedule, which is generated by the tool, is a complex construct involving geographical space, time, and heterogeneous objects (people and vehicles) with states and positions varying in time. We apply task-analytical approach to design techniques that could effectively support a human planner in the analysis of this complex information H. 1.2 [User/Machine Systems]: Human information processing - Visual Analytics; 1.6.9 [Visualization]: information visualization.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4388995",
            "id": "r_515",
            "s_ids": [
                "s_1195",
                "s_11",
                "s_1013"
            ],
            "type": "rich",
            "x": 6.154789447784424,
            "y": 7.499543190002441
        },
        {
            "title": "MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on Multiple Scales",
            "data": "Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030386",
            "id": "r_516",
            "s_ids": [
                "s_573",
                "s_1504",
                "s_248",
                "s_334",
                "s_1038"
            ],
            "type": "rich",
            "x": 5.320591926574707,
            "y": 6.559731483459473
        },
        {
            "title": "STULL: Unbiased Online Sampling for Visual Exploration of Large Spatiotemporal Data",
            "data": "Online sampling-supported visual analytics is increasingly important, as it allows users to explore large datasets with acceptable approximate answers at interactive rates. However, existing online spatiotemporal sampling techniques are often biased, as most researchers have primarily focused on reducing computational latency. Biased sampling approaches select data with unequal probabilities and produce results that do not match the exact data distribution, leading end users to incorrect interpretations. In this paper, we propose a novel approach to perform unbiased online sampling of large spatiotemporal data. The proposed approach ensures the same probability of selection to every point that qualifies the specifications of a user\u2019s multidimensional query. To achieve unbiased sampling for accurate representative interactive visualizations, we design a novel data index and an associated sample retrieval plan. Our proposed sampling approach is suitable for a wide variety of visual analytics tasks, e.g., tasks that run aggregate queries of spatiotemporal data. Extensive experiments confirm the superiority of our approach over a state-of-the-art spatial online sampling technique, demonstrating that within the same computational time, data samples generated in our approach are at least 50% more accurate in representing the actual spatial distribution of the data and enable approximate visualizations to present closer visual appearances to the exact ones.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00012",
            "id": "r_517",
            "s_ids": [
                "s_1161",
                "s_217",
                "s_794",
                "s_639",
                "s_625",
                "s_856",
                "s_1394",
                "s_589",
                "s_952"
            ],
            "type": "rich",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "title": "A Visual Analytics Approach to Debugging Cooperative, Autonomous Multi-Robot Systems\u2019 Worldviews",
            "data": "Autonomous multi-robot systems, where a team of robots shares information to perform tasks that are beyond an individual robot\u2019s abilities, hold great promise for a number of applications, such as planetary exploration missions. Each robot in a multi-robot system that uses the shared-world coordination paradigm autonomously schedules which robot should perform a given task, and when, using its worldview\u2013the robot\u2019s internal representation of its belief about both its own state, and other robots\u2019 states. A key problem for operators is that robots\u2019 worldviews can fall out of sync (often due to weak communication links), leading to desynchronization of the robots\u2019 scheduling decisions and inconsistent emergent behavior (e.g., tasks not performed, or performed by multiple robots). Operators face the time-consuming and difficult task of making sense of the robots\u2019 scheduling decisions, detecting de-synchronizations, and pinpointing the cause by comparing every robot\u2019s worldview. To address these challenges, we introduce MOSAIC Viewer, a visual analytics system that helps operators (i) make sense of the robots\u2019 schedules and (ii) detect and conduct a root cause analysis of the robots\u2019 desynchronized worldviews. Over a year-long partnership with roboticists at the NASA Jet Propulsion Laboratory, we conduct a formative study to identify the necessary system design requirements and a qualitative evaluation with 12 roboticists. We find that MOSAIC Viewer is faster- and easier-to-use than the users\u2019 current approaches, and it allows them to stitch low-level details to formulate a high-level understanding of the robots\u2019 schedules and detect and pinpoint the cause of the desynchronized worldviews.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00008",
            "id": "r_518",
            "s_ids": [
                "s_542",
                "s_1228",
                "s_1497",
                "s_964",
                "s_699"
            ],
            "type": "rich",
            "x": 3.307056188583374,
            "y": 8.17342758178711
        },
        {
            "title": "Visual analytics of terrorist activities related to epidemics",
            "data": "The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102498",
            "id": "r_519",
            "s_ids": [
                "s_473",
                "s_1516",
                "s_884",
                "s_34",
                "s_833",
                "s_357",
                "s_444",
                "s_695",
                "s_675",
                "s_1337",
                "s_1309",
                "s_511",
                "s_904",
                "s_1145",
                "s_58",
                "s_944",
                "s_1038"
            ],
            "type": "rich",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "title": "KD-photomap: Exploring photographs in space and time",
            "data": "KD-photomap is a web-based visual analytics system for browsing collections of geotagged Flickr photographs in search of interesting pictures, places, and events. Spatial filtering of the data is performed through zooming, moving or searching along the map. Temporal filtering is possible through defining time windows using interactive histograms and calendar controls. Information about the number and spatiotemporal distribution of photos captured in an explored area is continuously provided using various visual cues.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102479",
            "id": "r_520",
            "s_ids": [
                "s_1043",
                "s_969",
                "s_223",
                "s_11",
                "s_1195"
            ],
            "type": "rich",
            "x": 7.01570463180542,
            "y": 6.53007698059082
        },
        {
            "title": "Find distance function, hide model inference",
            "data": "Faced with a large, high-dimensional dataset, many turn to data analysis approaches that they understand less well than the domain of their data. An expert's knowledge can be leveraged into many types of analysis via a domain-specific distance function, but creating such a function is not intuitive to do by hand. We have created a system that shows an initial visualization, adapts to user feedback, and produces a distance function as a result. Specifically, we present a multidimensional scaling (MDS) visualization and an iterative feedback mechanism for a user to affect the distance function that informs the visualization without having to adjust the parameters of the visualization directly. An encouraging experimental result suggests that using this tool, data attributes with useless data are given low importance in the distance function.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102478",
            "id": "r_521",
            "s_ids": [
                "s_272",
                "s_45",
                "s_333"
            ],
            "type": "rich",
            "x": 4.429103374481201,
            "y": 6.936128616333008
        },
        {
            "title": "TreeVersity: Comparing tree structures by topology and node's attributes differences",
            "data": "It is common to classify data in hierarchies, they provide a comprehensible way of understanding big amounts of data. From budgets to organizational charts or even the stock market, trees are everywhere and people find them easy to use. However when analysts need to compare two versions of the same tree structure, or two related taxonomies, the task is not so easy. Much work has been done on this topic, but almost all of it has been restricted to either compare the trees by topology, or by the node attribute values. With this project we are proposing TreeVersity, a framework for comparing tree structures, both by structural changes and by differences in the node attributes. This paper is based on our previous work on comparing traffic agencies using LifeFlow [1, 2] and on a first prototype of TreeVersity.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102471",
            "id": "r_522",
            "s_ids": [
                "s_392",
                "s_1370",
                "s_1399",
                "s_219"
            ],
            "type": "rich",
            "x": 3.320497751235962,
            "y": 7.061007022857666
        },
        {
            "title": "Poster: Visual prediction of time series",
            "data": "Many well-known time series prediction methods have been used daily by analysts making decisions. To reach a good prediction, we introduce several new visual analysis techniques of smoothing, multi-scaling, and weighted average with the involvement of human expert knowledge. We combine them into a well-fitted method to perform prediction. We have applied this approach to predict resource consumption in data center for next day planning.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333420",
            "id": "r_523",
            "s_ids": [
                "s_211",
                "s_653",
                "s_768",
                "s_1169",
                "s_1038",
                "s_846"
            ],
            "type": "rich",
            "x": 6.655723571777344,
            "y": 6.452548503875732
        },
        {
            "title": "Visual Analytics with Jigsaw",
            "data": "This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST '07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389017",
            "id": "r_524",
            "s_ids": [
                "s_657",
                "s_1115",
                "s_753",
                "s_179",
                "s_756"
            ],
            "type": "rich",
            "x": 6.468072891235352,
            "y": 9.141068458557129
        },
        {
            "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization",
            "data": "LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934658",
            "id": "r_525",
            "s_ids": [
                "s_691",
                "s_60",
                "s_1486",
                "s_138",
                "s_172"
            ],
            "type": "rich",
            "x": 3.408552408218384,
            "y": 7.584595680236816
        },
        {
            "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics",
            "data": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934264",
            "id": "r_526",
            "s_ids": [
                "s_389",
                "s_1394",
                "s_952",
                "s_72"
            ],
            "type": "rich",
            "x": 4.131429195404053,
            "y": 9.27343463897705
        },
        {
            "title": "AnaFe: Visual Analytics of Image-derived Temporal Features Focusing on the Spleen",
            "data": "We present a novel visualization framework, AnaFe, targeted at observing changes in the spleen over time through multiple image-derived features. Accurate monitoring of progressive changes is crucial for diseases that result in enlargement of the organ. Our system is comprised of multiple linked views combining visualization of temporal 3D organ data, related measurements, and features. Thus it enables the observation of progression and allows for simultaneous comparison within and between the subjects. AnaFe offers insights into the overall distribution of robustly extracted and reproducible quantitative imaging features and their changes within the population, and also enables detailed analysis of individual cases. It performs similarity comparison of temporal series of one subject to all other series in both sick and healthy groups. We demonstrate our system through two use case scenarios on a population of 189 spleen datasets from 68 subjects with various conditions observed over time.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598463",
            "id": "r_527",
            "s_ids": [
                "s_149",
                "s_566",
                "s_750",
                "s_623"
            ],
            "type": "rich",
            "x": 5.92241907119751,
            "y": 6.868464469909668
        },
        {
            "title": "A System for Visual Analysis of Radio Signal Data",
            "data": "Analysis of radio transmissions is vital for military defense as it provides valuable information about enemy communication and infrastructure. One challenge to the data analysis task is that there are far too many signals for analysts to go through by hand. Even typical signal meta data (such as frequency band, duration, and geographic location) can be overwhelming. In this paper, we present a system for exploring and analyzing such radio signal meta-data. Our system incorporates several visual representations for signal data, designed for readability and ease of comparison, as well as novel algorithms for extracting and classifying consistent signal patterns. We demonstrate the effectiveness of our system using data collected from real missions with an airborne sensor platform.",
            "url": "http://dx.doi.org/10.1109/VAST.2014.7042479",
            "id": "r_528",
            "s_ids": [
                "s_1007",
                "s_1041",
                "s_699"
            ],
            "type": "rich",
            "x": 6.586615085601807,
            "y": 7.37317419052124
        },
        {
            "title": "Visualising variations in household energy consumption",
            "data": "There is limited understanding of the relationship between neighbourhoods, demographic characteristics and domestic energy consumption habits. We report upon research that combines datasets relating to household energy use with geodemographics to enable better understanding of UK energy user types. A novel interactive interface is planned to evaluate the performance of specifically created energy-based data classifications. The research aims to help local governments and the energy industry in targeting households and populations for new energy saving schemes and in improving efforts to promote sustainable energy consumption. The new classifications may also stimulate consumption awareness amongst domestic users. This poster reports on initial visual findings and describes the research methodology, data sources and future visualisation requirements.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400545",
            "id": "r_529",
            "s_ids": [
                "s_84",
                "s_514"
            ],
            "type": "rich",
            "x": 5.8617682456970215,
            "y": 8.003876686096191
        },
        {
            "title": "Using translational science in visual analytics",
            "data": "We introduce translational science, a research discipline from medicine, and show how adapting it for visual analytics can improve the design and evaluation of visual analytics interfaces. Translational science \u201ctranslates\u201d knowledge from the lab to the real-world to \u201cground truth\u201d by incorporating a 3 phase program of research. Phase 1 &amp; 2 include protocols for research in the lab and field and Phase 3 focuses on dissemination and documentation. We discuss these phases and how they may be applied to visual analytics research.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400543",
            "id": "r_530",
            "s_ids": [
                "s_627",
                "s_1047"
            ],
            "type": "rich",
            "x": 6.15588903427124,
            "y": 10.035637855529785
        },
        {
            "title": "Visual exploration of local interest points in sets of time series",
            "data": "Visual analysis of time series data is an important, yet challenging task with many application examples in fields such as financial or news stream data analysis. Many visual time series analysis approaches consider a global perspective on the time series. Fewer approaches consider visual analysis of local patterns in time series, and often rely on interactive specification of the local area of interest. We present initial results of an approach that is based on automatic detection of local interest points. We follow an overview-first approach to find useful parameters for the interest point detection, and details-on-demand to relate the found patterns. We present initial results and detail possible extensions of the approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400534",
            "id": "r_531",
            "s_ids": [
                "s_511",
                "s_1234",
                "s_906",
                "s_893",
                "s_99",
                "s_241",
                "s_609"
            ],
            "type": "rich",
            "x": 6.885515213012695,
            "y": 6.168759346008301
        },
        {
            "title": "A state transition approach to understanding users' interactions",
            "data": "Understanding users' interactions is considered as one of the important research topics in visual analytics. Although numerous empirical user studies have been performed to understand a user's interaction, a limited study has been successful in connecting the user's interaction to his/her reasoning. In this paper, we present an approach of understanding experts' interactive analysis by connecting their interactions to conclusions (i.e. findings) through a state transition approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102476",
            "id": "r_532",
            "s_ids": [
                "s_596",
                "s_1467",
                "s_233",
                "s_333"
            ],
            "type": "rich",
            "x": 5.313672065734863,
            "y": 10.065361976623535
        },
        {
            "title": "Analysts aren't machines: Inferring frustration through visualization interaction",
            "data": "Recent work in visual analytics has explored the extent to which information regarding analyst action and reasoning can be inferred from interaction. However, these methods typically rely on humans instead of automatic extraction techniques. Furthermore, there is little discussion regarding the role of user frustration when interacting with a visual interface. We demonstrate that automatic extraction of user frustration is possible given action-level visualization interaction logs. An experiment is described which collects data that accurately reflects user emotion transitions and corresponding interaction sequences. This data is then used in building HiddenMarkov Models (HMMs) which statistically connect interaction events with frustration. The capabilities of HMMs in predicting user frustration are tested using standard machine learning evaluation methods. The resulting classifier serves as a suitable predictor of user frustration that performs similarly across different users and datasets.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102473",
            "id": "r_533",
            "s_ids": [
                "s_120",
                "s_305",
                "s_403",
                "s_233",
                "s_1368"
            ],
            "type": "rich",
            "x": 5.001984596252441,
            "y": 10.0775146484375
        },
        {
            "title": "ALIDA: Using machine learning for intent discernment in visual analytics interfaces",
            "data": "In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5650854",
            "id": "r_534",
            "s_ids": [
                "s_627",
                "s_721",
                "s_416"
            ],
            "type": "rich",
            "x": 6.003656387329102,
            "y": 9.494745254516602
        },
        {
            "title": "Cell phone mini challenge award: Intuitive social network graphs visual analytics of cell phone data using mobivis and ontovis",
            "data": "MobiVis is a visual analytics tools to aid in the process of processing and understanding complex relational data, such as social networks. At the core of these tools is the ability to filter complex networks structurally and semantically, which helps us discover clusters and patterns in the organization of social networks. Semantic filtering is obtained via an ontology graph, based on another visual analytics tool, called OntoVis. In this summary, we describe how these tools where used to analyze one of the mini-challenges of the 2008 VAST challenge.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677391",
            "id": "r_535",
            "s_ids": [
                "s_883",
                "s_1007",
                "s_1041",
                "s_1033",
                "s_494",
                "s_109",
                "s_699"
            ],
            "type": "rich",
            "x": 4.940927505493164,
            "y": 5.271132946014404
        },
        {
            "title": "Grand challenge award 2008: Support for diverse analytic techniques - nSpace2 and GeoTime visual analytics",
            "data": "GeoTime and nSpace2 are interactive visual analytics tools that were used to examine and interpret all four of the 2008 VAST Challenge datasets. GeoTime excels in visualizing event patterns in time and space, or in time and any abstract landscape, while nSpace2 is a web-based analytical tool designed to support every step of the analytical process. nSpace2 is an integrating analytic environment. This paper highlights the VAST analytical experience with these tools that contributed to the success of these tools and this team for the third consecutive year.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677385",
            "id": "r_536",
            "s_ids": [
                "s_707",
                "s_786",
                "s_668",
                "s_366",
                "s_796"
            ],
            "type": "rich",
            "x": 6.673811435699463,
            "y": 8.611976623535156
        },
        {
            "title": "Supporting exploration awareness for visual analytics",
            "data": "While exploring data using information visualization, analysts try to make sense of the data, build cases, and present them to others. However, if the exploration is long or done in multiple sessions, it can be hard for analysts to remember all interesting visualizations and the relationships among them they have seen. Often, they will see the same or similar visualizations, and are unable to recall when, why and how they have seen something similar. Recalling and retrieving interesting visualizations are important tasks for the analysis processes such as problem solving, reasoning, and conceptualization. In this paper, we argue that offering support for thinking based on past analysis processes is important, and present a solution for this.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677378",
            "id": "r_537",
            "s_ids": [
                "s_754",
                "s_63"
            ],
            "type": "rich",
            "x": 5.27301549911499,
            "y": 9.239537239074707
        },
        {
            "title": "Generating hypotheses of trends in high-dimensional data skeletons",
            "data": "We seek an information-revealing representation for high-dimensional data distributions that may contain local trends in certain subspaces. Examples are data that have continuous support in simple shapes with identifiable branches. Such data can be represented by a graph that consists of segments of locally fit principal curves or surfaces summarizing each identifiable branch. We describe a new algorithm to find the optimal paths through such a principal graph. The paths are optimal in the sense that they represent the longest smooth trends through the data set, and jointly they cover the data set entirely with minimum overlap. The algorithm is suitable for hypothesizing trends in high-dimensional data, and can assist exploratory data analysis and visualization.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677367",
            "id": "r_538",
            "s_ids": [
                "s_747",
                "s_94",
                "s_450"
            ],
            "type": "rich",
            "x": 4.702369213104248,
            "y": 6.508816719055176
        },
        {
            "title": "C-GROUP: A Visual Analytic Tool for Pairwise Analysis of Dynamic Group Membership",
            "data": "C-GROUP is a tool for analyzing dynamic group membership in social networks over time. Unlike most network visualization tools, which show the group structure within an entire network, or the group membership for a single actor, C-GROUP allows users to focus their analysis on a pair of individuals of interest. And unlike most dynamic social network visualization tools, which focus on the addition and deletion of nodes (actors) and edges (relationships) over time, C-GROUP focuses on changing group memberships over time. C-GROUP provides users with a flexible interface for defining (and redefining) groups interactively, and allows users to view the changing group memberships for the pair over time. This helps to highlight the similarities and differences between the individuals and their evolving group memberships. C-GROUP allows users to dynamically select the time granularity of the temporal evolution and supports two novel visual representations of the evolving group memberships. This flexibility gives users alternate views that are appropriate for different network sizes and provides users with different insights into the grouping behavior.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389022",
            "id": "r_539",
            "s_ids": [
                "s_485",
                "s_145",
                "s_151"
            ],
            "type": "rich",
            "x": 5.308221340179443,
            "y": 5.474822998046875
        },
        {
            "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
            "data": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028958",
            "id": "r_540",
            "s_ids": [
                "s_183",
                "s_42",
                "s_401",
                "s_940",
                "s_721"
            ],
            "type": "rich",
            "x": 4.645948886871338,
            "y": 7.92765474319458
        },
        {
            "title": "MetricsVis: A Visual Analytics System for Evaluating Employee Performance in Public Safety Agencies",
            "data": "Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934603",
            "id": "r_541",
            "s_ids": [
                "s_1032",
                "s_1394",
                "s_442",
                "s_795",
                "s_979",
                "s_952"
            ],
            "type": "rich",
            "x": 5.395201206207275,
            "y": 9.531249046325684
        },
        {
            "title": "Visual data quality analysis for taxi GPS data",
            "data": "We present a novel visual analysis method to systematically discover data quality problems in raw taxi GPS data. It combines semi-supervised active learning and interactive visual exploration. It helps analysts interactively discover unknown data quality problems, and automatically extract known problems. We report analysis results on Beijing taxi GPS data.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347689",
            "id": "r_542",
            "s_ids": [
                "s_1163",
                "s_1274",
                "s_82",
                "s_685",
                "s_327",
                "s_1262",
                "s_1528",
                "s_277",
                "s_1060"
            ],
            "type": "rich",
            "x": 7.030353546142578,
            "y": 4.95527458190918
        },
        {
            "title": "Sequencing of categorical time series",
            "data": "Exploring and comparing categorical time series and finding temporal patterns are complex tasks in the field of time series data mining. Although different analysis approaches exist, these tasks remain challenging, especially when numerous time series are considered at once. We propose a visual analysis approach that supports exploring such data by ordering time series in meaningful ways. We provide interaction techniques to steer the automated arrangement and to allow users to investigate patterns in detail.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347684",
            "id": "r_543",
            "s_ids": [
                "s_839",
                "s_537",
                "s_1193",
                "s_757"
            ],
            "type": "rich",
            "x": 6.887332916259766,
            "y": 6.183637619018555
        },
        {
            "title": "Interactive semi-automatic categorization for spinel group minerals",
            "data": "Spinel group minerals are excellent indicators of geological environments (tectonic settings). In 2001, Barnes and Roeder defined a set of contours corresponding to compositional fields for spinel group minerals. Geologists typically use this contours to estimate the tectonic environment where a particular spinel composition could have been formed. This task is prone to errors and requires tedious manual comparison of overlapping diagrams. We introduce a semi-automatic, interactive detection of tectonic settings for an arbitrary dataset based on the Barnes and Roeder contours. The new approach integrates the mentioned contours and includes a novel interaction called contour brush. The new methodology is integrated in the Spinel Explorer system and it improves the scientist's workflow significantly.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347676",
            "id": "r_544",
            "s_ids": [
                "s_622",
                "s_282",
                "s_732",
                "s_175",
                "s_994",
                "s_531",
                "s_840"
            ],
            "type": "rich",
            "x": 5.519930362701416,
            "y": 6.475109100341797
        },
        {
            "title": "A software developer's guide to informal evaluation of Visual Analytics environments using VAST Challenge information",
            "data": "The VAST Challenge has been a popular venue for academic and industry participants for over ten years. Many participants comment that the majority of their time in preparing VAST Challenge entries is discovering elements in their software environments that need to be redesigned in order to solve the given task. Fortunately, there is no need to wait until the VAST Challenge is announced to test out software systems. The Visual Analytics Benchmark Repository contains all past VAST Challenge tasks, data, solutions and submissions. In this poster we describe how developers can perform informal evaluations of various aspects of their visual analytics environments using VAST Challenge information.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347674",
            "id": "r_545",
            "s_ids": [
                "s_684",
                "s_1244",
                "s_231"
            ],
            "type": "rich",
            "x": 6.792044639587402,
            "y": 8.776833534240723
        },
        {
            "title": "FPSSeer: Visual analysis of game frame rate data",
            "data": "The rate at which frames are rendered in a computer game directly influences both game playability and enjoyability. Players frequently have to deal with the trade-off between high frame rates and good resolution. Analyzing patterns in frame rate data and their correlation with the overall game performance is important in designing games (e.g., graphic card/display setting suggestion and game performance measurement). However, this task is challenging because game frame rates vary both temporally and spatially. In addition, players may adjust their display settings based on their gaming experience and hardware conditions, which further contributes to the unpredictability of frame rates. In this paper, we present a comprehensive visual analytics system FPSSeer, to help game designers gain insight into frame rate data. Our system consists of four major views: 1) a frame rate view to show the overall distribution in a geographic scale, 2) a grid view to show the frame rate distribution and grid element clusters based on their similarity, 3) a FootRiver view to reveal the temporal patterns in game condition changes and potential spatiotemporal correlations, and 4) a comparison view to evaluate game performance discrepancy under different game tests. The real-world case studies demonstrate the effectiveness of our system. The system has been applied to an online commercial game to monitor its performance and to provide feedbacks to designers and developers.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347633",
            "id": "r_546",
            "s_ids": [
                "s_492",
                "s_1332",
                "s_131"
            ],
            "type": "rich",
            "x": 8.474620819091797,
            "y": 6.063738822937012
        },
        {
            "title": "A visual analytics approach to understanding cycling behaviour",
            "data": "Existing research into cycling behaviours has either relied on detailed ethnographic studies or larger public attitude surveys [1] [9]. Instead, following recent contributions from information visualization [13] and data mining [5] [7], this design study uses visual analytics techniques to identify, describe and explain cycling behaviours within a large and attribute rich transactional dataset. Using data from London's bike share scheme&lt;sup&gt;1&lt;/sup&gt;, customer level classifications will be created, which consider the regularity of scheme use, journey length and travel times. Monitoring customer usage over time, user classifications will attend to the dynamics of cycling behaviour, asking substantive questions about how behaviours change under varying conditions. The 3-year PhD project will contribute to academic and strategic discussions around sustainable travel policy. A programme of research is outlined, along with an early visual analytics prototype for rapidly querying customer journeys.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400550",
            "id": "r_547",
            "s_ids": [
                "s_1240",
                "s_1126",
                "s_212"
            ],
            "type": "rich",
            "x": 7.270965099334717,
            "y": 4.867156505584717
        },
        {
            "title": "Priming Locus of Control to affect performance",
            "data": "Recent research suggests that the personality trait Locus of Control (LOC) can be a reliable predictor of performance when learning a new visualization tool. While these results are compelling and have direct implications to visualization design, the relationship between a user's LOC measure and their performance is not well understood. We hypothesize that there is a dependent relationship between LOC and performance; specifically, a person's orientation on the LOC scale directly influences their performance when learning new visualizations. To test this hypothesis, we conduct an experiment with 300 subjects using Amazon's Mechanical Turk. We adapt techniques from personality psychology to manipulate a user's LOC so that users are either primed to be more internally or externally oriented on the LOC scale. Replicating previous studies investigating the effect of LOC on performance, we measure users' speed and accuracy as they use visualizations with varying visual metaphors. Our findings demonstrate that changing a user's LOC impacts their performance. We find that a change in users' LOC results in performance changes.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400535",
            "id": "r_548",
            "s_ids": [
                "s_778",
                "s_865",
                "s_855",
                "s_333"
            ],
            "type": "rich",
            "x": 4.6886067390441895,
            "y": 10.004459381103516
        },
        {
            "title": "Pexel and heatmap visual analysis of multidimensional gun/homicide data",
            "data": "We present a visual analysis tool for mining correlations in county-level, multidimensional gun/homicide data. The tool uses 2D pexels, heatmaps, linked-views, dynamic queries and details-on-demand to analyze annual county-level data on firearm homicide rates and gun availability, as well as various socio-demographic measures. A statistical significance filter was implemented as a visual means to validate exploratory hypotheses. Results from expert evaluations indicate that our methods outperform typical graphical techniques used by statisticians, such as bar graphs, scatterplots and residual plots, to show spatial and temporal relationships. Our visualization has the potential to convey the impact of gun availability on firearm homicides to the public health arena and the general public.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102482",
            "id": "r_549",
            "s_ids": [
                "s_59",
                "s_586",
                "s_476"
            ],
            "type": "rich",
            "x": 6.26613712310791,
            "y": 6.6600165367126465
        },
        {
            "title": "Exploring proportions: Comparative visualization of categorical data",
            "data": "This poster describes an approach to facilitate comparisons in multi-dimensional categorical data. The key idea is to represent over- or under-proportional relationships explicitly. On an overview level, the visualization of various measures conveys pair-wise relationships between categorical dimensions. For more details, interaction supports to relate a single category to all categories of multiple dimensions. We discuss methods for representing relationships and visualization-driven strategies for ordering dimensions and categories, and we illustrate the approach by means of data from a social survey.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102481",
            "id": "r_550",
            "s_ids": [
                "s_275",
                "s_1311"
            ],
            "type": "rich",
            "x": 5.083412170410156,
            "y": 6.22314453125
        },
        {
            "title": "A continuous analysis process between desktop and collaborative visual analytics environments",
            "data": "Since its inception, the field of visual analytics has undergone tremendous growth in understanding how to create interactive visual tools to solve analytical problems. However, with few exceptions, most of these tools have been designed for single users in desktop environments. While often effective on their own, most single-user systems do not reflect the collaborative nature of solving real-world analytical tasks. Many intelligence analysts, for example, have been observed to switch repeatedly between working alone and collaborating with members of a small team. In this paper, we propose that a complete visual analytical system designed for solving real-world tasks ought to have two integrated components: a single-user desktop system and a mirroring system suitable for a collaborative environment.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5652958",
            "id": "r_551",
            "s_ids": [
                "s_596",
                "s_224",
                "s_658",
                "s_233",
                "s_333"
            ],
            "type": "rich",
            "x": 5.243402004241943,
            "y": 10.550222396850586
        },
        {
            "title": "Visual analysis of frequent patterns in large time series",
            "data": "The detection of previously unknown, frequently occurring patterns in time series, often called motifs, has been recognized as an important task. To find these motifs, we use an advanced temporal data mining algorithm. Since our algorithm usually finds hundreds of motifs, we need to analyze and access the discovered motifs. For this purpose, we introduce three novel visual analytics methods: (1) motif layout, using colored rectangles for visualizing the occurrences and hierarchical relationships of motifs in a multivariate time series, (2) motif distortion, for enlarging or shrinking motifs as appropriate for easy analysis and (3) motif merging, to combine a number of identical adjacent motif instances without cluttering the display. We have applied and evaluated our methods using two real-world data sets: data center cooling and oil well production.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5650766",
            "id": "r_552",
            "s_ids": [
                "s_211",
                "s_168",
                "s_1531",
                "s_1038",
                "s_1169",
                "s_768",
                "s_647",
                "s_1461"
            ],
            "type": "rich",
            "x": 6.74947452545166,
            "y": 6.071475505828857
        },
        {
            "title": "A radial visualization tool for depicting hierarchically structured video content",
            "data": "The visual analysis of video content is an important research topic due to the huge amount of video data that is generated every day. Annotating this data will become a major problem since the amount of videos further increases. With this work we introduce a system that combines a visualization tool with automatic video segmentation techniques and a characteristic key-frame extraction. A summary of the content of a whole video in one view is realized. Furthermore, the user can interactively browse through the video via our visualization interface to get more detailed information. The system is adapted to two application scenarios and a third application is discussed for future work.",
            "url": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5650177",
            "id": "r_553",
            "s_ids": [
                "s_99",
                "s_85"
            ],
            "type": "rich",
            "x": 5.932639122009277,
            "y": 8.259029388427734
        },
        {
            "title": "Using projection and 2D plots to visually reveal genetic mechanisms of complex human disorders",
            "data": "Gene mapping is a statistical method used to localize human disease genes to particular regions of the human genome. When performing such analysis, a genetic likelihood space is generated and sampled, which results in a multidimensional scalar field. Researchers are interested in exploring this likelihood space through the use of visualization. Previous efforts at visualizing this space, though, were slow and cumbersome, only showing a small portion of the space at a time, thus requiring the user to keep a mental picture of several views. We have developed a new technique that displays much more data at once by projecting the multidimensional data into several 2D plots. One plot is created for each parameter that shows the change along that parameter. A radial projection is used to create another plot that provides an overview of the high dimensional surface from the perspective of a single point. Linking and brushing between all the plots are used to determine relationships between parameters. We demonstrate our techniques on real world autism data, showing how to visually examine features of the high dimensional space.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333917",
            "id": "r_554",
            "s_ids": [
                "s_1017",
                "s_1049",
                "s_363",
                "s_760"
            ],
            "type": "rich",
            "x": 4.628226280212402,
            "y": 6.6852898597717285
        },
        {
            "title": "Articulate: a conversational interface for visual analytics",
            "data": "While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333099",
            "id": "r_555",
            "s_ids": [
                "s_1328",
                "s_1506",
                "s_770",
                "s_1136"
            ],
            "type": "rich",
            "x": 5.082072734832764,
            "y": 9.60634708404541
        },
        {
            "title": "Using SocialAction to uncover structure in social networks over time",
            "data": "I describe how SocialAction was used to find insights in an evolving social structure VAST Challenge 2008psilas Mini-Challenge 3. This analysis and SocialAction were given the award, ldquoCell Phone Mini Challenge Award: Time Visualizations of Cell Phone Activityrdquo.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677392",
            "id": "r_556",
            "s_ids": [
                "s_1177"
            ],
            "type": "rich",
            "x": 6.996157646179199,
            "y": 8.599920272827148
        },
        {
            "title": "VisPad: Integrating Visualization, Navigation and Synthesis",
            "data": "We present a new framework - VisPad - to support the user to revisit the visual exploration process, and to synthesize and disseminate information. It offers three integrated views. The data view allows the user to interactively explore the data. The navigation view captures the exploration process. It enables the user to revisit any particular state and reuse it. The knowledge view enables the user to record his/her findings and the relations between these findings.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389021",
            "id": "r_557",
            "s_ids": [
                "s_754",
                "s_63"
            ],
            "type": "rich",
            "x": 5.974727630615234,
            "y": 9.443111419677734
        },
        {
            "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning",
            "data": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934289",
            "id": "r_558",
            "s_ids": [
                "s_815",
                "s_793",
                "s_1203",
                "s_63",
                "s_905"
            ],
            "type": "rich",
            "x": 6.628056526184082,
            "y": 6.06528377532959
        },
        {
            "title": "Tell me what do you see: Detecting perceptually-separable visual patterns via clustering of image-space features in visualizations",
            "data": "Visualization helps users infer structures and relationships in the data by encoding information as visual features that can be processed by the human visual-perceptual system. However, users would typically need to expend significant effort to scan and analyze a large number of views before they can begin to recognize relationships in a visualization. We propose a technique to partially automate the process of analyzing visualizations. By deriving and analyzing image-space features from visualizations, we can detect perceptually-separable patterns in the information space. We summarize these patterns with a tree-based meta-visualization and present it to the user to aid exploration. We illustrate this technique with an example scenario involving the analysis of census data.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347683",
            "id": "r_559",
            "s_ids": [
                "s_1015",
                "s_1498",
                "s_1506",
                "s_666"
            ],
            "type": "rich",
            "x": 4.927656650543213,
            "y": 6.493618965148926
        },
        {
            "title": "Visual scalability of spatial ensemble uncertainty",
            "data": "Weather Research and Forecasting (WRF) models simulate weather conditions by generating 2D numerical weather prediction ensemble members either through perturbing initial conditions or by changing different parameterization schemes, e.g., cumulus and microphysics schemes. These simulations are often used by weather analysts to analyze the nature of uncertainty attributed by these simulations to forecast weather conditions with good accuracy. The number of simulations used for forecasting is growing with the advent of increase in computing power. Hence, there is a need for providing better visual insights of uncertainty with growing number of ensemble members. We propose a geo visual analytical framework that uses visual analytics approach to resolve visual scalability of these ensemble members. Our approach naturally fits with the workflow of an analyst analyzing ensemble spatial uncertainty. Meteorologists evaluated our framework qualitatively and found it to be effective in acquiring insights of spatial uncertainty associated with multiple ensemble runs that are simulated using multiple parameterization schemes.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347671",
            "id": "r_560",
            "s_ids": [
                "s_853",
                "s_178",
                "s_670",
                "s_1360",
                "s_10"
            ],
            "type": "rich",
            "x": 6.29480504989624,
            "y": 6.361831188201904
        },
        {
            "title": "SocialNetSense: Supporting sensemaking of social and structural features in networks with interactive visualization",
            "data": "Increasingly, social network datasets contain social attribute information about actors and their relationship. Analyzing such network with social attributes requires making sense of not only its structural features, but also the relationship between social features in attributes and network structures. Existing social network analysis tools are usually weak in supporting complex analytical tasks involving both structural and social features, and often overlook users' needs for sensemaking tools that help to gather, synthesize, and organize information of these features. To address these challenges, we propose a sensemaking framework of social-network visual analytics in this paper. This framework considers both bottom-up processes, which are about constructing new understandings based on collected information, and top-down processes, which concern using prior knowledge to guide information collection, in analyzing social networks from both social and structural perspectives. The framework also emphasizes the externalization of sensemaking processes through interactive visualization. Guided by the framework, we develop a system, SocialNetSense, to support the sensemaking in visual analytics of social networks with social attributes. The example of using our system to analyze a scholar collaboration network shows that our approach can help users gain insight into social networks both structurally and socially, and enhance their process awareness in visual analytics.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400558",
            "id": "r_561",
            "s_ids": [
                "s_116",
                "s_1405",
                "s_457",
                "s_35"
            ],
            "type": "rich",
            "x": 4.980503082275391,
            "y": 5.211485862731934
        },
        {
            "title": "Incorporating GOMS analysis into the design of an EEG data visual analysis tool",
            "data": "In this paper, we present a case study where we incorporate GOMS (Goals, Operators, Methods, and Selectors) [2] task analysis into the design process of a visual analysis tool. We performed GOMS analysis on an Electroencephalography (EEG) analyst's current data analysis strategy to identify important user tasks and unnecessary user actions in his current workflow. We then designed an EEG data visual analysis tool based on the GOMS analysis result. Evaluation results show that the tool we have developed, EEGVis, allows the user to analyze EEG data with reduced subjective cognitive load, faster speed and increased confidence in the analysis quality. The positive evaluation results suggest that our design process demonstrates an effective application of GOMS analysis to discover opportunities for designing better tools to support the user's visual analysis process.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400542",
            "id": "r_562",
            "s_ids": [
                "s_694",
                "s_1200",
                "s_1194"
            ],
            "type": "rich",
            "x": 5.303769588470459,
            "y": 9.71528148651123
        },
        {
            "title": "Exploring cyber physical data streams using Radial Pixel Visualizations",
            "data": "Cyber physical systems (CPS), such as smart buildings and data centers, are richly instrumented systems composed of tightly coupled computational and physical elements that generate large amounts of data. To explore CPS data and obtain actionable insights, we construct a Radial Pixel Visualization (RPV) system, which uses multiple concentric rings to show the data in a compact circular layout of small polygons (pixel cells), each of which represents an individual data value. RPV provides an effective visual representation of locality and periodicity of the high volume, multivariate data streams, and seamlessly combines them with the results of an automated analysis. In the outermost ring the results of correlation analysis and peak point detection are highlighted. Our explorations demonstrates how RPV can help administrators to identify periodic thermal hot spots, understand data center energy consumption, and optimize IT workload.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400541",
            "id": "r_563",
            "s_ids": [
                "s_211",
                "s_168",
                "s_890",
                "s_653",
                "s_1038",
                "s_1169",
                "s_615",
                "s_332",
                "s_610",
                "s_989",
                "s_1190"
            ],
            "type": "rich",
            "x": 4.998837947845459,
            "y": 7.000543594360352
        },
        {
            "title": "The spatiotemporal multivariate hypercube for discovery of patterns in event data",
            "data": "Event data can hold valuable decision making information, yet detecting interesting patterns in this type of data is not an easy task because the data is usually rich and contains spatial, temporal as well as multivariate dimensions. Research into visual analytics tools to support the discovery of patterns in event data often focuses on the spatiotemporal or spatiomultivariate dimension of the data only. Few research efforts focus on all three dimensions in one framework. An integral view on all three dimensions is, however, required to unlock the full potential of event datasets. In this poster, we present an event visualization, transition, and interaction framework that enables an integral view on all dimensions of spatiotemporal multivariate event data. The framework is built around the notion that the event data space can be considered a spatiotemporal multivariate hypercube. Results of a case study we performed suggest that a visual analytics tool based on the proposed framework is indeed capable to support users in the discovery of multidimensional spatiotemporal multivariate patterns in event data.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400536",
            "id": "r_564",
            "s_ids": [
                "s_655",
                "s_1206"
            ],
            "type": "rich",
            "x": 7.504979133605957,
            "y": 6.80190372467041
        },
        {
            "title": "LensingWikipedia: Parsing text for the interactive visualization of human history",
            "data": "Extracting information from text is challenging. Most current practices treat text as a bag of words or word clusters, ignoring valuable linguistic information. Leveraging this linguistic information, we propose a novel approach to visualize textual information. The novelty lies in using state-of-the-art Natural Language Processing (NLP) tools to automatically annotate text which provides a basis for new and powerful interactive visualizations. Using NLP tools, we built a web-based interactive visual browser for human history articles from Wikipedia.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400530",
            "id": "r_565",
            "s_ids": [
                "s_216",
                "s_1208",
                "s_1112",
                "s_551"
            ],
            "type": "rich",
            "x": 7.595698833465576,
            "y": 10.46611499786377
        },
        {
            "title": "Analyst's workspace: Protecting vastopolis",
            "data": "Analyst's Workspace is a sensemaking environment designed specifically for use of large, high-resolution displays. It employs a spatial workspace to integrate foraging and synthesis activities into a unified process. In this paper we describe how Analyst's Workspace solved the VAST 2011 mini-challenge #3 and discuss some of the unique features of the environment.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102495",
            "id": "r_566",
            "s_ids": [
                "s_1492",
                "s_1438",
                "s_283",
                "s_1461",
                "s_202"
            ],
            "type": "rich",
            "x": 5.472451210021973,
            "y": 10.237101554870605
        },
        {
            "title": "MobileAnalymator: Animating data changes on mobile devices",
            "data": "MobileAnalymator (Mobile Analysis Animator) is a visual analytic system designed to analyze geospatial-temporal data on mobile devices. The system is an Internet based application that allows analysts to work in flexile enviornments at anytime. Its client side is developed by Adobe Flash to animate and interact with data. The server side uses Java and MySQL to query, compute, and serve data. The analyst can run the analytical task from a tablet (or computer) with Internet connection. MobileAnalymator adopted spatial and temporal autocorrelations in the interface design and integrated tangible interaction in the navigation to support analysis process.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102490",
            "id": "r_567",
            "s_ids": [
                "s_388",
                "s_979",
                "s_148"
            ],
            "type": "rich",
            "x": 5.962850570678711,
            "y": 9.402202606201172
        },
        {
            "title": "epSpread - Storyboarding for visual analytics",
            "data": "We present epSpread, an analysis and storyboarding tool for geolocated microblogging data. Individual time points and ranges are analysed through queries, heatmaps, word clouds and streamgraphs. The underlying narrative is shown on a storyboard-style timeline for discussion, refinement and presentation. The tool was used to analyse data from the VAST Challenge 2011 Mini-Challenge 1, tracking the spread of an epidemic using microblogging data. In this article we describe how the tool was used to identify the origin and track the spread of the epidemic.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102489",
            "id": "r_568",
            "s_ids": [
                "s_453",
                "s_1532",
                "s_184",
                "s_417",
                "s_13",
                "s_813",
                "s_1245"
            ],
            "type": "rich",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "title": "PORGY: Interactive and visual reasoning with graph rewriting systems",
            "data": "Graph rewriting systems are easily described and explained. They can be seen as a game where one iterates transformation rules on an initial graph, until some condition is met. A rule describes a local pattern (i.e. a subgraph) that must be identified in a graph and specifies how to transform this subgraph. The graph rewriting formalism is at the same time extremely rich and complex, making the study of a model expressed in terms of graph rewriting quite challenging. For instance, predicting whether rules can be applied in any order is often difficult. When modelling complex systems, graphical formalisms have clear advantages: they are more intuitive and make it easier to visualize a system and convey intuitions about it. This work focuses on the design of an interactive visual graph rewriting system which supports graphical manipulations and computation to reason and simulate on a system. PORGY has been designed based on regular exchanges with graph rewriting systems experts and users over the past three years. The design choices relied on a careful methodology inspired from Munzner's nested process model for visualization design and validation [4].",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102480",
            "id": "r_569",
            "s_ids": [
                "s_53",
                "s_308",
                "s_93"
            ],
            "type": "rich",
            "x": 4.793405055999756,
            "y": 5.397699356079102
        },
        {
            "title": "Visual analytical approaches to evaluating uncertainty and bias in crowd sourced crisis information",
            "data": "Concerns about verification mean the humanitarian community are reluctant to use information collected during crisis events, even though such information could potentially enhance the response effort. Consequently, a program of research is presented that aims to evaluate the degree to which uncertainty and bias are found in public collections of incident reports gathered during crisis events. These datasets exemplify a class whose members have spatial and temporal attributes, are gathered from heterogeneous sources, and do not have readily available attribution information. An interactive software prototype, and existing software, are applied to a dataset related to the current armed conflict in Libya to identify `intrinsic' characteristics against which uncertainty and bias can be evaluated. Requirements on the prototype are identified, which in time will be expanded into full research objectives.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102470",
            "id": "r_570",
            "s_ids": [
                "s_46",
                "s_514",
                "s_1126"
            ],
            "type": "rich",
            "x": 5.794953346252441,
            "y": 8.35061264038086
        },
        {
            "title": "Combining statistical independence testing, visual attribute selection and automated analysis to find relevant attributes for classification",
            "data": "We present an iterative strategy for finding a relevant subset of attributes for the purpose of classification in high-dimensional, heterogeneous data sets. The attribute subset is used for the construction of a classifier function. In order to cope with the challenge of scalability, the analysis is split into an overview of all attributes and a detailed analysis of small groups of attributes. The overview provides generic information on statistical dependencies between attributes. With this information the user can select groups of attributes and an analytical method for their detailed analysis. The detailed analysis involves the identification of redundant attributes (via classification or regression) and the creation of summarizing attributes (via clustering or dimension reduction). Our strategy does not prescribe specific analytical methods. Instead, we recursively combine the results of different methods to find or generate a subset of attributes to use for classification.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5654445",
            "id": "r_571",
            "s_ids": [
                "s_1218",
                "s_326",
                "s_85"
            ],
            "type": "rich",
            "x": 4.0988640785217285,
            "y": 8.007610321044922
        },
        {
            "title": "Cluster correspondence views for enhanced analysis of SOM displays",
            "data": "The Self-Organizing Map (SOM) algorithm is a popular and widely used cluster algorithm. Its constraint to organize clusters on a grid structure makes it very amenable to visualization. On the other hand, the grid constraint may lead to reduced cluster accuracy and reliability, compared to other clustering methods not implementing this restriction. We propose a visual cluster analysis system that allows to validate the output of the SOM algorithm by comparison with alternative clustering methods. Specifically, visual mappings overlaying alternative clustering results onto the SOM are proposed. We apply our system on an example data set, and outline main analytical use cases.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5651676",
            "id": "r_572",
            "s_ids": [
                "s_893",
                "s_241",
                "s_889",
                "s_511"
            ],
            "type": "rich",
            "x": 3.6550514698028564,
            "y": 5.683559417724609
        },
        {
            "title": "Enhancing text-based chat with visuals for hazardous weather decision making",
            "data": "We created a visual chat application for use during hazardous weather events. The application, NWSChat2, allows National Weather Service forecasters, media members, and storm trackers to communicate with each other, basing their conversation on a common shared radar map of the storm. Users can additionally annotate the map with `pins' or draw notes with a stylus. These annotations are automatically shared with all other users. The collaborative nature of NWSChat2 makes it well-suited for disseminating information to all users during weather emergencies.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5650815",
            "id": "r_573",
            "s_ids": [
                "s_1482",
                "s_498",
                "s_1216",
                "s_309"
            ],
            "type": "rich",
            "x": 6.389932155609131,
            "y": 9.841611862182617
        },
        {
            "title": "Enron case study: Analysis of email behavior using EmailTime",
            "data": "This paper presents a case study with Enron email dataset to explore the behaviors of email users within different organizational positions. We defined email behavior as the email activity level of people regarding a series of measured metrics e.g. sent and received emails, numbers of email addresses, etc. These metrics were calculated through EmailTime, a visual analysis tool of email correspondence over the course of time. Results showed specific patterns in the email datasets of different organizational positions.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5649905",
            "id": "r_574",
            "s_ids": [
                "s_443",
                "s_848",
                "s_650",
                "s_1108"
            ],
            "type": "rich",
            "x": 7.695061683654785,
            "y": 8.196971893310547
        },
        {
            "title": "VIDI surveillance - embassy monitoring and oversight system",
            "data": "We hypothesized that potential spies would try to use other employees' terminals in order to not draw attention to themselves. We define one type of suspicious activity as IP use on a terminal when the owner is inside the classified area. We created a timeline visualization of IP usage, overlaid with classified area entrances and exits. The vertical axis divides the timelines into 31 rows, one for each day of the month. The horizontal axis represents the time of day from early morning to late evening. A single employee's entire month is viewed all at once using this visualization. The employee being viewed can be changed using the arrow keys. Every IP event is represented by a vertical bar positioned at the exact time of its appearance. We color the IP events by port number, which is either intranet, HTTP, tomcat, or email, and size the bar based on the outgoing data size. Whenever an employee enters the classified area, a semi-transparent yellow region is drawn until that user exits the classified area. In rare cases when the user double enters, the region is twice as opaque, and in the other rare case where a user leaves the exits without entering, a red region is drawn until the next time the employee enters. The legend key and office diagram showing the current selected employee, highlighted in red, can be seen in the top left-hand corner.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333950",
            "id": "r_575",
            "s_ids": [
                "s_193",
                "s_900",
                "s_109",
                "s_1281",
                "s_699"
            ],
            "type": "rich",
            "x": 6.836374282836914,
            "y": 7.674856662750244
        },
        {
            "title": "Comparing two interface tools in performing visual analytics tasks",
            "data": "In visual analytics, menu systems are commonly adopted as supporting tools because of the complex nature of data. However, it is still unknown how much the interaction implicit to the interface impacts the performance of visual analysis. To show the effectiveness of two interface tools, one a floating text-based menu (Floating Menu) and the other a more interactive iconic tool (Interactive-Icon), we evaluated the use and human performance of both tools within one highly interactive visual analytics system. We asked participants to answer similarly constructed, straightforward questions in a genomic visualization, first with one tool, and then the other. During task performance we tracked completion times, task errors, and captured coarse-grained interactive behaviors. Based on the participants accuracy, speed, behaviors and post-task qualitative feedback, we observed that although the Interactive-Icon tool supports continuous interactions, task-oriented user evaluation did not find a significant difference between the two tools because there is a familiarity effect on the performance of solving the task questions with using Floating-Menu interface tool.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333469",
            "id": "r_576",
            "s_ids": [
                "s_596",
                "s_627",
                "s_233",
                "s_333"
            ],
            "type": "rich",
            "x": 5.075603485107422,
            "y": 9.981369972229004
        },
        {
            "title": "Evacuation traces mini challenge: User testing to obtain consensus discovering the terrorist",
            "data": "The adoption of visual analytics methodologies in security applications is an approach that could lead to interesting results. Usually, the data that has to be analyzed finds in a graphical representation its preferred nature, such as spatial or temporal relationships. Due to the nature of these applications, it is very important that key-details are made easy to identify. In the context of the VAST 2008 Challenge, we developed a visualization tool that graphically displays the movement of 82 employees of the Miami Department of Health (USA). We also asked 13 users to identify potential suspects and observe what happened during an evacuation of the building caused by an explosion. In this paper we explain the results of the user testing we conducted and how the users interpreted the event taken into account.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677390",
            "id": "r_577",
            "s_ids": [
                "s_234",
                "s_1473"
            ],
            "type": "rich",
            "x": 6.872135639190674,
            "y": 7.794391632080078
        },
        {
            "title": "Evacuation trace Mini Challenge award: Tool integration analysis of movements with Geospatial Visual Analytics Toolkit",
            "data": "The Geospatial Visual Analytics Toolkit intended for exploratory analysis of spatial and spatio-temporal data has been recently enriched with specific visual and computational techniques supporting analysis of data about movement. We applied these and other techniques to the data and tasks of Mini Challenge 4, where it was necessary to analyze tracks of moving people.CR Categories and Subject Descriptors: H.1.2 [User/Machine Systems]: Human information processing - Visual Analytics; 1.6.9 [Visualization]: information visualization.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677388",
            "id": "r_578",
            "s_ids": [
                "s_11",
                "s_1195"
            ],
            "type": "rich",
            "x": 7.481263160705566,
            "y": 5.375572204589844
        },
        {
            "title": "Migrant boat mini challenge award: Simple and effective integrated display geo-temporal analysis of migrant boats",
            "data": "We provide a description of the tools and techniques used in our analysis of the VAST 2008 Challenge dealing with mass movement of persons departing Isla Del Sue.no on boats for the United States during 2005-2007. We used visual analytics to explore migration patterns, characterize the choice and evolution of landing sites, characterize the geographical patterns of interdictions and determine the successful landing rate. Our ComVis tool, in connection with some helper applications and Google Earth, allowed us to explore geo-temporal characteristics of the data set and answer the challenge questions. The ComVis project file captures the visual analysis context and facilitates better collaboration among team members.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677387",
            "id": "r_579",
            "s_ids": [
                "s_730",
                "s_682",
                "s_1353",
                "s_1152",
                "s_588",
                "s_840",
                "s_990"
            ],
            "type": "rich",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "title": "Grand challenge award: Data integration visualization and collaboration in the VAST 2008 Challenge",
            "data": "The VAST 2008 Challenge consisted of four heterogeneous synthetic data sets each organized into separate mini-challenges. The Grand Challenge required integrating the raw data from these four data sets as well as integrating results and findings from team members working on specific mini-challenges. Modeling the problem with a semantic network provided a means for integrating both the raw data and the subjective findings.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677384",
            "id": "r_580",
            "s_ids": [
                "s_1340",
                "s_1109",
                "s_907",
                "s_1331",
                "s_901",
                "s_309",
                "s_998",
                "s_161",
                "s_1342",
                "s_546"
            ],
            "type": "rich",
            "x": 6.881535053253174,
            "y": 8.784066200256348
        },
        {
            "title": "Visual Analysis of Dynamic Networks with Geological Clustering",
            "data": "Many dynamic networks have associated geological information. Here we present two complementing visual analysis methods for such networks. The first one provides an overview with summerized information while the second one presents a more detailed view. The geological information is encoded in the network layout, which is designed to help maintain user's mental map. We also combined visualization with social network analysis to facilitate knowledge discovery, especially to understand network changes in the context overall evolution. Both methods are applied to the \"History of the FIFA World Cup Competition\" data set.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389027",
            "id": "r_581",
            "s_ids": [
                "s_1051",
                "s_549",
                "s_902",
                "s_1290",
                "s_638"
            ],
            "type": "rich",
            "x": 5.461853981018066,
            "y": 5.2681779861450195
        },
        {
            "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop Analysis",
            "data": "Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of ancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022 Rosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in seeking signs of past life on Mars. Geologists measure and interpret 3D DOMs, create sedimentary logs and combine them in \u2018correlation panels\u2019 to map the extents of key geological horizons, and build a stratigraphic model to understand their position in the ancient landscape. Currently, the creation of correlation panels is completely manual and therefore time-consuming, and inflexible. With InCorr we present a visualization solution that encompasses a 3D logging tool and an interactive data-driven correlation panel that evolves with the stratigraphic analysis. For the creation of InCorr we closely cooperated with leading planetary geologists in the form of a design study. We verify our results by recreating an existing correlation analysis with InCorr and validate our correlation panel against a manually created illustration. Further, we conducted a user-study with a wider circle of geologists. Our evaluation shows that InCorr efficiently supports the domain experts in tackling their research questions and that it has the potential to significantly impact how geologists work with digital outcrop representations in general.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030409",
            "id": "r_582",
            "s_ids": [
                "s_92",
                "s_691",
                "s_310",
                "s_22",
                "s_1204",
                "s_531"
            ],
            "type": "rich",
            "x": 5.602627754211426,
            "y": 6.447721481323242
        },
        {
            "title": "StreamVisND: Visualizing relationships in streaming multivariate data",
            "data": "In streaming acquisitions the data changes over time. ThemeRiver and line charts are common methods to display data over time. However, these methods can only show the values of the variables (or attributes) but not the relationships among them over time. We propose a framework we call StreamVis&lt;sup&gt;ND&lt;/sup&gt; that can display these types of streaming data relations. It first slices the data stream into different time slices, then it visualizes each slice with a sequence of multivariate 2D data layouts, and finally it flattens this series of displays into a parallel coordinate type display. Our framework is fully interactive and lends itself well to real-time displays.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347673",
            "id": "r_583",
            "s_ids": [
                "s_1462",
                "s_925",
                "s_1079",
                "s_503",
                "s_254"
            ],
            "type": "rich",
            "x": 5.112260341644287,
            "y": 7.125860214233398
        },
        {
            "title": "Augmenting visual representation of affectively charged information using sound graphs",
            "data": "Within the Visual Analytics research agenda there is an interest on studying the applicability of multimodal information representation and interaction techniques for the analytical reasoning process. The present study summarizes a pilot experiment conducted to understand the effects of augmenting visualizations of affectively-charged information using auditory graphs. We designed an audiovisual representation of social comments made to different news posted on a popular website, and their affective dimension using a sentiment analysis tool for short texts. Participants of the study were asked to create an assessment of the affective valence trend (positive or negative) of the news articles using for it, the visualizations and sonifications. The conditions were tested looking for speed/accuracy trade off comparing the visual representation with an audiovisual one. We discuss our preliminary findings regarding the design of augmented information-representation.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400547",
            "id": "r_584",
            "s_ids": [
                "s_302",
                "s_164",
                "s_1047"
            ],
            "type": "rich",
            "x": 5.203633785247803,
            "y": 9.702972412109375
        },
        {
            "title": "Optimizing an SPT-tree for visual analytics",
            "data": "Despite the extensive work done in the scientific visualization community on the creation and optimization of spatial data structures, there has been little adaptation of these structures in visual analytics and information visualization. In this work we present how we modify a space-partioning time (SPT) tree - a structure normally used in direct-volume rendering - for geospatial-temporal visualizations. We also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons. Finally, we present the results of an experiment that quantitatively evaluates our modified SPT tree with and without our optimizations. Our results indicate that retrieval was nearly three times faster when using our optimizations, and are consistent across multiple trials. Our finding could have implications for performance in using our modified SPT tree in large-scale geospatial temporal visual analytics software.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400544",
            "id": "r_585",
            "s_ids": [
                "s_838",
                "s_333"
            ],
            "type": "rich",
            "x": 3.7646729946136475,
            "y": 6.220944881439209
        },
        {
            "title": "Using visual analytics to detect problems in datasets collected from photo-sharing services",
            "data": "Datasets that are collected for research often contain millions of records and may carry hidden pitfalls that are hard to detect. This work demonstrates how visual analytics can be used for identifying problems in the spatial distribution of crawled photographic data in different datasets: Picasa Web Albums, Panoramio, Flickr and Geograph, chosen to be potential data sources for ongoing doctoral research. This poster summary describes a number of problems found in the datasets using visual analytics and suggests that greater attention should be paid to assessing the quality of data gathered from user-generated photographic content. This work is the first part of a three-year PhD project aimed at producing a pedestrian-routing system that can suggest attractive pathways extracted from user-generated photographic content.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400538",
            "id": "r_586",
            "s_ids": [
                "s_1324",
                "s_1126"
            ],
            "type": "rich",
            "x": 5.428104877471924,
            "y": 7.266650199890137
        },
        {
            "title": "VDQAM: A toolkit for database quality evaluation based on visual morphology",
            "data": "Data quality evaluation is one of the most critical steps during the data mining processes. Data with poor quality often leads to poor performance in data mining, low efficiency in data analysis, wrong decision which bring great economic loss to users and organizations further. Although many researches have been carried out from various aspects of the extracting, transforming, and loading processes in data mining, most researches pay more attention to analysis automation than to data quality evaluation. To address the data quality evaluation issues, we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer, and develop a visual analysis method for data quality evaluation based on visual morphology.",
            "url": "http://dx.doi.org/10.1109/VAST.2012.6400531",
            "id": "r_587",
            "s_ids": [
                "s_1398",
                "s_1093",
                "s_190",
                "s_1445"
            ],
            "type": "rich",
            "x": 4.659048080444336,
            "y": 7.696251392364502
        },
        {
            "title": "Jigsaw to save vastopolis",
            "data": "This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw's computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102496",
            "id": "r_588",
            "s_ids": [
                "s_965",
                "s_657",
                "s_1115",
                "s_756"
            ],
            "type": "rich",
            "x": 6.491633415222168,
            "y": 9.125612258911133
        },
        {
            "title": "Guiding security analysis through visualization",
            "data": "We present a multiple views visualization for the security data in the VAST 2010 Mini Challenge 2. The visualization is used to monitor log event activity on the network log data included in the challenge. Interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed. Additionally, a database application is used to allow SQL queries for more detailed investigation.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102492",
            "id": "r_589",
            "s_ids": [
                "s_120",
                "s_305",
                "s_403",
                "s_233",
                "s_1368"
            ],
            "type": "rich",
            "x": 6.846879005432129,
            "y": 7.738626956939697
        },
        {
            "title": "Exploring agent-based simulations using temporal graphs",
            "data": "Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102469",
            "id": "r_590",
            "s_ids": [
                "s_865",
                "s_347",
                "s_333"
            ],
            "type": "rich",
            "x": 3.1261494159698486,
            "y": 7.796170234680176
        },
        {
            "title": "Reasonable abstractions: Semantics for dynamic data visualization",
            "data": "Chi showed how to treat visualization programing models abstractly. This provided a firm theoretical basis for the data-state model of visualization. However, Chi's models did not look deeper into fine-grained program properties, such as execution semantics. We present conditionally deterministic and resource bounded semantics for the data flow model of visualization based on E-FRP. These semantics are used in the Stencil system to move between data state and data flow execution, build task-based parallelism, and build complex analysis chains for dynamic data. This initial work also shows promise for other complex operators, compilation techniques to enable efficient use of time and space, and mixing task and data parallelism.",
            "url": "http://dx.doi.org/10.1109/VAST.2011.6102468",
            "id": "r_591",
            "s_ids": [
                "s_1225",
                "s_1457"
            ],
            "type": "rich",
            "x": 4.849803924560547,
            "y": 7.343348979949951
        },
        {
            "title": "Poster: Translating cross-filtered queries into questions",
            "data": "Complex combinations of coordinated multiple views are increasingly used to design tools for highly interactive visual exploration and analysis of multidimensional data. While complex coordination patterns provide substantial utility through expressive querying, they also exhibit usability problems for users when learning required interaction sequences, recalling past queries, and interpreting visual states. As visual analysis tools grow more sophisticated, there is a growing need to make them more understandable as well. Our long-term goal is to exploit natural language familiarity and literacy to directly facilitate individual and collaborative use of visual analysis tools. In this poster, we present work in progress on an automatically generated query-to-question user interface to translate interactive states during visual analysis into an accompanying visual log of formatted text. Our effort currently focuses on a symmetric and thus relatively simple coordination pattern: cross-filtered views. We describe our current thinking about query-to-question translation in a typical cross-filtered visualization of movies, people, and genres in the Internet Movie Database.",
            "url": "http://dx.doi.org/10.1109/VAST.2010.5650251",
            "id": "r_592",
            "s_ids": [
                "s_26",
                "s_309"
            ],
            "type": "rich",
            "x": 5.229276657104492,
            "y": 9.590843200683594
        },
        {
            "title": "Timeline analysis of undercover activities VAST 2009 traffic mini challenge award: Good analytical technique",
            "data": "Our visualization tool for the VAST 2009 traffic mini challenge, Timeliner, visualizes badge and network traffic data together in a single timeline. The two views of per-employee and per-day with various filtering interactions enable users to analyze easily employees activities at a particular moment of interest as well as their general daily patterns. Using Timeliner, we present several hypotheses for the task at hand and their validation processes, which reveals various aspects of the data.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5334460",
            "id": "r_593",
            "s_ids": [
                "s_331",
                "s_493",
                "s_468",
                "s_462"
            ],
            "type": "rich",
            "x": 7.202186584472656,
            "y": 6.9594502449035645
        },
        {
            "title": "Solving the traffic and flitter challenges with tulip",
            "data": "We present our visualization systems and findings for the badge and network traffic as well as the social network and geospatial challenges of the 2009 VAST contest. The summary starts by presenting an overview of our time series encoding of badge information and network traffic. Our findings suggest that employee 30 may be of interest. In the second part of the paper, we describe our system for finding subgraphs in the social network subject to degree constraints. Subsequently, we present our most likely candidate network which is similar to scenario B.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5334456",
            "id": "r_594",
            "s_ids": [
                "s_941",
                "s_879",
                "s_105",
                "s_420",
                "s_972",
                "s_209",
                "s_616",
                "s_579",
                "s_308",
                "s_504",
                "s_552",
                "s_769",
                "s_93"
            ],
            "type": "rich",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "title": "Integrative visual analytics for suspicious behavior detection",
            "data": "In the VAST Challenge 2009 suspicious behavior had to be detected applying visual analytics to heterogeneous data, such as network traffic, social network enriched with geo-spatial attributes, and finally video surveillance data. This paper describes some of the awarded parts from our solution entry.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5334430",
            "id": "r_595",
            "s_ids": [
                "s_637",
                "s_1337",
                "s_1167",
                "s_1346",
                "s_1303",
                "s_278",
                "s_630",
                "s_1038"
            ],
            "type": "rich",
            "x": 6.847413539886475,
            "y": 8.73234748840332
        },
        {
            "title": "Visualization of uncertainty and analysis of geographical data",
            "data": "A team of five worked on this challenge to identify a possible criminal structure within the Flitter social network. Initially we worked on the problem individually, deliberately not sharing any data, results or conclusions. This maximised the chances of spotting any blunders, unjustified assumptions or inferences and allowed us to triangulate any common conclusions. After an agreed period we shared our results demonstrating the visualization applications we had built and the reasoning behind our conclusions. This sharing of assumptions encouraged us to incorporate uncertainty in our visualization approaches as it became clear that there was a number of possible interpretations of the rules and assumptions governing the challenge. This summary of the work emphasises one of those applications detailing the geographic analysis and uncertainty handling of the network data.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333965",
            "id": "r_596",
            "s_ids": [
                "s_1126",
                "s_1411",
                "s_156",
                "s_514",
                "s_1469"
            ],
            "type": "rich",
            "x": 5.795925617218018,
            "y": 8.556072235107422
        },
        {
            "title": "Interactive poster: A proposal for sharing user requirements for visual analytic tools",
            "data": "Although many in the community have advocated user-centered evaluations for visual analytic environments, a significant barrier exists. The users targeted by the visual analytics community (law enforcement personnel, professional information analysts, financial analysts, health care analysts, etc.) are often inaccessible to researchers. These analysts are extremely busy and their work environments and data are often classified or at least confidential. Furthermore, their tasks often last weeks or even months. It is simply not feasible to do such long-term observations to understand their jobs. How then can we hope to gather enough information about the diverse user populations to understand their needs? Some researchers, including the author, have been successful in getting access to specific end-users. A reasonable approach, therefore, would be to find a way to share user information. This work outlines a proposal for developing a handbook of user profiles for use by researchers, developers, and evaluators.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333474",
            "id": "r_597",
            "s_ids": [
                "s_1244"
            ],
            "type": "rich",
            "x": 5.984583377838135,
            "y": 10.259687423706055
        },
        {
            "title": "A scalable architecture for visual data exploration",
            "data": "Intelligence analysts in the areas of defense and homeland security are now faced with the difficult problem of discerning the relevant details amidst massive data stores. We propose a component-based visualization architecture that is built specifically to encourage the flexible exploration of geospatial event databases. The proposed system is designed to deploy on a variety of display layouts, from a single laptop screen to a multi-monitor tiled-display. By utilizing a combination of parallel coordinates, principal components plots, and other data views, analysts may reduce the dimensionality of a data set to its most salient features. Of particular value to our target applications are understanding correlations between data layers, both within a single view and across multiple views. Our proposed system aims to address the limited scalability associated with coordinated multiple views (CMVs) through the implementation of an efficient core application which is extensible by the end-user.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333451",
            "id": "r_598",
            "s_ids": [
                "s_565",
                "s_1519",
                "s_1070",
                "s_515"
            ],
            "type": "rich",
            "x": 5.102502822875977,
            "y": 7.049795150756836
        },
        {
            "title": "ProcessLine: Visualizing time-series data in process industry",
            "data": "In modern process industry, it is often difficult to analyze a manufacture process due to its numerous time-series data. Analysts wish to not only interpret the evolution of data over time in a working procedure, but also examine the changes in the whole production process through time. To meet such analytic requirements, we have developed ProcessLine, an interactive visualization tool for a large amount of time-series data in process industry. The data are displayed in a fisheye timeline. ProcessLine provides good overviews for the whole production process and details for the focused working procedure. A preliminary user study using beer industry production data has shown that the tool is effective.",
            "url": "http://dx.doi.org/10.1109/VAST.2009.5333421",
            "id": "r_599",
            "s_ids": [
                "s_903",
                "s_1445",
                "s_258",
                "s_1415",
                "s_1398",
                "s_516"
            ],
            "type": "rich",
            "x": 5.462973117828369,
            "y": 7.948393821716309
        },
        {
            "title": "Interactive poster: Visual data mining of unevenly-spaced event sequences",
            "data": "We present a process for the exploration and analysis of large databases of events. A typical database is characterized by the sequential actions of a number of individual entities. These entities can be compared by their similarities in sequence and changes in sequence over time. The correlation of two sequences can provide important clues as to the possibility of a connection between the responsible entities, but an analyst might not be able to specify the type of connection sought prior to examination. Our process incorporates extensive automated calculation and data mining but permits diversity of analysis by providing visualization of results at multiple levels, taking advantage of human intuition and visual processing to generate avenues of inquiry.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677379",
            "id": "r_600",
            "s_ids": [
                "s_1519",
                "s_333",
                "s_68",
                "s_233"
            ],
            "type": "rich",
            "x": 7.523910999298096,
            "y": 6.697447299957275
        },
        {
            "title": "Visual analysis for mutual fund performance",
            "data": "Mutual funds are one of the most important investment instruments available. However, choosing among mutual funds is not an easy task because they vary in many different dimensions, such as asset size, turnover and fee structure, and these characteristics may affect fund returns. It is thus important to understand the relation between fund performance and these properties. In this work, we use a new visual analytical tool, the density-based distribution map, to assist in this task. By visualizing various important fund characteristics from a real-world database of the US stock funds, our new visual representations greatly help understand the relation between fund characteristics and returns.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677376",
            "id": "r_601",
            "s_ids": [
                "s_975",
                "s_1023",
                "s_1526"
            ],
            "type": "rich",
            "x": 8.840062141418457,
            "y": 7.1926984786987305
        },
        {
            "title": "A compound approach for interactive visualization of time-oriented data",
            "data": "Many real-world visual analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data.",
            "url": "http://dx.doi.org/10.1109/VAST.2008.4677374",
            "id": "r_602",
            "s_ids": [
                "s_427"
            ],
            "type": "rich",
            "x": 6.721049785614014,
            "y": 6.973466396331787
        },
        {
            "title": "University of British Columbia & Simon Fraser University - The Bricolage",
            "data": "",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4470207",
            "id": "r_603",
            "s_ids": [
                "s_296",
                "s_126",
                "s_1410",
                "s_663",
                "s_396",
                "s_866",
                "s_1047"
            ],
            "type": "rich",
            "x": 6.216020107269287,
            "y": 6.907812118530273
        },
        {
            "title": "VAST 2007 Contest TexPlorer",
            "data": "TexPlorer is an integrated system for exploring and analyzing vast amount of text documents. The data processing modules of TexPlorer consist of named entity extraction, entity relation extraction, hierarchical clustering, and text summarization tools. Using time line tool, tree-view, table-view, and concept maps, TexPlorer provides visualizations from different aspects and allows analysts to explore vast amount of text documents efficiently.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389037",
            "id": "r_604",
            "s_ids": [
                "s_1109",
                "s_513",
                "s_901",
                "s_907",
                "s_998",
                "s_546",
                "s_1342"
            ],
            "type": "rich",
            "x": 7.336312294006348,
            "y": 10.417177200317383
        },
        {
            "title": "VAST 2007 Contest - Analysis with nSpace and GeoTime",
            "data": "GeoTime and nSpace are two interactive visual analytics tools that support the process of analyzing massive and complex datasets. The two tools were used to examine and interpret the 2007 VAST contest dataset. This paper describes how the capabilities of the tools were used to facilitate and expedite every stage of the analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389033",
            "id": "r_605",
            "s_ids": [
                "s_707",
                "s_786",
                "s_1220",
                "s_1468",
                "s_477",
                "s_796"
            ],
            "type": "rich",
            "x": 6.660202503204346,
            "y": 8.713521003723145
        },
        {
            "title": "Outlook for Visual Analytics Research Funding",
            "data": "Visual Analytics has become a rapidly growing field of study. It is also a field that is addressing very significant real world problems in homeland security, business analytics, emergency management, genetics and bioinformatics, investigative analysis, medical analytics, and other areas. For both these reasons, it is attracting new funding and will continue to do so in the future. Visual analytics has also become an international field, with significant research efforts in Canada, Europe, and Australia, as well as the U.S. There is significant new research funding in Canada and Germany with other efforts being discussed, including a major program sponsored by the European Union. The contributors to this panel are some of the primary thought leaders providing research funding or involved in setting up the funding apparatus. We have asked them to present their needs, funding programs, and expectations from the research community. They all come from different perspectives, different missions, and different expectations. They will present their views of the range of activity in both the U.S. and internationally and discuss what is coming. Come learn about these programs, initiatives, and plans, and how you can contribute.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389030",
            "id": "r_606",
            "s_ids": [
                "s_1403",
                "s_1038",
                "s_83",
                "s_95"
            ],
            "type": "rich",
            "x": 6.236343860626221,
            "y": 10.160306930541992
        },
        {
            "title": "From Tasks to Tools: A Field Study in Collaborative Visual Analytics",
            "data": "This poster presents an exploratory field study of a VAST 2007 contest entry. We applied cognitive task analysis (CTA), grounded theory (GT), and activity theory (AT), to analysis of field notes and interviews from participants. Our results are described in the context of activity theory and sensemaking, two theoretical perspectives that we have found to be particularly useful in understanding analytic tasks.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389028",
            "id": "r_607",
            "s_ids": [
                "s_126",
                "s_396",
                "s_866",
                "s_296",
                "s_1410",
                "s_663",
                "s_1047",
                "s_551"
            ],
            "type": "rich",
            "x": 5.62161111831665,
            "y": 10.346427917480469
        },
        {
            "title": "University of British Columbia & Simon Fraser University - The Bricolage",
            "data": "This abstract presents a<i>bricolage</i>approach to the 2007 VAST contest. The analytical process we used is presented across four stages of sensemaking. Several tools were used throughout our approach, and we present their strengths and weaknesses for specific aspects of the analytical process. In addition, we review the details of both individual and collaborative techniques for solving visual analytics problems.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389020",
            "id": "r_608",
            "s_ids": [
                "s_296",
                "s_126",
                "s_1410",
                "s_663",
                "s_396",
                "s_866"
            ],
            "type": "rich",
            "x": 5.42981481552124,
            "y": 10.247477531433105
        },
        {
            "title": "TextPlorer: An application supporting text analysis",
            "data": "TexPlorer is an integrated system for exploring and analyzing large amounts of text documents. The data processing modules of TexPlorer consist of named entity extraction, entity relation extraction, hierarchical clustering, and text summarization tools. Using a timeline tool, tree-view, table-view, and concept maps, TexPlorer provides an analytical interface for exploring a set of text documents from different perspectives and allows users to explore vast amount of text documents efficiently.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389019",
            "id": "r_609",
            "s_ids": [
                "s_1109",
                "s_513",
                "s_901",
                "s_907"
            ],
            "type": "rich",
            "x": 7.357332229614258,
            "y": 10.343387603759766
        },
        {
            "title": "Something's \"Fishy\" at Global Ways and Gill Breeders - Analysis with nSpace and GeoTime",
            "data": "GeoTime and nSpace are two interactive visual analytics tools that support the process of analyzing massive and complex datasets. The two tools were used to examine and interpret the 2007 VAST contest dataset. This poster paper describes how the capabilities of the tools were used to facilitate and expedite every stage of an analyst workflow.",
            "url": "http://dx.doi.org/10.1109/VAST.2007.4389018",
            "id": "r_610",
            "s_ids": [
                "s_707",
                "s_786",
                "s_796"
            ],
            "type": "rich",
            "x": 6.627882480621338,
            "y": 8.726107597351074
        },
        {
            "id": "s_0",
            "name": "Heather Richter Lipford",
            "type": "sparse",
            "x": 5.377330303192139,
            "y": 9.783858299255371
        },
        {
            "id": "s_1",
            "name": "Selim Balcisoy",
            "type": "sparse",
            "x": 5.311036586761475,
            "y": 9.03400707244873
        },
        {
            "id": "s_2",
            "name": "Werner Purgathofer",
            "type": "sparse",
            "x": 3.272869348526001,
            "y": 7.7402143478393555
        },
        {
            "id": "s_3",
            "name": "J\u00f6rn Schneidewind",
            "type": "sparse",
            "x": 5.32595157623291,
            "y": 7.126411437988281
        },
        {
            "id": "s_4",
            "name": "Miaoxin Hu",
            "type": "sparse",
            "x": 4.8352508544921875,
            "y": 7.234743595123291
        },
        {
            "id": "s_5",
            "name": "Patrick Hertzog",
            "type": "sparse",
            "x": 6.889247417449951,
            "y": 7.69767951965332
        },
        {
            "id": "s_6",
            "name": "Phong H. Nguyen",
            "type": "sparse",
            "x": 5.912964820861816,
            "y": 9.671320915222168
        },
        {
            "id": "s_7",
            "name": "Zhibin Wang",
            "type": "sparse",
            "x": 6.819993019104004,
            "y": 6.766763210296631
        },
        {
            "id": "s_8",
            "name": "Deryck W. Holdsworth",
            "type": "sparse",
            "x": 7.1402506828308105,
            "y": 7.083461284637451
        },
        {
            "id": "s_9",
            "name": "Emily Wall",
            "type": "sparse",
            "x": 4.63115930557251,
            "y": 8.236598014831543
        },
        {
            "id": "s_10",
            "name": "J. Edward Swan II",
            "type": "sparse",
            "x": 6.3353271484375,
            "y": 6.393352508544922
        },
        {
            "id": "s_11",
            "name": "Natalia V. Andrienko",
            "type": "sparse",
            "x": 6.701501846313477,
            "y": 6.239125728607178
        },
        {
            "id": "s_12",
            "name": "Annette Haworth",
            "type": "sparse",
            "x": 3.569378137588501,
            "y": 7.797091007232666
        },
        {
            "id": "s_13",
            "name": "Chris J. Hughes",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_14",
            "name": "Matthew Brehmer",
            "type": "sparse",
            "x": 7.942614555358887,
            "y": 7.702369689941406
        },
        {
            "id": "s_15",
            "name": "Leanna House",
            "type": "sparse",
            "x": 5.145179271697998,
            "y": 8.35034465789795
        },
        {
            "id": "s_16",
            "name": "Shahid Latif",
            "type": "sparse",
            "x": 7.256138801574707,
            "y": 10.13315200805664
        },
        {
            "id": "s_17",
            "name": "Mark W. Jones 0001",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_18",
            "name": "Ravish Chawla",
            "type": "sparse",
            "x": 4.688411235809326,
            "y": 8.034207344055176
        },
        {
            "id": "s_19",
            "name": "Roger A. Leite",
            "type": "sparse",
            "x": 8.872180938720703,
            "y": 7.168383598327637
        },
        {
            "id": "s_20",
            "name": "Christoph Heinzl",
            "type": "sparse",
            "x": 5.075896739959717,
            "y": 6.747625827789307
        },
        {
            "id": "s_21",
            "name": "Janu Verma",
            "type": "sparse",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "id": "s_22",
            "name": "Robert Barnes",
            "type": "sparse",
            "x": 5.602627754211426,
            "y": 6.447721481323242
        },
        {
            "id": "s_23",
            "name": "Yuan He 0004",
            "type": "sparse",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "id": "s_24",
            "name": "Yuli Gao",
            "type": "sparse",
            "x": 5.232822895050049,
            "y": 8.288137435913086
        },
        {
            "id": "s_25",
            "name": "Rui A. P. Perdig\u00e3o",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_26",
            "name": "Maryam Nafari",
            "type": "sparse",
            "x": 5.229276657104492,
            "y": 9.590843200683594
        },
        {
            "id": "s_27",
            "name": "Subhajit Das 0002",
            "type": "sparse",
            "x": 5.173712730407715,
            "y": 8.365744590759277
        },
        {
            "id": "s_28",
            "name": "Ke Xu",
            "type": "sparse",
            "x": 7.1652092933654785,
            "y": 7.150138854980469
        },
        {
            "id": "s_29",
            "name": "Haekyu Park",
            "type": "sparse",
            "x": 2.162933349609375,
            "y": 8.92276668548584
        },
        {
            "id": "s_30",
            "name": "Harish Doraiswamy",
            "type": "sparse",
            "x": 5.522463798522949,
            "y": 6.873469352722168
        },
        {
            "id": "s_31",
            "name": "Arjun Srinivasan",
            "type": "sparse",
            "x": 5.053277969360352,
            "y": 5.204998970031738
        },
        {
            "id": "s_32",
            "name": "Lei Shi 0002",
            "type": "sparse",
            "x": 6.1663689613342285,
            "y": 7.046450614929199
        },
        {
            "id": "s_33",
            "name": "Dandelion Man\u00e9",
            "type": "sparse",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "id": "s_34",
            "name": "Stephan Huber",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_35",
            "name": "Patricia F. Anderson",
            "type": "sparse",
            "x": 4.980503082275391,
            "y": 5.211485862731934
        },
        {
            "id": "s_36",
            "name": "Matthew Hoffman 0001",
            "type": "sparse",
            "x": 7.511104106903076,
            "y": 6.730006217956543
        },
        {
            "id": "s_37",
            "name": "Daniela Oelke",
            "type": "sparse",
            "x": 7.591629981994629,
            "y": 9.956622123718262
        },
        {
            "id": "s_38",
            "name": "Zhendong Yang",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_39",
            "name": "Dominik J\u00e4ckle",
            "type": "sparse",
            "x": 6.644218444824219,
            "y": 6.188513278961182
        },
        {
            "id": "s_40",
            "name": "Melanie G\u00f6rner",
            "type": "sparse",
            "x": 4.9069132804870605,
            "y": 5.314910411834717
        },
        {
            "id": "s_41",
            "name": "Tai-Quan Peng",
            "type": "sparse",
            "x": 7.806752681732178,
            "y": 8.2239408493042
        },
        {
            "id": "s_42",
            "name": "Yuxin Ma",
            "type": "sparse",
            "x": 3.4246668815612793,
            "y": 8.055705070495605
        },
        {
            "id": "s_43",
            "name": "Tolga Bolukbasi",
            "type": "sparse",
            "x": 3.356947183609009,
            "y": 8.667536735534668
        },
        {
            "id": "s_44",
            "name": "Kejie Yu",
            "type": "sparse",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "id": "s_45",
            "name": "Eli T. Brown",
            "type": "sparse",
            "x": 4.598292350769043,
            "y": 7.967381000518799
        },
        {
            "id": "s_46",
            "name": "Iain Dillingham",
            "type": "sparse",
            "x": 5.794953346252441,
            "y": 8.35061264038086
        },
        {
            "id": "s_47",
            "name": "Michelle Annett",
            "type": "sparse",
            "x": 7.414912700653076,
            "y": 10.379096031188965
        },
        {
            "id": "s_48",
            "name": "Zuobin Wang",
            "type": "sparse",
            "x": 8.534769058227539,
            "y": 5.998741626739502
        },
        {
            "id": "s_49",
            "name": "Erich Gstrein",
            "type": "sparse",
            "x": 8.872180938720703,
            "y": 7.168383598327637
        },
        {
            "id": "s_50",
            "name": "Guihua Shan",
            "type": "sparse",
            "x": 2.099738597869873,
            "y": 8.981844902038574
        },
        {
            "id": "s_51",
            "name": "Elisa Portes dos Santos Amorim",
            "type": "sparse",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "id": "s_52",
            "name": "Abish Malik",
            "type": "sparse",
            "x": 6.357044696807861,
            "y": 6.702111721038818
        },
        {
            "id": "s_53",
            "name": "Bruno Pinaud",
            "type": "sparse",
            "x": 4.793405055999756,
            "y": 5.397699356079102
        },
        {
            "id": "s_54",
            "name": "William S. Cleveland",
            "type": "sparse",
            "x": 6.470664978027344,
            "y": 6.918655872344971
        },
        {
            "id": "s_55",
            "name": "Qi Liao 0002",
            "type": "sparse",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "id": "s_56",
            "name": "Scotland Leman",
            "type": "sparse",
            "x": 4.987693786621094,
            "y": 7.952511787414551
        },
        {
            "id": "s_57",
            "name": "Phong Hai Nguyen",
            "type": "sparse",
            "x": 6.049070835113525,
            "y": 10.11598014831543
        },
        {
            "id": "s_58",
            "name": "Mark Tautzenberger",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_59",
            "name": "Scott D. Rothenberger",
            "type": "sparse",
            "x": 6.26613712310791,
            "y": 6.6600165367126465
        },
        {
            "id": "s_60",
            "name": "Michael Schw\u00e4rzler",
            "type": "sparse",
            "x": 3.377743721008301,
            "y": 7.585751533508301
        },
        {
            "id": "s_61",
            "name": "Tim Althoff",
            "type": "sparse",
            "x": 4.1113362312316895,
            "y": 8.209181785583496
        },
        {
            "id": "s_62",
            "name": "Jiaxin Bai",
            "type": "sparse",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "id": "s_63",
            "name": "Jarke J. van Wijk",
            "type": "sparse",
            "x": 5.044285297393799,
            "y": 7.768483638763428
        },
        {
            "id": "s_64",
            "name": "Li Yu",
            "type": "sparse",
            "x": 7.198326110839844,
            "y": 10.895029067993164
        },
        {
            "id": "s_65",
            "name": "Florian Heimerl",
            "type": "sparse",
            "x": 6.418086051940918,
            "y": 9.080625534057617
        },
        {
            "id": "s_66",
            "name": "Binjie Chen",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_67",
            "name": "Mingliang Xu 0001",
            "type": "sparse",
            "x": 7.150640487670898,
            "y": 5.3722243309021
        },
        {
            "id": "s_68",
            "name": "Robert Kosara",
            "type": "sparse",
            "x": 8.188132286071777,
            "y": 6.917491436004639
        },
        {
            "id": "s_69",
            "name": "Jiawei Zhang 0003",
            "type": "sparse",
            "x": 3.061535596847534,
            "y": 8.740524291992188
        },
        {
            "id": "s_70",
            "name": "Ashish Kapoor",
            "type": "sparse",
            "x": 7.385095596313477,
            "y": 10.23172664642334
        },
        {
            "id": "s_71",
            "name": "Steven Landis",
            "type": "sparse",
            "x": 8.675040245056152,
            "y": 7.296395301818848
        },
        {
            "id": "s_72",
            "name": "Arif Ghafoor",
            "type": "sparse",
            "x": 4.131429195404053,
            "y": 9.27343463897705
        },
        {
            "id": "s_73",
            "name": "Samuel Gratzl",
            "type": "sparse",
            "x": 4.957309722900391,
            "y": 9.034566879272461
        },
        {
            "id": "s_74",
            "name": "Vikram Aggarwal",
            "type": "sparse",
            "x": 4.942386150360107,
            "y": 9.645306587219238
        },
        {
            "id": "s_75",
            "name": "Simon Attfield",
            "type": "sparse",
            "x": 5.61575984954834,
            "y": 10.328414916992188
        },
        {
            "id": "s_76",
            "name": "Zhiyong Guo",
            "type": "sparse",
            "x": 4.8352508544921875,
            "y": 7.234743595123291
        },
        {
            "id": "s_77",
            "name": "Dan Imre",
            "type": "sparse",
            "x": 3.615382671356201,
            "y": 5.652681827545166
        },
        {
            "id": "s_78",
            "name": "Markus B\u00f6gl",
            "type": "sparse",
            "x": 3.9276840686798096,
            "y": 7.97027063369751
        },
        {
            "id": "s_79",
            "name": "Bernhard Sadransky",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_80",
            "name": "Markus H\u00f6hn",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_81",
            "name": "Cagatay Turkay",
            "type": "sparse",
            "x": 6.130212306976318,
            "y": 7.887388706207275
        },
        {
            "id": "s_82",
            "name": "Tangzhi Ye",
            "type": "sparse",
            "x": 7.296698093414307,
            "y": 4.995692729949951
        },
        {
            "id": "s_83",
            "name": "Joe Kielman",
            "type": "sparse",
            "x": 6.236343860626221,
            "y": 10.160306930541992
        },
        {
            "id": "s_84",
            "name": "Sarah Goodwin",
            "type": "sparse",
            "x": 5.073339462280273,
            "y": 8.816460609436035
        },
        {
            "id": "s_85",
            "name": "J\u00f6rn Kohlhammer",
            "type": "sparse",
            "x": 5.377479076385498,
            "y": 7.021363735198975
        },
        {
            "id": "s_86",
            "name": "Qiaoan Chen",
            "type": "sparse",
            "x": 4.743052959442139,
            "y": 5.725571155548096
        },
        {
            "id": "s_87",
            "name": "Jorge Piazentin Ono",
            "type": "sparse",
            "x": 3.2641701698303223,
            "y": 8.662834167480469
        },
        {
            "id": "s_88",
            "name": "W. Sabrina Lin",
            "type": "sparse",
            "x": 7.491602897644043,
            "y": 8.011017799377441
        },
        {
            "id": "s_89",
            "name": "Yu Zheng 0004",
            "type": "sparse",
            "x": 6.504770755767822,
            "y": 6.255224227905273
        },
        {
            "id": "s_90",
            "name": "Jean-Daniel Fekete",
            "type": "sparse",
            "x": 5.216064453125,
            "y": 7.594047546386719
        },
        {
            "id": "s_91",
            "name": "Rudolf Benedik",
            "type": "sparse",
            "x": 6.642140865325928,
            "y": 7.3651442527771
        },
        {
            "id": "s_92",
            "name": "Thomas Ortner",
            "type": "sparse",
            "x": 4.474781036376953,
            "y": 7.017314434051514
        },
        {
            "id": "s_93",
            "name": "Guy Melan\u00e7on",
            "type": "sparse",
            "x": 5.152979373931885,
            "y": 5.32603120803833
        },
        {
            "id": "s_94",
            "name": "Snehal Pokharkar",
            "type": "sparse",
            "x": 4.702369213104248,
            "y": 6.508816719055176
        },
        {
            "id": "s_95",
            "name": "Larry Rosenblum",
            "type": "sparse",
            "x": 6.236343860626221,
            "y": 10.160306930541992
        },
        {
            "id": "s_96",
            "name": "Lauren Bradel",
            "type": "sparse",
            "x": 5.656345367431641,
            "y": 9.335917472839355
        },
        {
            "id": "s_97",
            "name": "Abishek Puri",
            "type": "sparse",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "id": "s_98",
            "name": "Michelle X. Zhou",
            "type": "sparse",
            "x": 6.754618167877197,
            "y": 9.303261756896973
        },
        {
            "id": "s_99",
            "name": "Tobias Ruppert",
            "type": "sparse",
            "x": 5.6615891456604,
            "y": 7.48773193359375
        },
        {
            "id": "s_100",
            "name": "Rahul C. Basole",
            "type": "sparse",
            "x": 5.8318562507629395,
            "y": 6.94346809387207
        },
        {
            "id": "s_101",
            "name": "Jimeng Sun 0001",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_102",
            "name": "Maoyuan Sun",
            "type": "sparse",
            "x": 5.146889686584473,
            "y": 6.692412376403809
        },
        {
            "id": "s_103",
            "name": "R\u00fcdiger Westermann",
            "type": "sparse",
            "x": 4.73779821395874,
            "y": 6.055338382720947
        },
        {
            "id": "s_104",
            "name": "Pramod Chundury",
            "type": "sparse",
            "x": 7.877171039581299,
            "y": 7.68709135055542
        },
        {
            "id": "s_105",
            "name": "Faraz Zaidi",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_106",
            "name": "Kenney Ng",
            "type": "sparse",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "id": "s_107",
            "name": "Fangfang Zhou",
            "type": "sparse",
            "x": 5.910553932189941,
            "y": 6.502710342407227
        },
        {
            "id": "s_108",
            "name": "Srikanth Kandula",
            "type": "sparse",
            "x": 6.996976375579834,
            "y": 7.708852291107178
        },
        {
            "id": "s_109",
            "name": "James Shearer",
            "type": "sparse",
            "x": 5.888650894165039,
            "y": 6.472994804382324
        },
        {
            "id": "s_110",
            "name": "Jiahui Chen",
            "type": "sparse",
            "x": 6.819993019104004,
            "y": 6.766763210296631
        },
        {
            "id": "s_111",
            "name": "Eli Packer",
            "type": "sparse",
            "x": 3.7874088287353516,
            "y": 6.028797149658203
        },
        {
            "id": "s_112",
            "name": "Dirk J. Lehmann",
            "type": "sparse",
            "x": 5.354574680328369,
            "y": 7.3530755043029785
        },
        {
            "id": "s_113",
            "name": "Jonathan Zhang",
            "type": "sparse",
            "x": 6.040262222290039,
            "y": 7.4920125007629395
        },
        {
            "id": "s_114",
            "name": "J\u00fcrgen Lerner",
            "type": "sparse",
            "x": 7.301630973815918,
            "y": 9.870648384094238
        },
        {
            "id": "s_115",
            "name": "Leishi Zhang",
            "type": "sparse",
            "x": 4.055224895477295,
            "y": 8.621465682983398
        },
        {
            "id": "s_116",
            "name": "Liang Gou",
            "type": "sparse",
            "x": 4.978421688079834,
            "y": 8.20222282409668
        },
        {
            "id": "s_117",
            "name": "Dustin T. Dunsmuir",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_118",
            "name": "Iwan W. Griffiths",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_119",
            "name": "Tom Polk",
            "type": "sparse",
            "x": 6.549075126647949,
            "y": 7.099808216094971
        },
        {
            "id": "s_120",
            "name": "Lane Harrison",
            "type": "sparse",
            "x": 5.400420188903809,
            "y": 9.486410140991211
        },
        {
            "id": "s_121",
            "name": "Rudolf Netzel",
            "type": "sparse",
            "x": 4.178314208984375,
            "y": 8.581392288208008
        },
        {
            "id": "s_122",
            "name": "Yuan Chen",
            "type": "sparse",
            "x": 4.907010555267334,
            "y": 8.000067710876465
        },
        {
            "id": "s_123",
            "name": "Xinran Hu",
            "type": "sparse",
            "x": 4.294029712677002,
            "y": 6.903424263000488
        },
        {
            "id": "s_124",
            "name": "Ivan Baclija",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_125",
            "name": "Weikai Yang",
            "type": "sparse",
            "x": 6.844028949737549,
            "y": 6.4676408767700195
        },
        {
            "id": "s_126",
            "name": "Daniel Ha",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_127",
            "name": "Bongwon Suh",
            "type": "sparse",
            "x": 6.555689811706543,
            "y": 9.125818252563477
        },
        {
            "id": "s_128",
            "name": "Matthias Schlachter",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_129",
            "name": "Subhashis Hazarika",
            "type": "sparse",
            "x": 3.0904009342193604,
            "y": 7.828068256378174
        },
        {
            "id": "s_130",
            "name": "Jens-Peter Kreiss",
            "type": "sparse",
            "x": 3.6500320434570312,
            "y": 7.781797885894775
        },
        {
            "id": "s_131",
            "name": "Huamin Qu",
            "type": "sparse",
            "x": 5.819926738739014,
            "y": 7.679962635040283
        },
        {
            "id": "s_132",
            "name": "Daniel Weiskopf",
            "type": "sparse",
            "x": 6.0506463050842285,
            "y": 8.567193031311035
        },
        {
            "id": "s_133",
            "name": "R. Arthur Bouwman",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_134",
            "name": "Yongjun Zheng",
            "type": "sparse",
            "x": 6.659133434295654,
            "y": 9.591086387634277
        },
        {
            "id": "s_135",
            "name": "Haibin Liu",
            "type": "sparse",
            "x": 7.857691764831543,
            "y": 8.961512565612793
        },
        {
            "id": "s_136",
            "name": "Daniel R. Tesone",
            "type": "sparse",
            "x": 5.447331428527832,
            "y": 8.479851722717285
        },
        {
            "id": "s_137",
            "name": "Changhyun Lee",
            "type": "sparse",
            "x": 7.193861961364746,
            "y": 10.908971786499023
        },
        {
            "id": "s_138",
            "name": "Elmar Eisemann",
            "type": "sparse",
            "x": 3.322061777114868,
            "y": 7.62338399887085
        },
        {
            "id": "s_139",
            "name": "Xiaoxiao Lian",
            "type": "sparse",
            "x": 7.562387943267822,
            "y": 10.414213180541992
        },
        {
            "id": "s_140",
            "name": "Patrick K\u00f6thur",
            "type": "sparse",
            "x": 6.648183345794678,
            "y": 6.226779937744141
        },
        {
            "id": "s_141",
            "name": "Zaixian Xie",
            "type": "sparse",
            "x": 4.6266655921936035,
            "y": 7.6206278800964355
        },
        {
            "id": "s_142",
            "name": "\u00c1ngel Alexander Cabrera",
            "type": "sparse",
            "x": 4.593509197235107,
            "y": 8.633565902709961
        },
        {
            "id": "s_143",
            "name": "Shiping Huang",
            "type": "sparse",
            "x": 4.6266655921936035,
            "y": 7.6206278800964355
        },
        {
            "id": "s_144",
            "name": "Artem Konev",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_145",
            "name": "Lise Getoor",
            "type": "sparse",
            "x": 4.56176233291626,
            "y": 6.238988399505615
        },
        {
            "id": "s_146",
            "name": "Xinyu Zhu",
            "type": "sparse",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "id": "s_147",
            "name": "Shamal Al-Dohuki",
            "type": "sparse",
            "x": 7.330615997314453,
            "y": 4.847436428070068
        },
        {
            "id": "s_148",
            "name": "Li Zhang",
            "type": "sparse",
            "x": 5.962850570678711,
            "y": 9.402202606201172
        },
        {
            "id": "s_149",
            "name": "Ievgeniia Gutenko",
            "type": "sparse",
            "x": 5.136153697967529,
            "y": 7.1557488441467285
        },
        {
            "id": "s_150",
            "name": "Michael Gleicher",
            "type": "sparse",
            "x": 6.105371475219727,
            "y": 8.784189224243164
        },
        {
            "id": "s_151",
            "name": "Lisa Singh",
            "type": "sparse",
            "x": 4.486476898193359,
            "y": 6.442342758178711
        },
        {
            "id": "s_152",
            "name": "Martin Wattenberg",
            "type": "sparse",
            "x": 4.436699390411377,
            "y": 8.30589485168457
        },
        {
            "id": "s_153",
            "name": "Guan Li",
            "type": "sparse",
            "x": 2.099738597869873,
            "y": 8.981844902038574
        },
        {
            "id": "s_154",
            "name": "Helen Zhao 0001",
            "type": "sparse",
            "x": 4.95570182800293,
            "y": 10.061023712158203
        },
        {
            "id": "s_155",
            "name": "Eveline H. J. Mestrom",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_156",
            "name": "Naz Khalili-Shavarini",
            "type": "sparse",
            "x": 5.795925617218018,
            "y": 8.556072235107422
        },
        {
            "id": "s_157",
            "name": "Karoline Villinger",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_158",
            "name": "Arnold K\u00f6chl",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_159",
            "name": "Alan Wilson 0004",
            "type": "sparse",
            "x": 7.511104106903076,
            "y": 6.730006217956543
        },
        {
            "id": "s_160",
            "name": "Yang Wang",
            "type": "sparse",
            "x": 5.286319732666016,
            "y": 7.735265254974365
        },
        {
            "id": "s_161",
            "name": "Chaomei Chen",
            "type": "sparse",
            "x": 7.388840675354004,
            "y": 8.937654495239258
        },
        {
            "id": "s_162",
            "name": "Jeremy Millar",
            "type": "sparse",
            "x": 3.569378137588501,
            "y": 7.797091007232666
        },
        {
            "id": "s_163",
            "name": "Rick Riensche",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_164",
            "name": "Bernhard E. Riecke",
            "type": "sparse",
            "x": 5.203633785247803,
            "y": 9.702972412109375
        },
        {
            "id": "s_165",
            "name": "Vivek Kothari",
            "type": "sparse",
            "x": 4.601602077484131,
            "y": 9.161087036132812
        },
        {
            "id": "s_166",
            "name": "David Scarlatti",
            "type": "sparse",
            "x": 7.486071586608887,
            "y": 5.270229339599609
        },
        {
            "id": "s_167",
            "name": "Rock Leung",
            "type": "sparse",
            "x": 5.484528541564941,
            "y": 10.792652130126953
        },
        {
            "id": "s_168",
            "name": "Manish Marwah",
            "type": "sparse",
            "x": 5.8741559982299805,
            "y": 6.536009788513184
        },
        {
            "id": "s_169",
            "name": "Piero Molino",
            "type": "sparse",
            "x": 3.061535596847534,
            "y": 8.740524291992188
        },
        {
            "id": "s_170",
            "name": "Sung-Hee Kim",
            "type": "sparse",
            "x": 5.768592834472656,
            "y": 10.660682678222656
        },
        {
            "id": "s_171",
            "name": "Gregory D. Abowd",
            "type": "sparse",
            "x": 6.5693864822387695,
            "y": 7.4537224769592285
        },
        {
            "id": "s_172",
            "name": "Theresia Gschwandtner",
            "type": "sparse",
            "x": 5.974344730377197,
            "y": 7.765809059143066
        },
        {
            "id": "s_173",
            "name": "Fanny Chevalier",
            "type": "sparse",
            "x": 6.248006820678711,
            "y": 8.343260765075684
        },
        {
            "id": "s_174",
            "name": "Zhicong Lu",
            "type": "sparse",
            "x": 7.414912700653076,
            "y": 10.379096031188965
        },
        {
            "id": "s_175",
            "name": "Silvia Mabel Castro",
            "type": "sparse",
            "x": 5.554603576660156,
            "y": 6.4707207679748535
        },
        {
            "id": "s_176",
            "name": "Ben Eysenbach",
            "type": "sparse",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "id": "s_177",
            "name": "Gregory J. Zelinsky",
            "type": "sparse",
            "x": 5.165153503417969,
            "y": 9.47240924835205
        },
        {
            "id": "s_178",
            "name": "Song Zhang 0004",
            "type": "sparse",
            "x": 6.29480504989624,
            "y": 6.361831188201904
        },
        {
            "id": "s_179",
            "name": "Kanupriya Singhal",
            "type": "sparse",
            "x": 6.423430442810059,
            "y": 9.495590209960938
        },
        {
            "id": "s_180",
            "name": "Thomas Seidl 0001",
            "type": "sparse",
            "x": 4.426377296447754,
            "y": 6.390743732452393
        },
        {
            "id": "s_181",
            "name": "Jinyan Chen",
            "type": "sparse",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "id": "s_182",
            "name": "Xiaoming Liu 0002",
            "type": "sparse",
            "x": 2.1292340755462646,
            "y": 8.979385375976562
        },
        {
            "id": "s_183",
            "name": "Tiankai Xie",
            "type": "sparse",
            "x": 3.5202388763427734,
            "y": 8.396490097045898
        },
        {
            "id": "s_184",
            "name": "Serban R. Pop",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_185",
            "name": "Joon-Yong Lee",
            "type": "sparse",
            "x": 4.675631999969482,
            "y": 9.515111923217773
        },
        {
            "id": "s_186",
            "name": "David J. Israel",
            "type": "sparse",
            "x": 5.411427021026611,
            "y": 9.972384452819824
        },
        {
            "id": "s_187",
            "name": "Zhao-Peng Meng",
            "type": "sparse",
            "x": 6.323979377746582,
            "y": 6.447656631469727
        },
        {
            "id": "s_188",
            "name": "Albert Amor-Amoros",
            "type": "sparse",
            "x": 4.098504543304443,
            "y": 9.517640113830566
        },
        {
            "id": "s_189",
            "name": "Rusheng Pan",
            "type": "sparse",
            "x": 6.170205593109131,
            "y": 8.641339302062988
        },
        {
            "id": "s_190",
            "name": "CuiXia Ma",
            "type": "sparse",
            "x": 4.659048080444336,
            "y": 7.696251392364502
        },
        {
            "id": "s_191",
            "name": "Megan Monroe",
            "type": "sparse",
            "x": 5.925091743469238,
            "y": 7.095569610595703
        },
        {
            "id": "s_192",
            "name": "Eric Sauda",
            "type": "sparse",
            "x": 7.275294780731201,
            "y": 10.136569023132324
        },
        {
            "id": "s_193",
            "name": "Chad Jones",
            "type": "sparse",
            "x": 6.836374282836914,
            "y": 7.674856662750244
        },
        {
            "id": "s_194",
            "name": "Wentao Gu",
            "type": "sparse",
            "x": 4.472042560577393,
            "y": 6.537318706512451
        },
        {
            "id": "s_195",
            "name": "Tim Dwyer",
            "type": "sparse",
            "x": 4.143867492675781,
            "y": 8.534170150756836
        },
        {
            "id": "s_196",
            "name": "Harold J. Ship",
            "type": "sparse",
            "x": 3.7874088287353516,
            "y": 6.028797149658203
        },
        {
            "id": "s_197",
            "name": "George Chin Jr.",
            "type": "sparse",
            "x": 4.890290260314941,
            "y": 5.3867316246032715
        },
        {
            "id": "s_198",
            "name": "Dennis Dingen",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_199",
            "name": "Xing Mu",
            "type": "sparse",
            "x": 6.7766804695129395,
            "y": 7.190415382385254
        },
        {
            "id": "s_200",
            "name": "Junghoon Chae",
            "type": "sparse",
            "x": 6.551141738891602,
            "y": 7.311921119689941
        },
        {
            "id": "s_201",
            "name": "Julian Thijssen",
            "type": "sparse",
            "x": 4.3963398933410645,
            "y": 6.363718032836914
        },
        {
            "id": "s_202",
            "name": "Chris North 0001",
            "type": "sparse",
            "x": 5.345881462097168,
            "y": 8.79746150970459
        },
        {
            "id": "s_203",
            "name": "Baldur van Lew",
            "type": "sparse",
            "x": 4.3963398933410645,
            "y": 6.363718032836914
        },
        {
            "id": "s_204",
            "name": "Preeti Raghavan",
            "type": "sparse",
            "x": 5.91933536529541,
            "y": 6.946452617645264
        },
        {
            "id": "s_205",
            "name": "Alexander Lex",
            "type": "sparse",
            "x": 5.235106945037842,
            "y": 5.753823280334473
        },
        {
            "id": "s_206",
            "name": "Stephen Rudolph",
            "type": "sparse",
            "x": 7.707914352416992,
            "y": 7.081083297729492
        },
        {
            "id": "s_207",
            "name": "Deborah N. Huntzinger",
            "type": "sparse",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "id": "s_208",
            "name": "A. Dalpke",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_209",
            "name": "Trung-Tien Phan-Quang",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_210",
            "name": "Sujin Jang",
            "type": "sparse",
            "x": 7.460609436035156,
            "y": 5.697226047515869
        },
        {
            "id": "s_211",
            "name": "Ming C. Hao",
            "type": "sparse",
            "x": 6.570365905761719,
            "y": 7.32098388671875
        },
        {
            "id": "s_212",
            "name": "Audrey Bowerman",
            "type": "sparse",
            "x": 7.270965099334717,
            "y": 4.867156505584717
        },
        {
            "id": "s_213",
            "name": "Susan M. Mniszewski",
            "type": "sparse",
            "x": 3.117333173751831,
            "y": 7.8499674797058105
        },
        {
            "id": "s_214",
            "name": "Harald Bosch",
            "type": "sparse",
            "x": 6.558578968048096,
            "y": 8.110800743103027
        },
        {
            "id": "s_215",
            "name": "Liting Sun",
            "type": "sparse",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "id": "s_216",
            "name": "Ravikiran Vadlapudi",
            "type": "sparse",
            "x": 7.595698833465576,
            "y": 10.46611499786377
        },
        {
            "id": "s_217",
            "name": "Jingjing Guo",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_218",
            "name": "Shayan Monadjemi",
            "type": "sparse",
            "x": 5.984103202819824,
            "y": 9.63005256652832
        },
        {
            "id": "s_219",
            "name": "Ben Shneiderman",
            "type": "sparse",
            "x": 6.101319789886475,
            "y": 6.7484564781188965
        },
        {
            "id": "s_220",
            "name": "Jurim Lee",
            "type": "sparse",
            "x": 7.521107196807861,
            "y": 10.293533325195312
        },
        {
            "id": "s_221",
            "name": "Siwei Fu",
            "type": "sparse",
            "x": 4.5082902908325195,
            "y": 8.85177993774414
        },
        {
            "id": "s_222",
            "name": "Stefan M\u00fcller Arisona",
            "type": "sparse",
            "x": 7.451045989990234,
            "y": 4.985848426818848
        },
        {
            "id": "s_223",
            "name": "Katerina Vrotsou",
            "type": "sparse",
            "x": 6.730813980102539,
            "y": 6.651250839233398
        },
        {
            "id": "s_224",
            "name": "Evan A. Suma",
            "type": "sparse",
            "x": 7.047878265380859,
            "y": 8.843878746032715
        },
        {
            "id": "s_225",
            "name": "Awalin Sopan",
            "type": "sparse",
            "x": 3.6647329330444336,
            "y": 7.409862041473389
        },
        {
            "id": "s_226",
            "name": "Benjamin Karer",
            "type": "sparse",
            "x": 5.147592544555664,
            "y": 9.267566680908203
        },
        {
            "id": "s_227",
            "name": "Won-Dong Jang",
            "type": "sparse",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "id": "s_228",
            "name": "Fatih Korkmaz",
            "type": "sparse",
            "x": 5.032182216644287,
            "y": 8.1648588180542
        },
        {
            "id": "s_229",
            "name": "Nilaksh Das",
            "type": "sparse",
            "x": 2.1530587673187256,
            "y": 8.92031478881836
        },
        {
            "id": "s_230",
            "name": "Qingyang Xu",
            "type": "sparse",
            "x": 8.534769058227539,
            "y": 5.998741626739502
        },
        {
            "id": "s_231",
            "name": "Mark A. Whiting",
            "type": "sparse",
            "x": 6.5443806648254395,
            "y": 8.239991188049316
        },
        {
            "id": "s_232",
            "name": "Marcus Dostie",
            "type": "sparse",
            "x": 7.453543663024902,
            "y": 5.439891338348389
        },
        {
            "id": "s_233",
            "name": "William Ribarsky",
            "type": "sparse",
            "x": 6.172811031341553,
            "y": 9.06862735748291
        },
        {
            "id": "s_234",
            "name": "Adalberto Lafcadio Simeone",
            "type": "sparse",
            "x": 6.872135639190674,
            "y": 7.794391632080078
        },
        {
            "id": "s_235",
            "name": "Michael Hofmann 0010",
            "type": "sparse",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "id": "s_236",
            "name": "Amit Kumar",
            "type": "sparse",
            "x": 7.462177753448486,
            "y": 10.412565231323242
        },
        {
            "id": "s_237",
            "name": "Jerry Alan Fails",
            "type": "sparse",
            "x": 7.580902099609375,
            "y": 6.7360711097717285
        },
        {
            "id": "s_238",
            "name": "Anne Mai Wassermann",
            "type": "sparse",
            "x": 5.349819660186768,
            "y": 6.387389659881592
        },
        {
            "id": "s_239",
            "name": "Tom Baumgartl",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_240",
            "name": "Anjul Kumar Tyagi",
            "type": "sparse",
            "x": 3.9564976692199707,
            "y": 7.766167163848877
        },
        {
            "id": "s_241",
            "name": "Tatiana von Landesberger",
            "type": "sparse",
            "x": 5.667938232421875,
            "y": 6.1486968994140625
        },
        {
            "id": "s_242",
            "name": "Steven R. Gomez",
            "type": "sparse",
            "x": 5.286588191986084,
            "y": 9.481925964355469
        },
        {
            "id": "s_243",
            "name": "Arnaud Prouzeau",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_244",
            "name": "Rongwen Zhao",
            "type": "sparse",
            "x": 7.656327247619629,
            "y": 6.817885875701904
        },
        {
            "id": "s_245",
            "name": "Ziyang Guo",
            "type": "sparse",
            "x": 8.534769058227539,
            "y": 5.998741626739502
        },
        {
            "id": "s_246",
            "name": "Sietse J. Luk",
            "type": "sparse",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "id": "s_247",
            "name": "Ji Hwan Park",
            "type": "sparse",
            "x": 5.832940101623535,
            "y": 7.073251247406006
        },
        {
            "id": "s_248",
            "name": "Daniel Seebacher",
            "type": "sparse",
            "x": 5.063028812408447,
            "y": 6.030219078063965
        },
        {
            "id": "s_249",
            "name": "Aviv Madar",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_250",
            "name": "Salvatore Rinzivillo",
            "type": "sparse",
            "x": 5.554372787475586,
            "y": 5.490920543670654
        },
        {
            "id": "s_251",
            "name": "Marcus A. Magnor",
            "type": "sparse",
            "x": 4.372778415679932,
            "y": 7.045327663421631
        },
        {
            "id": "s_252",
            "name": "Bob Baddeley",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_253",
            "name": "Bongshin Lee",
            "type": "sparse",
            "x": 5.731845378875732,
            "y": 7.929624080657959
        },
        {
            "id": "s_254",
            "name": "Klaus Mueller 0001",
            "type": "sparse",
            "x": 4.485874176025391,
            "y": 7.429938793182373
        },
        {
            "id": "s_255",
            "name": "Azam Khan",
            "type": "sparse",
            "x": 6.03320837020874,
            "y": 8.066607475280762
        },
        {
            "id": "s_256",
            "name": "Enamul Hoque",
            "type": "sparse",
            "x": 5.195247650146484,
            "y": 9.794124603271484
        },
        {
            "id": "s_257",
            "name": "Xiaolong Zhang 0001",
            "type": "sparse",
            "x": 6.99454402923584,
            "y": 8.42131233215332
        },
        {
            "id": "s_258",
            "name": "Feng Tian 0001",
            "type": "sparse",
            "x": 5.462973117828369,
            "y": 7.948393821716309
        },
        {
            "id": "s_259",
            "name": "Lei Zhang",
            "type": "sparse",
            "x": 6.134147644042969,
            "y": 8.68932056427002
        },
        {
            "id": "s_260",
            "name": "Mingming Fan 0001",
            "type": "sparse",
            "x": 7.414912700653076,
            "y": 10.379096031188965
        },
        {
            "id": "s_261",
            "name": "Hanna Sch\u00e4fer",
            "type": "sparse",
            "x": 3.1717755794525146,
            "y": 8.746894836425781
        },
        {
            "id": "s_262",
            "name": "Paulo Joia",
            "type": "sparse",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "id": "s_263",
            "name": "Dongshi Xu",
            "type": "sparse",
            "x": 6.856532096862793,
            "y": 6.264466762542725
        },
        {
            "id": "s_264",
            "name": "Ulrik Brandes",
            "type": "sparse",
            "x": 7.384159088134766,
            "y": 7.670565605163574
        },
        {
            "id": "s_265",
            "name": "Eva Mayr",
            "type": "sparse",
            "x": 4.754398345947266,
            "y": 9.436184883117676
        },
        {
            "id": "s_266",
            "name": "Runlin Li",
            "type": "sparse",
            "x": 8.89720630645752,
            "y": 7.1770339012146
        },
        {
            "id": "s_267",
            "name": "Qing Chen 0001",
            "type": "sparse",
            "x": 3.8067502975463867,
            "y": 8.83480167388916
        },
        {
            "id": "s_268",
            "name": "William A. Pike",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_269",
            "name": "Ronghua Liang",
            "type": "sparse",
            "x": 7.605733394622803,
            "y": 6.749001502990723
        },
        {
            "id": "s_270",
            "name": "Lionel M. Ni",
            "type": "sparse",
            "x": 7.01460075378418,
            "y": 5.634749412536621
        },
        {
            "id": "s_271",
            "name": "Bastian Goldl\u00fccke",
            "type": "sparse",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "id": "s_272",
            "name": "Jingjing Liu",
            "type": "sparse",
            "x": 4.374527931213379,
            "y": 6.887147426605225
        },
        {
            "id": "s_273",
            "name": "Min Chen 0001",
            "type": "sparse",
            "x": 4.741181373596191,
            "y": 8.707047462463379
        },
        {
            "id": "s_274",
            "name": "Finale Doshi-Velez",
            "type": "sparse",
            "x": 5.8732194900512695,
            "y": 6.8065009117126465
        },
        {
            "id": "s_275",
            "name": "Harald Piringer",
            "type": "sparse",
            "x": 4.445923805236816,
            "y": 7.760870933532715
        },
        {
            "id": "s_276",
            "name": "Mark Wallace 0001",
            "type": "sparse",
            "x": 3.7070932388305664,
            "y": 7.860748291015625
        },
        {
            "id": "s_277",
            "name": "Haiyang Wang",
            "type": "sparse",
            "x": 7.030353546142578,
            "y": 4.95527458190918
        },
        {
            "id": "s_278",
            "name": "Simon Butscher",
            "type": "sparse",
            "x": 5.770631790161133,
            "y": 7.734119415283203
        },
        {
            "id": "s_279",
            "name": "Xiaoke Huang",
            "type": "sparse",
            "x": 7.365744590759277,
            "y": 4.923183917999268
        },
        {
            "id": "s_280",
            "name": "Guodao Sun",
            "type": "sparse",
            "x": 7.939236164093018,
            "y": 8.651774406433105
        },
        {
            "id": "s_281",
            "name": "Xu-Meng Wang",
            "type": "sparse",
            "x": 6.1468915939331055,
            "y": 8.70013427734375
        },
        {
            "id": "s_282",
            "name": "Maria Florencia Gargiulo",
            "type": "sparse",
            "x": 5.554603576660156,
            "y": 6.4707207679748535
        },
        {
            "id": "s_283",
            "name": "Samah Gad",
            "type": "sparse",
            "x": 5.472451210021973,
            "y": 10.237101554870605
        },
        {
            "id": "s_284",
            "name": "Robert Harper 0002",
            "type": "sparse",
            "x": 6.958956241607666,
            "y": 7.864200592041016
        },
        {
            "id": "s_285",
            "name": "Hong Zhou 0004",
            "type": "sparse",
            "x": 7.652134895324707,
            "y": 8.588372230529785
        },
        {
            "id": "s_286",
            "name": "Jiazhi Xia",
            "type": "sparse",
            "x": 5.2220234870910645,
            "y": 6.560883045196533
        },
        {
            "id": "s_287",
            "name": "Bo Dong 0001",
            "type": "sparse",
            "x": 7.421921730041504,
            "y": 7.7123517990112305
        },
        {
            "id": "s_288",
            "name": "Mihaela Vorvoreanu",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_289",
            "name": "Edwin Puttmann",
            "type": "sparse",
            "x": 7.733405113220215,
            "y": 8.00662612915039
        },
        {
            "id": "s_290",
            "name": "Johannes Kehrer",
            "type": "sparse",
            "x": 3.9637699127197266,
            "y": 6.8339524269104
        },
        {
            "id": "s_291",
            "name": "Alexis Pister",
            "type": "sparse",
            "x": 3.5988824367523193,
            "y": 5.591180801391602
        },
        {
            "id": "s_292",
            "name": "Ruixiang Zhang",
            "type": "sparse",
            "x": 2.921347141265869,
            "y": 8.863079071044922
        },
        {
            "id": "s_293",
            "name": "Michael Glueck",
            "type": "sparse",
            "x": 6.03320837020874,
            "y": 8.066607475280762
        },
        {
            "id": "s_294",
            "name": "Julia Eunju Nam",
            "type": "sparse",
            "x": 3.451568126678467,
            "y": 8.536810874938965
        },
        {
            "id": "s_295",
            "name": "Rhodri Bown",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_296",
            "name": "William Chao",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_297",
            "name": "Carrie Varley",
            "type": "sparse",
            "x": 6.7198567390441895,
            "y": 8.789078712463379
        },
        {
            "id": "s_298",
            "name": "Joseph M. Hellerstein",
            "type": "sparse",
            "x": 5.356631278991699,
            "y": 9.290544509887695
        },
        {
            "id": "s_299",
            "name": "Xi Ye",
            "type": "sparse",
            "x": 4.549491882324219,
            "y": 8.062954902648926
        },
        {
            "id": "s_300",
            "name": "Peter J. Polack Jr.",
            "type": "sparse",
            "x": 7.04141902923584,
            "y": 6.825443267822266
        },
        {
            "id": "s_301",
            "name": "Qingsong Liu",
            "type": "sparse",
            "x": 5.17265510559082,
            "y": 5.230408668518066
        },
        {
            "id": "s_302",
            "name": "Nadya A. Calder\u00f3n",
            "type": "sparse",
            "x": 5.203633785247803,
            "y": 9.702972412109375
        },
        {
            "id": "s_303",
            "name": "Ben Maule",
            "type": "sparse",
            "x": 6.568792343139648,
            "y": 7.300233840942383
        },
        {
            "id": "s_304",
            "name": "Omar ElTayeby",
            "type": "sparse",
            "x": 7.529202938079834,
            "y": 9.4118013381958
        },
        {
            "id": "s_305",
            "name": "Wenwen Dou",
            "type": "sparse",
            "x": 6.356529235839844,
            "y": 9.501063346862793
        },
        {
            "id": "s_306",
            "name": "Michal Jacovi",
            "type": "sparse",
            "x": 7.6279706954956055,
            "y": 8.709637641906738
        },
        {
            "id": "s_307",
            "name": "Julie Heiser",
            "type": "sparse",
            "x": 7.2240824699401855,
            "y": 10.312528610229492
        },
        {
            "id": "s_308",
            "name": "Jonathan Dubois",
            "type": "sparse",
            "x": 5.152979373931885,
            "y": 5.32603120803833
        },
        {
            "id": "s_309",
            "name": "Chris E. Weaver",
            "type": "sparse",
            "x": 5.75516414642334,
            "y": 7.829800605773926
        },
        {
            "id": "s_310",
            "name": "Rebecca Nowak",
            "type": "sparse",
            "x": 5.602627754211426,
            "y": 6.447721481323242
        },
        {
            "id": "s_311",
            "name": "Duen Horng Chau",
            "type": "sparse",
            "x": 5.514884948730469,
            "y": 6.951671123504639
        },
        {
            "id": "s_312",
            "name": "Mor Naaman",
            "type": "sparse",
            "x": 8.004839897155762,
            "y": 8.350887298583984
        },
        {
            "id": "s_313",
            "name": "Kang Zhang 0001",
            "type": "sparse",
            "x": 6.323979377746582,
            "y": 6.447656631469727
        },
        {
            "id": "s_314",
            "name": "Thomas Ertl",
            "type": "sparse",
            "x": 6.51603364944458,
            "y": 8.879149436950684
        },
        {
            "id": "s_315",
            "name": "Xinxin Huang",
            "type": "sparse",
            "x": 6.923262119293213,
            "y": 6.862221717834473
        },
        {
            "id": "s_316",
            "name": "Shixia Liu",
            "type": "sparse",
            "x": 5.664394378662109,
            "y": 8.241772651672363
        },
        {
            "id": "s_317",
            "name": "Stefan J\u00e4nicke",
            "type": "sparse",
            "x": 7.2519378662109375,
            "y": 10.143952369689941
        },
        {
            "id": "s_318",
            "name": "Sihang Li",
            "type": "sparse",
            "x": 7.80152702331543,
            "y": 8.48024845123291
        },
        {
            "id": "s_319",
            "name": "Jie Lu 0002",
            "type": "sparse",
            "x": 7.491602897644043,
            "y": 8.011017799377441
        },
        {
            "id": "s_320",
            "name": "Christof Seeger",
            "type": "sparse",
            "x": 6.034650802612305,
            "y": 7.87465238571167
        },
        {
            "id": "s_321",
            "name": "Alexander Kumpf",
            "type": "sparse",
            "x": 4.963197708129883,
            "y": 5.9895172119140625
        },
        {
            "id": "s_322",
            "name": "Daniel M. Best",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_323",
            "name": "Lijing Lin",
            "type": "sparse",
            "x": 7.854940891265869,
            "y": 8.153303146362305
        },
        {
            "id": "s_324",
            "name": "David Scott Warren",
            "type": "sparse",
            "x": 5.165153503417969,
            "y": 9.47240924835205
        },
        {
            "id": "s_325",
            "name": "Robert A. Bridges",
            "type": "sparse",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        },
        {
            "id": "s_326",
            "name": "James Davey",
            "type": "sparse",
            "x": 5.055766582489014,
            "y": 7.4191131591796875
        },
        {
            "id": "s_327",
            "name": "Siming Chen 0001",
            "type": "sparse",
            "x": 6.9419050216674805,
            "y": 7.307591915130615
        },
        {
            "id": "s_328",
            "name": "Haojin Jiang",
            "type": "sparse",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "id": "s_329",
            "name": "Jose Manuel Cordero Garcia",
            "type": "sparse",
            "x": 7.484586715698242,
            "y": 5.297567367553711
        },
        {
            "id": "s_330",
            "name": "Xin Chen",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_331",
            "name": "Jaegul Choo",
            "type": "sparse",
            "x": 5.541679382324219,
            "y": 8.557923316955566
        },
        {
            "id": "s_332",
            "name": "Carlos J. Felix",
            "type": "sparse",
            "x": 4.998837947845459,
            "y": 7.000543594360352
        },
        {
            "id": "s_333",
            "name": "Remco Chang",
            "type": "sparse",
            "x": 5.142661094665527,
            "y": 9.0299711227417
        },
        {
            "id": "s_334",
            "name": "Martin Wikelski",
            "type": "sparse",
            "x": 5.320591926574707,
            "y": 6.559731483459473
        },
        {
            "id": "s_335",
            "name": "Dieter W. Fellner",
            "type": "sparse",
            "x": 4.2764739990234375,
            "y": 8.524543762207031
        },
        {
            "id": "s_336",
            "name": "Ren Liu",
            "type": "sparse",
            "x": 6.819993019104004,
            "y": 6.766763210296631
        },
        {
            "id": "s_337",
            "name": "Jonathan J. H. Zhu",
            "type": "sparse",
            "x": 7.966888904571533,
            "y": 8.550166130065918
        },
        {
            "id": "s_338",
            "name": "Josef Focht",
            "type": "sparse",
            "x": 6.975711345672607,
            "y": 9.864171981811523
        },
        {
            "id": "s_339",
            "name": "Haipeng Zeng",
            "type": "sparse",
            "x": 5.177468776702881,
            "y": 9.842877388000488
        },
        {
            "id": "s_340",
            "name": "Dino Pedreschi",
            "type": "sparse",
            "x": 3.618051052093506,
            "y": 5.657589912414551
        },
        {
            "id": "s_341",
            "name": "Markus Petzold",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_342",
            "name": "Marlene Baumgart",
            "type": "sparse",
            "x": 3.5826504230499268,
            "y": 5.609038829803467
        },
        {
            "id": "s_343",
            "name": "Alireza Karduni",
            "type": "sparse",
            "x": 4.635231018066406,
            "y": 9.437541007995605
        },
        {
            "id": "s_344",
            "name": "Mike Sips",
            "type": "sparse",
            "x": 6.0440592765808105,
            "y": 6.3947954177856445
        },
        {
            "id": "s_345",
            "name": "Alexander Rind",
            "type": "sparse",
            "x": 4.695831775665283,
            "y": 8.137592315673828
        },
        {
            "id": "s_346",
            "name": "SungYe Kim",
            "type": "sparse",
            "x": 6.643281936645508,
            "y": 7.454643249511719
        },
        {
            "id": "s_347",
            "name": "Jeremy G. Freeman",
            "type": "sparse",
            "x": 3.1261494159698486,
            "y": 7.796170234680176
        },
        {
            "id": "s_348",
            "name": "Siyuan Liu",
            "type": "sparse",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "id": "s_349",
            "name": "Dirk Bartz",
            "type": "sparse",
            "x": 4.756052494049072,
            "y": 6.701610088348389
        },
        {
            "id": "s_350",
            "name": "Nils Wilhelm",
            "type": "sparse",
            "x": 7.492415904998779,
            "y": 5.876190662384033
        },
        {
            "id": "s_351",
            "name": "Wesley Willett",
            "type": "sparse",
            "x": 5.919469833374023,
            "y": 10.14952278137207
        },
        {
            "id": "s_352",
            "name": "Chris Bryan",
            "type": "sparse",
            "x": 4.643769264221191,
            "y": 8.24565315246582
        },
        {
            "id": "s_353",
            "name": "Yifan Hu 0001",
            "type": "sparse",
            "x": 5.147846698760986,
            "y": 5.0413899421691895
        },
        {
            "id": "s_354",
            "name": "Teresa O'Connell",
            "type": "sparse",
            "x": 6.814907550811768,
            "y": 8.75832462310791
        },
        {
            "id": "s_355",
            "name": "Rolf Mueller",
            "type": "sparse",
            "x": 3.6070477962493896,
            "y": 5.6530327796936035
        },
        {
            "id": "s_356",
            "name": "Anqi Cao",
            "type": "sparse",
            "x": 8.519206047058105,
            "y": 6.030973434448242
        },
        {
            "id": "s_357",
            "name": "Fabian Maass",
            "type": "sparse",
            "x": 5.899907112121582,
            "y": 7.304012298583984
        },
        {
            "id": "s_358",
            "name": "Patrick Fiaux",
            "type": "sparse",
            "x": 5.474067211151123,
            "y": 9.94048023223877
        },
        {
            "id": "s_359",
            "name": "Xinnan Du",
            "type": "sparse",
            "x": 6.867059707641602,
            "y": 7.571639060974121
        },
        {
            "id": "s_360",
            "name": "Luis Gustavo Nonato",
            "type": "sparse",
            "x": 5.803957462310791,
            "y": 6.575812816619873
        },
        {
            "id": "s_361",
            "name": "Sharmin Choudhury",
            "type": "sparse",
            "x": 5.564159870147705,
            "y": 10.273418426513672
        },
        {
            "id": "s_362",
            "name": "Haidong Zhang",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_363",
            "name": "Han-Wei Shen",
            "type": "sparse",
            "x": 4.354574680328369,
            "y": 7.957513332366943
        },
        {
            "id": "s_364",
            "name": "Naveen Pitipornvivat",
            "type": "sparse",
            "x": 5.1919989585876465,
            "y": 5.070548057556152
        },
        {
            "id": "s_365",
            "name": "Eric D. Ragan",
            "type": "sparse",
            "x": 6.833795070648193,
            "y": 8.67609691619873
        },
        {
            "id": "s_366",
            "name": "Adeel Khamisa",
            "type": "sparse",
            "x": 6.673811435699463,
            "y": 8.611976623535156
        },
        {
            "id": "s_367",
            "name": "Shoubin Cheng",
            "type": "sparse",
            "x": 8.536551475524902,
            "y": 5.959712982177734
        },
        {
            "id": "s_368",
            "name": "Randall M. Rohrer",
            "type": "sparse",
            "x": 5.126974582672119,
            "y": 5.073885440826416
        },
        {
            "id": "s_369",
            "name": "Jiannan Xiao",
            "type": "sparse",
            "x": 3.3129467964172363,
            "y": 7.173401355743408
        },
        {
            "id": "s_370",
            "name": "Robin Valenza",
            "type": "sparse",
            "x": 7.1605377197265625,
            "y": 10.400833129882812
        },
        {
            "id": "s_371",
            "name": "Weichao Wang",
            "type": "sparse",
            "x": 6.884183406829834,
            "y": 7.541064262390137
        },
        {
            "id": "s_372",
            "name": "Stephen Ingram",
            "type": "sparse",
            "x": 4.042430877685547,
            "y": 7.574769496917725
        },
        {
            "id": "s_373",
            "name": "Shuai Chen 0001",
            "type": "sparse",
            "x": 7.825162887573242,
            "y": 8.398510932922363
        },
        {
            "id": "s_374",
            "name": "Yuan Gao",
            "type": "sparse",
            "x": 7.3848042488098145,
            "y": 4.858085632324219
        },
        {
            "id": "s_375",
            "name": "Meredith Ringel Morris",
            "type": "sparse",
            "x": 5.419025897979736,
            "y": 10.711797714233398
        },
        {
            "id": "s_376",
            "name": "Doug Fritz",
            "type": "sparse",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "id": "s_377",
            "name": "Jinsong Wang",
            "type": "sparse",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "id": "s_378",
            "name": "Helwig Hauser",
            "type": "sparse",
            "x": 4.004556655883789,
            "y": 8.247773170471191
        },
        {
            "id": "s_379",
            "name": "Gordon Woodhull",
            "type": "sparse",
            "x": 5.817330360412598,
            "y": 10.671513557434082
        },
        {
            "id": "s_380",
            "name": "Fraser Anderson",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_381",
            "name": "Jun Yuan 0003",
            "type": "sparse",
            "x": 4.388597011566162,
            "y": 6.54218053817749
        },
        {
            "id": "s_382",
            "name": "Lorenz Linhardt",
            "type": "sparse",
            "x": 3.3038711547851562,
            "y": 7.186951160430908
        },
        {
            "id": "s_383",
            "name": "Emilio Vital Brazil",
            "type": "sparse",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "id": "s_384",
            "name": "Kun Zhou 0001",
            "type": "sparse",
            "x": 4.472042560577393,
            "y": 6.537318706512451
        },
        {
            "id": "s_385",
            "name": "Christian Tominski",
            "type": "sparse",
            "x": 4.256865501403809,
            "y": 9.337084770202637
        },
        {
            "id": "s_386",
            "name": "Shouxing Xiang",
            "type": "sparse",
            "x": 4.4690446853637695,
            "y": 7.302567481994629
        },
        {
            "id": "s_387",
            "name": "Krist Wongsuphasawat",
            "type": "sparse",
            "x": 7.321428298950195,
            "y": 7.277231693267822
        },
        {
            "id": "s_388",
            "name": "Yingjie Victor Chen",
            "type": "sparse",
            "x": 5.962850570678711,
            "y": 9.402202606201172
        },
        {
            "id": "s_389",
            "name": "Mosab Khayat",
            "type": "sparse",
            "x": 5.883443832397461,
            "y": 8.729681968688965
        },
        {
            "id": "s_390",
            "name": "Florence Y. Wang",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_391",
            "name": "Georgia Albuquerque",
            "type": "sparse",
            "x": 4.372778415679932,
            "y": 7.045327663421631
        },
        {
            "id": "s_392",
            "name": "John Alexis Guerra G\u00f3mez",
            "type": "sparse",
            "x": 3.320497751235962,
            "y": 7.061007022857666
        },
        {
            "id": "s_393",
            "name": "Cl\u00e1udio T. Silva",
            "type": "sparse",
            "x": 5.870697498321533,
            "y": 6.524290561676025
        },
        {
            "id": "s_394",
            "name": "Derek Xiaoyu Wang",
            "type": "sparse",
            "x": 7.275294780731201,
            "y": 10.136569023132324
        },
        {
            "id": "s_395",
            "name": "Pierre-Luc Hemery",
            "type": "sparse",
            "x": 5.43972110748291,
            "y": 8.626875877380371
        },
        {
            "id": "s_396",
            "name": "Minjung Kim",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_397",
            "name": "Puripant Ruchikachorn",
            "type": "sparse",
            "x": 4.627654552459717,
            "y": 7.064582347869873
        },
        {
            "id": "s_398",
            "name": "Artem Amirkhanov",
            "type": "sparse",
            "x": 5.075896739959717,
            "y": 6.747625827789307
        },
        {
            "id": "s_399",
            "name": "Fabian Beck 0001",
            "type": "sparse",
            "x": 6.868045330047607,
            "y": 9.973257064819336
        },
        {
            "id": "s_400",
            "name": "Dirk Streeb",
            "type": "sparse",
            "x": 4.8054656982421875,
            "y": 5.500706195831299
        },
        {
            "id": "s_401",
            "name": "Hanghang Tong",
            "type": "sparse",
            "x": 4.921706199645996,
            "y": 6.673541069030762
        },
        {
            "id": "s_402",
            "name": "Agus Sudjianto",
            "type": "sparse",
            "x": 8.852354049682617,
            "y": 7.137535572052002
        },
        {
            "id": "s_403",
            "name": "Aidong Lu",
            "type": "sparse",
            "x": 5.853851318359375,
            "y": 8.812668800354004
        },
        {
            "id": "s_404",
            "name": "Steffen Koch 0001",
            "type": "sparse",
            "x": 6.621237277984619,
            "y": 9.123343467712402
        },
        {
            "id": "s_405",
            "name": "Simone Kriglstein",
            "type": "sparse",
            "x": 8.856771469116211,
            "y": 7.150928497314453
        },
        {
            "id": "s_406",
            "name": "Simon J. Walton",
            "type": "sparse",
            "x": 5.301107883453369,
            "y": 6.747445583343506
        },
        {
            "id": "s_407",
            "name": "Udo Schlegel",
            "type": "sparse",
            "x": 4.1301984786987305,
            "y": 7.000141143798828
        },
        {
            "id": "s_408",
            "name": "Jie Li 0006",
            "type": "sparse",
            "x": 5.53632926940918,
            "y": 6.185448169708252
        },
        {
            "id": "s_409",
            "name": "Christian P\u00f6litz",
            "type": "sparse",
            "x": 7.473939895629883,
            "y": 7.139910697937012
        },
        {
            "id": "s_410",
            "name": "Sara L. Su",
            "type": "sparse",
            "x": 4.692488193511963,
            "y": 9.84524154663086
        },
        {
            "id": "s_411",
            "name": "Joel Daniels II",
            "type": "sparse",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "id": "s_412",
            "name": "Marco Hutter 0002",
            "type": "sparse",
            "x": 4.2764739990234375,
            "y": 8.524543762207031
        },
        {
            "id": "s_413",
            "name": "Quan Lin",
            "type": "sparse",
            "x": 4.95570182800293,
            "y": 10.061023712158203
        },
        {
            "id": "s_414",
            "name": "Shunan Guo",
            "type": "sparse",
            "x": 7.654257297515869,
            "y": 7.013844966888428
        },
        {
            "id": "s_415",
            "name": "Janko Dietzsch",
            "type": "sparse",
            "x": 4.756052494049072,
            "y": 6.701610088348389
        },
        {
            "id": "s_416",
            "name": "Steve DiPaola",
            "type": "sparse",
            "x": 6.003656387329102,
            "y": 9.494745254516602
        },
        {
            "id": "s_417",
            "name": "Helen C. Miles",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_418",
            "name": "Juncong Lin",
            "type": "sparse",
            "x": 7.166633605957031,
            "y": 4.956838130950928
        },
        {
            "id": "s_419",
            "name": "Kelly M. T. Huffer",
            "type": "sparse",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        },
        {
            "id": "s_420",
            "name": "Daniel Archambault",
            "type": "sparse",
            "x": 5.977020740509033,
            "y": 6.10413932800293
        },
        {
            "id": "s_421",
            "name": "Pak Chung Wong",
            "type": "sparse",
            "x": 5.008632659912109,
            "y": 5.230308532714844
        },
        {
            "id": "s_422",
            "name": "Yeting Xu",
            "type": "sparse",
            "x": 8.83371639251709,
            "y": 7.131134986877441
        },
        {
            "id": "s_423",
            "name": "Chao Han 0005",
            "type": "sparse",
            "x": 5.255892276763916,
            "y": 9.593609809875488
        },
        {
            "id": "s_424",
            "name": "Simone Scheithauer",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_425",
            "name": "Rong Zhang 0011",
            "type": "sparse",
            "x": 8.882956504821777,
            "y": 7.1464362144470215
        },
        {
            "id": "s_426",
            "name": "Vanessa Eichel",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_427",
            "name": "Tim Lammarsch",
            "type": "sparse",
            "x": 5.351468563079834,
            "y": 7.5015482902526855
        },
        {
            "id": "s_428",
            "name": "Yating Wei",
            "type": "sparse",
            "x": 4.4847564697265625,
            "y": 6.689899921417236
        },
        {
            "id": "s_429",
            "name": "Sumeet Tandon",
            "type": "sparse",
            "x": 6.7051167488098145,
            "y": 8.124053955078125
        },
        {
            "id": "s_430",
            "name": "Qianwen Wang",
            "type": "sparse",
            "x": 4.543193817138672,
            "y": 9.233100891113281
        },
        {
            "id": "s_431",
            "name": "Roxanne Ryan",
            "type": "sparse",
            "x": 5.070459365844727,
            "y": 10.236028671264648
        },
        {
            "id": "s_432",
            "name": "Xinsong Yang",
            "type": "sparse",
            "x": 5.197463035583496,
            "y": 5.419427871704102
        },
        {
            "id": "s_433",
            "name": "Hossein Siadati",
            "type": "sparse",
            "x": 6.941458702087402,
            "y": 8.855831146240234
        },
        {
            "id": "s_434",
            "name": "Anna Vilanova",
            "type": "sparse",
            "x": 3.2788166999816895,
            "y": 7.642777442932129
        },
        {
            "id": "s_435",
            "name": "Thomas M\u00fchlbacher",
            "type": "sparse",
            "x": 3.6378157138824463,
            "y": 7.717992305755615
        },
        {
            "id": "s_436",
            "name": "Yuanzhe Chen",
            "type": "sparse",
            "x": 4.918525695800781,
            "y": 8.100534439086914
        },
        {
            "id": "s_437",
            "name": "Erel Uziel",
            "type": "sparse",
            "x": 7.6279706954956055,
            "y": 8.709637641906738
        },
        {
            "id": "s_438",
            "name": "Qi'an Chen",
            "type": "sparse",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "id": "s_439",
            "name": "Zhen Wen",
            "type": "sparse",
            "x": 7.767972469329834,
            "y": 8.18466567993164
        },
        {
            "id": "s_440",
            "name": "Sriram Karthik Badam",
            "type": "sparse",
            "x": 5.138099670410156,
            "y": 10.092191696166992
        },
        {
            "id": "s_441",
            "name": "Shenyu Xu",
            "type": "sparse",
            "x": 5.659013748168945,
            "y": 8.697281837463379
        },
        {
            "id": "s_442",
            "name": "Luke S. Snyder",
            "type": "sparse",
            "x": 4.765856742858887,
            "y": 9.071268081665039
        },
        {
            "id": "s_443",
            "name": "Minoo Erfani Joorabchi",
            "type": "sparse",
            "x": 7.693778991699219,
            "y": 8.201777458190918
        },
        {
            "id": "s_444",
            "name": "Florian Mansmann",
            "type": "sparse",
            "x": 7.12154483795166,
            "y": 7.952777862548828
        },
        {
            "id": "s_445",
            "name": "Alexander Mordvintsev",
            "type": "sparse",
            "x": 4.3963398933410645,
            "y": 6.363718032836914
        },
        {
            "id": "s_446",
            "name": "Hongye Liang",
            "type": "sparse",
            "x": 8.536551475524902,
            "y": 5.959712982177734
        },
        {
            "id": "s_447",
            "name": "John R. Goodall",
            "type": "sparse",
            "x": 6.207950592041016,
            "y": 8.04184341430664
        },
        {
            "id": "s_448",
            "name": "Greg Abram",
            "type": "sparse",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "id": "s_449",
            "name": "Chufan Lai",
            "type": "sparse",
            "x": 7.39639139175415,
            "y": 4.817005157470703
        },
        {
            "id": "s_450",
            "name": "Tin Kam Ho",
            "type": "sparse",
            "x": 4.702369213104248,
            "y": 6.508816719055176
        },
        {
            "id": "s_451",
            "name": "Min-Je Choi",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_452",
            "name": "Seungyoon Lee",
            "type": "sparse",
            "x": 4.951333045959473,
            "y": 5.35188627243042
        },
        {
            "id": "s_453",
            "name": "Llyr ap Cenydd",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_454",
            "name": "Huihua Guan",
            "type": "sparse",
            "x": 6.1468915939331055,
            "y": 8.70013427734375
        },
        {
            "id": "s_455",
            "name": "Dilip Krishnan",
            "type": "sparse",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "id": "s_456",
            "name": "Patrick Oesterling",
            "type": "sparse",
            "x": 7.229828834533691,
            "y": 10.067737579345703
        },
        {
            "id": "s_457",
            "name": "Airong Luo",
            "type": "sparse",
            "x": 4.980503082275391,
            "y": 5.211485862731934
        },
        {
            "id": "s_458",
            "name": "Sebastian Baltes",
            "type": "sparse",
            "x": 5.900495529174805,
            "y": 9.673620223999023
        },
        {
            "id": "s_459",
            "name": "David Borland",
            "type": "sparse",
            "x": 6.040262222290039,
            "y": 7.4920125007629395
        },
        {
            "id": "s_460",
            "name": "Guowei Huang 0002",
            "type": "sparse",
            "x": 5.1919989585876465,
            "y": 5.070548057556152
        },
        {
            "id": "s_461",
            "name": "Peter K. Sorger",
            "type": "sparse",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "id": "s_462",
            "name": "Pedro R. Walteros",
            "type": "sparse",
            "x": 7.202186584472656,
            "y": 6.9594502449035645
        },
        {
            "id": "s_463",
            "name": "Alexander M. Rush",
            "type": "sparse",
            "x": 2.9387755393981934,
            "y": 8.857743263244629
        },
        {
            "id": "s_464",
            "name": "Benjamin Tyner",
            "type": "sparse",
            "x": 6.365241527557373,
            "y": 6.8800506591796875
        },
        {
            "id": "s_465",
            "name": "Inbal Ronen",
            "type": "sparse",
            "x": 7.6279706954956055,
            "y": 8.709637641906738
        },
        {
            "id": "s_466",
            "name": "Xiaolong Luke Zhang",
            "type": "sparse",
            "x": 7.425845623016357,
            "y": 5.407773494720459
        },
        {
            "id": "s_467",
            "name": "Josua Krause",
            "type": "sparse",
            "x": 4.271104335784912,
            "y": 8.069756507873535
        },
        {
            "id": "s_468",
            "name": "Hanseung Lee",
            "type": "sparse",
            "x": 5.7978644371032715,
            "y": 6.848394393920898
        },
        {
            "id": "s_469",
            "name": "Yanhong Wu",
            "type": "sparse",
            "x": 5.214345455169678,
            "y": 7.641722202301025
        },
        {
            "id": "s_470",
            "name": "Chaoguang Lin",
            "type": "sparse",
            "x": 7.012173652648926,
            "y": 7.094387531280518
        },
        {
            "id": "s_471",
            "name": "Boyd Kenkhuis",
            "type": "sparse",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "id": "s_472",
            "name": "Kostiantyn Kucher",
            "type": "sparse",
            "x": 3.587266445159912,
            "y": 8.453961372375488
        },
        {
            "id": "s_473",
            "name": "Enrico Bertini",
            "type": "sparse",
            "x": 5.103095054626465,
            "y": 8.210104942321777
        },
        {
            "id": "s_474",
            "name": "Lezhi Li",
            "type": "sparse",
            "x": 3.061535596847534,
            "y": 8.740524291992188
        },
        {
            "id": "s_475",
            "name": "Binghan Xu",
            "type": "sparse",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "id": "s_476",
            "name": "G. Elisabeta Marai",
            "type": "sparse",
            "x": 6.26613712310791,
            "y": 6.6600165367126465
        },
        {
            "id": "s_477",
            "name": "Winniefried Kuan",
            "type": "sparse",
            "x": 6.660202503204346,
            "y": 8.713521003723145
        },
        {
            "id": "s_478",
            "name": "Vidya Setlur",
            "type": "sparse",
            "x": 5.159065246582031,
            "y": 9.94525146484375
        },
        {
            "id": "s_479",
            "name": "Martin Mladenov",
            "type": "sparse",
            "x": 7.473939895629883,
            "y": 7.139910697937012
        },
        {
            "id": "s_480",
            "name": "Lyndsey Franklin",
            "type": "sparse",
            "x": 5.090229511260986,
            "y": 9.739994049072266
        },
        {
            "id": "s_481",
            "name": "Paolo Federico 0001",
            "type": "sparse",
            "x": 4.098504543304443,
            "y": 9.517640113830566
        },
        {
            "id": "s_482",
            "name": "Francine Chen 0001",
            "type": "sparse",
            "x": 5.016319751739502,
            "y": 5.535304069519043
        },
        {
            "id": "s_483",
            "name": "Mario Luis Arrieta-Ortiz",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_484",
            "name": "Christian Partl",
            "type": "sparse",
            "x": 5.349819660186768,
            "y": 6.387389659881592
        },
        {
            "id": "s_485",
            "name": "Hyunmo Kang",
            "type": "sparse",
            "x": 5.1796979904174805,
            "y": 5.322124481201172
        },
        {
            "id": "s_486",
            "name": "Jack Pegg",
            "type": "sparse",
            "x": 4.543193817138672,
            "y": 9.233100891113281
        },
        {
            "id": "s_487",
            "name": "Xiaotong Liu",
            "type": "sparse",
            "x": 7.074164867401123,
            "y": 7.665656089782715
        },
        {
            "id": "s_488",
            "name": "Michael Mock",
            "type": "sparse",
            "x": 7.473939895629883,
            "y": 7.139910697937012
        },
        {
            "id": "s_489",
            "name": "Wolfgang Aigner",
            "type": "sparse",
            "x": 4.79535436630249,
            "y": 7.740059852600098
        },
        {
            "id": "s_490",
            "name": "Frederik L. Dennig",
            "type": "sparse",
            "x": 4.621323108673096,
            "y": 8.148672103881836
        },
        {
            "id": "s_491",
            "name": "Mengchen Liu",
            "type": "sparse",
            "x": 4.827512264251709,
            "y": 8.428983688354492
        },
        {
            "id": "s_492",
            "name": "Quan Li",
            "type": "sparse",
            "x": 6.733369827270508,
            "y": 6.9086809158325195
        },
        {
            "id": "s_493",
            "name": "Emily Fujimoto",
            "type": "sparse",
            "x": 7.202186584472656,
            "y": 6.9594502449035645
        },
        {
            "id": "s_494",
            "name": "Ryan Armstrong",
            "type": "sparse",
            "x": 4.940927505493164,
            "y": 5.271132946014404
        },
        {
            "id": "s_495",
            "name": "Smiti Kaul",
            "type": "sparse",
            "x": 4.845474720001221,
            "y": 8.684896469116211
        },
        {
            "id": "s_496",
            "name": "Minghui Chen",
            "type": "sparse",
            "x": 3.5461413860321045,
            "y": 5.572312831878662
        },
        {
            "id": "s_497",
            "name": "Christoph Hafemeister",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_498",
            "name": "Gina Eosco",
            "type": "sparse",
            "x": 6.389932155609131,
            "y": 9.841611862182617
        },
        {
            "id": "s_499",
            "name": "Mahima Pushkarna",
            "type": "sparse",
            "x": 3.356947183609009,
            "y": 8.667536735534668
        },
        {
            "id": "s_500",
            "name": "Philipp Weil",
            "type": "sparse",
            "x": 3.3674137592315674,
            "y": 6.971466064453125
        },
        {
            "id": "s_501",
            "name": "Ashley Rye Yauilla",
            "type": "sparse",
            "x": 4.692488193511963,
            "y": 9.84524154663086
        },
        {
            "id": "s_502",
            "name": "Fereshteh Amini",
            "type": "sparse",
            "x": 5.138099670410156,
            "y": 10.092191696166992
        },
        {
            "id": "s_503",
            "name": "Zhifang Jiang",
            "type": "sparse",
            "x": 5.112260341644287,
            "y": 7.125860214233398
        },
        {
            "id": "s_504",
            "name": "Ronan Sicre",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_505",
            "name": "Xizhou Zhu",
            "type": "sparse",
            "x": 7.730375289916992,
            "y": 8.678378105163574
        },
        {
            "id": "s_506",
            "name": "Jan C. van Gemert",
            "type": "sparse",
            "x": 3.242910623550415,
            "y": 8.485912322998047
        },
        {
            "id": "s_507",
            "name": "Jordan Swartz",
            "type": "sparse",
            "x": 3.168092966079712,
            "y": 8.754652976989746
        },
        {
            "id": "s_508",
            "name": "Huan He",
            "type": "sparse",
            "x": 5.9608869552612305,
            "y": 8.278266906738281
        },
        {
            "id": "s_509",
            "name": "Heidi Lam",
            "type": "sparse",
            "x": 5.484500408172607,
            "y": 8.928224563598633
        },
        {
            "id": "s_510",
            "name": "Tianxiang Chen",
            "type": "sparse",
            "x": 6.134147644042969,
            "y": 8.68932056427002
        },
        {
            "id": "s_511",
            "name": "Tobias Schreck",
            "type": "sparse",
            "x": 5.540051460266113,
            "y": 6.703611850738525
        },
        {
            "id": "s_512",
            "name": "Marc Streit",
            "type": "sparse",
            "x": 5.034231185913086,
            "y": 8.345975875854492
        },
        {
            "id": "s_513",
            "name": "Anuj R. Jaiswal",
            "type": "sparse",
            "x": 7.510919570922852,
            "y": 9.659215927124023
        },
        {
            "id": "s_514",
            "name": "Jason Dykes",
            "type": "sparse",
            "x": 6.027945041656494,
            "y": 8.625411987304688
        },
        {
            "id": "s_515",
            "name": "Denise Royle",
            "type": "sparse",
            "x": 5.102502822875977,
            "y": 7.049795150756836
        },
        {
            "id": "s_516",
            "name": "Guozhong Dai",
            "type": "sparse",
            "x": 5.462973117828369,
            "y": 7.948393821716309
        },
        {
            "id": "s_517",
            "name": "Jorji Nonaka",
            "type": "sparse",
            "x": 4.579966068267822,
            "y": 6.905692100524902
        },
        {
            "id": "s_518",
            "name": "Katarina Younkin",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_519",
            "name": "Jaakko Peltonen",
            "type": "sparse",
            "x": 4.047524452209473,
            "y": 7.783985614776611
        },
        {
            "id": "s_520",
            "name": "Sohaib Ghani",
            "type": "sparse",
            "x": 4.951333045959473,
            "y": 5.35188627243042
        },
        {
            "id": "s_521",
            "name": "Neesha Kodagoda",
            "type": "sparse",
            "x": 5.564159870147705,
            "y": 10.273418426513672
        },
        {
            "id": "s_522",
            "name": "Michael Witmore",
            "type": "sparse",
            "x": 7.1605377197265625,
            "y": 10.400833129882812
        },
        {
            "id": "s_523",
            "name": "Zhaosong Huang",
            "type": "sparse",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "id": "s_524",
            "name": "Yifang Wang 0001",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_525",
            "name": "John Aldo Lee",
            "type": "sparse",
            "x": 4.047524452209473,
            "y": 7.783985614776611
        },
        {
            "id": "s_526",
            "name": "Caleb Robinson",
            "type": "sparse",
            "x": 2.1728076934814453,
            "y": 8.92521858215332
        },
        {
            "id": "s_527",
            "name": "Andreas Kerren",
            "type": "sparse",
            "x": 5.53901481628418,
            "y": 6.938285827636719
        },
        {
            "id": "s_528",
            "name": "Xue Wu",
            "type": "sparse",
            "x": 3.117333173751831,
            "y": 7.8499674797058105
        },
        {
            "id": "s_529",
            "name": "Yale Song",
            "type": "sparse",
            "x": 7.767972469329834,
            "y": 8.18466567993164
        },
        {
            "id": "s_530",
            "name": "Jing Ma",
            "type": "sparse",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "id": "s_531",
            "name": "M. Eduard Gr\u00f6ller",
            "type": "sparse",
            "x": 4.910147190093994,
            "y": 6.8387451171875
        },
        {
            "id": "s_532",
            "name": "Johannes Sorger",
            "type": "sparse",
            "x": 3.3469347953796387,
            "y": 7.586907386779785
        },
        {
            "id": "s_533",
            "name": "Avin Pattath",
            "type": "sparse",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "id": "s_534",
            "name": "Kejian Zhao",
            "type": "sparse",
            "x": 8.519206047058105,
            "y": 6.030973434448242
        },
        {
            "id": "s_535",
            "name": "Tino Gruse",
            "type": "sparse",
            "x": 8.669734954833984,
            "y": 7.024385452270508
        },
        {
            "id": "s_536",
            "name": "Deborah R. Wahl",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_537",
            "name": "Martin Luboschik",
            "type": "sparse",
            "x": 5.326026916503906,
            "y": 7.089005470275879
        },
        {
            "id": "s_538",
            "name": "James R. Eagan",
            "type": "sparse",
            "x": 5.635706424713135,
            "y": 10.295689582824707
        },
        {
            "id": "s_539",
            "name": "Patrick Butler",
            "type": "sparse",
            "x": 7.316325664520264,
            "y": 10.24779987335205
        },
        {
            "id": "s_540",
            "name": "Zezheng Feng",
            "type": "sparse",
            "x": 4.558706283569336,
            "y": 6.38838005065918
        },
        {
            "id": "s_541",
            "name": "Joshua Shrestha",
            "type": "sparse",
            "x": 6.637656211853027,
            "y": 6.895570755004883
        },
        {
            "id": "s_542",
            "name": "Suyun \"Sandra\" Bae",
            "type": "sparse",
            "x": 3.307056188583374,
            "y": 8.17342758178711
        },
        {
            "id": "s_543",
            "name": "Pourang Irani",
            "type": "sparse",
            "x": 6.310723304748535,
            "y": 8.06081485748291
        },
        {
            "id": "s_544",
            "name": "Mingze Ma",
            "type": "sparse",
            "x": 7.309029579162598,
            "y": 4.781009197235107
        },
        {
            "id": "s_545",
            "name": "Kori Inkpen",
            "type": "sparse",
            "x": 5.419025897979736,
            "y": 10.711797714233398
        },
        {
            "id": "s_546",
            "name": "Alan M. MacEachren",
            "type": "sparse",
            "x": 7.0095319747924805,
            "y": 8.959933280944824
        },
        {
            "id": "s_547",
            "name": "Michael Behrisch 0001",
            "type": "sparse",
            "x": 4.457268238067627,
            "y": 7.309556484222412
        },
        {
            "id": "s_548",
            "name": "Mark Last",
            "type": "sparse",
            "x": 7.499957084655762,
            "y": 10.237372398376465
        },
        {
            "id": "s_549",
            "name": "Xiaoyan Fu",
            "type": "sparse",
            "x": 5.461853981018066,
            "y": 5.2681779861450195
        },
        {
            "id": "s_550",
            "name": "Nadia Boukhelifa",
            "type": "sparse",
            "x": 6.467045783996582,
            "y": 10.109938621520996
        },
        {
            "id": "s_551",
            "name": "John Dill",
            "type": "sparse",
            "x": 6.418212413787842,
            "y": 10.062971115112305
        },
        {
            "id": "s_552",
            "name": "Mathieu Brulin",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_553",
            "name": "Naohisa Sakamoto",
            "type": "sparse",
            "x": 4.579966068267822,
            "y": 6.905692100524902
        },
        {
            "id": "s_554",
            "name": "Tianyu Gu",
            "type": "sparse",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "id": "s_555",
            "name": "Ratul Mahajan",
            "type": "sparse",
            "x": 6.996976375579834,
            "y": 7.708852291107178
        },
        {
            "id": "s_556",
            "name": "Zhan Guo",
            "type": "sparse",
            "x": 7.280351638793945,
            "y": 4.832824230194092
        },
        {
            "id": "s_557",
            "name": "Arjun Choudhry",
            "type": "sparse",
            "x": 7.877171039581299,
            "y": 7.68709135055542
        },
        {
            "id": "s_558",
            "name": "Michael Hund",
            "type": "sparse",
            "x": 4.466116905212402,
            "y": 6.274818420410156
        },
        {
            "id": "s_559",
            "name": "Cong Liu",
            "type": "sparse",
            "x": 5.659013748168945,
            "y": 8.697281837463379
        },
        {
            "id": "s_560",
            "name": "Chaitanya Chandurkar",
            "type": "sparse",
            "x": 5.957756519317627,
            "y": 9.685969352722168
        },
        {
            "id": "s_561",
            "name": "Bingru Lin",
            "type": "sparse",
            "x": 4.4847564697265625,
            "y": 6.689899921417236
        },
        {
            "id": "s_562",
            "name": "Amir H. Meghdadi",
            "type": "sparse",
            "x": 7.483346462249756,
            "y": 6.029438018798828
        },
        {
            "id": "s_563",
            "name": "Cheryl Z. Qian",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_564",
            "name": "Rafael Henkin",
            "type": "sparse",
            "x": 5.912964820861816,
            "y": 9.671320915222168
        },
        {
            "id": "s_565",
            "name": "Jonathan W. Decker",
            "type": "sparse",
            "x": 5.102502822875977,
            "y": 7.049795150756836
        },
        {
            "id": "s_566",
            "name": "Konstantin Dmitriev",
            "type": "sparse",
            "x": 5.92241907119751,
            "y": 6.868464469909668
        },
        {
            "id": "s_567",
            "name": "Hui Zhang 0051",
            "type": "sparse",
            "x": 8.530769348144531,
            "y": 5.983466625213623
        },
        {
            "id": "s_568",
            "name": "Ido Guy",
            "type": "sparse",
            "x": 7.6279706954956055,
            "y": 8.709637641906738
        },
        {
            "id": "s_569",
            "name": "Oh-Hyun Kwon",
            "type": "sparse",
            "x": 3.6037490367889404,
            "y": 5.632784843444824
        },
        {
            "id": "s_570",
            "name": "Alla Zelenyuk",
            "type": "sparse",
            "x": 3.615382671356201,
            "y": 5.652681827545166
        },
        {
            "id": "s_571",
            "name": "Patrick Houthuizen",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_572",
            "name": "Kevin A. Roundy",
            "type": "sparse",
            "x": 4.909725189208984,
            "y": 5.396003723144531
        },
        {
            "id": "s_573",
            "name": "Philipp Meschenmoser",
            "type": "sparse",
            "x": 5.320591926574707,
            "y": 6.559731483459473
        },
        {
            "id": "s_574",
            "name": "Muchan Park",
            "type": "sparse",
            "x": 5.759721755981445,
            "y": 8.101409912109375
        },
        {
            "id": "s_575",
            "name": "Duen Horng (Polo) Chau",
            "type": "sparse",
            "x": 2.1667537689208984,
            "y": 8.91409969329834
        },
        {
            "id": "s_576",
            "name": "Aaron Hoff",
            "type": "sparse",
            "x": 7.937456130981445,
            "y": 7.830206394195557
        },
        {
            "id": "s_577",
            "name": "David Fyfe",
            "type": "sparse",
            "x": 7.1402506828308105,
            "y": 7.083461284637451
        },
        {
            "id": "s_578",
            "name": "Daniel Cornel",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_579",
            "name": "Antoine Lambert",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_580",
            "name": "Rimma V. Nehme",
            "type": "sparse",
            "x": 6.365241527557373,
            "y": 6.8800506591796875
        },
        {
            "id": "s_581",
            "name": "Hamed Bouzari",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_582",
            "name": "Marcel Hlawatsch",
            "type": "sparse",
            "x": 6.034650802612305,
            "y": 7.87465238571167
        },
        {
            "id": "s_583",
            "name": "Chad A. Steed",
            "type": "sparse",
            "x": 6.672209739685059,
            "y": 7.014354228973389
        },
        {
            "id": "s_584",
            "name": "Michael Riemer",
            "type": "sparse",
            "x": 4.963197708129883,
            "y": 5.9895172119140625
        },
        {
            "id": "s_585",
            "name": "Yong Xu 0010",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_586",
            "name": "John E. Wenskovitch",
            "type": "sparse",
            "x": 5.279606819152832,
            "y": 7.644184112548828
        },
        {
            "id": "s_587",
            "name": "Christopher S. Gates",
            "type": "sparse",
            "x": 4.909725189208984,
            "y": 5.396003723144531
        },
        {
            "id": "s_588",
            "name": "Wolfgang Freiler",
            "type": "sparse",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "id": "s_589",
            "name": "Walid G. Aref",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_590",
            "name": "Jundong Li",
            "type": "sparse",
            "x": 2.394528865814209,
            "y": 8.865325927734375
        },
        {
            "id": "s_591",
            "name": "Jincheng Jiang",
            "type": "sparse",
            "x": 7.166633605957031,
            "y": 4.956838130950928
        },
        {
            "id": "s_592",
            "name": "Christophe Hurter",
            "type": "sparse",
            "x": 7.490694522857666,
            "y": 5.324251174926758
        },
        {
            "id": "s_593",
            "name": "B. L. William Wong",
            "type": "sparse",
            "x": 5.92784309387207,
            "y": 10.155340194702148
        },
        {
            "id": "s_594",
            "name": "Yaxing Wei",
            "type": "sparse",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "id": "s_595",
            "name": "Sharon J. Laskowski",
            "type": "sparse",
            "x": 6.760641574859619,
            "y": 8.775925636291504
        },
        {
            "id": "s_596",
            "name": "Dong Hyun Jeong",
            "type": "sparse",
            "x": 5.17218542098999,
            "y": 10.160517692565918
        },
        {
            "id": "s_597",
            "name": "Harald Reiterer",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_598",
            "name": "Michael Delz",
            "type": "sparse",
            "x": 4.52726936340332,
            "y": 6.233945369720459
        },
        {
            "id": "s_599",
            "name": "Johannes Weissenbock",
            "type": "sparse",
            "x": 5.075896739959717,
            "y": 6.747625827789307
        },
        {
            "id": "s_600",
            "name": "Anshul Vikram Pandey",
            "type": "sparse",
            "x": 7.566127777099609,
            "y": 10.470183372497559
        },
        {
            "id": "s_601",
            "name": "Seyedkoosha Mirhosseini",
            "type": "sparse",
            "x": 5.832940101623535,
            "y": 7.073251247406006
        },
        {
            "id": "s_602",
            "name": "Robert Kincaid",
            "type": "sparse",
            "x": 6.315182685852051,
            "y": 8.191092491149902
        },
        {
            "id": "s_603",
            "name": "Johanna Beyer",
            "type": "sparse",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "id": "s_604",
            "name": "Dylan Cashman",
            "type": "sparse",
            "x": 3.9551072120666504,
            "y": 8.776058197021484
        },
        {
            "id": "s_605",
            "name": "Yun Jang",
            "type": "sparse",
            "x": 6.603382110595703,
            "y": 7.09909200668335
        },
        {
            "id": "s_606",
            "name": "Mary Czerwinski",
            "type": "sparse",
            "x": 5.419025897979736,
            "y": 10.711797714233398
        },
        {
            "id": "s_607",
            "name": "Johanna Schmidt",
            "type": "sparse",
            "x": 4.331264019012451,
            "y": 6.314713478088379
        },
        {
            "id": "s_608",
            "name": "Shin'ichi Satoh 0001",
            "type": "sparse",
            "x": 5.913250923156738,
            "y": 8.269309997558594
        },
        {
            "id": "s_609",
            "name": "Benjamin Bustos",
            "type": "sparse",
            "x": 6.885515213012695,
            "y": 6.168759346008301
        },
        {
            "id": "s_610",
            "name": "Chandrakant D. Patel",
            "type": "sparse",
            "x": 4.998837947845459,
            "y": 7.000543594360352
        },
        {
            "id": "s_611",
            "name": "Lincan Zou",
            "type": "sparse",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "id": "s_612",
            "name": "Angelos Chatzimparmpas",
            "type": "sparse",
            "x": 3.587266445159912,
            "y": 8.453961372375488
        },
        {
            "id": "s_613",
            "name": "Eric A. Bier",
            "type": "sparse",
            "x": 5.4073805809021,
            "y": 10.088134765625
        },
        {
            "id": "s_614",
            "name": "Yuanyuan Chen",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_615",
            "name": "Cullen E. Bash",
            "type": "sparse",
            "x": 4.998837947845459,
            "y": 7.000543594360352
        },
        {
            "id": "s_616",
            "name": "Morgan Mathiaut",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_617",
            "name": "Haotian Li 0001",
            "type": "sparse",
            "x": 4.558706283569336,
            "y": 6.38838005065918
        },
        {
            "id": "s_618",
            "name": "Dipayan Maiti",
            "type": "sparse",
            "x": 4.774960994720459,
            "y": 8.248517036437988
        },
        {
            "id": "s_619",
            "name": "Prithwish Chakraborty",
            "type": "sparse",
            "x": 7.316325664520264,
            "y": 10.24779987335205
        },
        {
            "id": "s_620",
            "name": "Frank Kr\u00fcger 0001",
            "type": "sparse",
            "x": 3.764721155166626,
            "y": 7.994372844696045
        },
        {
            "id": "s_621",
            "name": "Stefan Wrobel",
            "type": "sparse",
            "x": 7.490694522857666,
            "y": 5.324251174926758
        },
        {
            "id": "s_622",
            "name": "Maria Luj\u00e1n Ganuza",
            "type": "sparse",
            "x": 5.554603576660156,
            "y": 6.4707207679748535
        },
        {
            "id": "s_623",
            "name": "Matthew A. Barish",
            "type": "sparse",
            "x": 5.92241907119751,
            "y": 6.868464469909668
        },
        {
            "id": "s_624",
            "name": "Eric C. Alexander",
            "type": "sparse",
            "x": 7.177129745483398,
            "y": 10.652694702148438
        },
        {
            "id": "s_625",
            "name": "Calvin Yau",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_626",
            "name": "Yujie Liu",
            "type": "sparse",
            "x": 4.889342308044434,
            "y": 6.623239517211914
        },
        {
            "id": "s_627",
            "name": "Tera Marie Green",
            "type": "sparse",
            "x": 5.371737957000732,
            "y": 9.819666862487793
        },
        {
            "id": "s_628",
            "name": "Anne Laprie",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_629",
            "name": "Jianping Fan 0001",
            "type": "sparse",
            "x": 5.686441421508789,
            "y": 8.275586128234863
        },
        {
            "id": "s_630",
            "name": "Patrick Jungk",
            "type": "sparse",
            "x": 6.847413539886475,
            "y": 8.73234748840332
        },
        {
            "id": "s_631",
            "name": "Wenyuan Wang",
            "type": "sparse",
            "x": 6.637656211853027,
            "y": 6.895570755004883
        },
        {
            "id": "s_632",
            "name": "Jacky Yuan",
            "type": "sparse",
            "x": 7.463348865509033,
            "y": 5.214798927307129
        },
        {
            "id": "s_633",
            "name": "Zhuochen Jin",
            "type": "sparse",
            "x": 7.65322208404541,
            "y": 7.111824035644531
        },
        {
            "id": "s_634",
            "name": "Joe Kohlmann",
            "type": "sparse",
            "x": 7.1605377197265625,
            "y": 10.400833129882812
        },
        {
            "id": "s_635",
            "name": "Jereme Haack",
            "type": "sparse",
            "x": 6.7198567390441895,
            "y": 8.789078712463379
        },
        {
            "id": "s_636",
            "name": "Layla Shahamat",
            "type": "sparse",
            "x": 7.580902099609375,
            "y": 6.7360711097717285
        },
        {
            "id": "s_637",
            "name": "Peter Bak",
            "type": "sparse",
            "x": 6.0472235679626465,
            "y": 7.855517864227295
        },
        {
            "id": "s_638",
            "name": "Kai Xu 0003",
            "type": "sparse",
            "x": 5.902266502380371,
            "y": 8.904029846191406
        },
        {
            "id": "s_639",
            "name": "Jos\u00e9 Florencio de Queiroz Neto",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_640",
            "name": "Kelei Cao",
            "type": "sparse",
            "x": 3.709033966064453,
            "y": 8.133350372314453
        },
        {
            "id": "s_641",
            "name": "Juliana Freire",
            "type": "sparse",
            "x": 5.93858528137207,
            "y": 6.126359462738037
        },
        {
            "id": "s_642",
            "name": "Daniel M. Russell",
            "type": "sparse",
            "x": 5.484500408172607,
            "y": 8.928224563598633
        },
        {
            "id": "s_643",
            "name": "Omar Shaikh",
            "type": "sparse",
            "x": 2.1530587673187256,
            "y": 8.92031478881836
        },
        {
            "id": "s_644",
            "name": "Artem Sokolov",
            "type": "sparse",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "id": "s_645",
            "name": "Markus Wagner 0008",
            "type": "sparse",
            "x": 4.098504543304443,
            "y": 9.517640113830566
        },
        {
            "id": "s_646",
            "name": "E. Yanli",
            "type": "sparse",
            "x": 5.684476852416992,
            "y": 7.335890769958496
        },
        {
            "id": "s_647",
            "name": "Debprakash Patnaik",
            "type": "sparse",
            "x": 6.74947452545166,
            "y": 6.071475505828857
        },
        {
            "id": "s_648",
            "name": "Di Weng",
            "type": "sparse",
            "x": 6.504770755767822,
            "y": 6.255224227905273
        },
        {
            "id": "s_649",
            "name": "Yiping Han",
            "type": "sparse",
            "x": 3.615382671356201,
            "y": 5.652681827545166
        },
        {
            "id": "s_650",
            "name": "Mona Erfani Joorabchi",
            "type": "sparse",
            "x": 7.695061683654785,
            "y": 8.196971893310547
        },
        {
            "id": "s_651",
            "name": "Yun Wang 0012",
            "type": "sparse",
            "x": 7.448137283325195,
            "y": 7.753540992736816
        },
        {
            "id": "s_652",
            "name": "Shade T. Shutters",
            "type": "sparse",
            "x": 8.675040245056152,
            "y": 7.296395301818848
        },
        {
            "id": "s_653",
            "name": "Halld\u00f3r Janetzko",
            "type": "sparse",
            "x": 6.9622979164123535,
            "y": 7.17667293548584
        },
        {
            "id": "s_654",
            "name": "Nivan Ferreira",
            "type": "sparse",
            "x": 6.515478134155273,
            "y": 6.492414474487305
        },
        {
            "id": "s_655",
            "name": "Fred Olislagers",
            "type": "sparse",
            "x": 7.504979133605957,
            "y": 6.80190372467041
        },
        {
            "id": "s_656",
            "name": "Carla E. Brodley",
            "type": "sparse",
            "x": 4.319952011108398,
            "y": 6.838166236877441
        },
        {
            "id": "s_657",
            "name": "Carsten G\u00f6rg",
            "type": "sparse",
            "x": 6.272757530212402,
            "y": 9.426734924316406
        },
        {
            "id": "s_658",
            "name": "Thomas Butkiewicz",
            "type": "sparse",
            "x": 5.273646354675293,
            "y": 10.018550872802734
        },
        {
            "id": "s_659",
            "name": "Len Kne",
            "type": "sparse",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "id": "s_660",
            "name": "Walter F. Stewart",
            "type": "sparse",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "id": "s_661",
            "name": "Maneesh Agrawala",
            "type": "sparse",
            "x": 5.727664947509766,
            "y": 10.393458366394043
        },
        {
            "id": "s_662",
            "name": "Haojing Jiang",
            "type": "sparse",
            "x": 4.4847564697265625,
            "y": 6.689899921417236
        },
        {
            "id": "s_663",
            "name": "Linda T. Kaastra",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_664",
            "name": "Ting-Chuen Pong",
            "type": "sparse",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "id": "s_665",
            "name": "Chengbo Zheng",
            "type": "sparse",
            "x": 7.309029579162598,
            "y": 4.781009197235107
        },
        {
            "id": "s_666",
            "name": "Michael E. Papka",
            "type": "sparse",
            "x": 4.8665313720703125,
            "y": 6.72496223449707
        },
        {
            "id": "s_667",
            "name": "Isaac Dykeman",
            "type": "sparse",
            "x": 5.195247650146484,
            "y": 9.794124603271484
        },
        {
            "id": "s_668",
            "name": "Pascale Proulx",
            "type": "sparse",
            "x": 6.689464092254639,
            "y": 8.36801528930664
        },
        {
            "id": "s_669",
            "name": "Emily R. Miraldi",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_670",
            "name": "Andrew Mercer 0001",
            "type": "sparse",
            "x": 6.29480504989624,
            "y": 6.361831188201904
        },
        {
            "id": "s_671",
            "name": "Sebastian Koch",
            "type": "sparse",
            "x": 7.059408187866211,
            "y": 9.953104019165039
        },
        {
            "id": "s_672",
            "name": "Cindy Grimm",
            "type": "sparse",
            "x": 3.6070477962493896,
            "y": 5.6530327796936035
        },
        {
            "id": "s_673",
            "name": "Yuet Ling Wong",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_674",
            "name": "Holger Stitz",
            "type": "sparse",
            "x": 6.838124752044678,
            "y": 9.806111335754395
        },
        {
            "id": "s_675",
            "name": "Michael Regenscheit",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_676",
            "name": "David Spretke",
            "type": "sparse",
            "x": 7.403539180755615,
            "y": 10.271008491516113
        },
        {
            "id": "s_677",
            "name": "Yindalon Aphinyanaphongs",
            "type": "sparse",
            "x": 3.168092966079712,
            "y": 8.754652976989746
        },
        {
            "id": "s_678",
            "name": "Haoyu Li",
            "type": "sparse",
            "x": 3.0904009342193604,
            "y": 7.828068256378174
        },
        {
            "id": "s_679",
            "name": "Fei Wang 0016",
            "type": "sparse",
            "x": 7.301423072814941,
            "y": 4.846832275390625
        },
        {
            "id": "s_680",
            "name": "Robert S. Pienta",
            "type": "sparse",
            "x": 4.909725189208984,
            "y": 5.396003723144531
        },
        {
            "id": "s_681",
            "name": "Andrea Batch",
            "type": "sparse",
            "x": 5.040587425231934,
            "y": 9.46938419342041
        },
        {
            "id": "s_682",
            "name": "Tomislav Lipic",
            "type": "sparse",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "id": "s_683",
            "name": "Clemens H. Cap",
            "type": "sparse",
            "x": 5.256996154785156,
            "y": 5.111698627471924
        },
        {
            "id": "s_684",
            "name": "Kristin A. Cook",
            "type": "sparse",
            "x": 5.379818916320801,
            "y": 8.649133682250977
        },
        {
            "id": "s_685",
            "name": "Youfeng Hao",
            "type": "sparse",
            "x": 7.030353546142578,
            "y": 4.95527458190918
        },
        {
            "id": "s_686",
            "name": "Ryan Wilson",
            "type": "sparse",
            "x": 4.675631999969482,
            "y": 9.515111923217773
        },
        {
            "id": "s_687",
            "name": "Reinhold Preiner",
            "type": "sparse",
            "x": 4.083239555358887,
            "y": 6.6136088371276855
        },
        {
            "id": "s_688",
            "name": "Fidelia Ibekwe-Sanjuan",
            "type": "sparse",
            "x": 7.896146297454834,
            "y": 9.091241836547852
        },
        {
            "id": "s_689",
            "name": "Peter Hamilton",
            "type": "sparse",
            "x": 5.901120662689209,
            "y": 6.837912082672119
        },
        {
            "id": "s_690",
            "name": "Manuel Stein",
            "type": "sparse",
            "x": 8.40367317199707,
            "y": 6.121192932128906
        },
        {
            "id": "s_691",
            "name": "Andreas Walch",
            "type": "sparse",
            "x": 4.505589962005615,
            "y": 7.016158580780029
        },
        {
            "id": "s_692",
            "name": "Bj\u00f6rn Kr\u00fcger",
            "type": "sparse",
            "x": 7.492415904998779,
            "y": 5.876190662384033
        },
        {
            "id": "s_693",
            "name": "Roland N. Boubela",
            "type": "sparse",
            "x": 3.554858684539795,
            "y": 7.912291526794434
        },
        {
            "id": "s_694",
            "name": "Hua Guo",
            "type": "sparse",
            "x": 5.2923150062561035,
            "y": 9.559711456298828
        },
        {
            "id": "s_695",
            "name": "Thomas Ramm",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_696",
            "name": "Daniel J. Wigdor",
            "type": "sparse",
            "x": 5.901120662689209,
            "y": 6.837912082672119
        },
        {
            "id": "s_697",
            "name": "Shuyue Zhou",
            "type": "sparse",
            "x": 4.4847564697265625,
            "y": 6.689899921417236
        },
        {
            "id": "s_698",
            "name": "Benjamin Rowland",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_699",
            "name": "Kwan-Liu Ma",
            "type": "sparse",
            "x": 5.220797061920166,
            "y": 7.164988994598389
        },
        {
            "id": "s_700",
            "name": "Peng Mi",
            "type": "sparse",
            "x": 4.947014808654785,
            "y": 5.668623447418213
        },
        {
            "id": "s_701",
            "name": "Ye Yuan",
            "type": "sparse",
            "x": 5.673964023590088,
            "y": 6.457597255706787
        },
        {
            "id": "s_702",
            "name": "Zeyu Li 0003",
            "type": "sparse",
            "x": 6.394692420959473,
            "y": 10.036843299865723
        },
        {
            "id": "s_703",
            "name": "Heungseok Park",
            "type": "sparse",
            "x": 3.6053433418273926,
            "y": 7.882735729217529
        },
        {
            "id": "s_704",
            "name": "David Koop",
            "type": "sparse",
            "x": 7.238384246826172,
            "y": 7.855299949645996
        },
        {
            "id": "s_705",
            "name": "Wei Xu 0020",
            "type": "sparse",
            "x": 7.053226470947266,
            "y": 7.47855806350708
        },
        {
            "id": "s_706",
            "name": "Shengjie Gao",
            "type": "sparse",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "id": "s_707",
            "name": "Lynn Chien",
            "type": "sparse",
            "x": 6.653965473175049,
            "y": 8.683868408203125
        },
        {
            "id": "s_708",
            "name": "James Wexler",
            "type": "sparse",
            "x": 4.0586161613464355,
            "y": 7.076250076293945
        },
        {
            "id": "s_709",
            "name": "Rainer Splechtna",
            "type": "sparse",
            "x": 3.277205467224121,
            "y": 7.747622489929199
        },
        {
            "id": "s_710",
            "name": "Dongjin Choi",
            "type": "sparse",
            "x": 7.219537734985352,
            "y": 10.913814544677734
        },
        {
            "id": "s_711",
            "name": "Holger Theisel",
            "type": "sparse",
            "x": 4.668849945068359,
            "y": 6.81472110748291
        },
        {
            "id": "s_712",
            "name": "Erich P. Stuntebeck",
            "type": "sparse",
            "x": 6.5693864822387695,
            "y": 7.4537224769592285
        },
        {
            "id": "s_713",
            "name": "Rongjian Lan",
            "type": "sparse",
            "x": 5.925091743469238,
            "y": 7.095569610595703
        },
        {
            "id": "s_714",
            "name": "Geoffrey P. Ellis",
            "type": "sparse",
            "x": 4.775978088378906,
            "y": 9.144189834594727
        },
        {
            "id": "s_715",
            "name": "Sana Malik",
            "type": "sparse",
            "x": 7.579283237457275,
            "y": 6.7240729331970215
        },
        {
            "id": "s_716",
            "name": "Yuhua Liu",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_717",
            "name": "Donna J. Peuquet",
            "type": "sparse",
            "x": 7.1402506828308105,
            "y": 7.083461284637451
        },
        {
            "id": "s_718",
            "name": "Changjian Chen",
            "type": "sparse",
            "x": 4.297730922698975,
            "y": 8.582598686218262
        },
        {
            "id": "s_719",
            "name": "Rui Li 0047",
            "type": "sparse",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "id": "s_720",
            "name": "Nicole Sultanum",
            "type": "sparse",
            "x": 7.287152290344238,
            "y": 10.204761505126953
        },
        {
            "id": "s_721",
            "name": "Ross Maciejewski",
            "type": "sparse",
            "x": 6.139348030090332,
            "y": 7.722855567932129
        },
        {
            "id": "s_722",
            "name": "Bo Qiao 0001",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_723",
            "name": "Zheng Zhou",
            "type": "sparse",
            "x": 8.519206047058105,
            "y": 6.030973434448242
        },
        {
            "id": "s_724",
            "name": "Yiyang Tang",
            "type": "sparse",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "id": "s_725",
            "name": "Nico T. Mutters",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_726",
            "name": "Charles D. Stolper",
            "type": "sparse",
            "x": 5.311514377593994,
            "y": 9.128715515136719
        },
        {
            "id": "s_727",
            "name": "Chong Zhang",
            "type": "sparse",
            "x": 7.365744590759277,
            "y": 4.923183917999268
        },
        {
            "id": "s_728",
            "name": "Tatiana Tekusov\u00e1",
            "type": "sparse",
            "x": 3.6725172996520996,
            "y": 5.735623359680176
        },
        {
            "id": "s_729",
            "name": "Ashley Wheat",
            "type": "sparse",
            "x": 5.667359352111816,
            "y": 10.38341236114502
        },
        {
            "id": "s_730",
            "name": "Ranko Miklin",
            "type": "sparse",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "id": "s_731",
            "name": "Arun Reddy Nelakurthi",
            "type": "sparse",
            "x": 2.2220568656921387,
            "y": 8.885293960571289
        },
        {
            "id": "s_732",
            "name": "Gabriela Ferracutti",
            "type": "sparse",
            "x": 5.554603576660156,
            "y": 6.4707207679748535
        },
        {
            "id": "s_733",
            "name": "Qinghua Zheng",
            "type": "sparse",
            "x": 7.421921730041504,
            "y": 7.7123517990112305
        },
        {
            "id": "s_734",
            "name": "Kalyan Veeramachaneni",
            "type": "sparse",
            "x": 3.8067502975463867,
            "y": 8.83480167388916
        },
        {
            "id": "s_735",
            "name": "Holger Last",
            "type": "sparse",
            "x": 4.062925338745117,
            "y": 9.458945274353027
        },
        {
            "id": "s_736",
            "name": "Shuang-Hua Yang",
            "type": "sparse",
            "x": 4.558706283569336,
            "y": 6.38838005065918
        },
        {
            "id": "s_737",
            "name": "Dongning Luo",
            "type": "sparse",
            "x": 7.862253189086914,
            "y": 7.706037521362305
        },
        {
            "id": "s_738",
            "name": "Youn ah Kang",
            "type": "sparse",
            "x": 5.385032653808594,
            "y": 9.595192909240723
        },
        {
            "id": "s_739",
            "name": "Mingxuan Yuan",
            "type": "sparse",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "id": "s_740",
            "name": "Michael Beham",
            "type": "sparse",
            "x": 4.04945182800293,
            "y": 6.402584075927734
        },
        {
            "id": "s_741",
            "name": "Amir Hossein Hajizadeh",
            "type": "sparse",
            "x": 5.484528541564941,
            "y": 10.792652130126953
        },
        {
            "id": "s_742",
            "name": "Samuel H. Payne",
            "type": "sparse",
            "x": 4.675631999969482,
            "y": 9.515111923217773
        },
        {
            "id": "s_743",
            "name": "Yi-Shan Lin",
            "type": "sparse",
            "x": 4.136512279510498,
            "y": 8.611287117004395
        },
        {
            "id": "s_744",
            "name": "Enxun Wei",
            "type": "sparse",
            "x": 7.994541645050049,
            "y": 8.44855785369873
        },
        {
            "id": "s_745",
            "name": "Hannah Kim 0001",
            "type": "sparse",
            "x": 5.415658473968506,
            "y": 8.24028491973877
        },
        {
            "id": "s_746",
            "name": "Dennis Thom",
            "type": "sparse",
            "x": 7.746993064880371,
            "y": 8.201763153076172
        },
        {
            "id": "s_747",
            "name": "Chandan K. Reddy",
            "type": "sparse",
            "x": 5.948115348815918,
            "y": 8.708894729614258
        },
        {
            "id": "s_748",
            "name": "Yingyu Wu",
            "type": "sparse",
            "x": 7.330615997314453,
            "y": 4.847436428070068
        },
        {
            "id": "s_749",
            "name": "Anya Savikhin",
            "type": "sparse",
            "x": 6.704798698425293,
            "y": 8.14471435546875
        },
        {
            "id": "s_750",
            "name": "Arie E. Kaufman",
            "type": "sparse",
            "x": 5.317600250244141,
            "y": 7.714289665222168
        },
        {
            "id": "s_751",
            "name": "Guido Reina",
            "type": "sparse",
            "x": 6.911941051483154,
            "y": 8.79002857208252
        },
        {
            "id": "s_752",
            "name": "Michael Brudno",
            "type": "sparse",
            "x": 6.241352081298828,
            "y": 7.674605846405029
        },
        {
            "id": "s_753",
            "name": "Neel Parekh",
            "type": "sparse",
            "x": 6.4771270751953125,
            "y": 9.133146286010742
        },
        {
            "id": "s_754",
            "name": "Yedendra Babu Shrinivasan",
            "type": "sparse",
            "x": 6.000173568725586,
            "y": 9.567950248718262
        },
        {
            "id": "s_755",
            "name": "Dawei Cheng",
            "type": "sparse",
            "x": 8.89720630645752,
            "y": 7.1770339012146
        },
        {
            "id": "s_756",
            "name": "John T. Stasko",
            "type": "sparse",
            "x": 5.870213508605957,
            "y": 8.569107055664062
        },
        {
            "id": "s_757",
            "name": "Heidrun Schumann",
            "type": "sparse",
            "x": 5.303016662597656,
            "y": 6.429903030395508
        },
        {
            "id": "s_758",
            "name": "Mandar Sharma",
            "type": "sparse",
            "x": 7.877171039581299,
            "y": 7.68709135055542
        },
        {
            "id": "s_759",
            "name": "Harry Stavropoulos",
            "type": "sparse",
            "x": 6.78038215637207,
            "y": 6.900979995727539
        },
        {
            "id": "s_760",
            "name": "Veronica J. Vieland",
            "type": "sparse",
            "x": 4.628226280212402,
            "y": 6.6852898597717285
        },
        {
            "id": "s_761",
            "name": "Patrick Chiu",
            "type": "sparse",
            "x": 4.838989734649658,
            "y": 5.702586650848389
        },
        {
            "id": "s_762",
            "name": "Michael Oppermann",
            "type": "sparse",
            "x": 7.126302242279053,
            "y": 9.960545539855957
        },
        {
            "id": "s_763",
            "name": "Jing Yang 0001",
            "type": "sparse",
            "x": 6.91139554977417,
            "y": 7.473057270050049
        },
        {
            "id": "s_764",
            "name": "Ed H. Chi",
            "type": "sparse",
            "x": 7.298556327819824,
            "y": 9.965882301330566
        },
        {
            "id": "s_765",
            "name": "Bing Wang 0007",
            "type": "sparse",
            "x": 4.627654552459717,
            "y": 7.064582347869873
        },
        {
            "id": "s_766",
            "name": "Yueqi Hu",
            "type": "sparse",
            "x": 8.78813648223877,
            "y": 7.098605155944824
        },
        {
            "id": "s_767",
            "name": "Luc Wilson",
            "type": "sparse",
            "x": 5.759721755981445,
            "y": 8.101409912109375
        },
        {
            "id": "s_768",
            "name": "Ratnesh K. Sharma",
            "type": "sparse",
            "x": 6.702599048614502,
            "y": 6.262012004852295
        },
        {
            "id": "s_769",
            "name": "R\u00e9my Vieux",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_770",
            "name": "Andrew E. Johnson 0001",
            "type": "sparse",
            "x": 5.082072734832764,
            "y": 9.60634708404541
        },
        {
            "id": "s_771",
            "name": "Galileo Namata",
            "type": "sparse",
            "x": 3.6647329330444336,
            "y": 7.409862041473389
        },
        {
            "id": "s_772",
            "name": "Qinying Liao",
            "type": "sparse",
            "x": 7.730375289916992,
            "y": 8.678378105163574
        },
        {
            "id": "s_773",
            "name": "Patrick J. Fitzpatrick",
            "type": "sparse",
            "x": 6.37584924697876,
            "y": 6.424874305725098
        },
        {
            "id": "s_774",
            "name": "Zheqing Yu",
            "type": "sparse",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "id": "s_775",
            "name": "Pierre Y. Andrews",
            "type": "sparse",
            "x": 2.157792329788208,
            "y": 8.928483963012695
        },
        {
            "id": "s_776",
            "name": "Praveen Kumar Reddy Ojili",
            "type": "sparse",
            "x": 3.6070477962493896,
            "y": 5.6530327796936035
        },
        {
            "id": "s_777",
            "name": "Saad Nadeem",
            "type": "sparse",
            "x": 5.832940101623535,
            "y": 7.073251247406006
        },
        {
            "id": "s_778",
            "name": "Alvitta Ottley",
            "type": "sparse",
            "x": 5.209470748901367,
            "y": 9.89851188659668
        },
        {
            "id": "s_779",
            "name": "Alex Kale",
            "type": "sparse",
            "x": 4.1113362312316895,
            "y": 8.209181785583496
        },
        {
            "id": "s_780",
            "name": "Seungwon Yang",
            "type": "sparse",
            "x": 5.334270477294922,
            "y": 10.654705047607422
        },
        {
            "id": "s_781",
            "name": "Aoyu Wu",
            "type": "sparse",
            "x": 5.177468776702881,
            "y": 9.842877388000488
        },
        {
            "id": "s_782",
            "name": "Shang-Tse Chen",
            "type": "sparse",
            "x": 7.04141902923584,
            "y": 6.825443267822266
        },
        {
            "id": "s_783",
            "name": "Nikhil Thorat",
            "type": "sparse",
            "x": 2.183356523513794,
            "y": 8.8823823928833
        },
        {
            "id": "s_784",
            "name": "Arlen Fan",
            "type": "sparse",
            "x": 2.2220568656921387,
            "y": 8.885293960571289
        },
        {
            "id": "s_785",
            "name": "Matthew L. Parry",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_786",
            "name": "Annie Tat",
            "type": "sparse",
            "x": 6.653965473175049,
            "y": 8.683868408203125
        },
        {
            "id": "s_787",
            "name": "Russ Burtner",
            "type": "sparse",
            "x": 5.411427021026611,
            "y": 9.972384452819824
        },
        {
            "id": "s_788",
            "name": "Junpeng Wang 0001",
            "type": "sparse",
            "x": 3.517043352127075,
            "y": 8.090068817138672
        },
        {
            "id": "s_789",
            "name": "Jie Liu 0046",
            "type": "sparse",
            "x": 3.6086106300354004,
            "y": 7.845585823059082
        },
        {
            "id": "s_790",
            "name": "Till Wollenberg",
            "type": "sparse",
            "x": 5.256996154785156,
            "y": 5.111698627471924
        },
        {
            "id": "s_791",
            "name": "Jiachen Wang",
            "type": "sparse",
            "x": 8.530769348144531,
            "y": 5.983466625213623
        },
        {
            "id": "s_792",
            "name": "Douglas M. Rice",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_793",
            "name": "Vincent Bloemen",
            "type": "sparse",
            "x": 6.628056526184082,
            "y": 6.06528377532959
        },
        {
            "id": "s_794",
            "name": "Mingjie Tang",
            "type": "sparse",
            "x": 5.8192877769470215,
            "y": 5.802861213684082
        },
        {
            "id": "s_795",
            "name": "Chittayong Surakitbanharn",
            "type": "sparse",
            "x": 5.395201206207275,
            "y": 9.531249046325684
        },
        {
            "id": "s_796",
            "name": "William Wright",
            "type": "sparse",
            "x": 6.806461334228516,
            "y": 8.27403450012207
        },
        {
            "id": "s_797",
            "name": "Elke A. Rundensteiner",
            "type": "sparse",
            "x": 4.930548667907715,
            "y": 7.509877681732178
        },
        {
            "id": "s_798",
            "name": "J. T. Fry",
            "type": "sparse",
            "x": 4.801039218902588,
            "y": 6.734798431396484
        },
        {
            "id": "s_799",
            "name": "Mennatallah El-Assady",
            "type": "sparse",
            "x": 6.286108016967773,
            "y": 10.335165023803711
        },
        {
            "id": "s_800",
            "name": "Diane Tang",
            "type": "sparse",
            "x": 5.484500408172607,
            "y": 8.928224563598633
        },
        {
            "id": "s_801",
            "name": "Zudi Lin",
            "type": "sparse",
            "x": 4.621323108673096,
            "y": 8.148672103881836
        },
        {
            "id": "s_802",
            "name": "Nadine Drager",
            "type": "sparse",
            "x": 6.390726089477539,
            "y": 6.075316429138184
        },
        {
            "id": "s_803",
            "name": "Layne T. Watson",
            "type": "sparse",
            "x": 3.6070477962493896,
            "y": 5.6530327796936035
        },
        {
            "id": "s_804",
            "name": "Yafeng Lu",
            "type": "sparse",
            "x": 7.0820770263671875,
            "y": 7.8309502601623535
        },
        {
            "id": "s_805",
            "name": "I. V. Ramakrishnan",
            "type": "sparse",
            "x": 4.116965293884277,
            "y": 7.970519542694092
        },
        {
            "id": "s_806",
            "name": "Edward J. Coyle",
            "type": "sparse",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "id": "s_807",
            "name": "Furui Cheng",
            "type": "sparse",
            "x": 3.104616641998291,
            "y": 8.795337677001953
        },
        {
            "id": "s_808",
            "name": "Sebastian Gehrmann",
            "type": "sparse",
            "x": 2.9387755393981934,
            "y": 8.857743263244629
        },
        {
            "id": "s_809",
            "name": "Simon Urbanek",
            "type": "sparse",
            "x": 5.817330360412598,
            "y": 10.671513557434082
        },
        {
            "id": "s_810",
            "name": "Davide Ceneda",
            "type": "sparse",
            "x": 4.256865501403809,
            "y": 9.337084770202637
        },
        {
            "id": "s_811",
            "name": "Michael Sedlmair",
            "type": "sparse",
            "x": 4.005371570587158,
            "y": 8.053948402404785
        },
        {
            "id": "s_812",
            "name": "Jia-Kai Chou",
            "type": "sparse",
            "x": 6.1468915939331055,
            "y": 8.70013427734375
        },
        {
            "id": "s_813",
            "name": "William John Teahan",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_814",
            "name": "Richard Bonneau",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_815",
            "name": "Ji Qi",
            "type": "sparse",
            "x": 6.628056526184082,
            "y": 6.06528377532959
        },
        {
            "id": "s_816",
            "name": "Tianyi Zhang",
            "type": "sparse",
            "x": 4.889342308044434,
            "y": 6.623239517211914
        },
        {
            "id": "s_817",
            "name": "Benedikt Stehno",
            "type": "sparse",
            "x": 3.272869348526001,
            "y": 7.7402143478393555
        },
        {
            "id": "s_818",
            "name": "Charilaos Papadopoulos",
            "type": "sparse",
            "x": 4.349888324737549,
            "y": 7.443033218383789
        },
        {
            "id": "s_819",
            "name": "Stuart K. Card",
            "type": "sparse",
            "x": 6.1577677726745605,
            "y": 9.878106117248535
        },
        {
            "id": "s_820",
            "name": "Ravin Balakrishnan",
            "type": "sparse",
            "x": 5.232478141784668,
            "y": 6.284097194671631
        },
        {
            "id": "s_821",
            "name": "Laura von R\u00fcden",
            "type": "sparse",
            "x": 4.52726936340332,
            "y": 6.233945369720459
        },
        {
            "id": "s_822",
            "name": "R\u00fcdiger Schernthaner",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_823",
            "name": "Aditya Kalro",
            "type": "sparse",
            "x": 2.157792329788208,
            "y": 8.928483963012695
        },
        {
            "id": "s_824",
            "name": "Jiali Liu",
            "type": "sparse",
            "x": 5.635706424713135,
            "y": 10.295689582824707
        },
        {
            "id": "s_825",
            "name": "David H. S. Chung",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_826",
            "name": "Eun Ju Nam",
            "type": "sparse",
            "x": 3.615382671356201,
            "y": 5.652681827545166
        },
        {
            "id": "s_827",
            "name": "Santhosh Nandhakumar",
            "type": "sparse",
            "x": 6.9483442306518555,
            "y": 10.12969970703125
        },
        {
            "id": "s_828",
            "name": "D. Morent",
            "type": "sparse",
            "x": 5.132715702056885,
            "y": 6.853967666625977
        },
        {
            "id": "s_829",
            "name": "Tamara Munzner",
            "type": "sparse",
            "x": 6.148962020874023,
            "y": 8.54147720336914
        },
        {
            "id": "s_830",
            "name": "Jason A. Laska",
            "type": "sparse",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        },
        {
            "id": "s_831",
            "name": "Christoph M\u00fcller 0001",
            "type": "sparse",
            "x": 6.911941051483154,
            "y": 8.79002857208252
        },
        {
            "id": "s_832",
            "name": "Matthias Kraus 0002",
            "type": "sparse",
            "x": 3.777678966522217,
            "y": 7.490976333618164
        },
        {
            "id": "s_833",
            "name": "Thomas Lindemeier",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_834",
            "name": "Donald J. Jacobs",
            "type": "sparse",
            "x": 4.889342308044434,
            "y": 6.623239517211914
        },
        {
            "id": "s_835",
            "name": "Mahdi Pakdaman Naeini",
            "type": "sparse",
            "x": 5.8732194900512695,
            "y": 6.8065009117126465
        },
        {
            "id": "s_836",
            "name": "Noel F. C. C. de Miranda",
            "type": "sparse",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "id": "s_837",
            "name": "Zhen Li 0044",
            "type": "sparse",
            "x": 3.9539642333984375,
            "y": 8.10319995880127
        },
        {
            "id": "s_838",
            "name": "Connor Gramazio",
            "type": "sparse",
            "x": 3.7646729946136475,
            "y": 6.220944881439209
        },
        {
            "id": "s_839",
            "name": "Christian Richter",
            "type": "sparse",
            "x": 6.887332916259766,
            "y": 6.183637619018555
        },
        {
            "id": "s_840",
            "name": "Kresimir Matkovic",
            "type": "sparse",
            "x": 5.116002559661865,
            "y": 6.950352191925049
        },
        {
            "id": "s_841",
            "name": "Roman Garnett",
            "type": "sparse",
            "x": 5.984103202819824,
            "y": 9.63005256652832
        },
        {
            "id": "s_842",
            "name": "Maximilian T. Fischer",
            "type": "sparse",
            "x": 4.8054656982421875,
            "y": 5.500706195831299
        },
        {
            "id": "s_843",
            "name": "Andreas M\u00fcller 0012",
            "type": "sparse",
            "x": 7.425528049468994,
            "y": 10.315485000610352
        },
        {
            "id": "s_844",
            "name": "Hendrikus H. M. Korsten",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_845",
            "name": "Jina Huh",
            "type": "sparse",
            "x": 5.768592834472656,
            "y": 10.660682678222656
        },
        {
            "id": "s_846",
            "name": "Mal\u00fa Castellanos",
            "type": "sparse",
            "x": 6.655723571777344,
            "y": 6.452548503875732
        },
        {
            "id": "s_847",
            "name": "Rita Sevastjanova",
            "type": "sparse",
            "x": 7.219534873962402,
            "y": 10.616594314575195
        },
        {
            "id": "s_848",
            "name": "Ji-Dong Yim",
            "type": "sparse",
            "x": 7.693778991699219,
            "y": 8.201777458190918
        },
        {
            "id": "s_849",
            "name": "Sonia Castelo",
            "type": "sparse",
            "x": 3.2641701698303223,
            "y": 8.662834167480469
        },
        {
            "id": "s_850",
            "name": "Prithiviraj K. Muthumanickam",
            "type": "sparse",
            "x": 6.588368892669678,
            "y": 6.7118377685546875
        },
        {
            "id": "s_851",
            "name": "Andr\u00e9s Lalama",
            "type": "sparse",
            "x": 4.120519161224365,
            "y": 7.768464088439941
        },
        {
            "id": "s_852",
            "name": "Sarang Joshi",
            "type": "sparse",
            "x": 5.052469253540039,
            "y": 9.884970664978027
        },
        {
            "id": "s_853",
            "name": "Sujan Anreddy",
            "type": "sparse",
            "x": 6.29480504989624,
            "y": 6.361831188201904
        },
        {
            "id": "s_854",
            "name": "Feng Wang 0012",
            "type": "sparse",
            "x": 8.197513580322266,
            "y": 8.10689926147461
        },
        {
            "id": "s_855",
            "name": "Caroline Ziemkiewicz",
            "type": "sparse",
            "x": 5.646180629730225,
            "y": 9.364519119262695
        },
        {
            "id": "s_856",
            "name": "Anas Daghistani",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_857",
            "name": "Johann Kastner",
            "type": "sparse",
            "x": 5.075896739959717,
            "y": 6.747625827789307
        },
        {
            "id": "s_858",
            "name": "Philipp Roskosch",
            "type": "sparse",
            "x": 7.490762710571289,
            "y": 5.422610759735107
        },
        {
            "id": "s_859",
            "name": "Gerik Scheuermann",
            "type": "sparse",
            "x": 7.10276985168457,
            "y": 9.965954780578613
        },
        {
            "id": "s_860",
            "name": "Jibonananda Sanyal",
            "type": "sparse",
            "x": 6.699019908905029,
            "y": 9.748359680175781
        },
        {
            "id": "s_861",
            "name": "Sarah E. Burke",
            "type": "sparse",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "id": "s_862",
            "name": "William J. Tolone",
            "type": "sparse",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "id": "s_863",
            "name": "Xun Zhao",
            "type": "sparse",
            "x": 4.117317199707031,
            "y": 7.614367485046387
        },
        {
            "id": "s_864",
            "name": "Rama Akkiraju",
            "type": "sparse",
            "x": 7.857691764831543,
            "y": 8.961512565612793
        },
        {
            "id": "s_865",
            "name": "R. Jordan Crouser",
            "type": "sparse",
            "x": 4.528852462768555,
            "y": 9.567140579223633
        },
        {
            "id": "s_866",
            "name": "Andrew Wade",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_867",
            "name": "Fosca Giannotti",
            "type": "sparse",
            "x": 3.618051052093506,
            "y": 5.657589912414551
        },
        {
            "id": "s_868",
            "name": "Chengqiao Lin",
            "type": "sparse",
            "x": 7.166633605957031,
            "y": 4.956838130950928
        },
        {
            "id": "s_869",
            "name": "Betul Salman",
            "type": "sparse",
            "x": 5.82072114944458,
            "y": 10.373441696166992
        },
        {
            "id": "s_870",
            "name": "Andreas Rauber",
            "type": "sparse",
            "x": 4.1527791023254395,
            "y": 8.461838722229004
        },
        {
            "id": "s_871",
            "name": "Frank Keul",
            "type": "sparse",
            "x": 5.561883926391602,
            "y": 6.028954029083252
        },
        {
            "id": "s_872",
            "name": "Patrice Y. Simard",
            "type": "sparse",
            "x": 7.385095596313477,
            "y": 10.23172664642334
        },
        {
            "id": "s_873",
            "name": "David Gotz",
            "type": "sparse",
            "x": 6.472065448760986,
            "y": 8.070807456970215
        },
        {
            "id": "s_874",
            "name": "Hao Yang 0007",
            "type": "sparse",
            "x": 2.1607532501220703,
            "y": 8.918561935424805
        },
        {
            "id": "s_875",
            "name": "Gunther Heidemann",
            "type": "sparse",
            "x": 4.178314208984375,
            "y": 8.581392288208008
        },
        {
            "id": "s_876",
            "name": "Qianliang Wu",
            "type": "sparse",
            "x": 7.463348865509033,
            "y": 5.214798927307129
        },
        {
            "id": "s_877",
            "name": "Haidong Chen",
            "type": "sparse",
            "x": 4.472042560577393,
            "y": 6.537318706512451
        },
        {
            "id": "s_878",
            "name": "Zengsheng Zhong",
            "type": "sparse",
            "x": 8.83371639251709,
            "y": 7.131134986877441
        },
        {
            "id": "s_879",
            "name": "Pierre-Yves Koenig",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_880",
            "name": "Zhiqiang Ma 0004",
            "type": "sparse",
            "x": 7.198326110839844,
            "y": 10.895029067993164
        },
        {
            "id": "s_881",
            "name": "Jie Tang 0001",
            "type": "sparse",
            "x": 5.147846698760986,
            "y": 5.0413899421691895
        },
        {
            "id": "s_882",
            "name": "Furu Wei",
            "type": "sparse",
            "x": 7.646381378173828,
            "y": 9.546295166015625
        },
        {
            "id": "s_883",
            "name": "Carlos D. Correa",
            "type": "sparse",
            "x": 5.404908180236816,
            "y": 6.289618015289307
        },
        {
            "id": "s_884",
            "name": "Fabian Fischer 0001",
            "type": "sparse",
            "x": 7.116809844970703,
            "y": 7.202565670013428
        },
        {
            "id": "s_885",
            "name": "Louis Licamele",
            "type": "sparse",
            "x": 4.7123332023620605,
            "y": 5.83228063583374
        },
        {
            "id": "s_886",
            "name": "John W. Bodnar",
            "type": "sparse",
            "x": 5.62460994720459,
            "y": 9.660894393920898
        },
        {
            "id": "s_887",
            "name": "Alexander Savelyev",
            "type": "sparse",
            "x": 7.839114665985107,
            "y": 8.217082977294922
        },
        {
            "id": "s_888",
            "name": "Nick Cramer",
            "type": "sparse",
            "x": 5.043529510498047,
            "y": 9.74374771118164
        },
        {
            "id": "s_889",
            "name": "Sebastian Bremm",
            "type": "sparse",
            "x": 4.904706954956055,
            "y": 6.332345008850098
        },
        {
            "id": "s_890",
            "name": "Sebastian Mittelst\u00e4dt",
            "type": "sparse",
            "x": 4.530881881713867,
            "y": 8.229743957519531
        },
        {
            "id": "s_891",
            "name": "Aaron Ault",
            "type": "sparse",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "id": "s_892",
            "name": "Nan Cao 0001",
            "type": "sparse",
            "x": 7.478777885437012,
            "y": 7.510499954223633
        },
        {
            "id": "s_893",
            "name": "J\u00fcrgen Bernard",
            "type": "sparse",
            "x": 4.929618835449219,
            "y": 6.263797283172607
        },
        {
            "id": "s_894",
            "name": "Hendrik Strobelt",
            "type": "sparse",
            "x": 3.369642734527588,
            "y": 8.23942756652832
        },
        {
            "id": "s_895",
            "name": "Andreas Bannach",
            "type": "sparse",
            "x": 4.166612148284912,
            "y": 8.035407066345215
        },
        {
            "id": "s_896",
            "name": "Zhonghua Lu",
            "type": "sparse",
            "x": 2.099738597869873,
            "y": 8.981844902038574
        },
        {
            "id": "s_897",
            "name": "Xidao Wen",
            "type": "sparse",
            "x": 7.012173652648926,
            "y": 7.094387531280518
        },
        {
            "id": "s_898",
            "name": "Michelle Dowling",
            "type": "sparse",
            "x": 5.366823196411133,
            "y": 8.537153244018555
        },
        {
            "id": "s_899",
            "name": "Kurt Dejgaard",
            "type": "sparse",
            "x": 5.504063129425049,
            "y": 6.4216389656066895
        },
        {
            "id": "s_900",
            "name": "Michael Ogawa",
            "type": "sparse",
            "x": 6.836374282836914,
            "y": 7.674856662750244
        },
        {
            "id": "s_901",
            "name": "Junyan Luo",
            "type": "sparse",
            "x": 7.1917266845703125,
            "y": 9.848210334777832
        },
        {
            "id": "s_902",
            "name": "Seok-Hee Hong 0001",
            "type": "sparse",
            "x": 5.461853981018066,
            "y": 5.2681779861450195
        },
        {
            "id": "s_903",
            "name": "XiongFei Luo",
            "type": "sparse",
            "x": 5.462973117828369,
            "y": 7.948393821716309
        },
        {
            "id": "s_904",
            "name": "Stephan Sellien",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_905",
            "name": "Huub van de Wetering",
            "type": "sparse",
            "x": 7.009634017944336,
            "y": 5.465327739715576
        },
        {
            "id": "s_906",
            "name": "Franz Wanner",
            "type": "sparse",
            "x": 6.885515213012695,
            "y": 6.168759346008301
        },
        {
            "id": "s_907",
            "name": "Anthony C. Robinson",
            "type": "sparse",
            "x": 7.0076727867126465,
            "y": 9.259106636047363
        },
        {
            "id": "s_908",
            "name": "Yi Chen 0007",
            "type": "sparse",
            "x": 5.054995059967041,
            "y": 6.512628555297852
        },
        {
            "id": "s_909",
            "name": "Bianca Tost",
            "type": "sparse",
            "x": 3.5826504230499268,
            "y": 5.609038829803467
        },
        {
            "id": "s_910",
            "name": "Danny Holten",
            "type": "sparse",
            "x": 5.216991424560547,
            "y": 5.118741035461426
        },
        {
            "id": "s_911",
            "name": "Philippas Tsigas",
            "type": "sparse",
            "x": 5.081255912780762,
            "y": 6.8517327308654785
        },
        {
            "id": "s_912",
            "name": "Wanqi Hu",
            "type": "sparse",
            "x": 5.058387756347656,
            "y": 6.625837802886963
        },
        {
            "id": "s_913",
            "name": "Xin Li",
            "type": "sparse",
            "x": 7.330615997314453,
            "y": 4.847436428070068
        },
        {
            "id": "s_914",
            "name": "Markus H\u00f6ferlin",
            "type": "sparse",
            "x": 5.545127868652344,
            "y": 8.685710906982422
        },
        {
            "id": "s_915",
            "name": "Funda Kivran-Swaine",
            "type": "sparse",
            "x": 8.004839897155762,
            "y": 8.350887298583984
        },
        {
            "id": "s_916",
            "name": "Michael Brooks",
            "type": "sparse",
            "x": 7.385095596313477,
            "y": 10.23172664642334
        },
        {
            "id": "s_917",
            "name": "Lu Lu",
            "type": "sparse",
            "x": 7.3848042488098145,
            "y": 4.858085632324219
        },
        {
            "id": "s_918",
            "name": "Jiawan Zhang",
            "type": "sparse",
            "x": 6.8173394203186035,
            "y": 7.458686828613281
        },
        {
            "id": "s_919",
            "name": "Susan E. Brennan",
            "type": "sparse",
            "x": 5.165153503417969,
            "y": 9.47240924835205
        },
        {
            "id": "s_920",
            "name": "Weiwei Cui",
            "type": "sparse",
            "x": 4.883158206939697,
            "y": 8.527709007263184
        },
        {
            "id": "s_921",
            "name": "Zac Lucarelli",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_922",
            "name": "Daniel Hubball",
            "type": "sparse",
            "x": 5.232822895050049,
            "y": 8.288137435913086
        },
        {
            "id": "s_923",
            "name": "Amin Jourabloo",
            "type": "sparse",
            "x": 2.1292340755462646,
            "y": 8.979385375976562
        },
        {
            "id": "s_924",
            "name": "Richard May 0001",
            "type": "sparse",
            "x": 5.662320137023926,
            "y": 9.607105255126953
        },
        {
            "id": "s_925",
            "name": "Yue Wang",
            "type": "sparse",
            "x": 5.112260341644287,
            "y": 7.125860214233398
        },
        {
            "id": "s_926",
            "name": "Hang Su 0006",
            "type": "sparse",
            "x": 2.165336847305298,
            "y": 8.943147659301758
        },
        {
            "id": "s_927",
            "name": "Stefan Bruckner",
            "type": "sparse",
            "x": 4.837636470794678,
            "y": 7.188283443450928
        },
        {
            "id": "s_928",
            "name": "Ryan Eccles",
            "type": "sparse",
            "x": 7.08587646484375,
            "y": 7.734273910522461
        },
        {
            "id": "s_929",
            "name": "Shaun Kennedy",
            "type": "sparse",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "id": "s_930",
            "name": "Zhipeng Wang",
            "type": "sparse",
            "x": 8.538335800170898,
            "y": 6.002535820007324
        },
        {
            "id": "s_931",
            "name": "Marcos Lage",
            "type": "sparse",
            "x": 5.759721755981445,
            "y": 8.101409912109375
        },
        {
            "id": "s_932",
            "name": "Dominik Sacha",
            "type": "sparse",
            "x": 4.912261962890625,
            "y": 7.877874851226807
        },
        {
            "id": "s_933",
            "name": "Yang Yang",
            "type": "sparse",
            "x": 5.301107883453369,
            "y": 6.747445583343506
        },
        {
            "id": "s_934",
            "name": "Theresa A. O'Connell",
            "type": "sparse",
            "x": 6.733508110046387,
            "y": 8.784727096557617
        },
        {
            "id": "s_935",
            "name": "Barrett Ens",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_936",
            "name": "Mohamed Yakout",
            "type": "sparse",
            "x": 6.5760884284973145,
            "y": 6.957261085510254
        },
        {
            "id": "s_937",
            "name": "Benjamin H\u00f6ferlin",
            "type": "sparse",
            "x": 5.545127868652344,
            "y": 8.685710906982422
        },
        {
            "id": "s_938",
            "name": "Supriya Garg",
            "type": "sparse",
            "x": 3.5928711891174316,
            "y": 7.219573974609375
        },
        {
            "id": "s_939",
            "name": "Daniel Wigdor",
            "type": "sparse",
            "x": 6.397348880767822,
            "y": 8.011615753173828
        },
        {
            "id": "s_940",
            "name": "My T. Thai",
            "type": "sparse",
            "x": 4.645948886871338,
            "y": 7.92765474319458
        },
        {
            "id": "s_941",
            "name": "Paolo Simonetto",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_942",
            "name": "Dan Goldwasser",
            "type": "sparse",
            "x": 4.136512279510498,
            "y": 8.611287117004395
        },
        {
            "id": "s_943",
            "name": "Zafar Ahmed",
            "type": "sparse",
            "x": 3.781320095062256,
            "y": 6.027749538421631
        },
        {
            "id": "s_944",
            "name": "Matthias Zieker",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_945",
            "name": "Shehzad Afzal",
            "type": "sparse",
            "x": 4.962870121002197,
            "y": 7.165988445281982
        },
        {
            "id": "s_946",
            "name": "Wolfgang Herzner",
            "type": "sparse",
            "x": 4.04945182800293,
            "y": 6.402584075927734
        },
        {
            "id": "s_947",
            "name": "Hao Dong 0008",
            "type": "sparse",
            "x": 3.2904086112976074,
            "y": 7.0784125328063965
        },
        {
            "id": "s_948",
            "name": "Jun Wang",
            "type": "sparse",
            "x": 4.261224746704102,
            "y": 8.188447952270508
        },
        {
            "id": "s_949",
            "name": "Whitney K. Huang",
            "type": "sparse",
            "x": 6.880544662475586,
            "y": 6.19067907333374
        },
        {
            "id": "s_950",
            "name": "Marcel Wunderlich",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_951",
            "name": "Guy Danon",
            "type": "sparse",
            "x": 7.499957084655762,
            "y": 10.237372398376465
        },
        {
            "id": "s_952",
            "name": "David S. Ebert",
            "type": "sparse",
            "x": 5.810538291931152,
            "y": 7.598243713378906
        },
        {
            "id": "s_953",
            "name": "Mao Ye",
            "type": "sparse",
            "x": 2.1292340755462646,
            "y": 8.979385375976562
        },
        {
            "id": "s_954",
            "name": "Valentin Polishchuk",
            "type": "sparse",
            "x": 3.7874088287353516,
            "y": 6.028797149658203
        },
        {
            "id": "s_955",
            "name": "Ronghua Shi",
            "type": "sparse",
            "x": 8.83371639251709,
            "y": 7.131134986877441
        },
        {
            "id": "s_956",
            "name": "Adam Bodnar",
            "type": "sparse",
            "x": 6.7051167488098145,
            "y": 8.124053955078125
        },
        {
            "id": "s_957",
            "name": "Ke Li",
            "type": "sparse",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "id": "s_958",
            "name": "Alex Bigelow",
            "type": "sparse",
            "x": 5.120394229888916,
            "y": 5.120257377624512
        },
        {
            "id": "s_959",
            "name": "Cristian Felix",
            "type": "sparse",
            "x": 7.253793239593506,
            "y": 9.663007736206055
        },
        {
            "id": "s_960",
            "name": "Gunther H. Weber",
            "type": "sparse",
            "x": 7.229828834533691,
            "y": 10.067737579345703
        },
        {
            "id": "s_961",
            "name": "Deokgun Park 0001",
            "type": "sparse",
            "x": 7.349143028259277,
            "y": 10.615827560424805
        },
        {
            "id": "s_962",
            "name": "Nanxiang Li",
            "type": "sparse",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "id": "s_963",
            "name": "Thomas Kirste",
            "type": "sparse",
            "x": 3.764721155166626,
            "y": 7.994372844696045
        },
        {
            "id": "s_964",
            "name": "Scott Davidoff",
            "type": "sparse",
            "x": 3.307056188583374,
            "y": 8.17342758178711
        },
        {
            "id": "s_965",
            "name": "Elizabeth Braunstein",
            "type": "sparse",
            "x": 6.491633415222168,
            "y": 9.125612258911133
        },
        {
            "id": "s_966",
            "name": "Jina Suh",
            "type": "sparse",
            "x": 3.4941341876983643,
            "y": 8.608490943908691
        },
        {
            "id": "s_967",
            "name": "Guang Lin",
            "type": "sparse",
            "x": 6.290637969970703,
            "y": 6.369799613952637
        },
        {
            "id": "s_968",
            "name": "Young Bin Kim",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_969",
            "name": "Haolin Zhi",
            "type": "sparse",
            "x": 7.01570463180542,
            "y": 6.53007698059082
        },
        {
            "id": "s_970",
            "name": "Matthew O. Ward",
            "type": "sparse",
            "x": 4.991003513336182,
            "y": 7.665530204772949
        },
        {
            "id": "s_971",
            "name": "Marco Jenny",
            "type": "sparse",
            "x": 8.669734954833984,
            "y": 7.024385452270508
        },
        {
            "id": "s_972",
            "name": "Fr\u00e9d\u00e9ric Gilbert 0001",
            "type": "sparse",
            "x": 5.512553691864014,
            "y": 5.254363059997559
        },
        {
            "id": "s_973",
            "name": "Bj\u00f6rn Hartmann",
            "type": "sparse",
            "x": 5.919469833374023,
            "y": 10.14952278137207
        },
        {
            "id": "s_974",
            "name": "Huixuan Xie",
            "type": "sparse",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "id": "s_975",
            "name": "Ye Zhao 0003",
            "type": "sparse",
            "x": 7.287503719329834,
            "y": 6.2570343017578125
        },
        {
            "id": "s_976",
            "name": "Linhao Meng",
            "type": "sparse",
            "x": 4.8352508544921875,
            "y": 7.234743595123291
        },
        {
            "id": "s_977",
            "name": "Romain Vuillemot",
            "type": "sparse",
            "x": 7.462177753448486,
            "y": 10.412565231323242
        },
        {
            "id": "s_978",
            "name": "Michael Wimmer 0001",
            "type": "sparse",
            "x": 4.083239555358887,
            "y": 6.6136088371276855
        },
        {
            "id": "s_979",
            "name": "Zhenyu Cheryl Qian",
            "type": "sparse",
            "x": 5.679025650024414,
            "y": 9.466726303100586
        },
        {
            "id": "s_980",
            "name": "Harlan Foote",
            "type": "sparse",
            "x": 5.008632659912109,
            "y": 5.230308532714844
        },
        {
            "id": "s_981",
            "name": "Martin Eisemann",
            "type": "sparse",
            "x": 4.613693714141846,
            "y": 6.799837589263916
        },
        {
            "id": "s_982",
            "name": "Fangzhao Wu",
            "type": "sparse",
            "x": 8.011079788208008,
            "y": 8.516590118408203
        },
        {
            "id": "s_983",
            "name": "Stefanie Schmid",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_984",
            "name": "Guido Tack",
            "type": "sparse",
            "x": 3.6774682998657227,
            "y": 7.877414703369141
        },
        {
            "id": "s_985",
            "name": "Jason D. Williams",
            "type": "sparse",
            "x": 3.4941341876983643,
            "y": 8.608490943908691
        },
        {
            "id": "s_986",
            "name": "Yeukyin Chan",
            "type": "sparse",
            "x": 8.538335800170898,
            "y": 6.002535820007324
        },
        {
            "id": "s_987",
            "name": "Christopher deFilippi",
            "type": "sparse",
            "x": 3.5829885005950928,
            "y": 5.610853672027588
        },
        {
            "id": "s_988",
            "name": "Ines F\u00e4rber",
            "type": "sparse",
            "x": 4.426377296447754,
            "y": 6.390743732452393
        },
        {
            "id": "s_989",
            "name": "Meichun Hsu",
            "type": "sparse",
            "x": 6.504193305969238,
            "y": 7.753623962402344
        },
        {
            "id": "s_990",
            "name": "Denis Gracanin",
            "type": "sparse",
            "x": 4.576113224029541,
            "y": 7.3307576179504395
        },
        {
            "id": "s_991",
            "name": "Jianping Kelvin Li",
            "type": "sparse",
            "x": 5.2668681144714355,
            "y": 8.542102813720703
        },
        {
            "id": "s_992",
            "name": "Ching-Yung Lin",
            "type": "sparse",
            "x": 7.491602897644043,
            "y": 8.011017799377441
        },
        {
            "id": "s_993",
            "name": "Brian D. Bue",
            "type": "sparse",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "id": "s_994",
            "name": "Ernesto A. Bjerg",
            "type": "sparse",
            "x": 5.554603576660156,
            "y": 6.4707207679748535
        },
        {
            "id": "s_995",
            "name": "Thomas Baudel",
            "type": "sparse",
            "x": 5.062215805053711,
            "y": 8.758380889892578
        },
        {
            "id": "s_996",
            "name": "Robert A. Lafrance",
            "type": "sparse",
            "x": 4.675631999969482,
            "y": 9.515111923217773
        },
        {
            "id": "s_997",
            "name": "Minfeng Zhu 0001",
            "type": "sparse",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "id": "s_998",
            "name": "Prasenjit Mitra",
            "type": "sparse",
            "x": 7.314278602600098,
            "y": 9.282108306884766
        },
        {
            "id": "s_999",
            "name": "Hartmut Ziegler",
            "type": "sparse",
            "x": 8.669734954833984,
            "y": 7.024385452270508
        },
        {
            "id": "s_1000",
            "name": "Nicola Pezzotti",
            "type": "sparse",
            "x": 3.2788166999816895,
            "y": 7.642777442932129
        },
        {
            "id": "s_1001",
            "name": "Bowen Yu 0004",
            "type": "sparse",
            "x": 5.032397747039795,
            "y": 6.75681734085083
        },
        {
            "id": "s_1002",
            "name": "Zeng Dai",
            "type": "sparse",
            "x": 4.913002967834473,
            "y": 5.747988224029541
        },
        {
            "id": "s_1003",
            "name": "Wenchao Wu",
            "type": "sparse",
            "x": 5.246304035186768,
            "y": 6.87199592590332
        },
        {
            "id": "s_1004",
            "name": "Meng Xia 0002",
            "type": "sparse",
            "x": 5.980079650878906,
            "y": 8.734273910522461
        },
        {
            "id": "s_1005",
            "name": "Seungyeon Kim",
            "type": "sparse",
            "x": 7.521107196807861,
            "y": 10.293533325195312
        },
        {
            "id": "s_1006",
            "name": "Aditya Johri",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_1007",
            "name": "Tarik Crnovrsanin",
            "type": "sparse",
            "x": 6.3358330726623535,
            "y": 6.023787021636963
        },
        {
            "id": "s_1008",
            "name": "Jay Koven",
            "type": "sparse",
            "x": 6.941458702087402,
            "y": 8.855831146240234
        },
        {
            "id": "s_1009",
            "name": "Denis Lalanne",
            "type": "sparse",
            "x": 6.889247417449951,
            "y": 7.69767951965332
        },
        {
            "id": "s_1010",
            "name": "Yixian Zheng",
            "type": "sparse",
            "x": 5.246304035186768,
            "y": 6.87199592590332
        },
        {
            "id": "s_1011",
            "name": "Carlos A. Dietrich",
            "type": "sparse",
            "x": 8.519011497497559,
            "y": 6.024631023406982
        },
        {
            "id": "s_1012",
            "name": "Jia Zeng",
            "type": "sparse",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "id": "s_1013",
            "name": "Ulrich Bartling",
            "type": "sparse",
            "x": 6.154789447784424,
            "y": 7.499543190002441
        },
        {
            "id": "s_1014",
            "name": "Olivier Thonnard",
            "type": "sparse",
            "x": 5.912964820861816,
            "y": 9.671320915222168
        },
        {
            "id": "s_1015",
            "name": "Khairi Reda",
            "type": "sparse",
            "x": 4.8665313720703125,
            "y": 6.72496223449707
        },
        {
            "id": "s_1016",
            "name": "Sai Prashanth Dasari",
            "type": "sparse",
            "x": 6.9483442306518555,
            "y": 10.12969970703125
        },
        {
            "id": "s_1017",
            "name": "Boonthanome Nouanesengsy",
            "type": "sparse",
            "x": 4.628226280212402,
            "y": 6.6852898597717285
        },
        {
            "id": "s_1018",
            "name": "Joe Bruce",
            "type": "sparse",
            "x": 5.536873817443848,
            "y": 9.789745330810547
        },
        {
            "id": "s_1019",
            "name": "Jiang Wu",
            "type": "sparse",
            "x": 8.534769058227539,
            "y": 5.998741626739502
        },
        {
            "id": "s_1020",
            "name": "Daniel Smilkov",
            "type": "sparse",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "id": "s_1021",
            "name": "Thomas L\u00f6we",
            "type": "sparse",
            "x": 3.6500320434570312,
            "y": 7.781797885894775
        },
        {
            "id": "s_1022",
            "name": "Leslie M. Blaha",
            "type": "sparse",
            "x": 4.715351581573486,
            "y": 9.705424308776855
        },
        {
            "id": "s_1023",
            "name": "Jamal Alsakran",
            "type": "sparse",
            "x": 7.235036373138428,
            "y": 8.927081108093262
        },
        {
            "id": "s_1024",
            "name": "Xian Teng",
            "type": "sparse",
            "x": 7.012173652648926,
            "y": 7.094387531280518
        },
        {
            "id": "s_1025",
            "name": "Patrick Mackey",
            "type": "sparse",
            "x": 5.008632659912109,
            "y": 5.230308532714844
        },
        {
            "id": "s_1026",
            "name": "Zhibin Niu",
            "type": "sparse",
            "x": 8.89720630645752,
            "y": 7.1770339012146
        },
        {
            "id": "s_1027",
            "name": "Jim Zhu",
            "type": "sparse",
            "x": 7.447577953338623,
            "y": 9.646245002746582
        },
        {
            "id": "s_1028",
            "name": "Sukwon Lee",
            "type": "sparse",
            "x": 5.768592834472656,
            "y": 10.660682678222656
        },
        {
            "id": "s_1029",
            "name": "Stefan Weber 0004",
            "type": "sparse",
            "x": 4.062925338745117,
            "y": 9.458945274353027
        },
        {
            "id": "s_1030",
            "name": "Glenn A. Fink",
            "type": "sparse",
            "x": 5.355887413024902,
            "y": 9.362960815429688
        },
        {
            "id": "s_1031",
            "name": "Johannes Kuntner",
            "type": "sparse",
            "x": 8.872180938720703,
            "y": 7.168383598327637
        },
        {
            "id": "s_1032",
            "name": "Jieqiong Zhao",
            "type": "sparse",
            "x": 5.402294158935547,
            "y": 8.5021390914917
        },
        {
            "id": "s_1033",
            "name": "Zeqian Shen",
            "type": "sparse",
            "x": 6.006004333496094,
            "y": 6.751768112182617
        },
        {
            "id": "s_1034",
            "name": "Neil Spring",
            "type": "sparse",
            "x": 7.681723117828369,
            "y": 7.083856582641602
        },
        {
            "id": "s_1035",
            "name": "Malgorzata Migut",
            "type": "sparse",
            "x": 4.389556884765625,
            "y": 8.165496826171875
        },
        {
            "id": "s_1036",
            "name": "Xi Chen",
            "type": "sparse",
            "x": 5.285205364227295,
            "y": 5.645528316497803
        },
        {
            "id": "s_1037",
            "name": "Jim Smiley",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_1038",
            "name": "Daniel A. Keim",
            "type": "sparse",
            "x": 6.115347862243652,
            "y": 7.7548298835754395
        },
        {
            "id": "s_1039",
            "name": "Xiaoru Lin",
            "type": "sparse",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "id": "s_1040",
            "name": "Derek Stephens",
            "type": "sparse",
            "x": 6.659133434295654,
            "y": 9.591086387634277
        },
        {
            "id": "s_1041",
            "name": "Chris Muelder",
            "type": "sparse",
            "x": 6.3358330726623535,
            "y": 6.023787021636963
        },
        {
            "id": "s_1042",
            "name": "Fred Hohman",
            "type": "sparse",
            "x": 3.457275390625,
            "y": 7.968775749206543
        },
        {
            "id": "s_1043",
            "name": "Iulian Peca",
            "type": "sparse",
            "x": 7.01570463180542,
            "y": 6.53007698059082
        },
        {
            "id": "s_1044",
            "name": "Hujun Bao",
            "type": "sparse",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "id": "s_1045",
            "name": "Xuanwu Yue",
            "type": "sparse",
            "x": 8.831314086914062,
            "y": 7.171049118041992
        },
        {
            "id": "s_1046",
            "name": "Zhiqi Liu",
            "type": "sparse",
            "x": 4.472042560577393,
            "y": 6.537318706512451
        },
        {
            "id": "s_1047",
            "name": "Brian D. Fisher",
            "type": "sparse",
            "x": 5.450980186462402,
            "y": 9.5066499710083
        },
        {
            "id": "s_1048",
            "name": "Danyel Fisher",
            "type": "sparse",
            "x": 6.678240776062012,
            "y": 9.271001815795898
        },
        {
            "id": "s_1049",
            "name": "Sang-Cheol Seok",
            "type": "sparse",
            "x": 4.628226280212402,
            "y": 6.6852898597717285
        },
        {
            "id": "s_1050",
            "name": "Wenlong Chen",
            "type": "sparse",
            "x": 6.1468915939331055,
            "y": 8.70013427734375
        },
        {
            "id": "s_1051",
            "name": "Adel Ahmed",
            "type": "sparse",
            "x": 5.461853981018066,
            "y": 5.2681779861450195
        },
        {
            "id": "s_1052",
            "name": "Tongshuang Wu",
            "type": "sparse",
            "x": 7.16457462310791,
            "y": 9.706252098083496
        },
        {
            "id": "s_1053",
            "name": "Georg Fuchs",
            "type": "sparse",
            "x": 7.483101844787598,
            "y": 5.324905872344971
        },
        {
            "id": "s_1054",
            "name": "Zhenhuang Wang",
            "type": "sparse",
            "x": 7.622433662414551,
            "y": 6.9848785400390625
        },
        {
            "id": "s_1055",
            "name": "Sherry Towers",
            "type": "sparse",
            "x": 6.677735328674316,
            "y": 6.570087909698486
        },
        {
            "id": "s_1056",
            "name": "Nazanin Kadivar",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_1057",
            "name": "Reshika Palaniyappan Velumani",
            "type": "sparse",
            "x": 5.183478832244873,
            "y": 10.278133392333984
        },
        {
            "id": "s_1058",
            "name": "Zhiguang Zhou",
            "type": "sparse",
            "x": 4.637801647186279,
            "y": 6.569194793701172
        },
        {
            "id": "s_1059",
            "name": "Steven R. Corman",
            "type": "sparse",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "id": "s_1060",
            "name": "Yadong Wu",
            "type": "sparse",
            "x": 7.424687385559082,
            "y": 6.758628845214844
        },
        {
            "id": "s_1061",
            "name": "David Joseph Wrisley",
            "type": "sparse",
            "x": 7.528163909912109,
            "y": 10.42373275756836
        },
        {
            "id": "s_1062",
            "name": "Sol\u00e9akh\u00e9na Ken",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_1063",
            "name": "Jorge Poco",
            "type": "sparse",
            "x": 5.903576850891113,
            "y": 5.4117536544799805
        },
        {
            "id": "s_1064",
            "name": "Joanne Taery Kim",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_1065",
            "name": "Yang Chen",
            "type": "sparse",
            "x": 5.981436729431152,
            "y": 8.937657356262207
        },
        {
            "id": "s_1066",
            "name": "Petra Isenberg",
            "type": "sparse",
            "x": 5.529303073883057,
            "y": 10.356632232666016
        },
        {
            "id": "s_1067",
            "name": "Shichao Jia",
            "type": "sparse",
            "x": 6.394692420959473,
            "y": 10.036843299865723
        },
        {
            "id": "s_1068",
            "name": "Steven Mark Drucker",
            "type": "sparse",
            "x": 7.385095596313477,
            "y": 10.23172664642334
        },
        {
            "id": "s_1069",
            "name": "Xiting Wang",
            "type": "sparse",
            "x": 6.082334518432617,
            "y": 8.130379676818848
        },
        {
            "id": "s_1070",
            "name": "Mark A. Livingston",
            "type": "sparse",
            "x": 5.102502822875977,
            "y": 7.049795150756836
        },
        {
            "id": "s_1071",
            "name": "Carlos Eduardo Scheidegger",
            "type": "sparse",
            "x": 5.817330360412598,
            "y": 10.671513557434082
        },
        {
            "id": "s_1072",
            "name": "Shamkant B. Navathe",
            "type": "sparse",
            "x": 4.99072265625,
            "y": 5.377012252807617
        },
        {
            "id": "s_1073",
            "name": "Kanupriyah Singhal",
            "type": "sparse",
            "x": 6.486181735992432,
            "y": 9.125225067138672
        },
        {
            "id": "s_1074",
            "name": "Mira Dontcheva",
            "type": "sparse",
            "x": 7.511104106903076,
            "y": 6.730006217956543
        },
        {
            "id": "s_1075",
            "name": "Kelly P. Gaither",
            "type": "sparse",
            "x": 4.0166425704956055,
            "y": 8.603402137756348
        },
        {
            "id": "s_1076",
            "name": "Han Hong",
            "type": "sparse",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "id": "s_1077",
            "name": "Isaac Cho",
            "type": "sparse",
            "x": 6.832483291625977,
            "y": 9.187666893005371
        },
        {
            "id": "s_1078",
            "name": "Scott Pezanowski",
            "type": "sparse",
            "x": 7.839114665985107,
            "y": 8.217082977294922
        },
        {
            "id": "s_1079",
            "name": "Dan Zhang",
            "type": "sparse",
            "x": 5.112260341644287,
            "y": 7.125860214233398
        },
        {
            "id": "s_1080",
            "name": "Yuhong Li",
            "type": "sparse",
            "x": 5.385289669036865,
            "y": 7.21790075302124
        },
        {
            "id": "s_1081",
            "name": "Boudewijn P. F. Lelieveldt",
            "type": "sparse",
            "x": 4.135839939117432,
            "y": 7.404669284820557
        },
        {
            "id": "s_1082",
            "name": "Madelaine Daianu",
            "type": "sparse",
            "x": 5.197463035583496,
            "y": 5.419427871704102
        },
        {
            "id": "s_1083",
            "name": "Mustafa Bilgic 0001",
            "type": "sparse",
            "x": 4.7123332023620605,
            "y": 5.83228063583374
        },
        {
            "id": "s_1084",
            "name": "Paolo Crisafulli",
            "type": "sparse",
            "x": 4.68471097946167,
            "y": 8.889886856079102
        },
        {
            "id": "s_1085",
            "name": "Paul M. Thompson",
            "type": "sparse",
            "x": 5.197463035583496,
            "y": 5.419427871704102
        },
        {
            "id": "s_1086",
            "name": "Yunhai Wang",
            "type": "sparse",
            "x": 3.5461413860321045,
            "y": 5.572312831878662
        },
        {
            "id": "s_1087",
            "name": "Ryan Wesslen",
            "type": "sparse",
            "x": 6.262717247009277,
            "y": 8.60114860534668
        },
        {
            "id": "s_1088",
            "name": "Neel Sundaresan",
            "type": "sparse",
            "x": 7.071080684661865,
            "y": 8.232402801513672
        },
        {
            "id": "s_1089",
            "name": "Will Epperson",
            "type": "sparse",
            "x": 4.593509197235107,
            "y": 8.633565902709961
        },
        {
            "id": "s_1090",
            "name": "Highmed Consortium",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_1091",
            "name": "Yitao Wu",
            "type": "sparse",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "id": "s_1092",
            "name": "Rafael Messias Martins",
            "type": "sparse",
            "x": 3.587266445159912,
            "y": 8.453961372375488
        },
        {
            "id": "s_1093",
            "name": "Haiyan Yang",
            "type": "sparse",
            "x": 5.911811351776123,
            "y": 8.701251983642578
        },
        {
            "id": "s_1094",
            "name": "Xuan Zhong",
            "type": "sparse",
            "x": 6.62894344329834,
            "y": 7.445334434509277
        },
        {
            "id": "s_1095",
            "name": "Dongyu Liu",
            "type": "sparse",
            "x": 5.296180725097656,
            "y": 6.965846061706543
        },
        {
            "id": "s_1096",
            "name": "Haesun Park",
            "type": "sparse",
            "x": 5.338520050048828,
            "y": 8.103423118591309
        },
        {
            "id": "s_1097",
            "name": "Yaqi Qin",
            "type": "sparse",
            "x": 4.69851016998291,
            "y": 5.8544511795043945
        },
        {
            "id": "s_1098",
            "name": "Jianfei Chen 0001",
            "type": "sparse",
            "x": 7.447577953338623,
            "y": 9.646245002746582
        },
        {
            "id": "s_1099",
            "name": "Aniket Kittur",
            "type": "sparse",
            "x": 7.373030662536621,
            "y": 9.61923599243164
        },
        {
            "id": "s_1100",
            "name": "William W. Hargrove",
            "type": "sparse",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "id": "s_1101",
            "name": "Cheng Zheng",
            "type": "sparse",
            "x": 6.365241527557373,
            "y": 6.8800506591796875
        },
        {
            "id": "s_1102",
            "name": "Julian Heinrich",
            "type": "sparse",
            "x": 5.833996772766113,
            "y": 7.745819091796875
        },
        {
            "id": "s_1103",
            "name": "Mihaela Jarema",
            "type": "sparse",
            "x": 4.286999702453613,
            "y": 6.186981201171875
        },
        {
            "id": "s_1104",
            "name": "Heidi Werner",
            "type": "sparse",
            "x": 5.759721755981445,
            "y": 8.101409912109375
        },
        {
            "id": "s_1105",
            "name": "Michael Blumenschein",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_1106",
            "name": "Preeti Malakar",
            "type": "sparse",
            "x": 4.80540657043457,
            "y": 6.956305503845215
        },
        {
            "id": "s_1107",
            "name": "Brendan Moyle",
            "type": "sparse",
            "x": 5.651156425476074,
            "y": 10.584758758544922
        },
        {
            "id": "s_1108",
            "name": "Christopher D. Shaw",
            "type": "sparse",
            "x": 7.141628265380859,
            "y": 8.593308448791504
        },
        {
            "id": "s_1109",
            "name": "Chi-Chun Pan",
            "type": "sparse",
            "x": 7.193832874298096,
            "y": 9.813684463500977
        },
        {
            "id": "s_1110",
            "name": "Jan Zah\u00e1lka",
            "type": "sparse",
            "x": 5.051943778991699,
            "y": 8.408015251159668
        },
        {
            "id": "s_1111",
            "name": "Arvind Kumar Shekar",
            "type": "sparse",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "id": "s_1112",
            "name": "Anoop Sarkar",
            "type": "sparse",
            "x": 7.595698833465576,
            "y": 10.46611499786377
        },
        {
            "id": "s_1113",
            "name": "Yu-Hsuan Chan",
            "type": "sparse",
            "x": 4.599374294281006,
            "y": 7.230142593383789
        },
        {
            "id": "s_1114",
            "name": "Zhong Su",
            "type": "sparse",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "id": "s_1115",
            "name": "Zhicheng Liu 0001",
            "type": "sparse",
            "x": 6.622970104217529,
            "y": 7.970371246337891
        },
        {
            "id": "s_1116",
            "name": "Robert Kr\u00fcger",
            "type": "sparse",
            "x": 6.1062421798706055,
            "y": 8.166038513183594
        },
        {
            "id": "s_1117",
            "name": "Brian M. Tomaszewski",
            "type": "sparse",
            "x": 5.850448131561279,
            "y": 10.297879219055176
        },
        {
            "id": "s_1118",
            "name": "Jie Bao 0003",
            "type": "sparse",
            "x": 6.504770755767822,
            "y": 6.255224227905273
        },
        {
            "id": "s_1119",
            "name": "Li Tan",
            "type": "sparse",
            "x": 7.562387943267822,
            "y": 10.414213180541992
        },
        {
            "id": "s_1120",
            "name": "Shiry Ginosar",
            "type": "sparse",
            "x": 5.919469833374023,
            "y": 10.14952278137207
        },
        {
            "id": "s_1121",
            "name": "Rachel Shadoan",
            "type": "sparse",
            "x": 5.138487339019775,
            "y": 6.1423020362854
        },
        {
            "id": "s_1122",
            "name": "Xinlong Zhang",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_1123",
            "name": "Hansi Senaratne",
            "type": "sparse",
            "x": 4.74345588684082,
            "y": 8.819846153259277
        },
        {
            "id": "s_1124",
            "name": "Silvia Miksch",
            "type": "sparse",
            "x": 5.389550685882568,
            "y": 7.883035182952881
        },
        {
            "id": "s_1125",
            "name": "Thilo Spinner",
            "type": "sparse",
            "x": 3.1717755794525146,
            "y": 8.746894836425781
        },
        {
            "id": "s_1126",
            "name": "Jo Wood",
            "type": "sparse",
            "x": 6.064800262451172,
            "y": 7.87640905380249
        },
        {
            "id": "s_1127",
            "name": "Bryan A. Pendleton",
            "type": "sparse",
            "x": 6.555689811706543,
            "y": 9.125818252563477
        },
        {
            "id": "s_1128",
            "name": "Shawn Bohn",
            "type": "sparse",
            "x": 4.323966979980469,
            "y": 6.500546455383301
        },
        {
            "id": "s_1129",
            "name": "Yumeng Hou",
            "type": "sparse",
            "x": 5.058387756347656,
            "y": 6.625837802886963
        },
        {
            "id": "s_1130",
            "name": "Lawrence T. Glickman",
            "type": "sparse",
            "x": 6.365241527557373,
            "y": 6.8800506591796875
        },
        {
            "id": "s_1131",
            "name": "Andrea Unger",
            "type": "sparse",
            "x": 6.642919540405273,
            "y": 6.131760597229004
        },
        {
            "id": "s_1132",
            "name": "Ko-Chih Wang",
            "type": "sparse",
            "x": 3.0904009342193604,
            "y": 7.828068256378174
        },
        {
            "id": "s_1133",
            "name": "Alice Chu",
            "type": "sparse",
            "x": 5.91933536529541,
            "y": 6.946452617645264
        },
        {
            "id": "s_1134",
            "name": "Michael Smuc",
            "type": "sparse",
            "x": 4.754398345947266,
            "y": 9.436184883117676
        },
        {
            "id": "s_1135",
            "name": "Mario Costa Sousa",
            "type": "sparse",
            "x": 4.561797142028809,
            "y": 6.675894737243652
        },
        {
            "id": "s_1136",
            "name": "Dennis Chau",
            "type": "sparse",
            "x": 5.082072734832764,
            "y": 9.60634708404541
        },
        {
            "id": "s_1137",
            "name": "Scott Barlowe",
            "type": "sparse",
            "x": 6.353636741638184,
            "y": 8.521591186523438
        },
        {
            "id": "s_1138",
            "name": "Feiran Wu",
            "type": "sparse",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "id": "s_1139",
            "name": "Britta Renner",
            "type": "sparse",
            "x": 4.693850040435791,
            "y": 6.735891342163086
        },
        {
            "id": "s_1140",
            "name": "Xiangyang Wu",
            "type": "sparse",
            "x": 6.856532096862793,
            "y": 6.264466762542725
        },
        {
            "id": "s_1141",
            "name": "Shaun J. Grannis",
            "type": "sparse",
            "x": 6.470664978027344,
            "y": 6.918655872344971
        },
        {
            "id": "s_1142",
            "name": "Feng Luo 0002",
            "type": "sparse",
            "x": 6.189929008483887,
            "y": 6.351723670959473
        },
        {
            "id": "s_1143",
            "name": "Dong Sun 0001",
            "type": "sparse",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "id": "s_1144",
            "name": "Panpan Xu",
            "type": "sparse",
            "x": 5.693430423736572,
            "y": 7.413105487823486
        },
        {
            "id": "s_1145",
            "name": "Florian Stoffel",
            "type": "sparse",
            "x": 6.090968608856201,
            "y": 8.842906951904297
        },
        {
            "id": "s_1146",
            "name": "Xiaobo Luo",
            "type": "sparse",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "id": "s_1147",
            "name": "Robert Turko",
            "type": "sparse",
            "x": 2.1530587673187256,
            "y": 8.92031478881836
        },
        {
            "id": "s_1148",
            "name": "Sven Teresniak",
            "type": "sparse",
            "x": 7.229828834533691,
            "y": 10.067737579345703
        },
        {
            "id": "s_1149",
            "name": "Matthew Cooper 0001",
            "type": "sparse",
            "x": 6.588368892669678,
            "y": 6.7118377685546875
        },
        {
            "id": "s_1150",
            "name": "Margit Pohl",
            "type": "sparse",
            "x": 6.805584907531738,
            "y": 8.293556213378906
        },
        {
            "id": "s_1151",
            "name": "Erez Zadok",
            "type": "sparse",
            "x": 3.9564976692199707,
            "y": 7.766167163848877
        },
        {
            "id": "s_1152",
            "name": "Mario Beric",
            "type": "sparse",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "id": "s_1153",
            "name": "Yong Wang 0021",
            "type": "sparse",
            "x": 5.916153907775879,
            "y": 8.637216567993164
        },
        {
            "id": "s_1154",
            "name": "Barry L. Drake",
            "type": "sparse",
            "x": 7.219537734985352,
            "y": 10.913814544677734
        },
        {
            "id": "s_1155",
            "name": "Yuki Asano 0003",
            "type": "sparse",
            "x": 3.595738649368286,
            "y": 5.5941081047058105
        },
        {
            "id": "s_1156",
            "name": "Olav Lenz",
            "type": "sparse",
            "x": 5.561883926391602,
            "y": 6.028954029083252
        },
        {
            "id": "s_1157",
            "name": "Jiaxin Shi",
            "type": "sparse",
            "x": 2.1071267127990723,
            "y": 8.984073638916016
        },
        {
            "id": "s_1158",
            "name": "David Van Riper",
            "type": "sparse",
            "x": 3.176222801208496,
            "y": 7.789238452911377
        },
        {
            "id": "s_1159",
            "name": "Jihoon Kim 0001",
            "type": "sparse",
            "x": 3.6053433418273926,
            "y": 7.882735729217529
        },
        {
            "id": "s_1160",
            "name": "Alexander Erath",
            "type": "sparse",
            "x": 7.451045989990234,
            "y": 4.985848426818848
        },
        {
            "id": "s_1161",
            "name": "Guizhen Wang",
            "type": "sparse",
            "x": 4.3522162437438965,
            "y": 6.534526348114014
        },
        {
            "id": "s_1162",
            "name": "Steffen Hadlak",
            "type": "sparse",
            "x": 5.256996154785156,
            "y": 5.111698627471924
        },
        {
            "id": "s_1163",
            "name": "Zuchao Wang",
            "type": "sparse",
            "x": 7.3276896476745605,
            "y": 5.110804557800293
        },
        {
            "id": "s_1164",
            "name": "Jing Xia",
            "type": "sparse",
            "x": 4.117305278778076,
            "y": 7.20753812789917
        },
        {
            "id": "s_1165",
            "name": "Kay Hamacher",
            "type": "sparse",
            "x": 4.464648723602295,
            "y": 6.500209808349609
        },
        {
            "id": "s_1166",
            "name": "Doris Dransch",
            "type": "sparse",
            "x": 6.648183345794678,
            "y": 6.226779937744141
        },
        {
            "id": "s_1167",
            "name": "Svenja Leifert",
            "type": "sparse",
            "x": 6.847413539886475,
            "y": 8.73234748840332
        },
        {
            "id": "s_1168",
            "name": "Benjamin Bach",
            "type": "sparse",
            "x": 4.52726936340332,
            "y": 6.233945369720459
        },
        {
            "id": "s_1169",
            "name": "Umeshwar Dayal",
            "type": "sparse",
            "x": 6.570365905761719,
            "y": 7.32098388671875
        },
        {
            "id": "s_1170",
            "name": "Jian Chen 0006",
            "type": "sparse",
            "x": 5.875744819641113,
            "y": 9.816665649414062
        },
        {
            "id": "s_1171",
            "name": "Christopher R. Schwalm",
            "type": "sparse",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "id": "s_1172",
            "name": "Eric SanJuan",
            "type": "sparse",
            "x": 7.896146297454834,
            "y": 9.091241836547852
        },
        {
            "id": "s_1173",
            "name": "Fan Du",
            "type": "sparse",
            "x": 6.541816711425781,
            "y": 7.441092014312744
        },
        {
            "id": "s_1174",
            "name": "Qi Han 0006",
            "type": "sparse",
            "x": 7.188792705535889,
            "y": 10.028514862060547
        },
        {
            "id": "s_1175",
            "name": "Simon Breslav",
            "type": "sparse",
            "x": 6.374663352966309,
            "y": 8.337908744812012
        },
        {
            "id": "s_1176",
            "name": "Hanjun Xian",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_1177",
            "name": "Adam Perer",
            "type": "sparse",
            "x": 4.815995693206787,
            "y": 7.822298526763916
        },
        {
            "id": "s_1178",
            "name": "Stephen C. North",
            "type": "sparse",
            "x": 4.932427406311035,
            "y": 9.227749824523926
        },
        {
            "id": "s_1179",
            "name": "Di Yang 0003",
            "type": "sparse",
            "x": 5.75692892074585,
            "y": 8.83036994934082
        },
        {
            "id": "s_1180",
            "name": "Jingrui He",
            "type": "sparse",
            "x": 2.2220568656921387,
            "y": 8.885293960571289
        },
        {
            "id": "s_1181",
            "name": "Bin Wang 0021",
            "type": "sparse",
            "x": 4.297730922698975,
            "y": 8.582598686218262
        },
        {
            "id": "s_1182",
            "name": "Karthik Ramani",
            "type": "sparse",
            "x": 7.460609436035156,
            "y": 5.697226047515869
        },
        {
            "id": "s_1183",
            "name": "Paola Valdivia",
            "type": "sparse",
            "x": 5.2648115158081055,
            "y": 5.848135471343994
        },
        {
            "id": "s_1184",
            "name": "Haris Mumtaz",
            "type": "sparse",
            "x": 7.38595724105835,
            "y": 10.323249816894531
        },
        {
            "id": "s_1185",
            "name": "Victor Y. Chen",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_1186",
            "name": "Jimmy Johansson 0001",
            "type": "sparse",
            "x": 6.588368892669678,
            "y": 6.7118377685546875
        },
        {
            "id": "s_1187",
            "name": "Richard Souvenir",
            "type": "sparse",
            "x": 4.95570182800293,
            "y": 10.061023712158203
        },
        {
            "id": "s_1188",
            "name": "Xingbo Wang 0001",
            "type": "sparse",
            "x": 5.177468776702881,
            "y": 9.842877388000488
        },
        {
            "id": "s_1189",
            "name": "Wolfgang Berger",
            "type": "sparse",
            "x": 3.634395122528076,
            "y": 7.800271511077881
        },
        {
            "id": "s_1190",
            "name": "Yuan Chen 0001",
            "type": "sparse",
            "x": 4.998837947845459,
            "y": 7.000543594360352
        },
        {
            "id": "s_1191",
            "name": "Niklas Elmqvist",
            "type": "sparse",
            "x": 6.007533073425293,
            "y": 8.271360397338867
        },
        {
            "id": "s_1192",
            "name": "Carolina Nobre",
            "type": "sparse",
            "x": 5.120394229888916,
            "y": 5.120257377624512
        },
        {
            "id": "s_1193",
            "name": "Martin Rohlig",
            "type": "sparse",
            "x": 5.326026916503906,
            "y": 7.089005470275879
        },
        {
            "id": "s_1194",
            "name": "David H. Laidlaw",
            "type": "sparse",
            "x": 5.2923150062561035,
            "y": 9.559711456298828
        },
        {
            "id": "s_1195",
            "name": "Gennady L. Andrienko",
            "type": "sparse",
            "x": 6.800786018371582,
            "y": 6.227477073669434
        },
        {
            "id": "s_1196",
            "name": "Hossam Sharara",
            "type": "sparse",
            "x": 3.6647329330444336,
            "y": 7.409862041473389
        },
        {
            "id": "s_1197",
            "name": "Louise Barrett",
            "type": "sparse",
            "x": 7.453543663024902,
            "y": 5.439891338348389
        },
        {
            "id": "s_1198",
            "name": "Hans Hagen",
            "type": "sparse",
            "x": 5.147592544555664,
            "y": 9.267566680908203
        },
        {
            "id": "s_1199",
            "name": "Baining Guo",
            "type": "sparse",
            "x": 7.467028617858887,
            "y": 8.608867645263672
        },
        {
            "id": "s_1200",
            "name": "Diem Tran",
            "type": "sparse",
            "x": 5.303769588470459,
            "y": 9.71528148651123
        },
        {
            "id": "s_1201",
            "name": "Seth Walker",
            "type": "sparse",
            "x": 7.511104106903076,
            "y": 6.730006217956543
        },
        {
            "id": "s_1202",
            "name": "Hans-J\u00f6rg Schulz",
            "type": "sparse",
            "x": 4.256865501403809,
            "y": 9.337084770202637
        },
        {
            "id": "s_1203",
            "name": "Shihan Wang 0001",
            "type": "sparse",
            "x": 6.628056526184082,
            "y": 6.06528377532959
        },
        {
            "id": "s_1204",
            "name": "Thomas H\u00f6llt",
            "type": "sparse",
            "x": 4.502536773681641,
            "y": 7.165432453155518
        },
        {
            "id": "s_1205",
            "name": "Georges G. Grinstein",
            "type": "sparse",
            "x": 6.792946815490723,
            "y": 8.76003646850586
        },
        {
            "id": "s_1206",
            "name": "Marcel Worring",
            "type": "sparse",
            "x": 5.198907852172852,
            "y": 7.574939250946045
        },
        {
            "id": "s_1207",
            "name": "Hans-Christian Hege",
            "type": "sparse",
            "x": 6.895112991333008,
            "y": 6.188204288482666
        },
        {
            "id": "s_1208",
            "name": "Maryam Siahbani",
            "type": "sparse",
            "x": 7.595698833465576,
            "y": 10.46611499786377
        },
        {
            "id": "s_1209",
            "name": "Fabian Sperrle",
            "type": "sparse",
            "x": 7.093414306640625,
            "y": 10.690457344055176
        },
        {
            "id": "s_1210",
            "name": "Drew Skau",
            "type": "sparse",
            "x": 7.922826290130615,
            "y": 7.827709674835205
        },
        {
            "id": "s_1211",
            "name": "Sean McCullough",
            "type": "sparse",
            "x": 6.677735328674316,
            "y": 6.570087909698486
        },
        {
            "id": "s_1212",
            "name": "Timothy W. Collins",
            "type": "sparse",
            "x": 6.643281936645508,
            "y": 7.454643249511719
        },
        {
            "id": "s_1213",
            "name": "Donghao Ren",
            "type": "sparse",
            "x": 3.4941341876983643,
            "y": 8.608490943908691
        },
        {
            "id": "s_1214",
            "name": "Bharath Kalidindi",
            "type": "sparse",
            "x": 4.688411235809326,
            "y": 8.034207344055176
        },
        {
            "id": "s_1215",
            "name": "Henryk Dobslaw",
            "type": "sparse",
            "x": 6.401253700256348,
            "y": 6.265356063842773
        },
        {
            "id": "s_1216",
            "name": "Monica Zappa",
            "type": "sparse",
            "x": 6.389932155609131,
            "y": 9.841611862182617
        },
        {
            "id": "s_1217",
            "name": "Ian Crandell",
            "type": "sparse",
            "x": 3.845111131668091,
            "y": 5.765905380249023
        },
        {
            "id": "s_1218",
            "name": "Thorsten May",
            "type": "sparse",
            "x": 5.003689289093018,
            "y": 7.81407356262207
        },
        {
            "id": "s_1219",
            "name": "Gromit Yeuk-Yin Chan",
            "type": "sparse",
            "x": 5.416169166564941,
            "y": 6.347220420837402
        },
        {
            "id": "s_1220",
            "name": "Thomas Kapler",
            "type": "sparse",
            "x": 7.177281379699707,
            "y": 7.967289924621582
        },
        {
            "id": "s_1221",
            "name": "Johanna Fulda",
            "type": "sparse",
            "x": 7.942614555358887,
            "y": 7.702369689941406
        },
        {
            "id": "s_1222",
            "name": "Mohammad Ghoniem",
            "type": "sparse",
            "x": 8.357303619384766,
            "y": 7.421786308288574
        },
        {
            "id": "s_1223",
            "name": "Lingyun Yu 0001",
            "type": "sparse",
            "x": 4.388597011566162,
            "y": 6.54218053817749
        },
        {
            "id": "s_1224",
            "name": "Michael Wade",
            "type": "sparse",
            "x": 6.5760884284973145,
            "y": 6.957261085510254
        },
        {
            "id": "s_1225",
            "name": "Joseph A. Cottam",
            "type": "sparse",
            "x": 4.939088344573975,
            "y": 8.239179611206055
        },
        {
            "id": "s_1226",
            "name": "Yu-Ru Lin",
            "type": "sparse",
            "x": 6.727012634277344,
            "y": 7.98590612411499
        },
        {
            "id": "s_1227",
            "name": "Changhong Zhang",
            "type": "sparse",
            "x": 6.394692420959473,
            "y": 10.036843299865723
        },
        {
            "id": "s_1228",
            "name": "Federico Rossi 0001",
            "type": "sparse",
            "x": 3.307056188583374,
            "y": 8.17342758178711
        },
        {
            "id": "s_1229",
            "name": "Yusi Wang",
            "type": "sparse",
            "x": 4.436132431030273,
            "y": 6.544544696807861
        },
        {
            "id": "s_1230",
            "name": "Yang Liu 0136",
            "type": "sparse",
            "x": 4.1113362312316895,
            "y": 8.209181785583496
        },
        {
            "id": "s_1231",
            "name": "Ren\u00e9 Pompl",
            "type": "sparse",
            "x": 4.062925338745117,
            "y": 9.458945274353027
        },
        {
            "id": "s_1232",
            "name": "Milos Sr\u00e1mek",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_1233",
            "name": "S. Peter Henzi",
            "type": "sparse",
            "x": 7.453543663024902,
            "y": 5.439891338348389
        },
        {
            "id": "s_1234",
            "name": "Lyubka Sharalieva",
            "type": "sparse",
            "x": 6.885515213012695,
            "y": 6.168759346008301
        },
        {
            "id": "s_1235",
            "name": "Bum Chul Kwon",
            "type": "sparse",
            "x": 4.570714473724365,
            "y": 8.211389541625977
        },
        {
            "id": "s_1236",
            "name": "Xiao Zhang 0019",
            "type": "sparse",
            "x": 7.839114665985107,
            "y": 8.217082977294922
        },
        {
            "id": "s_1237",
            "name": "Jishang Wei",
            "type": "sparse",
            "x": 7.071080684661865,
            "y": 8.232402801513672
        },
        {
            "id": "s_1238",
            "name": "Viswanath Aluru",
            "type": "sparse",
            "x": 5.91933536529541,
            "y": 6.946452617645264
        },
        {
            "id": "s_1239",
            "name": "Sungahn Ko",
            "type": "sparse",
            "x": 4.238665580749512,
            "y": 7.268342018127441
        },
        {
            "id": "s_1240",
            "name": "Roger Beecham",
            "type": "sparse",
            "x": 7.270965099334717,
            "y": 4.867156505584717
        },
        {
            "id": "s_1241",
            "name": "Zijie J. Wang",
            "type": "sparse",
            "x": 2.1530587673187256,
            "y": 8.92031478881836
        },
        {
            "id": "s_1242",
            "name": "Xiaoyan Kui",
            "type": "sparse",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "id": "s_1243",
            "name": "Miguel Nunes",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_1244",
            "name": "Jean Scholtz",
            "type": "sparse",
            "x": 6.476834774017334,
            "y": 9.069793701171875
        },
        {
            "id": "s_1245",
            "name": "Jonathan C. Roberts",
            "type": "sparse",
            "x": 7.865790843963623,
            "y": 8.0352144241333
        },
        {
            "id": "s_1246",
            "name": "Matthew Hurst",
            "type": "sparse",
            "x": 7.937456130981445,
            "y": 7.830206394195557
        },
        {
            "id": "s_1247",
            "name": "Kim Marriott",
            "type": "sparse",
            "x": 3.6086106300354004,
            "y": 7.845585823059082
        },
        {
            "id": "s_1248",
            "name": "Qinhan Liu",
            "type": "sparse",
            "x": 8.835519790649414,
            "y": 7.198888778686523
        },
        {
            "id": "s_1249",
            "name": "Zikun Deng",
            "type": "sparse",
            "x": 7.064511299133301,
            "y": 5.773886203765869
        },
        {
            "id": "s_1250",
            "name": "Shilpika",
            "type": "sparse",
            "x": 4.579966068267822,
            "y": 6.905692100524902
        },
        {
            "id": "s_1251",
            "name": "Hasan Davulcu",
            "type": "sparse",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "id": "s_1252",
            "name": "Leni Yang",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_1253",
            "name": "Arthur G. Telea",
            "type": "sparse",
            "x": 4.68471097946167,
            "y": 8.889886856079102
        },
        {
            "id": "s_1254",
            "name": "Yating Lin",
            "type": "sparse",
            "x": 8.882956504821777,
            "y": 7.1464362144470215
        },
        {
            "id": "s_1255",
            "name": "Chongxuan Li",
            "type": "sparse",
            "x": 2.0965170860290527,
            "y": 8.978882789611816
        },
        {
            "id": "s_1256",
            "name": "Dik Lun Lee",
            "type": "sparse",
            "x": 4.117317199707031,
            "y": 7.614367485046387
        },
        {
            "id": "s_1257",
            "name": "Long Wang",
            "type": "sparse",
            "x": 7.272230625152588,
            "y": 4.84622859954834
        },
        {
            "id": "s_1258",
            "name": "Ismail Demir",
            "type": "sparse",
            "x": 4.286999702453613,
            "y": 6.186981201171875
        },
        {
            "id": "s_1259",
            "name": "Takanori Fujiwara",
            "type": "sparse",
            "x": 4.5456929206848145,
            "y": 6.215700626373291
        },
        {
            "id": "s_1260",
            "name": "Ji Soo Yi",
            "type": "sparse",
            "x": 5.352029323577881,
            "y": 8.659893989562988
        },
        {
            "id": "s_1261",
            "name": "Lars-Erik Haug",
            "type": "sparse",
            "x": 7.942722320556641,
            "y": 8.773685455322266
        },
        {
            "id": "s_1262",
            "name": "Jie Liang 0004",
            "type": "sparse",
            "x": 7.50531005859375,
            "y": 6.379067897796631
        },
        {
            "id": "s_1263",
            "name": "Aaron Striegel",
            "type": "sparse",
            "x": 6.757778644561768,
            "y": 7.310772895812988
        },
        {
            "id": "s_1264",
            "name": "Yoonsoo Nam",
            "type": "sparse",
            "x": 3.6053433418273926,
            "y": 7.882735729217529
        },
        {
            "id": "s_1265",
            "name": "Kai Yan 0003",
            "type": "sparse",
            "x": 8.011079788208008,
            "y": 8.516590118408203
        },
        {
            "id": "s_1266",
            "name": "Melanie Tory",
            "type": "sparse",
            "x": 5.1936492919921875,
            "y": 9.793862342834473
        },
        {
            "id": "s_1267",
            "name": "Daniel A. Kern",
            "type": "sparse",
            "x": 8.852354049682617,
            "y": 7.137535572052002
        },
        {
            "id": "s_1268",
            "name": "Ahmad M. Abusalah",
            "type": "sparse",
            "x": 6.5760884284973145,
            "y": 6.957261085510254
        },
        {
            "id": "s_1269",
            "name": "Jimmy Lin",
            "type": "sparse",
            "x": 7.255488872528076,
            "y": 7.872509956359863
        },
        {
            "id": "s_1270",
            "name": "Emmy-Charlotte F\u00f6rster",
            "type": "sparse",
            "x": 3.6500320434570312,
            "y": 7.781797885894775
        },
        {
            "id": "s_1271",
            "name": "Sean Kandel",
            "type": "sparse",
            "x": 5.356631278991699,
            "y": 9.290544509887695
        },
        {
            "id": "s_1272",
            "name": "Andreas Lamprecht",
            "type": "sparse",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "id": "s_1273",
            "name": "Fernanda B. Vi\u00e9gas",
            "type": "sparse",
            "x": 4.436699390411377,
            "y": 8.30589485168457
        },
        {
            "id": "s_1274",
            "name": "Xiaoru Yuan",
            "type": "sparse",
            "x": 7.062631130218506,
            "y": 6.459317684173584
        },
        {
            "id": "s_1275",
            "name": "Michael Correll",
            "type": "sparse",
            "x": 6.427948951721191,
            "y": 6.106138706207275
        },
        {
            "id": "s_1276",
            "name": "Venkatram Vishwanath",
            "type": "sparse",
            "x": 4.80540657043457,
            "y": 6.956305503845215
        },
        {
            "id": "s_1277",
            "name": "Dazhen Deng",
            "type": "sparse",
            "x": 8.530769348144531,
            "y": 5.983466625213623
        },
        {
            "id": "s_1278",
            "name": "Cesar Palomo",
            "type": "sparse",
            "x": 7.280351638793945,
            "y": 4.832824230194092
        },
        {
            "id": "s_1279",
            "name": "Oliver Deussen",
            "type": "sparse",
            "x": 7.379476547241211,
            "y": 9.302892684936523
        },
        {
            "id": "s_1280",
            "name": "Tanja Blascheck",
            "type": "sparse",
            "x": 5.676453590393066,
            "y": 9.598196029663086
        },
        {
            "id": "s_1281",
            "name": "Anna Tikhonova",
            "type": "sparse",
            "x": 6.836374282836914,
            "y": 7.674856662750244
        },
        {
            "id": "s_1282",
            "name": "Kristanto Sean Njotoprawiro",
            "type": "sparse",
            "x": 4.743052959442139,
            "y": 5.725571155548096
        },
        {
            "id": "s_1283",
            "name": "Siyuan Liu 0001",
            "type": "sparse",
            "x": 7.3848042488098145,
            "y": 4.858085632324219
        },
        {
            "id": "s_1284",
            "name": "Roque Lopez",
            "type": "sparse",
            "x": 3.2641701698303223,
            "y": 8.662834167480469
        },
        {
            "id": "s_1285",
            "name": "Benjamin B. Bederson",
            "type": "sparse",
            "x": 5.051174640655518,
            "y": 5.1694254875183105
        },
        {
            "id": "s_1286",
            "name": "Xinzhu Mu",
            "type": "sparse",
            "x": 5.147846698760986,
            "y": 5.0413899421691895
        },
        {
            "id": "s_1287",
            "name": "Antonios Somarakis",
            "type": "sparse",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "id": "s_1288",
            "name": "Hongyuan Zha",
            "type": "sparse",
            "x": 7.614363670349121,
            "y": 6.788825035095215
        },
        {
            "id": "s_1289",
            "name": "Junlin Liu",
            "type": "sparse",
            "x": 3.3129467964172363,
            "y": 7.173401355743408
        },
        {
            "id": "s_1290",
            "name": "Quan Hoang Nguyen 0001",
            "type": "sparse",
            "x": 5.461853981018066,
            "y": 5.2681779861450195
        },
        {
            "id": "s_1291",
            "name": "Junqi Wu",
            "type": "sparse",
            "x": 8.89720630645752,
            "y": 7.1770339012146
        },
        {
            "id": "s_1292",
            "name": "Amy K. Karlson",
            "type": "sparse",
            "x": 7.580902099609375,
            "y": 6.7360711097717285
        },
        {
            "id": "s_1293",
            "name": "Shah Rukh Humayoun",
            "type": "sparse",
            "x": 5.659013748168945,
            "y": 8.697281837463379
        },
        {
            "id": "s_1294",
            "name": "Felesia Stukes",
            "type": "sparse",
            "x": 5.377330303192139,
            "y": 9.783858299255371
        },
        {
            "id": "s_1295",
            "name": "Kamkwai Wong",
            "type": "sparse",
            "x": 8.882956504821777,
            "y": 7.1464362144470215
        },
        {
            "id": "s_1296",
            "name": "Samira Shaikh",
            "type": "sparse",
            "x": 4.635231018066406,
            "y": 9.437541007995605
        },
        {
            "id": "s_1297",
            "name": "Yi Han 0005",
            "type": "sparse",
            "x": 6.5693864822387695,
            "y": 7.4537224769592285
        },
        {
            "id": "s_1298",
            "name": "Christopher Mears",
            "type": "sparse",
            "x": 3.7070932388305664,
            "y": 7.860748291015625
        },
        {
            "id": "s_1299",
            "name": "Shuirun Wei",
            "type": "sparse",
            "x": 8.83371639251709,
            "y": 7.131134986877441
        },
        {
            "id": "s_1300",
            "name": "Stef van den Elzen",
            "type": "sparse",
            "x": 4.463833808898926,
            "y": 6.38590145111084
        },
        {
            "id": "s_1301",
            "name": "Jiun-Yi Tsai",
            "type": "sparse",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "id": "s_1302",
            "name": "Rahul Kanna",
            "type": "sparse",
            "x": 5.334270477294922,
            "y": 10.654705047607422
        },
        {
            "id": "s_1303",
            "name": "Stefan Koch",
            "type": "sparse",
            "x": 6.847413539886475,
            "y": 8.73234748840332
        },
        {
            "id": "s_1304",
            "name": "Michael W\u00f6rner 0001",
            "type": "sparse",
            "x": 7.4510698318481445,
            "y": 8.779691696166992
        },
        {
            "id": "s_1305",
            "name": "Erdem Kaya",
            "type": "sparse",
            "x": 5.311036586761475,
            "y": 9.03400707244873
        },
        {
            "id": "s_1306",
            "name": "Qiuhan Zhu",
            "type": "sparse",
            "x": 7.012173652648926,
            "y": 7.094387531280518
        },
        {
            "id": "s_1307",
            "name": "Andrada Tatu",
            "type": "sparse",
            "x": 4.619335651397705,
            "y": 6.651921272277832
        },
        {
            "id": "s_1308",
            "name": "Marcel van 't Veer",
            "type": "sparse",
            "x": 3.5966358184814453,
            "y": 8.431370735168457
        },
        {
            "id": "s_1309",
            "name": "Christian Scheible",
            "type": "sparse",
            "x": 7.373436450958252,
            "y": 8.217280387878418
        },
        {
            "id": "s_1310",
            "name": "Po-Ming Law",
            "type": "sparse",
            "x": 5.5305891036987305,
            "y": 7.475362777709961
        },
        {
            "id": "s_1311",
            "name": "Matthias Buchetics",
            "type": "sparse",
            "x": 5.862776756286621,
            "y": 6.794144630432129
        },
        {
            "id": "s_1312",
            "name": "Mario Jelovic",
            "type": "sparse",
            "x": 3.277205467224121,
            "y": 7.747622489929199
        },
        {
            "id": "s_1313",
            "name": "Johannes Knittel",
            "type": "sparse",
            "x": 4.120519161224365,
            "y": 7.768464088439941
        },
        {
            "id": "s_1314",
            "name": "Matthew E. Hawkins",
            "type": "sparse",
            "x": 5.596868991851807,
            "y": 9.598109245300293
        },
        {
            "id": "s_1315",
            "name": "Gerhard Heyer",
            "type": "sparse",
            "x": 7.229828834533691,
            "y": 10.067737579345703
        },
        {
            "id": "s_1316",
            "name": "Veronika Irvine",
            "type": "sparse",
            "x": 4.042430877685547,
            "y": 7.574769496917725
        },
        {
            "id": "s_1317",
            "name": "Yahui Zhao",
            "type": "sparse",
            "x": 5.694990158081055,
            "y": 8.214183807373047
        },
        {
            "id": "s_1318",
            "name": "Nicholas Diakopoulos",
            "type": "sparse",
            "x": 7.762973785400391,
            "y": 9.322210311889648
        },
        {
            "id": "s_1319",
            "name": "Wei Chen 0001",
            "type": "sparse",
            "x": 6.194931983947754,
            "y": 6.608351230621338
        },
        {
            "id": "s_1320",
            "name": "Jeffrey Heer",
            "type": "sparse",
            "x": 5.019742965698242,
            "y": 8.329595565795898
        },
        {
            "id": "s_1321",
            "name": "Shimei Pan",
            "type": "sparse",
            "x": 7.730375289916992,
            "y": 8.678378105163574
        },
        {
            "id": "s_1322",
            "name": "Xinyue Ye",
            "type": "sparse",
            "x": 7.348180294036865,
            "y": 4.885310173034668
        },
        {
            "id": "s_1323",
            "name": "Nan Chen",
            "type": "sparse",
            "x": 7.734045028686523,
            "y": 7.463884353637695
        },
        {
            "id": "s_1324",
            "name": "Alexander Kachkaev",
            "type": "sparse",
            "x": 5.428104877471924,
            "y": 7.266650199890137
        },
        {
            "id": "s_1325",
            "name": "Hong Wang",
            "type": "sparse",
            "x": 8.27431869506836,
            "y": 7.59355354309082
        },
        {
            "id": "s_1326",
            "name": "Svitlana Volkova",
            "type": "sparse",
            "x": 7.89020299911499,
            "y": 7.764756679534912
        },
        {
            "id": "s_1327",
            "name": "Kay Nieselt",
            "type": "sparse",
            "x": 4.756052494049072,
            "y": 6.701610088348389
        },
        {
            "id": "s_1328",
            "name": "Yiwen Sun",
            "type": "sparse",
            "x": 5.082072734832764,
            "y": 9.60634708404541
        },
        {
            "id": "s_1329",
            "name": "Cong Xie",
            "type": "sparse",
            "x": 6.557981967926025,
            "y": 7.647650718688965
        },
        {
            "id": "s_1330",
            "name": "David Schroh",
            "type": "sparse",
            "x": 6.7051167488098145,
            "y": 8.124053955078125
        },
        {
            "id": "s_1331",
            "name": "Michael Stryker",
            "type": "sparse",
            "x": 6.881535053253174,
            "y": 8.784066200256348
        },
        {
            "id": "s_1332",
            "name": "Peng Xu",
            "type": "sparse",
            "x": 8.506478309631348,
            "y": 6.033137321472168
        },
        {
            "id": "s_1333",
            "name": "Cong Guo 0004",
            "type": "sparse",
            "x": 7.425845623016357,
            "y": 5.407773494720459
        },
        {
            "id": "s_1334",
            "name": "Lin Shao 0001",
            "type": "sparse",
            "x": 5.032182216644287,
            "y": 8.1648588180542
        },
        {
            "id": "s_1335",
            "name": "Nam Wook Kim",
            "type": "sparse",
            "x": 5.782403945922852,
            "y": 6.731578350067139
        },
        {
            "id": "s_1336",
            "name": "Andreas Stoffel",
            "type": "sparse",
            "x": 5.424988269805908,
            "y": 9.732829093933105
        },
        {
            "id": "s_1337",
            "name": "Christian Rohrdantz",
            "type": "sparse",
            "x": 7.526573657989502,
            "y": 8.624250411987305
        },
        {
            "id": "s_1338",
            "name": "Acar Tamersoy",
            "type": "sparse",
            "x": 4.909725189208984,
            "y": 5.396003723144531
        },
        {
            "id": "s_1339",
            "name": "Maria Garcia de la Banda",
            "type": "sparse",
            "x": 3.7070932388305664,
            "y": 7.860748291015625
        },
        {
            "id": "s_1340",
            "name": "Donald A. Pellegrino",
            "type": "sparse",
            "x": 6.881535053253174,
            "y": 8.784066200256348
        },
        {
            "id": "s_1341",
            "name": "Aida Nordman",
            "type": "sparse",
            "x": 6.459039211273193,
            "y": 7.370312690734863
        },
        {
            "id": "s_1342",
            "name": "Ian Turton",
            "type": "sparse",
            "x": 7.10892391204834,
            "y": 9.600622177124023
        },
        {
            "id": "s_1343",
            "name": "Justine I. Blanford",
            "type": "sparse",
            "x": 7.839114665985107,
            "y": 8.217082977294922
        },
        {
            "id": "s_1344",
            "name": "Fang-Xin Ou-Yang",
            "type": "sparse",
            "x": 4.297730922698975,
            "y": 8.582598686218262
        },
        {
            "id": "s_1345",
            "name": "William Alexander",
            "type": "sparse",
            "x": 4.543193817138672,
            "y": 9.233100891113281
        },
        {
            "id": "s_1346",
            "name": "Christoph Granacher",
            "type": "sparse",
            "x": 6.847413539886475,
            "y": 8.73234748840332
        },
        {
            "id": "s_1347",
            "name": "Yingcai Wu",
            "type": "sparse",
            "x": 7.467475891113281,
            "y": 7.116647243499756
        },
        {
            "id": "s_1348",
            "name": "Marieke E. Ijsselsteijn",
            "type": "sparse",
            "x": 5.849886417388916,
            "y": 6.928454399108887
        },
        {
            "id": "s_1349",
            "name": "George G. Robertson",
            "type": "sparse",
            "x": 7.937456130981445,
            "y": 7.830206394195557
        },
        {
            "id": "s_1350",
            "name": "Akhilesh Camisetty",
            "type": "sparse",
            "x": 5.957756519317627,
            "y": 9.685969352722168
        },
        {
            "id": "s_1351",
            "name": "Ching-Shan Chou",
            "type": "sparse",
            "x": 3.0904009342193604,
            "y": 7.828068256378174
        },
        {
            "id": "s_1352",
            "name": "Shaozu Cao",
            "type": "sparse",
            "x": 2.921347141265869,
            "y": 8.863079071044922
        },
        {
            "id": "s_1353",
            "name": "Zoltan Konyha",
            "type": "sparse",
            "x": 7.173929214477539,
            "y": 6.49702787399292
        },
        {
            "id": "s_1354",
            "name": "Jian Zhao 0010",
            "type": "sparse",
            "x": 5.903557300567627,
            "y": 7.950582504272461
        },
        {
            "id": "s_1355",
            "name": "Mikko Nikkil\u00e4",
            "type": "sparse",
            "x": 3.7874088287353516,
            "y": 6.028797149658203
        },
        {
            "id": "s_1356",
            "name": "Aritra Dasgupta",
            "type": "sparse",
            "x": 4.1265482902526855,
            "y": 8.069951057434082
        },
        {
            "id": "s_1357",
            "name": "Ying Zhao 0001",
            "type": "sparse",
            "x": 5.33455228805542,
            "y": 6.6505537033081055
        },
        {
            "id": "s_1358",
            "name": "Anthony K. H. Tung",
            "type": "sparse",
            "x": 4.436132431030273,
            "y": 6.544544696807861
        },
        {
            "id": "s_1359",
            "name": "Angela K. Mellema",
            "type": "sparse",
            "x": 6.643281936645508,
            "y": 7.454643249511719
        },
        {
            "id": "s_1360",
            "name": "Jamie L. Dyer",
            "type": "sparse",
            "x": 6.29480504989624,
            "y": 6.361831188201904
        },
        {
            "id": "s_1361",
            "name": "Hyunwoo Park",
            "type": "sparse",
            "x": 5.053277969360352,
            "y": 5.204998970031738
        },
        {
            "id": "s_1362",
            "name": "Andreas Paepcke",
            "type": "sparse",
            "x": 5.356631278991699,
            "y": 9.290544509887695
        },
        {
            "id": "s_1363",
            "name": "Fabiano Petronetto",
            "type": "sparse",
            "x": 6.9307403564453125,
            "y": 6.105090141296387
        },
        {
            "id": "s_1364",
            "name": "He Liu",
            "type": "sparse",
            "x": 7.3848042488098145,
            "y": 4.858085632324219
        },
        {
            "id": "s_1365",
            "name": "Andy Bardill",
            "type": "sparse",
            "x": 5.82072114944458,
            "y": 10.373441696166992
        },
        {
            "id": "s_1366",
            "name": "Wei Zeng 0004",
            "type": "sparse",
            "x": 6.3921284675598145,
            "y": 5.443688869476318
        },
        {
            "id": "s_1367",
            "name": "Krishna P. C. Madhavan",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_1368",
            "name": "Xiaoyu Wang 0001",
            "type": "sparse",
            "x": 6.139523983001709,
            "y": 9.377387046813965
        },
        {
            "id": "s_1369",
            "name": "Minjeong Kim",
            "type": "sparse",
            "x": 7.177179336547852,
            "y": 10.938121795654297
        },
        {
            "id": "s_1370",
            "name": "Audra Buck-Coleman",
            "type": "sparse",
            "x": 3.320497751235962,
            "y": 7.061007022857666
        },
        {
            "id": "s_1371",
            "name": "Gary K. L. Tam",
            "type": "sparse",
            "x": 4.601602077484131,
            "y": 9.161087036132812
        },
        {
            "id": "s_1372",
            "name": "Slava Kisilevich",
            "type": "sparse",
            "x": 6.054115295410156,
            "y": 6.42355489730835
        },
        {
            "id": "s_1373",
            "name": "Mingqian Zhao",
            "type": "sparse",
            "x": 3.8067502975463867,
            "y": 8.83480167388916
        },
        {
            "id": "s_1374",
            "name": "Eren Cakmak",
            "type": "sparse",
            "x": 6.27765417098999,
            "y": 5.361934661865234
        },
        {
            "id": "s_1375",
            "name": "Markus Jakobsson",
            "type": "sparse",
            "x": 6.941458702087402,
            "y": 8.855831146240234
        },
        {
            "id": "s_1376",
            "name": "Chris Rooney",
            "type": "sparse",
            "x": 5.564159870147705,
            "y": 10.273418426513672
        },
        {
            "id": "s_1377",
            "name": "Ali Sarvghad",
            "type": "sparse",
            "x": 5.485997676849365,
            "y": 9.772184371948242
        },
        {
            "id": "s_1378",
            "name": "Zhen Cao",
            "type": "sparse",
            "x": 3.9564976692199707,
            "y": 7.766167163848877
        },
        {
            "id": "s_1379",
            "name": "Kaixin Chen 0004",
            "type": "sparse",
            "x": 2.099738597869873,
            "y": 8.981844902038574
        },
        {
            "id": "s_1380",
            "name": "Zexian Chen",
            "type": "sparse",
            "x": 6.856532096862793,
            "y": 6.264466762542725
        },
        {
            "id": "s_1381",
            "name": "Fei Wang 0001",
            "type": "sparse",
            "x": 7.794764995574951,
            "y": 8.924461364746094
        },
        {
            "id": "s_1382",
            "name": "Derek W. S. Gray",
            "type": "sparse",
            "x": 7.877171039581299,
            "y": 7.68709135055542
        },
        {
            "id": "s_1383",
            "name": "Kai Kang",
            "type": "sparse",
            "x": 5.673964023590088,
            "y": 6.457597255706787
        },
        {
            "id": "s_1384",
            "name": "Zhenyu Guo",
            "type": "sparse",
            "x": 4.669299602508545,
            "y": 6.794257164001465
        },
        {
            "id": "s_1385",
            "name": "Wen Zhong",
            "type": "sparse",
            "x": 4.2564167976379395,
            "y": 7.324117660522461
        },
        {
            "id": "s_1386",
            "name": "Keiji Yamamoto",
            "type": "sparse",
            "x": 4.579966068267822,
            "y": 6.905692100524902
        },
        {
            "id": "s_1387",
            "name": "Eric Lee",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_1388",
            "name": "J\u00fcrgen Waser",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_1389",
            "name": "Michael Grossniklaus",
            "type": "sparse",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "id": "s_1390",
            "name": "Martin Hess",
            "type": "sparse",
            "x": 3.3674137592315674,
            "y": 6.971466064453125
        },
        {
            "id": "s_1391",
            "name": "Markus John",
            "type": "sparse",
            "x": 6.71522855758667,
            "y": 10.009221076965332
        },
        {
            "id": "s_1392",
            "name": "Allan Hanbury",
            "type": "sparse",
            "x": 4.1527791023254395,
            "y": 8.461838722229004
        },
        {
            "id": "s_1393",
            "name": "Liu Ren",
            "type": "sparse",
            "x": 4.341952800750732,
            "y": 7.714935779571533
        },
        {
            "id": "s_1394",
            "name": "Morteza Karimzadeh",
            "type": "sparse",
            "x": 5.130163669586182,
            "y": 8.427286148071289
        },
        {
            "id": "s_1395",
            "name": "Jorik Blaas",
            "type": "sparse",
            "x": 5.216991424560547,
            "y": 5.118741035461426
        },
        {
            "id": "s_1396",
            "name": "Saleema Amershi",
            "type": "sparse",
            "x": 5.439614772796631,
            "y": 9.420108795166016
        },
        {
            "id": "s_1397",
            "name": "Bilal Alsallakh",
            "type": "sparse",
            "x": 4.38133430480957,
            "y": 7.740362644195557
        },
        {
            "id": "s_1398",
            "name": "Dongxing Teng",
            "type": "sparse",
            "x": 5.061010360717773,
            "y": 7.822322845458984
        },
        {
            "id": "s_1399",
            "name": "Catherine Plaisant",
            "type": "sparse",
            "x": 6.0211334228515625,
            "y": 7.745375156402588
        },
        {
            "id": "s_1400",
            "name": "Michael Burch",
            "type": "sparse",
            "x": 6.037551403045654,
            "y": 7.78853702545166
        },
        {
            "id": "s_1401",
            "name": "Chi-Wing Fu",
            "type": "sparse",
            "x": 7.451045989990234,
            "y": 4.985848426818848
        },
        {
            "id": "s_1402",
            "name": "Johannes H\u00e4u\u00dfler",
            "type": "sparse",
            "x": 8.476827621459961,
            "y": 6.0509443283081055
        },
        {
            "id": "s_1403",
            "name": "James J. Thomas",
            "type": "sparse",
            "x": 5.9488301277160645,
            "y": 8.112038612365723
        },
        {
            "id": "s_1404",
            "name": "Thorsten Breitkreutz",
            "type": "sparse",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "id": "s_1405",
            "name": "Xiaolong (Luke) Zhang",
            "type": "sparse",
            "x": 4.980503082275391,
            "y": 5.211485862731934
        },
        {
            "id": "s_1406",
            "name": "Junping Zhang",
            "type": "sparse",
            "x": 7.39121150970459,
            "y": 4.8653717041015625
        },
        {
            "id": "s_1407",
            "name": "Brian C. McCann",
            "type": "sparse",
            "x": 4.857062816619873,
            "y": 9.41756534576416
        },
        {
            "id": "s_1408",
            "name": "Yingchao Wang",
            "type": "sparse",
            "x": 3.5461413860321045,
            "y": 5.572312831878662
        },
        {
            "id": "s_1409",
            "name": "Hanspeter Pfister",
            "type": "sparse",
            "x": 4.251657009124756,
            "y": 7.725236415863037
        },
        {
            "id": "s_1410",
            "name": "Kevin Ho",
            "type": "sparse",
            "x": 5.755815505981445,
            "y": 9.16723918914795
        },
        {
            "id": "s_1411",
            "name": "Aidan Slingsby",
            "type": "sparse",
            "x": 6.227529525756836,
            "y": 9.073579788208008
        },
        {
            "id": "s_1412",
            "name": "Douglas C. Montgomery",
            "type": "sparse",
            "x": 7.873596668243408,
            "y": 7.890711307525635
        },
        {
            "id": "s_1413",
            "name": "Yongsu Ahn",
            "type": "sparse",
            "x": 4.636303901672363,
            "y": 8.653554916381836
        },
        {
            "id": "s_1414",
            "name": "Honghui Mei",
            "type": "sparse",
            "x": 4.809410572052002,
            "y": 7.073798656463623
        },
        {
            "id": "s_1415",
            "name": "Wei Liu 0023",
            "type": "sparse",
            "x": 5.462973117828369,
            "y": 7.948393821716309
        },
        {
            "id": "s_1416",
            "name": "Philipp Zimmermann",
            "type": "sparse",
            "x": 8.488612174987793,
            "y": 6.029453277587891
        },
        {
            "id": "s_1417",
            "name": "Soonwook Kwon",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_1418",
            "name": "Conglei Shi",
            "type": "sparse",
            "x": 7.571868896484375,
            "y": 8.299695014953613
        },
        {
            "id": "s_1419",
            "name": "Robert F. Woodbury",
            "type": "sparse",
            "x": 6.037326335906982,
            "y": 9.376370429992676
        },
        {
            "id": "s_1420",
            "name": "Michael Marschollek",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_1421",
            "name": "Edward Choi",
            "type": "sparse",
            "x": 2.8849658966064453,
            "y": 8.842034339904785
        },
        {
            "id": "s_1422",
            "name": "Yao Ming",
            "type": "sparse",
            "x": 3.090121269226074,
            "y": 8.787849426269531
        },
        {
            "id": "s_1423",
            "name": "Haeyong Chung",
            "type": "sparse",
            "x": 5.778360843658447,
            "y": 10.223125457763672
        },
        {
            "id": "s_1424",
            "name": "Bob Fields",
            "type": "sparse",
            "x": 5.667359352111816,
            "y": 10.38341236114502
        },
        {
            "id": "s_1425",
            "name": "Dieter Schmalstieg",
            "type": "sparse",
            "x": 5.349819660186768,
            "y": 6.387389659881592
        },
        {
            "id": "s_1426",
            "name": "Si Qin",
            "type": "sparse",
            "x": 7.062619209289551,
            "y": 7.442115306854248
        },
        {
            "id": "s_1427",
            "name": "Xinhuan Shu",
            "type": "sparse",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "id": "s_1428",
            "name": "Miriah Meyer",
            "type": "sparse",
            "x": 5.120394229888916,
            "y": 5.120257377624512
        },
        {
            "id": "s_1429",
            "name": "Joel W. Reed",
            "type": "sparse",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        },
        {
            "id": "s_1430",
            "name": "Weifeng Chen 0002",
            "type": "sparse",
            "x": 4.454087257385254,
            "y": 6.540931701660156
        },
        {
            "id": "s_1431",
            "name": "Rebecca Kehlbeck",
            "type": "sparse",
            "x": 7.114018440246582,
            "y": 10.58177375793457
        },
        {
            "id": "s_1432",
            "name": "Bryan Heer",
            "type": "sparse",
            "x": 5.738348960876465,
            "y": 8.632399559020996
        },
        {
            "id": "s_1433",
            "name": "Jing Wu 0004",
            "type": "sparse",
            "x": 3.9312193393707275,
            "y": 7.618178367614746
        },
        {
            "id": "s_1434",
            "name": "Philip A. Legg",
            "type": "sparse",
            "x": 5.184502601623535,
            "y": 8.295247077941895
        },
        {
            "id": "s_1435",
            "name": "Mourad Ouzzani",
            "type": "sparse",
            "x": 6.470664978027344,
            "y": 6.918655872344971
        },
        {
            "id": "s_1436",
            "name": "Hammad Haleem",
            "type": "sparse",
            "x": 4.743052959442139,
            "y": 5.725571155548096
        },
        {
            "id": "s_1437",
            "name": "Farah Kamw",
            "type": "sparse",
            "x": 7.330615997314453,
            "y": 4.847436428070068
        },
        {
            "id": "s_1438",
            "name": "Mahmud Shahriar Hossain",
            "type": "sparse",
            "x": 4.539749622344971,
            "y": 7.945067405700684
        },
        {
            "id": "s_1439",
            "name": "Gabriel Mistelbauer",
            "type": "sparse",
            "x": 5.850381851196289,
            "y": 8.935423851013184
        },
        {
            "id": "s_1440",
            "name": "Sixiao Yang",
            "type": "sparse",
            "x": 5.1919989585876465,
            "y": 5.070548057556152
        },
        {
            "id": "s_1441",
            "name": "Kyeongpil Kang",
            "type": "sparse",
            "x": 7.177179336547852,
            "y": 10.938121795654297
        },
        {
            "id": "s_1442",
            "name": "Peter Filzmoser",
            "type": "sparse",
            "x": 3.8577301502227783,
            "y": 7.942910671234131
        },
        {
            "id": "s_1443",
            "name": "Marc Rautenhaus",
            "type": "sparse",
            "x": 4.963197708129883,
            "y": 5.9895172119140625
        },
        {
            "id": "s_1444",
            "name": "Xumeng Wang",
            "type": "sparse",
            "x": 6.856532096862793,
            "y": 6.264466762542725
        },
        {
            "id": "s_1445",
            "name": "Hongan Wang",
            "type": "sparse",
            "x": 5.061010360717773,
            "y": 7.822322845458984
        },
        {
            "id": "s_1446",
            "name": "Mark Giereth",
            "type": "sparse",
            "x": 5.787541389465332,
            "y": 7.80381965637207
        },
        {
            "id": "s_1447",
            "name": "Thomas Zichner",
            "type": "sparse",
            "x": 6.838124752044678,
            "y": 9.806111335754395
        },
        {
            "id": "s_1448",
            "name": "Nigel W. John",
            "type": "sparse",
            "x": 4.857062816619873,
            "y": 9.41756534576416
        },
        {
            "id": "s_1449",
            "name": "Chao Ma 0023",
            "type": "sparse",
            "x": 7.348180294036865,
            "y": 4.885310173034668
        },
        {
            "id": "s_1450",
            "name": "Kate Herd",
            "type": "sparse",
            "x": 5.82072114944458,
            "y": 10.373441696166992
        },
        {
            "id": "s_1451",
            "name": "Robert B. Cook",
            "type": "sparse",
            "x": 4.535919189453125,
            "y": 5.940088272094727
        },
        {
            "id": "s_1452",
            "name": "Ryan Hafen",
            "type": "sparse",
            "x": 6.5760884284973145,
            "y": 6.957261085510254
        },
        {
            "id": "s_1453",
            "name": "Tyler Estro",
            "type": "sparse",
            "x": 3.9564976692199707,
            "y": 7.766167163848877
        },
        {
            "id": "s_1454",
            "name": "Fenjin Ye",
            "type": "sparse",
            "x": 4.436132431030273,
            "y": 6.544544696807861
        },
        {
            "id": "s_1455",
            "name": "Bertjan Broeksema",
            "type": "sparse",
            "x": 4.68471097946167,
            "y": 8.889886856079102
        },
        {
            "id": "s_1456",
            "name": "Maria Tekusova",
            "type": "sparse",
            "x": 7.034478664398193,
            "y": 6.645401477813721
        },
        {
            "id": "s_1457",
            "name": "Andrew Lumsdaine",
            "type": "sparse",
            "x": 4.939088344573975,
            "y": 8.239179611206055
        },
        {
            "id": "s_1458",
            "name": "Alina Gvozdik",
            "type": "sparse",
            "x": 5.903914451599121,
            "y": 6.849249362945557
        },
        {
            "id": "s_1459",
            "name": "Jaeyeon Kihm",
            "type": "sparse",
            "x": 4.266314506530762,
            "y": 6.490163803100586
        },
        {
            "id": "s_1460",
            "name": "M. Lieser",
            "type": "sparse",
            "x": 6.441487789154053,
            "y": 6.953915119171143
        },
        {
            "id": "s_1461",
            "name": "Naren Ramakrishnan",
            "type": "sparse",
            "x": 5.58229923248291,
            "y": 7.130437850952148
        },
        {
            "id": "s_1462",
            "name": "Shenghui Cheng",
            "type": "sparse",
            "x": 4.72398567199707,
            "y": 6.792348861694336
        },
        {
            "id": "s_1463",
            "name": "Steven Bergner",
            "type": "sparse",
            "x": 4.042430877685547,
            "y": 7.574769496917725
        },
        {
            "id": "s_1464",
            "name": "Jin Wen",
            "type": "sparse",
            "x": 4.379642963409424,
            "y": 6.61838960647583
        },
        {
            "id": "s_1465",
            "name": "Minsuk Kahng",
            "type": "sparse",
            "x": 3.6258270740509033,
            "y": 8.438037872314453
        },
        {
            "id": "s_1466",
            "name": "Zsolt Horv\u00e1th",
            "type": "sparse",
            "x": 3.386897087097168,
            "y": 7.8124680519104
        },
        {
            "id": "s_1467",
            "name": "Soo-Yeon Ji",
            "type": "sparse",
            "x": 5.313672065734863,
            "y": 10.065361976623535
        },
        {
            "id": "s_1468",
            "name": "Patricia Enns",
            "type": "sparse",
            "x": 6.660202503204346,
            "y": 8.713521003723145
        },
        {
            "id": "s_1469",
            "name": "David M. Mountain",
            "type": "sparse",
            "x": 5.795925617218018,
            "y": 8.556072235107422
        },
        {
            "id": "s_1470",
            "name": "Xiaojuan Ma",
            "type": "sparse",
            "x": 6.154956340789795,
            "y": 7.335413455963135
        },
        {
            "id": "s_1471",
            "name": "Min Lu 0002",
            "type": "sparse",
            "x": 7.4169840812683105,
            "y": 4.965725421905518
        },
        {
            "id": "s_1472",
            "name": "Jamie Morgenstern",
            "type": "sparse",
            "x": 4.593509197235107,
            "y": 8.633565902709961
        },
        {
            "id": "s_1473",
            "name": "Paolo Buono",
            "type": "sparse",
            "x": 5.235508918762207,
            "y": 6.69278621673584
        },
        {
            "id": "s_1474",
            "name": "Qiaomu Shen",
            "type": "sparse",
            "x": 7.16457462310791,
            "y": 9.706252098083496
        },
        {
            "id": "s_1475",
            "name": "Avital Steinitz",
            "type": "sparse",
            "x": 5.919469833374023,
            "y": 10.14952278137207
        },
        {
            "id": "s_1476",
            "name": "Devin Singh",
            "type": "sparse",
            "x": 7.287152290344238,
            "y": 10.204761505126953
        },
        {
            "id": "s_1477",
            "name": "Lichan Hong",
            "type": "sparse",
            "x": 7.2240824699401855,
            "y": 10.312528610229492
        },
        {
            "id": "s_1478",
            "name": "Jimbo Wilson",
            "type": "sparse",
            "x": 4.0586161613464355,
            "y": 7.076250076293945
        },
        {
            "id": "s_1479",
            "name": "Carolina Ruiz",
            "type": "sparse",
            "x": 4.514427661895752,
            "y": 6.832135200500488
        },
        {
            "id": "s_1480",
            "name": "Katja B\u00fchler",
            "type": "sparse",
            "x": 5.858467102050781,
            "y": 6.768400192260742
        },
        {
            "id": "s_1481",
            "name": "Anbang Xu",
            "type": "sparse",
            "x": 7.857691764831543,
            "y": 8.961512565612793
        },
        {
            "id": "s_1482",
            "name": "Moshe Gutman",
            "type": "sparse",
            "x": 6.389932155609131,
            "y": 9.841611862182617
        },
        {
            "id": "s_1483",
            "name": "Michael Wolverton",
            "type": "sparse",
            "x": 5.411427021026611,
            "y": 9.972384452819824
        },
        {
            "id": "s_1484",
            "name": "Torsten M\u00f6ller",
            "type": "sparse",
            "x": 3.6731510162353516,
            "y": 7.380860328674316
        },
        {
            "id": "s_1485",
            "name": "Axel Wendt",
            "type": "sparse",
            "x": 2.0983948707580566,
            "y": 8.995092391967773
        },
        {
            "id": "s_1486",
            "name": "Christian Luksch",
            "type": "sparse",
            "x": 3.377743721008301,
            "y": 7.585751533508301
        },
        {
            "id": "s_1487",
            "name": "Zhihua Dong",
            "type": "sparse",
            "x": 5.778751850128174,
            "y": 10.62388801574707
        },
        {
            "id": "s_1488",
            "name": "Jun Zhu 0001",
            "type": "sparse",
            "x": 2.4231343269348145,
            "y": 8.521173477172852
        },
        {
            "id": "s_1489",
            "name": "Kanit Wongsuphasawat",
            "type": "sparse",
            "x": 4.760284900665283,
            "y": 5.4849629402160645
        },
        {
            "id": "s_1490",
            "name": "Huy T. Vo",
            "type": "sparse",
            "x": 7.183322906494141,
            "y": 6.33648681640625
        },
        {
            "id": "s_1491",
            "name": "Xiao Xie",
            "type": "sparse",
            "x": 7.490909099578857,
            "y": 6.607514381408691
        },
        {
            "id": "s_1492",
            "name": "Christopher Andrews 0001",
            "type": "sparse",
            "x": 5.727815628051758,
            "y": 10.14469051361084
        },
        {
            "id": "s_1493",
            "name": "Thomas Auzinger",
            "type": "sparse",
            "x": 4.083239555358887,
            "y": 6.6136088371276855
        },
        {
            "id": "s_1494",
            "name": "Tianyi Lao",
            "type": "sparse",
            "x": 6.123578071594238,
            "y": 8.758930206298828
        },
        {
            "id": "s_1495",
            "name": "Fabio Dias 0001",
            "type": "sparse",
            "x": 6.9307403564453125,
            "y": 6.105090141296387
        },
        {
            "id": "s_1496",
            "name": "Narges Mahyar",
            "type": "sparse",
            "x": 5.50348424911499,
            "y": 10.099703788757324
        },
        {
            "id": "s_1497",
            "name": "Joshua Vander Hook",
            "type": "sparse",
            "x": 3.307056188583374,
            "y": 8.17342758178711
        },
        {
            "id": "s_1498",
            "name": "Alberto Gonzalez",
            "type": "sparse",
            "x": 4.927656650543213,
            "y": 6.493618965148926
        },
        {
            "id": "s_1499",
            "name": "T. J. Jankun-Kelly",
            "type": "sparse",
            "x": 6.37584924697876,
            "y": 6.424874305725098
        },
        {
            "id": "s_1500",
            "name": "Moushumi Sharmin",
            "type": "sparse",
            "x": 7.04141902923584,
            "y": 6.825443267822266
        },
        {
            "id": "s_1501",
            "name": "Cheng Tang",
            "type": "sparse",
            "x": 4.8352508544921875,
            "y": 7.234743595123291
        },
        {
            "id": "s_1502",
            "name": "Yutao Zhang",
            "type": "sparse",
            "x": 5.147846698760986,
            "y": 5.0413899421691895
        },
        {
            "id": "s_1503",
            "name": "Hangzai Luo",
            "type": "sparse",
            "x": 5.686441421508789,
            "y": 8.275586128234863
        },
        {
            "id": "s_1504",
            "name": "Juri F. Buchm\u00fcller",
            "type": "sparse",
            "x": 5.320591926574707,
            "y": 6.559731483459473
        },
        {
            "id": "s_1505",
            "name": "Mirco Nanni",
            "type": "sparse",
            "x": 3.618051052093506,
            "y": 5.657589912414551
        },
        {
            "id": "s_1506",
            "name": "Jason Leigh",
            "type": "sparse",
            "x": 5.004864692687988,
            "y": 8.049983024597168
        },
        {
            "id": "s_1507",
            "name": "Dimitrios Papadopoulos 0001",
            "type": "sparse",
            "x": 8.827108383178711,
            "y": 7.143209934234619
        },
        {
            "id": "s_1508",
            "name": "Felix Brodkorb",
            "type": "sparse",
            "x": 7.490762710571289,
            "y": 5.422610759735107
        },
        {
            "id": "s_1509",
            "name": "Devanshu Arya",
            "type": "sparse",
            "x": 4.8054656982421875,
            "y": 5.500706195831299
        },
        {
            "id": "s_1510",
            "name": "Michael Steptoe",
            "type": "sparse",
            "x": 8.27431869506836,
            "y": 7.59355354309082
        },
        {
            "id": "s_1511",
            "name": "Naveed Massjouni",
            "type": "sparse",
            "x": 5.334270477294922,
            "y": 10.654705047607422
        },
        {
            "id": "s_1512",
            "name": "Kuno Kurzhals",
            "type": "sparse",
            "x": 5.854978561401367,
            "y": 8.353243827819824
        },
        {
            "id": "s_1513",
            "name": "Weixia Xu",
            "type": "sparse",
            "x": 7.2863593101501465,
            "y": 5.07119607925415
        },
        {
            "id": "s_1514",
            "name": "Chris Yi",
            "type": "sparse",
            "x": 4.743052959442139,
            "y": 5.725571155548096
        },
        {
            "id": "s_1515",
            "name": "Jing Su",
            "type": "sparse",
            "x": 7.48647928237915,
            "y": 7.571491241455078
        },
        {
            "id": "s_1516",
            "name": "Juri Buchm\u00fcller",
            "type": "sparse",
            "x": 7.4200615882873535,
            "y": 6.843881607055664
        },
        {
            "id": "s_1517",
            "name": "Alex Endert",
            "type": "sparse",
            "x": 5.365593433380127,
            "y": 8.76335334777832
        },
        {
            "id": "s_1518",
            "name": "Sashank Santhanam",
            "type": "sparse",
            "x": 4.635231018066406,
            "y": 9.437541007995605
        },
        {
            "id": "s_1519",
            "name": "Alex Godwin",
            "type": "sparse",
            "x": 6.313206672668457,
            "y": 6.873620986938477
        },
        {
            "id": "s_1520",
            "name": "Yangqiu Song",
            "type": "sparse",
            "x": 2.921347141265869,
            "y": 8.863079071044922
        },
        {
            "id": "s_1521",
            "name": "Dajian Liu",
            "type": "sparse",
            "x": 5.673964023590088,
            "y": 6.457597255706787
        },
        {
            "id": "s_1522",
            "name": "Christopher Collins 0001",
            "type": "sparse",
            "x": 6.909318447113037,
            "y": 9.546733856201172
        },
        {
            "id": "s_1523",
            "name": "Hairong Wang",
            "type": "sparse",
            "x": 6.563848972320557,
            "y": 7.452944755554199
        },
        {
            "id": "s_1524",
            "name": "Sebastien Boyer",
            "type": "sparse",
            "x": 3.8067502975463867,
            "y": 8.83480167388916
        },
        {
            "id": "s_1525",
            "name": "Jie Lu",
            "type": "sparse",
            "x": 6.752778053283691,
            "y": 10.02120304107666
        },
        {
            "id": "s_1526",
            "name": "Xinlei Zhao",
            "type": "sparse",
            "x": 8.840062141418457,
            "y": 7.1926984786987305
        },
        {
            "id": "s_1527",
            "name": "Tanya E. Clement",
            "type": "sparse",
            "x": 7.462177753448486,
            "y": 10.412565231323242
        },
        {
            "id": "s_1528",
            "name": "Qiusheng Li",
            "type": "sparse",
            "x": 7.030353546142578,
            "y": 4.95527458190918
        },
        {
            "id": "s_1529",
            "name": "Renfei Huang",
            "type": "sparse",
            "x": 5.429854393005371,
            "y": 7.918561935424805
        },
        {
            "id": "s_1530",
            "name": "Matthias Zeppelzauer",
            "type": "sparse",
            "x": 4.2764739990234375,
            "y": 8.524543762207031
        },
        {
            "id": "s_1531",
            "name": "Halldor Janetzko",
            "type": "sparse",
            "x": 7.74921178817749,
            "y": 6.869211196899414
        },
        {
            "id": "s_1532",
            "name": "Rick Walker",
            "type": "sparse",
            "x": 7.262462139129639,
            "y": 8.813150405883789
        },
        {
            "id": "s_1533",
            "name": "G. David Richardson",
            "type": "sparse",
            "x": 6.968570232391357,
            "y": 7.60383415222168
        }
    ],
    "links": [
        {
            "source": "r_0",
            "target": "s_654"
        },
        {
            "source": "r_0",
            "target": "s_1063"
        },
        {
            "source": "r_0",
            "target": "s_1490"
        },
        {
            "source": "r_0",
            "target": "s_641"
        },
        {
            "source": "r_0",
            "target": "s_393"
        },
        {
            "source": "r_1",
            "target": "s_491"
        },
        {
            "source": "r_1",
            "target": "s_1157"
        },
        {
            "source": "r_1",
            "target": "s_837"
        },
        {
            "source": "r_1",
            "target": "s_1255"
        },
        {
            "source": "r_1",
            "target": "s_1488"
        },
        {
            "source": "r_1",
            "target": "s_316"
        },
        {
            "source": "r_2",
            "target": "s_1271"
        },
        {
            "source": "r_2",
            "target": "s_1362"
        },
        {
            "source": "r_2",
            "target": "s_298"
        },
        {
            "source": "r_2",
            "target": "s_1320"
        },
        {
            "source": "r_3",
            "target": "s_1163"
        },
        {
            "source": "r_3",
            "target": "s_1471"
        },
        {
            "source": "r_3",
            "target": "s_1274"
        },
        {
            "source": "r_3",
            "target": "s_1406"
        },
        {
            "source": "r_3",
            "target": "s_905"
        },
        {
            "source": "r_4",
            "target": "s_932"
        },
        {
            "source": "r_4",
            "target": "s_1336"
        },
        {
            "source": "r_4",
            "target": "s_1145"
        },
        {
            "source": "r_4",
            "target": "s_1235"
        },
        {
            "source": "r_4",
            "target": "s_714"
        },
        {
            "source": "r_4",
            "target": "s_1038"
        },
        {
            "source": "r_5",
            "target": "s_1489"
        },
        {
            "source": "r_5",
            "target": "s_1020"
        },
        {
            "source": "r_5",
            "target": "s_708"
        },
        {
            "source": "r_5",
            "target": "s_1478"
        },
        {
            "source": "r_5",
            "target": "s_33"
        },
        {
            "source": "r_5",
            "target": "s_376"
        },
        {
            "source": "r_5",
            "target": "s_455"
        },
        {
            "source": "r_5",
            "target": "s_1273"
        },
        {
            "source": "r_5",
            "target": "s_152"
        },
        {
            "source": "r_6",
            "target": "s_546"
        },
        {
            "source": "r_6",
            "target": "s_513"
        },
        {
            "source": "r_6",
            "target": "s_907"
        },
        {
            "source": "r_6",
            "target": "s_1078"
        },
        {
            "source": "r_6",
            "target": "s_887"
        },
        {
            "source": "r_6",
            "target": "s_998"
        },
        {
            "source": "r_6",
            "target": "s_1236"
        },
        {
            "source": "r_6",
            "target": "s_1343"
        },
        {
            "source": "r_7",
            "target": "s_1465"
        },
        {
            "source": "r_7",
            "target": "s_775"
        },
        {
            "source": "r_7",
            "target": "s_823"
        },
        {
            "source": "r_7",
            "target": "s_575"
        },
        {
            "source": "r_8",
            "target": "s_191"
        },
        {
            "source": "r_8",
            "target": "s_713"
        },
        {
            "source": "r_8",
            "target": "s_468"
        },
        {
            "source": "r_8",
            "target": "s_1399"
        },
        {
            "source": "r_8",
            "target": "s_219"
        },
        {
            "source": "r_9",
            "target": "s_331"
        },
        {
            "source": "r_9",
            "target": "s_137"
        },
        {
            "source": "r_9",
            "target": "s_747"
        },
        {
            "source": "r_9",
            "target": "s_1096"
        },
        {
            "source": "r_10",
            "target": "s_1235"
        },
        {
            "source": "r_10",
            "target": "s_451"
        },
        {
            "source": "r_10",
            "target": "s_1064"
        },
        {
            "source": "r_10",
            "target": "s_1421"
        },
        {
            "source": "r_10",
            "target": "s_968"
        },
        {
            "source": "r_10",
            "target": "s_1417"
        },
        {
            "source": "r_10",
            "target": "s_101"
        },
        {
            "source": "r_10",
            "target": "s_331"
        },
        {
            "source": "r_11",
            "target": "s_932"
        },
        {
            "source": "r_11",
            "target": "s_1123"
        },
        {
            "source": "r_11",
            "target": "s_1235"
        },
        {
            "source": "r_11",
            "target": "s_714"
        },
        {
            "source": "r_11",
            "target": "s_1038"
        },
        {
            "source": "r_12",
            "target": "s_200"
        },
        {
            "source": "r_12",
            "target": "s_746"
        },
        {
            "source": "r_12",
            "target": "s_214"
        },
        {
            "source": "r_12",
            "target": "s_605"
        },
        {
            "source": "r_12",
            "target": "s_721"
        },
        {
            "source": "r_12",
            "target": "s_952"
        },
        {
            "source": "r_12",
            "target": "s_314"
        },
        {
            "source": "r_13",
            "target": "s_1422"
        },
        {
            "source": "r_13",
            "target": "s_131"
        },
        {
            "source": "r_13",
            "target": "s_473"
        },
        {
            "source": "r_14",
            "target": "s_810"
        },
        {
            "source": "r_14",
            "target": "s_172"
        },
        {
            "source": "r_14",
            "target": "s_1218"
        },
        {
            "source": "r_14",
            "target": "s_1124"
        },
        {
            "source": "r_14",
            "target": "s_1202"
        },
        {
            "source": "r_14",
            "target": "s_512"
        },
        {
            "source": "r_14",
            "target": "s_385"
        },
        {
            "source": "r_15",
            "target": "s_1241"
        },
        {
            "source": "r_15",
            "target": "s_1147"
        },
        {
            "source": "r_15",
            "target": "s_643"
        },
        {
            "source": "r_15",
            "target": "s_29"
        },
        {
            "source": "r_15",
            "target": "s_229"
        },
        {
            "source": "r_15",
            "target": "s_1042"
        },
        {
            "source": "r_15",
            "target": "s_1465"
        },
        {
            "source": "r_15",
            "target": "s_575"
        },
        {
            "source": "r_16",
            "target": "s_932"
        },
        {
            "source": "r_16",
            "target": "s_115"
        },
        {
            "source": "r_16",
            "target": "s_811"
        },
        {
            "source": "r_16",
            "target": "s_525"
        },
        {
            "source": "r_16",
            "target": "s_519"
        },
        {
            "source": "r_16",
            "target": "s_132"
        },
        {
            "source": "r_16",
            "target": "s_1178"
        },
        {
            "source": "r_16",
            "target": "s_1038"
        },
        {
            "source": "r_17",
            "target": "s_241"
        },
        {
            "source": "r_17",
            "target": "s_1508"
        },
        {
            "source": "r_17",
            "target": "s_858"
        },
        {
            "source": "r_17",
            "target": "s_11"
        },
        {
            "source": "r_17",
            "target": "s_1195"
        },
        {
            "source": "r_17",
            "target": "s_527"
        },
        {
            "source": "r_18",
            "target": "s_1522"
        },
        {
            "source": "r_18",
            "target": "s_1273"
        },
        {
            "source": "r_18",
            "target": "s_152"
        },
        {
            "source": "r_19",
            "target": "s_1195"
        },
        {
            "source": "r_19",
            "target": "s_11"
        },
        {
            "source": "r_20",
            "target": "s_1195"
        },
        {
            "source": "r_20",
            "target": "s_11"
        },
        {
            "source": "r_20",
            "target": "s_1400"
        },
        {
            "source": "r_20",
            "target": "s_132"
        },
        {
            "source": "r_21",
            "target": "s_365"
        },
        {
            "source": "r_21",
            "target": "s_1517"
        },
        {
            "source": "r_21",
            "target": "s_860"
        },
        {
            "source": "r_21",
            "target": "s_1170"
        },
        {
            "source": "r_22",
            "target": "s_1095"
        },
        {
            "source": "r_22",
            "target": "s_648"
        },
        {
            "source": "r_22",
            "target": "s_1080"
        },
        {
            "source": "r_22",
            "target": "s_1118"
        },
        {
            "source": "r_22",
            "target": "s_89"
        },
        {
            "source": "r_22",
            "target": "s_131"
        },
        {
            "source": "r_22",
            "target": "s_1347"
        },
        {
            "source": "r_23",
            "target": "s_1213"
        },
        {
            "source": "r_23",
            "target": "s_1396"
        },
        {
            "source": "r_23",
            "target": "s_253"
        },
        {
            "source": "r_23",
            "target": "s_966"
        },
        {
            "source": "r_23",
            "target": "s_985"
        },
        {
            "source": "r_24",
            "target": "s_1354"
        },
        {
            "source": "r_24",
            "target": "s_892"
        },
        {
            "source": "r_24",
            "target": "s_439"
        },
        {
            "source": "r_24",
            "target": "s_529"
        },
        {
            "source": "r_24",
            "target": "s_1226"
        },
        {
            "source": "r_24",
            "target": "s_1522"
        },
        {
            "source": "r_25",
            "target": "s_1195"
        },
        {
            "source": "r_25",
            "target": "s_11"
        },
        {
            "source": "r_25",
            "target": "s_250"
        },
        {
            "source": "r_25",
            "target": "s_1505"
        },
        {
            "source": "r_25",
            "target": "s_340"
        },
        {
            "source": "r_25",
            "target": "s_867"
        },
        {
            "source": "r_26",
            "target": "s_708"
        },
        {
            "source": "r_26",
            "target": "s_499"
        },
        {
            "source": "r_26",
            "target": "s_43"
        },
        {
            "source": "r_26",
            "target": "s_152"
        },
        {
            "source": "r_26",
            "target": "s_1273"
        },
        {
            "source": "r_26",
            "target": "s_1478"
        },
        {
            "source": "r_27",
            "target": "s_726"
        },
        {
            "source": "r_27",
            "target": "s_1177"
        },
        {
            "source": "r_27",
            "target": "s_873"
        },
        {
            "source": "r_28",
            "target": "s_1347"
        },
        {
            "source": "r_28",
            "target": "s_316"
        },
        {
            "source": "r_28",
            "target": "s_1265"
        },
        {
            "source": "r_28",
            "target": "s_491"
        },
        {
            "source": "r_28",
            "target": "s_982"
        },
        {
            "source": "r_29",
            "target": "s_1318"
        },
        {
            "source": "r_29",
            "target": "s_312"
        },
        {
            "source": "r_29",
            "target": "s_915"
        },
        {
            "source": "r_30",
            "target": "s_279"
        },
        {
            "source": "r_30",
            "target": "s_975"
        },
        {
            "source": "r_30",
            "target": "s_1449"
        },
        {
            "source": "r_30",
            "target": "s_763"
        },
        {
            "source": "r_30",
            "target": "s_1322"
        },
        {
            "source": "r_30",
            "target": "s_727"
        },
        {
            "source": "r_31",
            "target": "s_45"
        },
        {
            "source": "r_31",
            "target": "s_272"
        },
        {
            "source": "r_31",
            "target": "s_656"
        },
        {
            "source": "r_31",
            "target": "s_333"
        },
        {
            "source": "r_32",
            "target": "s_69"
        },
        {
            "source": "r_32",
            "target": "s_160"
        },
        {
            "source": "r_32",
            "target": "s_169"
        },
        {
            "source": "r_32",
            "target": "s_474"
        },
        {
            "source": "r_32",
            "target": "s_952"
        },
        {
            "source": "r_33",
            "target": "s_873"
        },
        {
            "source": "r_33",
            "target": "s_759"
        },
        {
            "source": "r_34",
            "target": "s_1422"
        },
        {
            "source": "r_34",
            "target": "s_1352"
        },
        {
            "source": "r_34",
            "target": "s_292"
        },
        {
            "source": "r_34",
            "target": "s_837"
        },
        {
            "source": "r_34",
            "target": "s_436"
        },
        {
            "source": "r_34",
            "target": "s_1520"
        },
        {
            "source": "r_34",
            "target": "s_131"
        },
        {
            "source": "r_35",
            "target": "s_1042"
        },
        {
            "source": "r_35",
            "target": "s_29"
        },
        {
            "source": "r_35",
            "target": "s_526"
        },
        {
            "source": "r_35",
            "target": "s_575"
        },
        {
            "source": "r_36",
            "target": "s_690"
        },
        {
            "source": "r_36",
            "target": "s_1531"
        },
        {
            "source": "r_36",
            "target": "s_1272"
        },
        {
            "source": "r_36",
            "target": "s_1404"
        },
        {
            "source": "r_36",
            "target": "s_1416"
        },
        {
            "source": "r_36",
            "target": "s_271"
        },
        {
            "source": "r_36",
            "target": "s_511"
        },
        {
            "source": "r_36",
            "target": "s_1195"
        },
        {
            "source": "r_36",
            "target": "s_1389"
        },
        {
            "source": "r_36",
            "target": "s_1038"
        },
        {
            "source": "r_37",
            "target": "s_467"
        },
        {
            "source": "r_37",
            "target": "s_1177"
        },
        {
            "source": "r_37",
            "target": "s_473"
        },
        {
            "source": "r_38",
            "target": "s_1000"
        },
        {
            "source": "r_38",
            "target": "s_1204"
        },
        {
            "source": "r_38",
            "target": "s_506"
        },
        {
            "source": "r_38",
            "target": "s_1081"
        },
        {
            "source": "r_38",
            "target": "s_138"
        },
        {
            "source": "r_38",
            "target": "s_434"
        },
        {
            "source": "r_39",
            "target": "s_863"
        },
        {
            "source": "r_39",
            "target": "s_469"
        },
        {
            "source": "r_39",
            "target": "s_1256"
        },
        {
            "source": "r_39",
            "target": "s_920"
        },
        {
            "source": "r_40",
            "target": "s_894"
        },
        {
            "source": "r_40",
            "target": "s_808"
        },
        {
            "source": "r_40",
            "target": "s_547"
        },
        {
            "source": "r_40",
            "target": "s_1177"
        },
        {
            "source": "r_40",
            "target": "s_1409"
        },
        {
            "source": "r_40",
            "target": "s_463"
        },
        {
            "source": "r_41",
            "target": "s_1397"
        },
        {
            "source": "r_41",
            "target": "s_923"
        },
        {
            "source": "r_41",
            "target": "s_953"
        },
        {
            "source": "r_41",
            "target": "s_182"
        },
        {
            "source": "r_41",
            "target": "s_1393"
        },
        {
            "source": "r_42",
            "target": "s_491"
        },
        {
            "source": "r_42",
            "target": "s_1157"
        },
        {
            "source": "r_42",
            "target": "s_640"
        },
        {
            "source": "r_42",
            "target": "s_1488"
        },
        {
            "source": "r_42",
            "target": "s_316"
        },
        {
            "source": "r_43",
            "target": "s_1300"
        },
        {
            "source": "r_43",
            "target": "s_910"
        },
        {
            "source": "r_43",
            "target": "s_1395"
        },
        {
            "source": "r_43",
            "target": "s_63"
        },
        {
            "source": "r_44",
            "target": "s_65"
        },
        {
            "source": "r_44",
            "target": "s_404"
        },
        {
            "source": "r_44",
            "target": "s_214"
        },
        {
            "source": "r_44",
            "target": "s_314"
        },
        {
            "source": "r_45",
            "target": "s_305"
        },
        {
            "source": "r_45",
            "target": "s_1368"
        },
        {
            "source": "r_45",
            "target": "s_1210"
        },
        {
            "source": "r_45",
            "target": "s_233"
        },
        {
            "source": "r_45",
            "target": "s_98"
        },
        {
            "source": "r_46",
            "target": "s_892"
        },
        {
            "source": "r_46",
            "target": "s_1418"
        },
        {
            "source": "r_46",
            "target": "s_88"
        },
        {
            "source": "r_46",
            "target": "s_319"
        },
        {
            "source": "r_46",
            "target": "s_1226"
        },
        {
            "source": "r_46",
            "target": "s_992"
        },
        {
            "source": "r_47",
            "target": "s_115"
        },
        {
            "source": "r_47",
            "target": "s_1336"
        },
        {
            "source": "r_47",
            "target": "s_547"
        },
        {
            "source": "r_47",
            "target": "s_890"
        },
        {
            "source": "r_47",
            "target": "s_511"
        },
        {
            "source": "r_47",
            "target": "s_1231"
        },
        {
            "source": "r_47",
            "target": "s_1029"
        },
        {
            "source": "r_47",
            "target": "s_735"
        },
        {
            "source": "r_47",
            "target": "s_1038"
        },
        {
            "source": "r_48",
            "target": "s_142"
        },
        {
            "source": "r_48",
            "target": "s_1089"
        },
        {
            "source": "r_48",
            "target": "s_1042"
        },
        {
            "source": "r_48",
            "target": "s_1465"
        },
        {
            "source": "r_48",
            "target": "s_1472"
        },
        {
            "source": "r_48",
            "target": "s_311"
        },
        {
            "source": "r_49",
            "target": "s_305"
        },
        {
            "source": "r_49",
            "target": "s_64"
        },
        {
            "source": "r_49",
            "target": "s_1368"
        },
        {
            "source": "r_49",
            "target": "s_880"
        },
        {
            "source": "r_49",
            "target": "s_233"
        },
        {
            "source": "r_50",
            "target": "s_1517"
        },
        {
            "source": "r_50",
            "target": "s_358"
        },
        {
            "source": "r_50",
            "target": "s_202"
        },
        {
            "source": "r_51",
            "target": "s_214"
        },
        {
            "source": "r_51",
            "target": "s_746"
        },
        {
            "source": "r_51",
            "target": "s_65"
        },
        {
            "source": "r_51",
            "target": "s_289"
        },
        {
            "source": "r_51",
            "target": "s_404"
        },
        {
            "source": "r_51",
            "target": "s_1116"
        },
        {
            "source": "r_51",
            "target": "s_1304"
        },
        {
            "source": "r_51",
            "target": "s_314"
        },
        {
            "source": "r_52",
            "target": "s_1517"
        },
        {
            "source": "r_52",
            "target": "s_423"
        },
        {
            "source": "r_52",
            "target": "s_618"
        },
        {
            "source": "r_52",
            "target": "s_15"
        },
        {
            "source": "r_52",
            "target": "s_56"
        },
        {
            "source": "r_52",
            "target": "s_202"
        },
        {
            "source": "r_53",
            "target": "s_1307"
        },
        {
            "source": "r_53",
            "target": "s_391"
        },
        {
            "source": "r_53",
            "target": "s_981"
        },
        {
            "source": "r_53",
            "target": "s_3"
        },
        {
            "source": "r_53",
            "target": "s_711"
        },
        {
            "source": "r_53",
            "target": "s_251"
        },
        {
            "source": "r_53",
            "target": "s_1038"
        },
        {
            "source": "r_54",
            "target": "s_327"
        },
        {
            "source": "r_54",
            "target": "s_1274"
        },
        {
            "source": "r_54",
            "target": "s_1054"
        },
        {
            "source": "r_54",
            "target": "s_1333"
        },
        {
            "source": "r_54",
            "target": "s_1262"
        },
        {
            "source": "r_54",
            "target": "s_1163"
        },
        {
            "source": "r_54",
            "target": "s_466"
        },
        {
            "source": "r_54",
            "target": "s_918"
        },
        {
            "source": "r_55",
            "target": "s_435"
        },
        {
            "source": "r_55",
            "target": "s_275"
        },
        {
            "source": "r_56",
            "target": "s_1125"
        },
        {
            "source": "r_56",
            "target": "s_407"
        },
        {
            "source": "r_56",
            "target": "s_261"
        },
        {
            "source": "r_56",
            "target": "s_799"
        },
        {
            "source": "r_57",
            "target": "s_1235"
        },
        {
            "source": "r_57",
            "target": "s_176"
        },
        {
            "source": "r_57",
            "target": "s_21"
        },
        {
            "source": "r_57",
            "target": "s_106"
        },
        {
            "source": "r_57",
            "target": "s_987"
        },
        {
            "source": "r_57",
            "target": "s_660"
        },
        {
            "source": "r_57",
            "target": "s_1177"
        },
        {
            "source": "r_58",
            "target": "s_1300"
        },
        {
            "source": "r_58",
            "target": "s_63"
        },
        {
            "source": "r_59",
            "target": "s_1465"
        },
        {
            "source": "r_59",
            "target": "s_783"
        },
        {
            "source": "r_59",
            "target": "s_575"
        },
        {
            "source": "r_59",
            "target": "s_1273"
        },
        {
            "source": "r_59",
            "target": "s_152"
        },
        {
            "source": "r_60",
            "target": "s_1366"
        },
        {
            "source": "r_60",
            "target": "s_1401"
        },
        {
            "source": "r_60",
            "target": "s_222"
        },
        {
            "source": "r_60",
            "target": "s_1160"
        },
        {
            "source": "r_60",
            "target": "s_131"
        },
        {
            "source": "r_61",
            "target": "s_45"
        },
        {
            "source": "r_61",
            "target": "s_778"
        },
        {
            "source": "r_61",
            "target": "s_154"
        },
        {
            "source": "r_61",
            "target": "s_413"
        },
        {
            "source": "r_61",
            "target": "s_1187"
        },
        {
            "source": "r_61",
            "target": "s_1517"
        },
        {
            "source": "r_61",
            "target": "s_333"
        },
        {
            "source": "r_62",
            "target": "s_280"
        },
        {
            "source": "r_62",
            "target": "s_1347"
        },
        {
            "source": "r_62",
            "target": "s_316"
        },
        {
            "source": "r_62",
            "target": "s_41"
        },
        {
            "source": "r_62",
            "target": "s_337"
        },
        {
            "source": "r_62",
            "target": "s_269"
        },
        {
            "source": "r_63",
            "target": "s_1144"
        },
        {
            "source": "r_63",
            "target": "s_1414"
        },
        {
            "source": "r_63",
            "target": "s_1393"
        },
        {
            "source": "r_63",
            "target": "s_1319"
        },
        {
            "source": "r_64",
            "target": "s_331"
        },
        {
            "source": "r_64",
            "target": "s_468"
        },
        {
            "source": "r_64",
            "target": "s_1459"
        },
        {
            "source": "r_64",
            "target": "s_1096"
        },
        {
            "source": "r_65",
            "target": "s_788"
        },
        {
            "source": "r_65",
            "target": "s_116"
        },
        {
            "source": "r_65",
            "target": "s_363"
        },
        {
            "source": "r_65",
            "target": "s_874"
        },
        {
            "source": "r_66",
            "target": "s_435"
        },
        {
            "source": "r_66",
            "target": "s_275"
        },
        {
            "source": "r_66",
            "target": "s_73"
        },
        {
            "source": "r_66",
            "target": "s_811"
        },
        {
            "source": "r_66",
            "target": "s_512"
        },
        {
            "source": "r_67",
            "target": "s_1144"
        },
        {
            "source": "r_67",
            "target": "s_1347"
        },
        {
            "source": "r_67",
            "target": "s_744"
        },
        {
            "source": "r_67",
            "target": "s_41"
        },
        {
            "source": "r_67",
            "target": "s_316"
        },
        {
            "source": "r_67",
            "target": "s_337"
        },
        {
            "source": "r_67",
            "target": "s_131"
        },
        {
            "source": "r_68",
            "target": "s_1163"
        },
        {
            "source": "r_68",
            "target": "s_82"
        },
        {
            "source": "r_68",
            "target": "s_1471"
        },
        {
            "source": "r_68",
            "target": "s_1274"
        },
        {
            "source": "r_68",
            "target": "s_131"
        },
        {
            "source": "r_68",
            "target": "s_632"
        },
        {
            "source": "r_68",
            "target": "s_876"
        },
        {
            "source": "r_69",
            "target": "s_893"
        },
        {
            "source": "r_69",
            "target": "s_412"
        },
        {
            "source": "r_69",
            "target": "s_1530"
        },
        {
            "source": "r_69",
            "target": "s_335"
        },
        {
            "source": "r_69",
            "target": "s_811"
        },
        {
            "source": "r_70",
            "target": "s_1397"
        },
        {
            "source": "r_70",
            "target": "s_1392"
        },
        {
            "source": "r_70",
            "target": "s_378"
        },
        {
            "source": "r_70",
            "target": "s_1124"
        },
        {
            "source": "r_70",
            "target": "s_870"
        },
        {
            "source": "r_71",
            "target": "s_333"
        },
        {
            "source": "r_71",
            "target": "s_1222"
        },
        {
            "source": "r_71",
            "target": "s_68"
        },
        {
            "source": "r_71",
            "target": "s_233"
        },
        {
            "source": "r_71",
            "target": "s_763"
        },
        {
            "source": "r_71",
            "target": "s_224"
        },
        {
            "source": "r_71",
            "target": "s_855"
        },
        {
            "source": "r_71",
            "target": "s_1267"
        },
        {
            "source": "r_71",
            "target": "s_402"
        },
        {
            "source": "r_72",
            "target": "s_1115"
        },
        {
            "source": "r_72",
            "target": "s_160"
        },
        {
            "source": "r_72",
            "target": "s_1074"
        },
        {
            "source": "r_72",
            "target": "s_36"
        },
        {
            "source": "r_72",
            "target": "s_1201"
        },
        {
            "source": "r_72",
            "target": "s_159"
        },
        {
            "source": "r_73",
            "target": "s_1364"
        },
        {
            "source": "r_73",
            "target": "s_374"
        },
        {
            "source": "r_73",
            "target": "s_917"
        },
        {
            "source": "r_73",
            "target": "s_1283"
        },
        {
            "source": "r_73",
            "target": "s_131"
        },
        {
            "source": "r_73",
            "target": "s_270"
        },
        {
            "source": "r_74",
            "target": "s_756"
        },
        {
            "source": "r_74",
            "target": "s_657"
        },
        {
            "source": "r_74",
            "target": "s_1115"
        },
        {
            "source": "r_74",
            "target": "s_179"
        },
        {
            "source": "r_75",
            "target": "s_399"
        },
        {
            "source": "r_75",
            "target": "s_671"
        },
        {
            "source": "r_75",
            "target": "s_132"
        },
        {
            "source": "r_76",
            "target": "s_237"
        },
        {
            "source": "r_76",
            "target": "s_1292"
        },
        {
            "source": "r_76",
            "target": "s_636"
        },
        {
            "source": "r_76",
            "target": "s_219"
        },
        {
            "source": "r_77",
            "target": "s_1347"
        },
        {
            "source": "r_77",
            "target": "s_1491"
        },
        {
            "source": "r_77",
            "target": "s_791"
        },
        {
            "source": "r_77",
            "target": "s_1277"
        },
        {
            "source": "r_77",
            "target": "s_446"
        },
        {
            "source": "r_77",
            "target": "s_567"
        },
        {
            "source": "r_77",
            "target": "s_367"
        },
        {
            "source": "r_77",
            "target": "s_1319"
        },
        {
            "source": "r_78",
            "target": "s_1001"
        },
        {
            "source": "r_78",
            "target": "s_393"
        },
        {
            "source": "r_79",
            "target": "s_1058"
        },
        {
            "source": "r_79",
            "target": "s_976"
        },
        {
            "source": "r_79",
            "target": "s_1501"
        },
        {
            "source": "r_79",
            "target": "s_1357"
        },
        {
            "source": "r_79",
            "target": "s_76"
        },
        {
            "source": "r_79",
            "target": "s_4"
        },
        {
            "source": "r_79",
            "target": "s_1319"
        },
        {
            "source": "r_80",
            "target": "s_147"
        },
        {
            "source": "r_80",
            "target": "s_748"
        },
        {
            "source": "r_80",
            "target": "s_1437"
        },
        {
            "source": "r_80",
            "target": "s_763"
        },
        {
            "source": "r_80",
            "target": "s_913"
        },
        {
            "source": "r_80",
            "target": "s_975"
        },
        {
            "source": "r_80",
            "target": "s_1322"
        },
        {
            "source": "r_80",
            "target": "s_1319"
        },
        {
            "source": "r_80",
            "target": "s_1449"
        },
        {
            "source": "r_80",
            "target": "s_679"
        },
        {
            "source": "r_81",
            "target": "s_877"
        },
        {
            "source": "r_81",
            "target": "s_1319"
        },
        {
            "source": "r_81",
            "target": "s_1414"
        },
        {
            "source": "r_81",
            "target": "s_1046"
        },
        {
            "source": "r_81",
            "target": "s_384"
        },
        {
            "source": "r_81",
            "target": "s_1430"
        },
        {
            "source": "r_81",
            "target": "s_194"
        },
        {
            "source": "r_81",
            "target": "s_699"
        },
        {
            "source": "r_82",
            "target": "s_414"
        },
        {
            "source": "r_82",
            "target": "s_28"
        },
        {
            "source": "r_82",
            "target": "s_244"
        },
        {
            "source": "r_82",
            "target": "s_873"
        },
        {
            "source": "r_82",
            "target": "s_1288"
        },
        {
            "source": "r_82",
            "target": "s_892"
        },
        {
            "source": "r_83",
            "target": "s_1496"
        },
        {
            "source": "r_83",
            "target": "s_1266"
        },
        {
            "source": "r_84",
            "target": "s_436"
        },
        {
            "source": "r_84",
            "target": "s_1144"
        },
        {
            "source": "r_84",
            "target": "s_1393"
        },
        {
            "source": "r_85",
            "target": "s_1354"
        },
        {
            "source": "r_85",
            "target": "s_1522"
        },
        {
            "source": "r_85",
            "target": "s_173"
        },
        {
            "source": "r_85",
            "target": "s_820"
        },
        {
            "source": "r_86",
            "target": "s_469"
        },
        {
            "source": "r_86",
            "target": "s_364"
        },
        {
            "source": "r_86",
            "target": "s_1354"
        },
        {
            "source": "r_86",
            "target": "s_1440"
        },
        {
            "source": "r_86",
            "target": "s_460"
        },
        {
            "source": "r_86",
            "target": "s_131"
        },
        {
            "source": "r_87",
            "target": "s_932"
        },
        {
            "source": "r_87",
            "target": "s_832"
        },
        {
            "source": "r_87",
            "target": "s_1038"
        },
        {
            "source": "r_87",
            "target": "s_273"
        },
        {
            "source": "r_88",
            "target": "s_305"
        },
        {
            "source": "r_88",
            "target": "s_1368"
        },
        {
            "source": "r_88",
            "target": "s_333"
        },
        {
            "source": "r_88",
            "target": "s_233"
        },
        {
            "source": "r_89",
            "target": "s_892"
        },
        {
            "source": "r_89",
            "target": "s_470"
        },
        {
            "source": "r_89",
            "target": "s_1306"
        },
        {
            "source": "r_89",
            "target": "s_1226"
        },
        {
            "source": "r_89",
            "target": "s_1024"
        },
        {
            "source": "r_89",
            "target": "s_897"
        },
        {
            "source": "r_90",
            "target": "s_788"
        },
        {
            "source": "r_90",
            "target": "s_487"
        },
        {
            "source": "r_90",
            "target": "s_363"
        },
        {
            "source": "r_90",
            "target": "s_967"
        },
        {
            "source": "r_91",
            "target": "s_65"
        },
        {
            "source": "r_91",
            "target": "s_1174"
        },
        {
            "source": "r_91",
            "target": "s_404"
        },
        {
            "source": "r_91",
            "target": "s_314"
        },
        {
            "source": "r_92",
            "target": "s_1512"
        },
        {
            "source": "r_92",
            "target": "s_132"
        },
        {
            "source": "r_93",
            "target": "s_624"
        },
        {
            "source": "r_93",
            "target": "s_634"
        },
        {
            "source": "r_93",
            "target": "s_370"
        },
        {
            "source": "r_93",
            "target": "s_522"
        },
        {
            "source": "r_93",
            "target": "s_150"
        },
        {
            "source": "r_94",
            "target": "s_1195"
        },
        {
            "source": "r_94",
            "target": "s_11"
        },
        {
            "source": "r_94",
            "target": "s_592"
        },
        {
            "source": "r_94",
            "target": "s_250"
        },
        {
            "source": "r_94",
            "target": "s_621"
        },
        {
            "source": "r_95",
            "target": "s_1066"
        },
        {
            "source": "r_95",
            "target": "s_1048"
        },
        {
            "source": "r_95",
            "target": "s_375"
        },
        {
            "source": "r_95",
            "target": "s_545"
        },
        {
            "source": "r_95",
            "target": "s_606"
        },
        {
            "source": "r_96",
            "target": "s_745"
        },
        {
            "source": "r_96",
            "target": "s_331"
        },
        {
            "source": "r_96",
            "target": "s_1096"
        },
        {
            "source": "r_96",
            "target": "s_1517"
        },
        {
            "source": "r_97",
            "target": "s_9"
        },
        {
            "source": "r_97",
            "target": "s_1022"
        },
        {
            "source": "r_97",
            "target": "s_480"
        },
        {
            "source": "r_97",
            "target": "s_1517"
        },
        {
            "source": "r_98",
            "target": "s_586"
        },
        {
            "source": "r_98",
            "target": "s_1217"
        },
        {
            "source": "r_98",
            "target": "s_1461"
        },
        {
            "source": "r_98",
            "target": "s_15"
        },
        {
            "source": "r_98",
            "target": "s_56"
        },
        {
            "source": "r_98",
            "target": "s_202"
        },
        {
            "source": "r_99",
            "target": "s_1280"
        },
        {
            "source": "r_99",
            "target": "s_1391"
        },
        {
            "source": "r_99",
            "target": "s_1512"
        },
        {
            "source": "r_99",
            "target": "s_404"
        },
        {
            "source": "r_99",
            "target": "s_314"
        },
        {
            "source": "r_100",
            "target": "s_893"
        },
        {
            "source": "r_100",
            "target": "s_350"
        },
        {
            "source": "r_100",
            "target": "s_692"
        },
        {
            "source": "r_100",
            "target": "s_1218"
        },
        {
            "source": "r_100",
            "target": "s_511"
        },
        {
            "source": "r_100",
            "target": "s_85"
        },
        {
            "source": "r_101",
            "target": "s_883"
        },
        {
            "source": "r_101",
            "target": "s_1113"
        },
        {
            "source": "r_101",
            "target": "s_699"
        },
        {
            "source": "r_102",
            "target": "s_387"
        },
        {
            "source": "r_102",
            "target": "s_219"
        },
        {
            "source": "r_103",
            "target": "s_256"
        },
        {
            "source": "r_103",
            "target": "s_478"
        },
        {
            "source": "r_103",
            "target": "s_1266"
        },
        {
            "source": "r_103",
            "target": "s_667"
        },
        {
            "source": "r_104",
            "target": "s_1259"
        },
        {
            "source": "r_104",
            "target": "s_569"
        },
        {
            "source": "r_104",
            "target": "s_699"
        },
        {
            "source": "r_105",
            "target": "s_467"
        },
        {
            "source": "r_105",
            "target": "s_1177"
        },
        {
            "source": "r_105",
            "target": "s_759"
        },
        {
            "source": "r_106",
            "target": "s_1357"
        },
        {
            "source": "r_106",
            "target": "s_328"
        },
        {
            "source": "r_106",
            "target": "s_438"
        },
        {
            "source": "r_106",
            "target": "s_1097"
        },
        {
            "source": "r_106",
            "target": "s_974"
        },
        {
            "source": "r_106",
            "target": "s_1091"
        },
        {
            "source": "r_106",
            "target": "s_316"
        },
        {
            "source": "r_106",
            "target": "s_1058"
        },
        {
            "source": "r_106",
            "target": "s_286"
        },
        {
            "source": "r_106",
            "target": "s_107"
        },
        {
            "source": "r_107",
            "target": "s_316"
        },
        {
            "source": "r_107",
            "target": "s_369"
        },
        {
            "source": "r_107",
            "target": "s_1289"
        },
        {
            "source": "r_107",
            "target": "s_1069"
        },
        {
            "source": "r_107",
            "target": "s_1433"
        },
        {
            "source": "r_107",
            "target": "s_1488"
        },
        {
            "source": "r_108",
            "target": "s_52"
        },
        {
            "source": "r_108",
            "target": "s_721"
        },
        {
            "source": "r_108",
            "target": "s_1055"
        },
        {
            "source": "r_108",
            "target": "s_1211"
        },
        {
            "source": "r_108",
            "target": "s_952"
        },
        {
            "source": "r_109",
            "target": "s_1237"
        },
        {
            "source": "r_109",
            "target": "s_1033"
        },
        {
            "source": "r_109",
            "target": "s_1088"
        },
        {
            "source": "r_109",
            "target": "s_699"
        },
        {
            "source": "r_110",
            "target": "s_1195"
        },
        {
            "source": "r_110",
            "target": "s_11"
        },
        {
            "source": "r_110",
            "target": "s_1053"
        },
        {
            "source": "r_110",
            "target": "s_329"
        },
        {
            "source": "r_111",
            "target": "s_694"
        },
        {
            "source": "r_111",
            "target": "s_242"
        },
        {
            "source": "r_111",
            "target": "s_855"
        },
        {
            "source": "r_111",
            "target": "s_1194"
        },
        {
            "source": "r_112",
            "target": "s_1307"
        },
        {
            "source": "r_112",
            "target": "s_357"
        },
        {
            "source": "r_112",
            "target": "s_988"
        },
        {
            "source": "r_112",
            "target": "s_473"
        },
        {
            "source": "r_112",
            "target": "s_511"
        },
        {
            "source": "r_112",
            "target": "s_180"
        },
        {
            "source": "r_112",
            "target": "s_1038"
        },
        {
            "source": "r_113",
            "target": "s_627"
        },
        {
            "source": "r_113",
            "target": "s_233"
        },
        {
            "source": "r_113",
            "target": "s_1047"
        },
        {
            "source": "r_114",
            "target": "s_1095"
        },
        {
            "source": "r_114",
            "target": "s_1144"
        },
        {
            "source": "r_114",
            "target": "s_1393"
        },
        {
            "source": "r_115",
            "target": "s_681"
        },
        {
            "source": "r_115",
            "target": "s_1191"
        },
        {
            "source": "r_116",
            "target": "s_562"
        },
        {
            "source": "r_116",
            "target": "s_543"
        },
        {
            "source": "r_117",
            "target": "s_150"
        },
        {
            "source": "r_118",
            "target": "s_81"
        },
        {
            "source": "r_118",
            "target": "s_1305"
        },
        {
            "source": "r_118",
            "target": "s_1"
        },
        {
            "source": "r_118",
            "target": "s_378"
        },
        {
            "source": "r_119",
            "target": "s_1371"
        },
        {
            "source": "r_119",
            "target": "s_165"
        },
        {
            "source": "r_119",
            "target": "s_273"
        },
        {
            "source": "r_120",
            "target": "s_1007"
        },
        {
            "source": "r_120",
            "target": "s_1041"
        },
        {
            "source": "r_120",
            "target": "s_883"
        },
        {
            "source": "r_120",
            "target": "s_699"
        },
        {
            "source": "r_121",
            "target": "s_1038"
        },
        {
            "source": "r_121",
            "target": "s_37"
        },
        {
            "source": "r_122",
            "target": "s_935"
        },
        {
            "source": "r_122",
            "target": "s_84"
        },
        {
            "source": "r_122",
            "target": "s_243"
        },
        {
            "source": "r_122",
            "target": "s_380"
        },
        {
            "source": "r_122",
            "target": "s_390"
        },
        {
            "source": "r_122",
            "target": "s_73"
        },
        {
            "source": "r_122",
            "target": "s_921"
        },
        {
            "source": "r_122",
            "target": "s_1107"
        },
        {
            "source": "r_122",
            "target": "s_1037"
        },
        {
            "source": "r_122",
            "target": "s_195"
        },
        {
            "source": "r_123",
            "target": "s_1512"
        },
        {
            "source": "r_123",
            "target": "s_582"
        },
        {
            "source": "r_123",
            "target": "s_320"
        },
        {
            "source": "r_123",
            "target": "s_132"
        },
        {
            "source": "r_124",
            "target": "s_945"
        },
        {
            "source": "r_124",
            "target": "s_721"
        },
        {
            "source": "r_124",
            "target": "s_952"
        },
        {
            "source": "r_125",
            "target": "s_932"
        },
        {
            "source": "r_125",
            "target": "s_832"
        },
        {
            "source": "r_125",
            "target": "s_893"
        },
        {
            "source": "r_125",
            "target": "s_547"
        },
        {
            "source": "r_125",
            "target": "s_511"
        },
        {
            "source": "r_125",
            "target": "s_1155"
        },
        {
            "source": "r_125",
            "target": "s_1038"
        },
        {
            "source": "r_126",
            "target": "s_961"
        },
        {
            "source": "r_126",
            "target": "s_1005"
        },
        {
            "source": "r_126",
            "target": "s_220"
        },
        {
            "source": "r_126",
            "target": "s_331"
        },
        {
            "source": "r_126",
            "target": "s_1318"
        },
        {
            "source": "r_126",
            "target": "s_1191"
        },
        {
            "source": "r_127",
            "target": "s_520"
        },
        {
            "source": "r_127",
            "target": "s_1235"
        },
        {
            "source": "r_127",
            "target": "s_452"
        },
        {
            "source": "r_127",
            "target": "s_1260"
        },
        {
            "source": "r_127",
            "target": "s_1191"
        },
        {
            "source": "r_128",
            "target": "s_37"
        },
        {
            "source": "r_128",
            "target": "s_211"
        },
        {
            "source": "r_128",
            "target": "s_1337"
        },
        {
            "source": "r_128",
            "target": "s_1038"
        },
        {
            "source": "r_128",
            "target": "s_1169"
        },
        {
            "source": "r_128",
            "target": "s_1261"
        },
        {
            "source": "r_128",
            "target": "s_653"
        },
        {
            "source": "r_129",
            "target": "s_612"
        },
        {
            "source": "r_129",
            "target": "s_1092"
        },
        {
            "source": "r_129",
            "target": "s_472"
        },
        {
            "source": "r_129",
            "target": "s_527"
        },
        {
            "source": "r_130",
            "target": "s_1369"
        },
        {
            "source": "r_130",
            "target": "s_1441"
        },
        {
            "source": "r_130",
            "target": "s_961"
        },
        {
            "source": "r_130",
            "target": "s_331"
        },
        {
            "source": "r_130",
            "target": "s_1191"
        },
        {
            "source": "r_131",
            "target": "s_372"
        },
        {
            "source": "r_131",
            "target": "s_829"
        },
        {
            "source": "r_131",
            "target": "s_1316"
        },
        {
            "source": "r_131",
            "target": "s_1266"
        },
        {
            "source": "r_131",
            "target": "s_1463"
        },
        {
            "source": "r_131",
            "target": "s_1484"
        },
        {
            "source": "r_132",
            "target": "s_907"
        },
        {
            "source": "r_133",
            "target": "s_791"
        },
        {
            "source": "r_133",
            "target": "s_534"
        },
        {
            "source": "r_133",
            "target": "s_1277"
        },
        {
            "source": "r_133",
            "target": "s_356"
        },
        {
            "source": "r_133",
            "target": "s_1491"
        },
        {
            "source": "r_133",
            "target": "s_723"
        },
        {
            "source": "r_133",
            "target": "s_567"
        },
        {
            "source": "r_133",
            "target": "s_1347"
        },
        {
            "source": "r_134",
            "target": "s_889"
        },
        {
            "source": "r_134",
            "target": "s_241"
        },
        {
            "source": "r_134",
            "target": "s_1390"
        },
        {
            "source": "r_134",
            "target": "s_511"
        },
        {
            "source": "r_134",
            "target": "s_500"
        },
        {
            "source": "r_134",
            "target": "s_1165"
        },
        {
            "source": "r_135",
            "target": "s_39"
        },
        {
            "source": "r_135",
            "target": "s_884"
        },
        {
            "source": "r_135",
            "target": "s_511"
        },
        {
            "source": "r_135",
            "target": "s_1038"
        },
        {
            "source": "r_136",
            "target": "s_1354"
        },
        {
            "source": "r_136",
            "target": "s_116"
        },
        {
            "source": "r_136",
            "target": "s_1381"
        },
        {
            "source": "r_136",
            "target": "s_98"
        },
        {
            "source": "r_137",
            "target": "s_161"
        },
        {
            "source": "r_137",
            "target": "s_688"
        },
        {
            "source": "r_137",
            "target": "s_1172"
        },
        {
            "source": "r_137",
            "target": "s_309"
        },
        {
            "source": "r_138",
            "target": "s_807"
        },
        {
            "source": "r_138",
            "target": "s_1422"
        },
        {
            "source": "r_138",
            "target": "s_131"
        },
        {
            "source": "r_139",
            "target": "s_738"
        },
        {
            "source": "r_139",
            "target": "s_756"
        },
        {
            "source": "r_140",
            "target": "s_738"
        },
        {
            "source": "r_140",
            "target": "s_657"
        },
        {
            "source": "r_140",
            "target": "s_756"
        },
        {
            "source": "r_141",
            "target": "s_1357"
        },
        {
            "source": "r_141",
            "target": "s_1142"
        },
        {
            "source": "r_141",
            "target": "s_496"
        },
        {
            "source": "r_141",
            "target": "s_1408"
        },
        {
            "source": "r_141",
            "target": "s_286"
        },
        {
            "source": "r_141",
            "target": "s_107"
        },
        {
            "source": "r_141",
            "target": "s_1086"
        },
        {
            "source": "r_141",
            "target": "s_908"
        },
        {
            "source": "r_141",
            "target": "s_1319"
        },
        {
            "source": "r_142",
            "target": "s_9"
        },
        {
            "source": "r_142",
            "target": "s_27"
        },
        {
            "source": "r_142",
            "target": "s_18"
        },
        {
            "source": "r_142",
            "target": "s_1214"
        },
        {
            "source": "r_142",
            "target": "s_45"
        },
        {
            "source": "r_142",
            "target": "s_1517"
        },
        {
            "source": "r_143",
            "target": "s_286"
        },
        {
            "source": "r_143",
            "target": "s_1454"
        },
        {
            "source": "r_143",
            "target": "s_1319"
        },
        {
            "source": "r_143",
            "target": "s_1229"
        },
        {
            "source": "r_143",
            "target": "s_1430"
        },
        {
            "source": "r_143",
            "target": "s_42"
        },
        {
            "source": "r_143",
            "target": "s_1358"
        },
        {
            "source": "r_144",
            "target": "s_221"
        },
        {
            "source": "r_144",
            "target": "s_1354"
        },
        {
            "source": "r_144",
            "target": "s_920"
        },
        {
            "source": "r_144",
            "target": "s_131"
        },
        {
            "source": "r_145",
            "target": "s_316"
        },
        {
            "source": "r_145",
            "target": "s_1069"
        },
        {
            "source": "r_145",
            "target": "s_1098"
        },
        {
            "source": "r_145",
            "target": "s_1027"
        },
        {
            "source": "r_145",
            "target": "s_1199"
        },
        {
            "source": "r_146",
            "target": "s_547"
        },
        {
            "source": "r_146",
            "target": "s_228"
        },
        {
            "source": "r_146",
            "target": "s_1334"
        },
        {
            "source": "r_146",
            "target": "s_511"
        },
        {
            "source": "r_147",
            "target": "s_467"
        },
        {
            "source": "r_147",
            "target": "s_1356"
        },
        {
            "source": "r_147",
            "target": "s_507"
        },
        {
            "source": "r_147",
            "target": "s_677"
        },
        {
            "source": "r_147",
            "target": "s_473"
        },
        {
            "source": "r_148",
            "target": "s_1218"
        },
        {
            "source": "r_148",
            "target": "s_895"
        },
        {
            "source": "r_148",
            "target": "s_326"
        },
        {
            "source": "r_148",
            "target": "s_99"
        },
        {
            "source": "r_148",
            "target": "s_85"
        },
        {
            "source": "r_149",
            "target": "s_391"
        },
        {
            "source": "r_149",
            "target": "s_981"
        },
        {
            "source": "r_149",
            "target": "s_112"
        },
        {
            "source": "r_149",
            "target": "s_711"
        },
        {
            "source": "r_149",
            "target": "s_251"
        },
        {
            "source": "r_150",
            "target": "s_873"
        },
        {
            "source": "r_150",
            "target": "s_98"
        },
        {
            "source": "r_151",
            "target": "s_648"
        },
        {
            "source": "r_151",
            "target": "s_665"
        },
        {
            "source": "r_151",
            "target": "s_1249"
        },
        {
            "source": "r_151",
            "target": "s_544"
        },
        {
            "source": "r_151",
            "target": "s_1118"
        },
        {
            "source": "r_151",
            "target": "s_89"
        },
        {
            "source": "r_151",
            "target": "s_67"
        },
        {
            "source": "r_151",
            "target": "s_1347"
        },
        {
            "source": "r_152",
            "target": "s_948"
        },
        {
            "source": "r_152",
            "target": "s_254"
        },
        {
            "source": "r_153",
            "target": "s_916"
        },
        {
            "source": "r_153",
            "target": "s_1396"
        },
        {
            "source": "r_153",
            "target": "s_253"
        },
        {
            "source": "r_153",
            "target": "s_1068"
        },
        {
            "source": "r_153",
            "target": "s_70"
        },
        {
            "source": "r_153",
            "target": "s_872"
        },
        {
            "source": "r_154",
            "target": "s_826"
        },
        {
            "source": "r_154",
            "target": "s_649"
        },
        {
            "source": "r_154",
            "target": "s_254"
        },
        {
            "source": "r_154",
            "target": "s_570"
        },
        {
            "source": "r_154",
            "target": "s_77"
        },
        {
            "source": "r_155",
            "target": "s_799"
        },
        {
            "source": "r_155",
            "target": "s_847"
        },
        {
            "source": "r_155",
            "target": "s_1209"
        },
        {
            "source": "r_155",
            "target": "s_1038"
        },
        {
            "source": "r_155",
            "target": "s_1522"
        },
        {
            "source": "r_156",
            "target": "s_491"
        },
        {
            "source": "r_156",
            "target": "s_316"
        },
        {
            "source": "r_156",
            "target": "s_505"
        },
        {
            "source": "r_156",
            "target": "s_772"
        },
        {
            "source": "r_156",
            "target": "s_882"
        },
        {
            "source": "r_156",
            "target": "s_1321"
        },
        {
            "source": "r_157",
            "target": "s_1173"
        },
        {
            "source": "r_157",
            "target": "s_1399"
        },
        {
            "source": "r_157",
            "target": "s_1034"
        },
        {
            "source": "r_157",
            "target": "s_219"
        },
        {
            "source": "r_158",
            "target": "s_653"
        },
        {
            "source": "r_158",
            "target": "s_932"
        },
        {
            "source": "r_158",
            "target": "s_690"
        },
        {
            "source": "r_158",
            "target": "s_511"
        },
        {
            "source": "r_158",
            "target": "s_1038"
        },
        {
            "source": "r_158",
            "target": "s_1279"
        },
        {
            "source": "r_159",
            "target": "s_42"
        },
        {
            "source": "r_159",
            "target": "s_183"
        },
        {
            "source": "r_159",
            "target": "s_590"
        },
        {
            "source": "r_159",
            "target": "s_721"
        },
        {
            "source": "r_160",
            "target": "s_414"
        },
        {
            "source": "r_160",
            "target": "s_633"
        },
        {
            "source": "r_160",
            "target": "s_873"
        },
        {
            "source": "r_160",
            "target": "s_1173"
        },
        {
            "source": "r_160",
            "target": "s_1288"
        },
        {
            "source": "r_160",
            "target": "s_892"
        },
        {
            "source": "r_161",
            "target": "s_624"
        },
        {
            "source": "r_161",
            "target": "s_150"
        },
        {
            "source": "r_162",
            "target": "s_11"
        },
        {
            "source": "r_162",
            "target": "s_1195"
        },
        {
            "source": "r_162",
            "target": "s_1197"
        },
        {
            "source": "r_162",
            "target": "s_232"
        },
        {
            "source": "r_162",
            "target": "s_1233"
        },
        {
            "source": "r_163",
            "target": "s_96"
        },
        {
            "source": "r_163",
            "target": "s_202"
        },
        {
            "source": "r_163",
            "target": "s_15"
        },
        {
            "source": "r_163",
            "target": "s_56"
        },
        {
            "source": "r_164",
            "target": "s_127"
        },
        {
            "source": "r_164",
            "target": "s_764"
        },
        {
            "source": "r_164",
            "target": "s_1127"
        },
        {
            "source": "r_164",
            "target": "s_1099"
        },
        {
            "source": "r_165",
            "target": "s_447"
        },
        {
            "source": "r_165",
            "target": "s_365"
        },
        {
            "source": "r_165",
            "target": "s_583"
        },
        {
            "source": "r_165",
            "target": "s_1429"
        },
        {
            "source": "r_165",
            "target": "s_1533"
        },
        {
            "source": "r_165",
            "target": "s_419"
        },
        {
            "source": "r_165",
            "target": "s_325"
        },
        {
            "source": "r_165",
            "target": "s_830"
        },
        {
            "source": "r_166",
            "target": "s_1221"
        },
        {
            "source": "r_166",
            "target": "s_14"
        },
        {
            "source": "r_166",
            "target": "s_829"
        },
        {
            "source": "r_167",
            "target": "s_937"
        },
        {
            "source": "r_167",
            "target": "s_121"
        },
        {
            "source": "r_167",
            "target": "s_914"
        },
        {
            "source": "r_167",
            "target": "s_132"
        },
        {
            "source": "r_167",
            "target": "s_875"
        },
        {
            "source": "r_168",
            "target": "s_116"
        },
        {
            "source": "r_168",
            "target": "s_611"
        },
        {
            "source": "r_168",
            "target": "s_962"
        },
        {
            "source": "r_168",
            "target": "s_235"
        },
        {
            "source": "r_168",
            "target": "s_1111"
        },
        {
            "source": "r_168",
            "target": "s_1485"
        },
        {
            "source": "r_168",
            "target": "s_1393"
        },
        {
            "source": "r_169",
            "target": "s_1357"
        },
        {
            "source": "r_169",
            "target": "s_1146"
        },
        {
            "source": "r_169",
            "target": "s_1039"
        },
        {
            "source": "r_169",
            "target": "s_1523"
        },
        {
            "source": "r_169",
            "target": "s_1242"
        },
        {
            "source": "r_169",
            "target": "s_107"
        },
        {
            "source": "r_169",
            "target": "s_377"
        },
        {
            "source": "r_169",
            "target": "s_908"
        },
        {
            "source": "r_169",
            "target": "s_1319"
        },
        {
            "source": "r_170",
            "target": "s_386"
        },
        {
            "source": "r_170",
            "target": "s_299"
        },
        {
            "source": "r_170",
            "target": "s_286"
        },
        {
            "source": "r_170",
            "target": "s_1433"
        },
        {
            "source": "r_170",
            "target": "s_1065"
        },
        {
            "source": "r_170",
            "target": "s_316"
        },
        {
            "source": "r_171",
            "target": "s_1077"
        },
        {
            "source": "r_171",
            "target": "s_1087"
        },
        {
            "source": "r_171",
            "target": "s_343"
        },
        {
            "source": "r_171",
            "target": "s_1518"
        },
        {
            "source": "r_171",
            "target": "s_1296"
        },
        {
            "source": "r_171",
            "target": "s_305"
        },
        {
            "source": "r_172",
            "target": "s_1056"
        },
        {
            "source": "r_172",
            "target": "s_1185"
        },
        {
            "source": "r_172",
            "target": "s_117"
        },
        {
            "source": "r_172",
            "target": "s_1387"
        },
        {
            "source": "r_172",
            "target": "s_563"
        },
        {
            "source": "r_172",
            "target": "s_551"
        },
        {
            "source": "r_172",
            "target": "s_1108"
        },
        {
            "source": "r_172",
            "target": "s_1419"
        },
        {
            "source": "r_173",
            "target": "s_763"
        },
        {
            "source": "r_173",
            "target": "s_629"
        },
        {
            "source": "r_173",
            "target": "s_922"
        },
        {
            "source": "r_173",
            "target": "s_24"
        },
        {
            "source": "r_173",
            "target": "s_1503"
        },
        {
            "source": "r_173",
            "target": "s_233"
        },
        {
            "source": "r_173",
            "target": "s_970"
        },
        {
            "source": "r_174",
            "target": "s_1000"
        },
        {
            "source": "r_174",
            "target": "s_201"
        },
        {
            "source": "r_174",
            "target": "s_445"
        },
        {
            "source": "r_174",
            "target": "s_1204"
        },
        {
            "source": "r_174",
            "target": "s_203"
        },
        {
            "source": "r_174",
            "target": "s_1081"
        },
        {
            "source": "r_174",
            "target": "s_138"
        },
        {
            "source": "r_174",
            "target": "s_434"
        },
        {
            "source": "r_175",
            "target": "s_316"
        },
        {
            "source": "r_175",
            "target": "s_718"
        },
        {
            "source": "r_175",
            "target": "s_804"
        },
        {
            "source": "r_175",
            "target": "s_1344"
        },
        {
            "source": "r_175",
            "target": "s_1181"
        },
        {
            "source": "r_176",
            "target": "s_1235"
        },
        {
            "source": "r_176",
            "target": "s_745"
        },
        {
            "source": "r_176",
            "target": "s_9"
        },
        {
            "source": "r_176",
            "target": "s_331"
        },
        {
            "source": "r_176",
            "target": "s_1096"
        },
        {
            "source": "r_176",
            "target": "s_1517"
        },
        {
            "source": "r_177",
            "target": "s_241"
        },
        {
            "source": "r_177",
            "target": "s_889"
        },
        {
            "source": "r_177",
            "target": "s_11"
        },
        {
            "source": "r_177",
            "target": "s_1195"
        },
        {
            "source": "r_177",
            "target": "s_1456"
        },
        {
            "source": "r_178",
            "target": "s_999"
        },
        {
            "source": "r_178",
            "target": "s_971"
        },
        {
            "source": "r_178",
            "target": "s_535"
        },
        {
            "source": "r_178",
            "target": "s_1038"
        },
        {
            "source": "r_179",
            "target": "s_1113"
        },
        {
            "source": "r_179",
            "target": "s_883"
        },
        {
            "source": "r_179",
            "target": "s_699"
        },
        {
            "source": "r_180",
            "target": "s_1320"
        },
        {
            "source": "r_180",
            "target": "s_661"
        },
        {
            "source": "r_181",
            "target": "s_928"
        },
        {
            "source": "r_181",
            "target": "s_1220"
        },
        {
            "source": "r_181",
            "target": "s_284"
        },
        {
            "source": "r_181",
            "target": "s_796"
        },
        {
            "source": "r_182",
            "target": "s_1045"
        },
        {
            "source": "r_182",
            "target": "s_1427"
        },
        {
            "source": "r_182",
            "target": "s_146"
        },
        {
            "source": "r_182",
            "target": "s_359"
        },
        {
            "source": "r_182",
            "target": "s_774"
        },
        {
            "source": "r_182",
            "target": "s_1507"
        },
        {
            "source": "r_182",
            "target": "s_348"
        },
        {
            "source": "r_183",
            "target": "s_1001"
        },
        {
            "source": "r_183",
            "target": "s_393"
        },
        {
            "source": "r_184",
            "target": "s_1329"
        },
        {
            "source": "r_184",
            "target": "s_1319"
        },
        {
            "source": "r_184",
            "target": "s_315"
        },
        {
            "source": "r_184",
            "target": "s_766"
        },
        {
            "source": "r_184",
            "target": "s_1137"
        },
        {
            "source": "r_184",
            "target": "s_763"
        },
        {
            "source": "r_185",
            "target": "s_78"
        },
        {
            "source": "r_185",
            "target": "s_489"
        },
        {
            "source": "r_185",
            "target": "s_1442"
        },
        {
            "source": "r_185",
            "target": "s_427"
        },
        {
            "source": "r_185",
            "target": "s_1124"
        },
        {
            "source": "r_185",
            "target": "s_345"
        },
        {
            "source": "r_186",
            "target": "s_436"
        },
        {
            "source": "r_186",
            "target": "s_267"
        },
        {
            "source": "r_186",
            "target": "s_1373"
        },
        {
            "source": "r_186",
            "target": "s_1524"
        },
        {
            "source": "r_186",
            "target": "s_734"
        },
        {
            "source": "r_186",
            "target": "s_131"
        },
        {
            "source": "r_187",
            "target": "s_1462"
        },
        {
            "source": "r_187",
            "target": "s_254"
        },
        {
            "source": "r_188",
            "target": "s_491"
        },
        {
            "source": "r_188",
            "target": "s_316"
        },
        {
            "source": "r_188",
            "target": "s_926"
        },
        {
            "source": "r_188",
            "target": "s_640"
        },
        {
            "source": "r_188",
            "target": "s_1488"
        },
        {
            "source": "r_189",
            "target": "s_1491"
        },
        {
            "source": "r_189",
            "target": "s_791"
        },
        {
            "source": "r_189",
            "target": "s_446"
        },
        {
            "source": "r_189",
            "target": "s_1277"
        },
        {
            "source": "r_189",
            "target": "s_367"
        },
        {
            "source": "r_189",
            "target": "s_567"
        },
        {
            "source": "r_189",
            "target": "s_1319"
        },
        {
            "source": "r_189",
            "target": "s_1347"
        },
        {
            "source": "r_190",
            "target": "s_1116"
        },
        {
            "source": "r_190",
            "target": "s_603"
        },
        {
            "source": "r_190",
            "target": "s_227"
        },
        {
            "source": "r_190",
            "target": "s_1335"
        },
        {
            "source": "r_190",
            "target": "s_644"
        },
        {
            "source": "r_190",
            "target": "s_461"
        },
        {
            "source": "r_190",
            "target": "s_1409"
        },
        {
            "source": "r_191",
            "target": "s_198"
        },
        {
            "source": "r_191",
            "target": "s_1308"
        },
        {
            "source": "r_191",
            "target": "s_571"
        },
        {
            "source": "r_191",
            "target": "s_155"
        },
        {
            "source": "r_191",
            "target": "s_844"
        },
        {
            "source": "r_191",
            "target": "s_133"
        },
        {
            "source": "r_191",
            "target": "s_63"
        },
        {
            "source": "r_192",
            "target": "s_1354"
        },
        {
            "source": "r_192",
            "target": "s_293"
        },
        {
            "source": "r_192",
            "target": "s_1066"
        },
        {
            "source": "r_192",
            "target": "s_173"
        },
        {
            "source": "r_192",
            "target": "s_255"
        },
        {
            "source": "r_193",
            "target": "s_1356"
        },
        {
            "source": "r_193",
            "target": "s_185"
        },
        {
            "source": "r_193",
            "target": "s_686"
        },
        {
            "source": "r_193",
            "target": "s_996"
        },
        {
            "source": "r_193",
            "target": "s_888"
        },
        {
            "source": "r_193",
            "target": "s_684"
        },
        {
            "source": "r_193",
            "target": "s_742"
        },
        {
            "source": "r_194",
            "target": "s_855"
        },
        {
            "source": "r_194",
            "target": "s_865"
        },
        {
            "source": "r_194",
            "target": "s_501"
        },
        {
            "source": "r_194",
            "target": "s_410"
        },
        {
            "source": "r_194",
            "target": "s_233"
        },
        {
            "source": "r_194",
            "target": "s_333"
        },
        {
            "source": "r_195",
            "target": "s_391"
        },
        {
            "source": "r_195",
            "target": "s_981"
        },
        {
            "source": "r_195",
            "target": "s_251"
        },
        {
            "source": "r_196",
            "target": "s_1422"
        },
        {
            "source": "r_196",
            "target": "s_1144"
        },
        {
            "source": "r_196",
            "target": "s_807"
        },
        {
            "source": "r_196",
            "target": "s_131"
        },
        {
            "source": "r_196",
            "target": "s_1393"
        },
        {
            "source": "r_197",
            "target": "s_799"
        },
        {
            "source": "r_197",
            "target": "s_1209"
        },
        {
            "source": "r_197",
            "target": "s_1279"
        },
        {
            "source": "r_197",
            "target": "s_1038"
        },
        {
            "source": "r_197",
            "target": "s_1522"
        },
        {
            "source": "r_198",
            "target": "s_1077"
        },
        {
            "source": "r_198",
            "target": "s_305"
        },
        {
            "source": "r_198",
            "target": "s_394"
        },
        {
            "source": "r_198",
            "target": "s_192"
        },
        {
            "source": "r_198",
            "target": "s_233"
        },
        {
            "source": "r_199",
            "target": "s_1492"
        },
        {
            "source": "r_199",
            "target": "s_202"
        },
        {
            "source": "r_200",
            "target": "s_627"
        },
        {
            "source": "r_200",
            "target": "s_1047"
        },
        {
            "source": "r_201",
            "target": "s_32"
        },
        {
            "source": "r_201",
            "target": "s_882"
        },
        {
            "source": "r_201",
            "target": "s_316"
        },
        {
            "source": "r_201",
            "target": "s_1119"
        },
        {
            "source": "r_201",
            "target": "s_139"
        },
        {
            "source": "r_201",
            "target": "s_98"
        },
        {
            "source": "r_202",
            "target": "s_1038"
        },
        {
            "source": "r_202",
            "target": "s_444"
        },
        {
            "source": "r_202",
            "target": "s_3"
        },
        {
            "source": "r_202",
            "target": "s_511"
        },
        {
            "source": "r_203",
            "target": "s_435"
        },
        {
            "source": "r_203",
            "target": "s_382"
        },
        {
            "source": "r_203",
            "target": "s_1484"
        },
        {
            "source": "r_203",
            "target": "s_275"
        },
        {
            "source": "r_204",
            "target": "s_404"
        },
        {
            "source": "r_204",
            "target": "s_1391"
        },
        {
            "source": "r_204",
            "target": "s_1304"
        },
        {
            "source": "r_204",
            "target": "s_843"
        },
        {
            "source": "r_204",
            "target": "s_314"
        },
        {
            "source": "r_205",
            "target": "s_52"
        },
        {
            "source": "r_205",
            "target": "s_721"
        },
        {
            "source": "r_205",
            "target": "s_1191"
        },
        {
            "source": "r_205",
            "target": "s_605"
        },
        {
            "source": "r_205",
            "target": "s_952"
        },
        {
            "source": "r_205",
            "target": "s_949"
        },
        {
            "source": "r_206",
            "target": "s_211"
        },
        {
            "source": "r_206",
            "target": "s_1337"
        },
        {
            "source": "r_206",
            "target": "s_1531"
        },
        {
            "source": "r_206",
            "target": "s_1169"
        },
        {
            "source": "r_206",
            "target": "s_1038"
        },
        {
            "source": "r_206",
            "target": "s_1261"
        },
        {
            "source": "r_206",
            "target": "s_989"
        },
        {
            "source": "r_207",
            "target": "s_509"
        },
        {
            "source": "r_207",
            "target": "s_642"
        },
        {
            "source": "r_207",
            "target": "s_800"
        },
        {
            "source": "r_207",
            "target": "s_829"
        },
        {
            "source": "r_208",
            "target": "s_1244"
        },
        {
            "source": "r_209",
            "target": "s_321"
        },
        {
            "source": "r_209",
            "target": "s_909"
        },
        {
            "source": "r_209",
            "target": "s_342"
        },
        {
            "source": "r_209",
            "target": "s_584"
        },
        {
            "source": "r_209",
            "target": "s_103"
        },
        {
            "source": "r_209",
            "target": "s_1443"
        },
        {
            "source": "r_210",
            "target": "s_102"
        },
        {
            "source": "r_210",
            "target": "s_700"
        },
        {
            "source": "r_210",
            "target": "s_202"
        },
        {
            "source": "r_210",
            "target": "s_1461"
        },
        {
            "source": "r_211",
            "target": "s_511"
        },
        {
            "source": "r_211",
            "target": "s_893"
        },
        {
            "source": "r_211",
            "target": "s_728"
        },
        {
            "source": "r_211",
            "target": "s_85"
        },
        {
            "source": "r_212",
            "target": "s_1083"
        },
        {
            "source": "r_212",
            "target": "s_885"
        },
        {
            "source": "r_212",
            "target": "s_145"
        },
        {
            "source": "r_212",
            "target": "s_219"
        },
        {
            "source": "r_213",
            "target": "s_1413"
        },
        {
            "source": "r_213",
            "target": "s_1226"
        },
        {
            "source": "r_214",
            "target": "s_680"
        },
        {
            "source": "r_214",
            "target": "s_1042"
        },
        {
            "source": "r_214",
            "target": "s_1517"
        },
        {
            "source": "r_214",
            "target": "s_1338"
        },
        {
            "source": "r_214",
            "target": "s_572"
        },
        {
            "source": "r_214",
            "target": "s_587"
        },
        {
            "source": "r_214",
            "target": "s_1072"
        },
        {
            "source": "r_214",
            "target": "s_311"
        },
        {
            "source": "r_215",
            "target": "s_19"
        },
        {
            "source": "r_215",
            "target": "s_172"
        },
        {
            "source": "r_215",
            "target": "s_1124"
        },
        {
            "source": "r_215",
            "target": "s_405"
        },
        {
            "source": "r_215",
            "target": "s_1150"
        },
        {
            "source": "r_215",
            "target": "s_49"
        },
        {
            "source": "r_215",
            "target": "s_1031"
        },
        {
            "source": "r_216",
            "target": "s_492"
        },
        {
            "source": "r_216",
            "target": "s_1332"
        },
        {
            "source": "r_216",
            "target": "s_986"
        },
        {
            "source": "r_216",
            "target": "s_651"
        },
        {
            "source": "r_216",
            "target": "s_930"
        },
        {
            "source": "r_216",
            "target": "s_131"
        },
        {
            "source": "r_216",
            "target": "s_1470"
        },
        {
            "source": "r_217",
            "target": "s_327"
        },
        {
            "source": "r_217",
            "target": "s_373"
        },
        {
            "source": "r_217",
            "target": "s_1054"
        },
        {
            "source": "r_217",
            "target": "s_1262"
        },
        {
            "source": "r_217",
            "target": "s_1274"
        },
        {
            "source": "r_217",
            "target": "s_892"
        },
        {
            "source": "r_217",
            "target": "s_1060"
        },
        {
            "source": "r_218",
            "target": "s_804"
        },
        {
            "source": "r_218",
            "target": "s_1116"
        },
        {
            "source": "r_218",
            "target": "s_746"
        },
        {
            "source": "r_218",
            "target": "s_854"
        },
        {
            "source": "r_218",
            "target": "s_404"
        },
        {
            "source": "r_218",
            "target": "s_314"
        },
        {
            "source": "r_218",
            "target": "s_721"
        },
        {
            "source": "r_219",
            "target": "s_1011"
        },
        {
            "source": "r_219",
            "target": "s_704"
        },
        {
            "source": "r_219",
            "target": "s_1490"
        },
        {
            "source": "r_219",
            "target": "s_393"
        },
        {
            "source": "r_220",
            "target": "s_206"
        },
        {
            "source": "r_220",
            "target": "s_749"
        },
        {
            "source": "r_220",
            "target": "s_952"
        },
        {
            "source": "r_221",
            "target": "s_281"
        },
        {
            "source": "r_221",
            "target": "s_812"
        },
        {
            "source": "r_221",
            "target": "s_1319"
        },
        {
            "source": "r_221",
            "target": "s_454"
        },
        {
            "source": "r_221",
            "target": "s_1050"
        },
        {
            "source": "r_221",
            "target": "s_1494"
        },
        {
            "source": "r_221",
            "target": "s_699"
        },
        {
            "source": "r_222",
            "target": "s_1278"
        },
        {
            "source": "r_222",
            "target": "s_556"
        },
        {
            "source": "r_222",
            "target": "s_393"
        },
        {
            "source": "r_222",
            "target": "s_641"
        },
        {
            "source": "r_223",
            "target": "s_865"
        },
        {
            "source": "r_223",
            "target": "s_333"
        },
        {
            "source": "r_224",
            "target": "s_1065"
        },
        {
            "source": "r_224",
            "target": "s_1137"
        },
        {
            "source": "r_224",
            "target": "s_763"
        },
        {
            "source": "r_225",
            "target": "s_613"
        },
        {
            "source": "r_225",
            "target": "s_819"
        },
        {
            "source": "r_225",
            "target": "s_886"
        },
        {
            "source": "r_226",
            "target": "s_873"
        },
        {
            "source": "r_226",
            "target": "s_98"
        },
        {
            "source": "r_226",
            "target": "s_74"
        },
        {
            "source": "r_227",
            "target": "s_3"
        },
        {
            "source": "r_227",
            "target": "s_344"
        },
        {
            "source": "r_227",
            "target": "s_1038"
        },
        {
            "source": "r_228",
            "target": "s_1230"
        },
        {
            "source": "r_228",
            "target": "s_779"
        },
        {
            "source": "r_228",
            "target": "s_61"
        },
        {
            "source": "r_228",
            "target": "s_1320"
        },
        {
            "source": "r_229",
            "target": "s_492"
        },
        {
            "source": "r_229",
            "target": "s_1282"
        },
        {
            "source": "r_229",
            "target": "s_1436"
        },
        {
            "source": "r_229",
            "target": "s_86"
        },
        {
            "source": "r_229",
            "target": "s_1514"
        },
        {
            "source": "r_229",
            "target": "s_1470"
        },
        {
            "source": "r_230",
            "target": "s_679"
        },
        {
            "source": "r_230",
            "target": "s_1319"
        },
        {
            "source": "r_230",
            "target": "s_1138"
        },
        {
            "source": "r_230",
            "target": "s_975"
        },
        {
            "source": "r_230",
            "target": "s_1076"
        },
        {
            "source": "r_230",
            "target": "s_554"
        },
        {
            "source": "r_230",
            "target": "s_1257"
        },
        {
            "source": "r_230",
            "target": "s_269"
        },
        {
            "source": "r_230",
            "target": "s_1044"
        },
        {
            "source": "r_231",
            "target": "s_1115"
        },
        {
            "source": "r_231",
            "target": "s_1072"
        },
        {
            "source": "r_231",
            "target": "s_756"
        },
        {
            "source": "r_232",
            "target": "s_919"
        },
        {
            "source": "r_232",
            "target": "s_254"
        },
        {
            "source": "r_232",
            "target": "s_177"
        },
        {
            "source": "r_232",
            "target": "s_805"
        },
        {
            "source": "r_232",
            "target": "s_324"
        },
        {
            "source": "r_232",
            "target": "s_750"
        },
        {
            "source": "r_233",
            "target": "s_153"
        },
        {
            "source": "r_233",
            "target": "s_788"
        },
        {
            "source": "r_233",
            "target": "s_363"
        },
        {
            "source": "r_233",
            "target": "s_1379"
        },
        {
            "source": "r_233",
            "target": "s_50"
        },
        {
            "source": "r_233",
            "target": "s_896"
        },
        {
            "source": "r_234",
            "target": "s_1329"
        },
        {
            "source": "r_234",
            "target": "s_705"
        },
        {
            "source": "r_234",
            "target": "s_254"
        },
        {
            "source": "r_235",
            "target": "s_16"
        },
        {
            "source": "r_235",
            "target": "s_399"
        },
        {
            "source": "r_236",
            "target": "s_863"
        },
        {
            "source": "r_236",
            "target": "s_469"
        },
        {
            "source": "r_236",
            "target": "s_920"
        },
        {
            "source": "r_236",
            "target": "s_359"
        },
        {
            "source": "r_236",
            "target": "s_122"
        },
        {
            "source": "r_236",
            "target": "s_1153"
        },
        {
            "source": "r_236",
            "target": "s_1256"
        },
        {
            "source": "r_236",
            "target": "s_131"
        },
        {
            "source": "r_237",
            "target": "s_607"
        },
        {
            "source": "r_237",
            "target": "s_531"
        },
        {
            "source": "r_237",
            "target": "s_927"
        },
        {
            "source": "r_238",
            "target": "s_123"
        },
        {
            "source": "r_238",
            "target": "s_96"
        },
        {
            "source": "r_238",
            "target": "s_618"
        },
        {
            "source": "r_238",
            "target": "s_15"
        },
        {
            "source": "r_238",
            "target": "s_202"
        },
        {
            "source": "r_238",
            "target": "s_56"
        },
        {
            "source": "r_239",
            "target": "s_654"
        },
        {
            "source": "r_239",
            "target": "s_931"
        },
        {
            "source": "r_239",
            "target": "s_30"
        },
        {
            "source": "r_239",
            "target": "s_1490"
        },
        {
            "source": "r_239",
            "target": "s_767"
        },
        {
            "source": "r_239",
            "target": "s_1104"
        },
        {
            "source": "r_239",
            "target": "s_574"
        },
        {
            "source": "r_239",
            "target": "s_393"
        },
        {
            "source": "r_240",
            "target": "s_331"
        },
        {
            "source": "r_240",
            "target": "s_1128"
        },
        {
            "source": "r_240",
            "target": "s_1096"
        },
        {
            "source": "r_241",
            "target": "s_346"
        },
        {
            "source": "r_241",
            "target": "s_605"
        },
        {
            "source": "r_241",
            "target": "s_1359"
        },
        {
            "source": "r_241",
            "target": "s_952"
        },
        {
            "source": "r_241",
            "target": "s_1212"
        },
        {
            "source": "r_242",
            "target": "s_1354"
        },
        {
            "source": "r_242",
            "target": "s_293"
        },
        {
            "source": "r_242",
            "target": "s_1175"
        },
        {
            "source": "r_242",
            "target": "s_173"
        },
        {
            "source": "r_242",
            "target": "s_255"
        },
        {
            "source": "r_243",
            "target": "s_740"
        },
        {
            "source": "r_243",
            "target": "s_946"
        },
        {
            "source": "r_243",
            "target": "s_531"
        },
        {
            "source": "r_243",
            "target": "s_290"
        },
        {
            "source": "r_244",
            "target": "s_87"
        },
        {
            "source": "r_244",
            "target": "s_849"
        },
        {
            "source": "r_244",
            "target": "s_1284"
        },
        {
            "source": "r_244",
            "target": "s_473"
        },
        {
            "source": "r_244",
            "target": "s_641"
        },
        {
            "source": "r_244",
            "target": "s_393"
        },
        {
            "source": "r_245",
            "target": "s_281"
        },
        {
            "source": "r_245",
            "target": "s_1319"
        },
        {
            "source": "r_245",
            "target": "s_812"
        },
        {
            "source": "r_245",
            "target": "s_352"
        },
        {
            "source": "r_245",
            "target": "s_454"
        },
        {
            "source": "r_245",
            "target": "s_1050"
        },
        {
            "source": "r_245",
            "target": "s_189"
        },
        {
            "source": "r_245",
            "target": "s_699"
        },
        {
            "source": "r_246",
            "target": "s_344"
        },
        {
            "source": "r_246",
            "target": "s_140"
        },
        {
            "source": "r_246",
            "target": "s_1131"
        },
        {
            "source": "r_246",
            "target": "s_1207"
        },
        {
            "source": "r_246",
            "target": "s_1166"
        },
        {
            "source": "r_247",
            "target": "s_481"
        },
        {
            "source": "r_247",
            "target": "s_645"
        },
        {
            "source": "r_247",
            "target": "s_345"
        },
        {
            "source": "r_247",
            "target": "s_188"
        },
        {
            "source": "r_247",
            "target": "s_1124"
        },
        {
            "source": "r_247",
            "target": "s_489"
        },
        {
            "source": "r_248",
            "target": "s_1110"
        },
        {
            "source": "r_248",
            "target": "s_1206"
        },
        {
            "source": "r_249",
            "target": "s_1205"
        },
        {
            "source": "r_249",
            "target": "s_1399"
        },
        {
            "source": "r_249",
            "target": "s_595"
        },
        {
            "source": "r_249",
            "target": "s_354"
        },
        {
            "source": "r_249",
            "target": "s_1244"
        },
        {
            "source": "r_249",
            "target": "s_231"
        },
        {
            "source": "r_250",
            "target": "s_633"
        },
        {
            "source": "r_250",
            "target": "s_414"
        },
        {
            "source": "r_250",
            "target": "s_1323"
        },
        {
            "source": "r_250",
            "target": "s_132"
        },
        {
            "source": "r_250",
            "target": "s_873"
        },
        {
            "source": "r_250",
            "target": "s_892"
        },
        {
            "source": "r_251",
            "target": "s_381"
        },
        {
            "source": "r_251",
            "target": "s_386"
        },
        {
            "source": "r_251",
            "target": "s_286"
        },
        {
            "source": "r_251",
            "target": "s_1223"
        },
        {
            "source": "r_251",
            "target": "s_316"
        },
        {
            "source": "r_252",
            "target": "s_1259"
        },
        {
            "source": "r_252",
            "target": "s_1250"
        },
        {
            "source": "r_252",
            "target": "s_553"
        },
        {
            "source": "r_252",
            "target": "s_517"
        },
        {
            "source": "r_252",
            "target": "s_1386"
        },
        {
            "source": "r_252",
            "target": "s_699"
        },
        {
            "source": "r_253",
            "target": "s_1249"
        },
        {
            "source": "r_253",
            "target": "s_648"
        },
        {
            "source": "r_253",
            "target": "s_110"
        },
        {
            "source": "r_253",
            "target": "s_336"
        },
        {
            "source": "r_253",
            "target": "s_7"
        },
        {
            "source": "r_253",
            "target": "s_1118"
        },
        {
            "source": "r_253",
            "target": "s_89"
        },
        {
            "source": "r_253",
            "target": "s_1347"
        },
        {
            "source": "r_254",
            "target": "s_674"
        },
        {
            "source": "r_254",
            "target": "s_73"
        },
        {
            "source": "r_254",
            "target": "s_275"
        },
        {
            "source": "r_254",
            "target": "s_1447"
        },
        {
            "source": "r_254",
            "target": "s_512"
        },
        {
            "source": "r_255",
            "target": "s_28"
        },
        {
            "source": "r_255",
            "target": "s_1004"
        },
        {
            "source": "r_255",
            "target": "s_199"
        },
        {
            "source": "r_255",
            "target": "s_651"
        },
        {
            "source": "r_255",
            "target": "s_892"
        },
        {
            "source": "r_256",
            "target": "s_432"
        },
        {
            "source": "r_256",
            "target": "s_32"
        },
        {
            "source": "r_256",
            "target": "s_1082"
        },
        {
            "source": "r_256",
            "target": "s_401"
        },
        {
            "source": "r_256",
            "target": "s_301"
        },
        {
            "source": "r_256",
            "target": "s_1085"
        },
        {
            "source": "r_257",
            "target": "s_948"
        },
        {
            "source": "r_257",
            "target": "s_254"
        },
        {
            "source": "r_258",
            "target": "s_473"
        },
        {
            "source": "r_258",
            "target": "s_5"
        },
        {
            "source": "r_258",
            "target": "s_1009"
        },
        {
            "source": "r_259",
            "target": "s_738"
        },
        {
            "source": "r_259",
            "target": "s_756"
        },
        {
            "source": "r_260",
            "target": "s_1103"
        },
        {
            "source": "r_260",
            "target": "s_1258"
        },
        {
            "source": "r_260",
            "target": "s_290"
        },
        {
            "source": "r_260",
            "target": "s_103"
        },
        {
            "source": "r_261",
            "target": "s_720"
        },
        {
            "source": "r_261",
            "target": "s_1476"
        },
        {
            "source": "r_261",
            "target": "s_752"
        },
        {
            "source": "r_261",
            "target": "s_173"
        },
        {
            "source": "r_262",
            "target": "s_804"
        },
        {
            "source": "r_262",
            "target": "s_1510"
        },
        {
            "source": "r_262",
            "target": "s_861"
        },
        {
            "source": "r_262",
            "target": "s_1325"
        },
        {
            "source": "r_262",
            "target": "s_1301"
        },
        {
            "source": "r_262",
            "target": "s_1251"
        },
        {
            "source": "r_262",
            "target": "s_1412"
        },
        {
            "source": "r_262",
            "target": "s_1059"
        },
        {
            "source": "r_262",
            "target": "s_721"
        },
        {
            "source": "r_263",
            "target": "s_57"
        },
        {
            "source": "r_263",
            "target": "s_638"
        },
        {
            "source": "r_263",
            "target": "s_729"
        },
        {
            "source": "r_263",
            "target": "s_593"
        },
        {
            "source": "r_263",
            "target": "s_75"
        },
        {
            "source": "r_263",
            "target": "s_1424"
        },
        {
            "source": "r_264",
            "target": "s_1150"
        },
        {
            "source": "r_264",
            "target": "s_1134"
        },
        {
            "source": "r_264",
            "target": "s_265"
        },
        {
            "source": "r_265",
            "target": "s_1444"
        },
        {
            "source": "r_265",
            "target": "s_1319"
        },
        {
            "source": "r_265",
            "target": "s_286"
        },
        {
            "source": "r_265",
            "target": "s_1380"
        },
        {
            "source": "r_265",
            "target": "s_263"
        },
        {
            "source": "r_265",
            "target": "s_1140"
        },
        {
            "source": "r_265",
            "target": "s_67"
        },
        {
            "source": "r_265",
            "target": "s_511"
        },
        {
            "source": "r_266",
            "target": "s_1423"
        },
        {
            "source": "r_266",
            "target": "s_780"
        },
        {
            "source": "r_266",
            "target": "s_1511"
        },
        {
            "source": "r_266",
            "target": "s_1492"
        },
        {
            "source": "r_266",
            "target": "s_1302"
        },
        {
            "source": "r_266",
            "target": "s_202"
        },
        {
            "source": "r_267",
            "target": "s_977"
        },
        {
            "source": "r_267",
            "target": "s_1527"
        },
        {
            "source": "r_267",
            "target": "s_1399"
        },
        {
            "source": "r_267",
            "target": "s_236"
        },
        {
            "source": "r_268",
            "target": "s_1048"
        },
        {
            "source": "r_268",
            "target": "s_576"
        },
        {
            "source": "r_268",
            "target": "s_1349"
        },
        {
            "source": "r_268",
            "target": "s_1246"
        },
        {
            "source": "r_269",
            "target": "s_1366"
        },
        {
            "source": "r_269",
            "target": "s_868"
        },
        {
            "source": "r_269",
            "target": "s_418"
        },
        {
            "source": "r_269",
            "target": "s_591"
        },
        {
            "source": "r_269",
            "target": "s_286"
        },
        {
            "source": "r_269",
            "target": "s_81"
        },
        {
            "source": "r_269",
            "target": "s_1319"
        },
        {
            "source": "r_270",
            "target": "s_762"
        },
        {
            "source": "r_270",
            "target": "s_602"
        },
        {
            "source": "r_270",
            "target": "s_829"
        },
        {
            "source": "r_271",
            "target": "s_898"
        },
        {
            "source": "r_271",
            "target": "s_586"
        },
        {
            "source": "r_271",
            "target": "s_798"
        },
        {
            "source": "r_271",
            "target": "s_56"
        },
        {
            "source": "r_271",
            "target": "s_15"
        },
        {
            "source": "r_271",
            "target": "s_202"
        },
        {
            "source": "r_272",
            "target": "s_1162"
        },
        {
            "source": "r_272",
            "target": "s_757"
        },
        {
            "source": "r_272",
            "target": "s_683"
        },
        {
            "source": "r_272",
            "target": "s_790"
        },
        {
            "source": "r_273",
            "target": "s_1275"
        },
        {
            "source": "r_273",
            "target": "s_150"
        },
        {
            "source": "r_274",
            "target": "s_440"
        },
        {
            "source": "r_274",
            "target": "s_502"
        },
        {
            "source": "r_274",
            "target": "s_1191"
        },
        {
            "source": "r_274",
            "target": "s_543"
        },
        {
            "source": "r_275",
            "target": "s_211"
        },
        {
            "source": "r_275",
            "target": "s_1169"
        },
        {
            "source": "r_275",
            "target": "s_1038"
        },
        {
            "source": "r_275",
            "target": "s_828"
        },
        {
            "source": "r_275",
            "target": "s_3"
        },
        {
            "source": "r_276",
            "target": "s_604"
        },
        {
            "source": "r_276",
            "target": "s_1177"
        },
        {
            "source": "r_276",
            "target": "s_333"
        },
        {
            "source": "r_276",
            "target": "s_894"
        },
        {
            "source": "r_277",
            "target": "s_1377"
        },
        {
            "source": "r_277",
            "target": "s_1266"
        },
        {
            "source": "r_277",
            "target": "s_1496"
        },
        {
            "source": "r_278",
            "target": "s_210"
        },
        {
            "source": "r_278",
            "target": "s_1191"
        },
        {
            "source": "r_278",
            "target": "s_1182"
        },
        {
            "source": "r_279",
            "target": "s_918"
        },
        {
            "source": "r_279",
            "target": "s_646"
        },
        {
            "source": "r_279",
            "target": "s_530"
        },
        {
            "source": "r_279",
            "target": "s_1317"
        },
        {
            "source": "r_279",
            "target": "s_475"
        },
        {
            "source": "r_279",
            "target": "s_215"
        },
        {
            "source": "r_279",
            "target": "s_181"
        },
        {
            "source": "r_279",
            "target": "s_1274"
        },
        {
            "source": "r_280",
            "target": "s_102"
        },
        {
            "source": "r_280",
            "target": "s_202"
        },
        {
            "source": "r_280",
            "target": "s_1461"
        },
        {
            "source": "r_281",
            "target": "s_65"
        },
        {
            "source": "r_281",
            "target": "s_1391"
        },
        {
            "source": "r_281",
            "target": "s_1174"
        },
        {
            "source": "r_281",
            "target": "s_404"
        },
        {
            "source": "r_281",
            "target": "s_314"
        },
        {
            "source": "r_282",
            "target": "s_0"
        },
        {
            "source": "r_282",
            "target": "s_1294"
        },
        {
            "source": "r_282",
            "target": "s_305"
        },
        {
            "source": "r_282",
            "target": "s_1314"
        },
        {
            "source": "r_282",
            "target": "s_333"
        },
        {
            "source": "r_283",
            "target": "s_1195"
        },
        {
            "source": "r_283",
            "target": "s_11"
        },
        {
            "source": "r_283",
            "target": "s_479"
        },
        {
            "source": "r_283",
            "target": "s_488"
        },
        {
            "source": "r_283",
            "target": "s_409"
        },
        {
            "source": "r_284",
            "target": "s_938"
        },
        {
            "source": "r_284",
            "target": "s_294"
        },
        {
            "source": "r_284",
            "target": "s_805"
        },
        {
            "source": "r_284",
            "target": "s_254"
        },
        {
            "source": "r_285",
            "target": "s_540"
        },
        {
            "source": "r_285",
            "target": "s_617"
        },
        {
            "source": "r_285",
            "target": "s_1366"
        },
        {
            "source": "r_285",
            "target": "s_736"
        },
        {
            "source": "r_285",
            "target": "s_131"
        },
        {
            "source": "r_286",
            "target": "s_824"
        },
        {
            "source": "r_286",
            "target": "s_550"
        },
        {
            "source": "r_286",
            "target": "s_538"
        },
        {
            "source": "r_287",
            "target": "s_547"
        },
        {
            "source": "r_287",
            "target": "s_1168"
        },
        {
            "source": "r_287",
            "target": "s_558"
        },
        {
            "source": "r_287",
            "target": "s_598"
        },
        {
            "source": "r_287",
            "target": "s_821"
        },
        {
            "source": "r_287",
            "target": "s_90"
        },
        {
            "source": "r_287",
            "target": "s_511"
        },
        {
            "source": "r_288",
            "target": "s_1474"
        },
        {
            "source": "r_288",
            "target": "s_1052"
        },
        {
            "source": "r_288",
            "target": "s_1093"
        },
        {
            "source": "r_288",
            "target": "s_469"
        },
        {
            "source": "r_288",
            "target": "s_131"
        },
        {
            "source": "r_288",
            "target": "s_920"
        },
        {
            "source": "r_289",
            "target": "s_317"
        },
        {
            "source": "r_289",
            "target": "s_338"
        },
        {
            "source": "r_289",
            "target": "s_859"
        },
        {
            "source": "r_290",
            "target": "s_1235"
        },
        {
            "source": "r_290",
            "target": "s_170"
        },
        {
            "source": "r_290",
            "target": "s_1028"
        },
        {
            "source": "r_290",
            "target": "s_331"
        },
        {
            "source": "r_290",
            "target": "s_845"
        },
        {
            "source": "r_290",
            "target": "s_1260"
        },
        {
            "source": "r_291",
            "target": "s_1418"
        },
        {
            "source": "r_291",
            "target": "s_1347"
        },
        {
            "source": "r_291",
            "target": "s_316"
        },
        {
            "source": "r_291",
            "target": "s_285"
        },
        {
            "source": "r_291",
            "target": "s_131"
        },
        {
            "source": "r_292",
            "target": "s_1532"
        },
        {
            "source": "r_292",
            "target": "s_1411"
        },
        {
            "source": "r_292",
            "target": "s_514"
        },
        {
            "source": "r_292",
            "target": "s_638"
        },
        {
            "source": "r_292",
            "target": "s_1126"
        },
        {
            "source": "r_292",
            "target": "s_57"
        },
        {
            "source": "r_292",
            "target": "s_1040"
        },
        {
            "source": "r_292",
            "target": "s_593"
        },
        {
            "source": "r_292",
            "target": "s_134"
        },
        {
            "source": "r_293",
            "target": "s_684"
        },
        {
            "source": "r_293",
            "target": "s_888"
        },
        {
            "source": "r_293",
            "target": "s_186"
        },
        {
            "source": "r_293",
            "target": "s_1483"
        },
        {
            "source": "r_293",
            "target": "s_1018"
        },
        {
            "source": "r_293",
            "target": "s_787"
        },
        {
            "source": "r_293",
            "target": "s_1517"
        },
        {
            "source": "r_294",
            "target": "s_51"
        },
        {
            "source": "r_294",
            "target": "s_383"
        },
        {
            "source": "r_294",
            "target": "s_411"
        },
        {
            "source": "r_294",
            "target": "s_262"
        },
        {
            "source": "r_294",
            "target": "s_360"
        },
        {
            "source": "r_294",
            "target": "s_1135"
        },
        {
            "source": "r_295",
            "target": "s_32"
        },
        {
            "source": "r_295",
            "target": "s_55"
        },
        {
            "source": "r_295",
            "target": "s_23"
        },
        {
            "source": "r_295",
            "target": "s_719"
        },
        {
            "source": "r_295",
            "target": "s_1263"
        },
        {
            "source": "r_295",
            "target": "s_1114"
        },
        {
            "source": "r_296",
            "target": "s_241"
        },
        {
            "source": "r_296",
            "target": "s_40"
        },
        {
            "source": "r_296",
            "target": "s_511"
        },
        {
            "source": "r_297",
            "target": "s_583"
        },
        {
            "source": "r_297",
            "target": "s_10"
        },
        {
            "source": "r_297",
            "target": "s_1499"
        },
        {
            "source": "r_297",
            "target": "s_773"
        },
        {
            "source": "r_298",
            "target": "s_6"
        },
        {
            "source": "r_298",
            "target": "s_564"
        },
        {
            "source": "r_298",
            "target": "s_327"
        },
        {
            "source": "r_298",
            "target": "s_11"
        },
        {
            "source": "r_298",
            "target": "s_1195"
        },
        {
            "source": "r_298",
            "target": "s_1014"
        },
        {
            "source": "r_298",
            "target": "s_81"
        },
        {
            "source": "r_299",
            "target": "s_840"
        },
        {
            "source": "r_299",
            "target": "s_990"
        },
        {
            "source": "r_299",
            "target": "s_709"
        },
        {
            "source": "r_299",
            "target": "s_1312"
        },
        {
            "source": "r_299",
            "target": "s_817"
        },
        {
            "source": "r_299",
            "target": "s_378"
        },
        {
            "source": "r_299",
            "target": "s_2"
        },
        {
            "source": "r_300",
            "target": "s_1434"
        },
        {
            "source": "r_300",
            "target": "s_825"
        },
        {
            "source": "r_300",
            "target": "s_785"
        },
        {
            "source": "r_300",
            "target": "s_295"
        },
        {
            "source": "r_300",
            "target": "s_17"
        },
        {
            "source": "r_300",
            "target": "s_118"
        },
        {
            "source": "r_300",
            "target": "s_273"
        },
        {
            "source": "r_301",
            "target": "s_327"
        },
        {
            "source": "r_301",
            "target": "s_373"
        },
        {
            "source": "r_301",
            "target": "s_323"
        },
        {
            "source": "r_301",
            "target": "s_1274"
        },
        {
            "source": "r_301",
            "target": "s_1262"
        },
        {
            "source": "r_301",
            "target": "s_257"
        },
        {
            "source": "r_302",
            "target": "s_57"
        },
        {
            "source": "r_302",
            "target": "s_638"
        },
        {
            "source": "r_302",
            "target": "s_1365"
        },
        {
            "source": "r_302",
            "target": "s_869"
        },
        {
            "source": "r_302",
            "target": "s_1450"
        },
        {
            "source": "r_302",
            "target": "s_593"
        },
        {
            "source": "r_303",
            "target": "s_1069"
        },
        {
            "source": "r_303",
            "target": "s_316"
        },
        {
            "source": "r_303",
            "target": "s_1065"
        },
        {
            "source": "r_303",
            "target": "s_41"
        },
        {
            "source": "r_303",
            "target": "s_1515"
        },
        {
            "source": "r_303",
            "target": "s_763"
        },
        {
            "source": "r_303",
            "target": "s_1199"
        },
        {
            "source": "r_304",
            "target": "s_1384"
        },
        {
            "source": "r_304",
            "target": "s_970"
        },
        {
            "source": "r_304",
            "target": "s_797"
        },
        {
            "source": "r_305",
            "target": "s_1491"
        },
        {
            "source": "r_305",
            "target": "s_1173"
        },
        {
            "source": "r_305",
            "target": "s_1347"
        },
        {
            "source": "r_306",
            "target": "s_959"
        },
        {
            "source": "r_306",
            "target": "s_600"
        },
        {
            "source": "r_306",
            "target": "s_473"
        },
        {
            "source": "r_307",
            "target": "s_351"
        },
        {
            "source": "r_307",
            "target": "s_1120"
        },
        {
            "source": "r_307",
            "target": "s_1475"
        },
        {
            "source": "r_307",
            "target": "s_973"
        },
        {
            "source": "r_307",
            "target": "s_661"
        },
        {
            "source": "r_308",
            "target": "s_1266"
        },
        {
            "source": "r_308",
            "target": "s_478"
        },
        {
            "source": "r_309",
            "target": "s_214"
        },
        {
            "source": "r_309",
            "target": "s_746"
        },
        {
            "source": "r_309",
            "target": "s_1304"
        },
        {
            "source": "r_309",
            "target": "s_404"
        },
        {
            "source": "r_309",
            "target": "s_289"
        },
        {
            "source": "r_309",
            "target": "s_39"
        },
        {
            "source": "r_309",
            "target": "s_314"
        },
        {
            "source": "r_310",
            "target": "s_52"
        },
        {
            "source": "r_310",
            "target": "s_721"
        },
        {
            "source": "r_310",
            "target": "s_303"
        },
        {
            "source": "r_310",
            "target": "s_952"
        },
        {
            "source": "r_311",
            "target": "s_749"
        },
        {
            "source": "r_311",
            "target": "s_721"
        },
        {
            "source": "r_311",
            "target": "s_952"
        },
        {
            "source": "r_312",
            "target": "s_1503"
        },
        {
            "source": "r_312",
            "target": "s_629"
        },
        {
            "source": "r_312",
            "target": "s_763"
        },
        {
            "source": "r_312",
            "target": "s_233"
        },
        {
            "source": "r_312",
            "target": "s_608"
        },
        {
            "source": "r_313",
            "target": "s_42"
        },
        {
            "source": "r_313",
            "target": "s_784"
        },
        {
            "source": "r_313",
            "target": "s_1180"
        },
        {
            "source": "r_313",
            "target": "s_731"
        },
        {
            "source": "r_313",
            "target": "s_721"
        },
        {
            "source": "r_314",
            "target": "s_28"
        },
        {
            "source": "r_314",
            "target": "s_651"
        },
        {
            "source": "r_314",
            "target": "s_1252"
        },
        {
            "source": "r_314",
            "target": "s_524"
        },
        {
            "source": "r_314",
            "target": "s_722"
        },
        {
            "source": "r_314",
            "target": "s_1426"
        },
        {
            "source": "r_314",
            "target": "s_585"
        },
        {
            "source": "r_314",
            "target": "s_362"
        },
        {
            "source": "r_314",
            "target": "s_131"
        },
        {
            "source": "r_315",
            "target": "s_1310"
        },
        {
            "source": "r_315",
            "target": "s_100"
        },
        {
            "source": "r_315",
            "target": "s_469"
        },
        {
            "source": "r_316",
            "target": "s_1239"
        },
        {
            "source": "r_316",
            "target": "s_945"
        },
        {
            "source": "r_316",
            "target": "s_406"
        },
        {
            "source": "r_316",
            "target": "s_933"
        },
        {
            "source": "r_316",
            "target": "s_200"
        },
        {
            "source": "r_316",
            "target": "s_52"
        },
        {
            "source": "r_316",
            "target": "s_605"
        },
        {
            "source": "r_316",
            "target": "s_273"
        },
        {
            "source": "r_316",
            "target": "s_952"
        },
        {
            "source": "r_317",
            "target": "s_539"
        },
        {
            "source": "r_317",
            "target": "s_619"
        },
        {
            "source": "r_317",
            "target": "s_1461"
        },
        {
            "source": "r_318",
            "target": "s_456"
        },
        {
            "source": "r_318",
            "target": "s_859"
        },
        {
            "source": "r_318",
            "target": "s_1148"
        },
        {
            "source": "r_318",
            "target": "s_1315"
        },
        {
            "source": "r_318",
            "target": "s_404"
        },
        {
            "source": "r_318",
            "target": "s_314"
        },
        {
            "source": "r_318",
            "target": "s_960"
        },
        {
            "source": "r_319",
            "target": "s_1191"
        },
        {
            "source": "r_319",
            "target": "s_756"
        },
        {
            "source": "r_319",
            "target": "s_911"
        },
        {
            "source": "r_320",
            "target": "s_141"
        },
        {
            "source": "r_320",
            "target": "s_143"
        },
        {
            "source": "r_320",
            "target": "s_970"
        },
        {
            "source": "r_320",
            "target": "s_797"
        },
        {
            "source": "r_321",
            "target": "s_293"
        },
        {
            "source": "r_321",
            "target": "s_835"
        },
        {
            "source": "r_321",
            "target": "s_274"
        },
        {
            "source": "r_321",
            "target": "s_173"
        },
        {
            "source": "r_321",
            "target": "s_255"
        },
        {
            "source": "r_321",
            "target": "s_939"
        },
        {
            "source": "r_321",
            "target": "s_752"
        },
        {
            "source": "r_322",
            "target": "s_293"
        },
        {
            "source": "r_322",
            "target": "s_689"
        },
        {
            "source": "r_322",
            "target": "s_173"
        },
        {
            "source": "r_322",
            "target": "s_1175"
        },
        {
            "source": "r_322",
            "target": "s_255"
        },
        {
            "source": "r_322",
            "target": "s_696"
        },
        {
            "source": "r_322",
            "target": "s_752"
        },
        {
            "source": "r_323",
            "target": "s_140"
        },
        {
            "source": "r_323",
            "target": "s_344"
        },
        {
            "source": "r_323",
            "target": "s_1215"
        },
        {
            "source": "r_323",
            "target": "s_1166"
        },
        {
            "source": "r_324",
            "target": "s_125"
        },
        {
            "source": "r_324",
            "target": "s_837"
        },
        {
            "source": "r_324",
            "target": "s_491"
        },
        {
            "source": "r_324",
            "target": "s_804"
        },
        {
            "source": "r_324",
            "target": "s_640"
        },
        {
            "source": "r_324",
            "target": "s_721"
        },
        {
            "source": "r_324",
            "target": "s_316"
        },
        {
            "source": "r_325",
            "target": "s_1235"
        },
        {
            "source": "r_325",
            "target": "s_1047"
        },
        {
            "source": "r_325",
            "target": "s_1260"
        },
        {
            "source": "r_326",
            "target": "s_1035"
        },
        {
            "source": "r_326",
            "target": "s_1206"
        },
        {
            "source": "r_327",
            "target": "s_84"
        },
        {
            "source": "r_327",
            "target": "s_1298"
        },
        {
            "source": "r_327",
            "target": "s_195"
        },
        {
            "source": "r_327",
            "target": "s_1339"
        },
        {
            "source": "r_327",
            "target": "s_984"
        },
        {
            "source": "r_327",
            "target": "s_276"
        },
        {
            "source": "r_328",
            "target": "s_532"
        },
        {
            "source": "r_328",
            "target": "s_92"
        },
        {
            "source": "r_328",
            "target": "s_1486"
        },
        {
            "source": "r_328",
            "target": "s_60"
        },
        {
            "source": "r_328",
            "target": "s_531"
        },
        {
            "source": "r_328",
            "target": "s_275"
        },
        {
            "source": "r_329",
            "target": "s_1063"
        },
        {
            "source": "r_329",
            "target": "s_1356"
        },
        {
            "source": "r_329",
            "target": "s_594"
        },
        {
            "source": "r_329",
            "target": "s_1100"
        },
        {
            "source": "r_329",
            "target": "s_1171"
        },
        {
            "source": "r_329",
            "target": "s_207"
        },
        {
            "source": "r_329",
            "target": "s_1451"
        },
        {
            "source": "r_329",
            "target": "s_473"
        },
        {
            "source": "r_329",
            "target": "s_393"
        },
        {
            "source": "r_330",
            "target": "s_111"
        },
        {
            "source": "r_330",
            "target": "s_637"
        },
        {
            "source": "r_330",
            "target": "s_1355"
        },
        {
            "source": "r_330",
            "target": "s_954"
        },
        {
            "source": "r_330",
            "target": "s_196"
        },
        {
            "source": "r_331",
            "target": "s_1492"
        },
        {
            "source": "r_331",
            "target": "s_202"
        },
        {
            "source": "r_332",
            "target": "s_1438"
        },
        {
            "source": "r_332",
            "target": "s_776"
        },
        {
            "source": "r_332",
            "target": "s_672"
        },
        {
            "source": "r_332",
            "target": "s_355"
        },
        {
            "source": "r_332",
            "target": "s_803"
        },
        {
            "source": "r_332",
            "target": "s_1461"
        },
        {
            "source": "r_333",
            "target": "s_1019"
        },
        {
            "source": "r_333",
            "target": "s_245"
        },
        {
            "source": "r_333",
            "target": "s_48"
        },
        {
            "source": "r_333",
            "target": "s_230"
        },
        {
            "source": "r_333",
            "target": "s_1347"
        },
        {
            "source": "r_334",
            "target": "s_387"
        },
        {
            "source": "r_334",
            "target": "s_1269"
        },
        {
            "source": "r_335",
            "target": "s_242"
        },
        {
            "source": "r_335",
            "target": "s_694"
        },
        {
            "source": "r_335",
            "target": "s_855"
        },
        {
            "source": "r_335",
            "target": "s_1194"
        },
        {
            "source": "r_336",
            "target": "s_754"
        },
        {
            "source": "r_336",
            "target": "s_873"
        },
        {
            "source": "r_336",
            "target": "s_1525"
        },
        {
            "source": "r_337",
            "target": "s_239"
        },
        {
            "source": "r_337",
            "target": "s_341"
        },
        {
            "source": "r_337",
            "target": "s_950"
        },
        {
            "source": "r_337",
            "target": "s_80"
        },
        {
            "source": "r_337",
            "target": "s_420"
        },
        {
            "source": "r_337",
            "target": "s_1460"
        },
        {
            "source": "r_337",
            "target": "s_208"
        },
        {
            "source": "r_337",
            "target": "s_424"
        },
        {
            "source": "r_337",
            "target": "s_1420"
        },
        {
            "source": "r_337",
            "target": "s_426"
        },
        {
            "source": "r_337",
            "target": "s_725"
        },
        {
            "source": "r_337",
            "target": "s_1090"
        },
        {
            "source": "r_337",
            "target": "s_241"
        },
        {
            "source": "r_338",
            "target": "s_339"
        },
        {
            "source": "r_338",
            "target": "s_1188"
        },
        {
            "source": "r_338",
            "target": "s_781"
        },
        {
            "source": "r_338",
            "target": "s_1153"
        },
        {
            "source": "r_338",
            "target": "s_492"
        },
        {
            "source": "r_338",
            "target": "s_1517"
        },
        {
            "source": "r_338",
            "target": "s_131"
        },
        {
            "source": "r_339",
            "target": "s_808"
        },
        {
            "source": "r_339",
            "target": "s_894"
        },
        {
            "source": "r_339",
            "target": "s_1116"
        },
        {
            "source": "r_339",
            "target": "s_1409"
        },
        {
            "source": "r_339",
            "target": "s_463"
        },
        {
            "source": "r_340",
            "target": "s_1325"
        },
        {
            "source": "r_340",
            "target": "s_804"
        },
        {
            "source": "r_340",
            "target": "s_652"
        },
        {
            "source": "r_340",
            "target": "s_1510"
        },
        {
            "source": "r_340",
            "target": "s_854"
        },
        {
            "source": "r_340",
            "target": "s_71"
        },
        {
            "source": "r_340",
            "target": "s_721"
        },
        {
            "source": "r_341",
            "target": "s_1354"
        },
        {
            "source": "r_341",
            "target": "s_102"
        },
        {
            "source": "r_341",
            "target": "s_482"
        },
        {
            "source": "r_341",
            "target": "s_761"
        },
        {
            "source": "r_342",
            "target": "s_293"
        },
        {
            "source": "r_342",
            "target": "s_1458"
        },
        {
            "source": "r_342",
            "target": "s_173"
        },
        {
            "source": "r_342",
            "target": "s_255"
        },
        {
            "source": "r_342",
            "target": "s_752"
        },
        {
            "source": "r_342",
            "target": "s_939"
        },
        {
            "source": "r_343",
            "target": "s_345"
        },
        {
            "source": "r_343",
            "target": "s_427"
        },
        {
            "source": "r_343",
            "target": "s_489"
        },
        {
            "source": "r_343",
            "target": "s_1397"
        },
        {
            "source": "r_343",
            "target": "s_1124"
        },
        {
            "source": "r_344",
            "target": "s_1397"
        },
        {
            "source": "r_344",
            "target": "s_489"
        },
        {
            "source": "r_344",
            "target": "s_1124"
        },
        {
            "source": "r_344",
            "target": "s_531"
        },
        {
            "source": "r_345",
            "target": "s_1177"
        },
        {
            "source": "r_345",
            "target": "s_568"
        },
        {
            "source": "r_345",
            "target": "s_437"
        },
        {
            "source": "r_345",
            "target": "s_465"
        },
        {
            "source": "r_345",
            "target": "s_306"
        },
        {
            "source": "r_346",
            "target": "s_404"
        },
        {
            "source": "r_346",
            "target": "s_214"
        },
        {
            "source": "r_346",
            "target": "s_1446"
        },
        {
            "source": "r_346",
            "target": "s_314"
        },
        {
            "source": "r_347",
            "target": "s_819"
        },
        {
            "source": "r_347",
            "target": "s_127"
        },
        {
            "source": "r_347",
            "target": "s_1127"
        },
        {
            "source": "r_347",
            "target": "s_1432"
        },
        {
            "source": "r_347",
            "target": "s_886"
        },
        {
            "source": "r_348",
            "target": "s_1254"
        },
        {
            "source": "r_348",
            "target": "s_1295"
        },
        {
            "source": "r_348",
            "target": "s_1153"
        },
        {
            "source": "r_348",
            "target": "s_425"
        },
        {
            "source": "r_348",
            "target": "s_287"
        },
        {
            "source": "r_348",
            "target": "s_131"
        },
        {
            "source": "r_348",
            "target": "s_733"
        },
        {
            "source": "r_349",
            "target": "s_1516"
        },
        {
            "source": "r_349",
            "target": "s_39"
        },
        {
            "source": "r_349",
            "target": "s_1374"
        },
        {
            "source": "r_349",
            "target": "s_264"
        },
        {
            "source": "r_349",
            "target": "s_1038"
        },
        {
            "source": "r_350",
            "target": "s_484"
        },
        {
            "source": "r_350",
            "target": "s_205"
        },
        {
            "source": "r_350",
            "target": "s_512"
        },
        {
            "source": "r_350",
            "target": "s_894"
        },
        {
            "source": "r_350",
            "target": "s_238"
        },
        {
            "source": "r_350",
            "target": "s_1409"
        },
        {
            "source": "r_350",
            "target": "s_1425"
        },
        {
            "source": "r_351",
            "target": "s_408"
        },
        {
            "source": "r_351",
            "target": "s_313"
        },
        {
            "source": "r_351",
            "target": "s_187"
        },
        {
            "source": "r_352",
            "target": "s_1195"
        },
        {
            "source": "r_352",
            "target": "s_11"
        },
        {
            "source": "r_353",
            "target": "s_1195"
        },
        {
            "source": "r_353",
            "target": "s_11"
        },
        {
            "source": "r_353",
            "target": "s_637"
        },
        {
            "source": "r_353",
            "target": "s_1372"
        },
        {
            "source": "r_353",
            "target": "s_1038"
        },
        {
            "source": "r_354",
            "target": "s_309"
        },
        {
            "source": "r_355",
            "target": "s_596"
        },
        {
            "source": "r_355",
            "target": "s_305"
        },
        {
            "source": "r_355",
            "target": "s_0"
        },
        {
            "source": "r_355",
            "target": "s_1294"
        },
        {
            "source": "r_355",
            "target": "s_333"
        },
        {
            "source": "r_355",
            "target": "s_233"
        },
        {
            "source": "r_356",
            "target": "s_842"
        },
        {
            "source": "r_356",
            "target": "s_1509"
        },
        {
            "source": "r_356",
            "target": "s_400"
        },
        {
            "source": "r_356",
            "target": "s_248"
        },
        {
            "source": "r_356",
            "target": "s_1038"
        },
        {
            "source": "r_356",
            "target": "s_1206"
        },
        {
            "source": "r_357",
            "target": "s_799"
        },
        {
            "source": "r_357",
            "target": "s_1431"
        },
        {
            "source": "r_357",
            "target": "s_1522"
        },
        {
            "source": "r_357",
            "target": "s_1038"
        },
        {
            "source": "r_357",
            "target": "s_1279"
        },
        {
            "source": "r_358",
            "target": "s_428"
        },
        {
            "source": "r_358",
            "target": "s_1414"
        },
        {
            "source": "r_358",
            "target": "s_1357"
        },
        {
            "source": "r_358",
            "target": "s_697"
        },
        {
            "source": "r_358",
            "target": "s_561"
        },
        {
            "source": "r_358",
            "target": "s_662"
        },
        {
            "source": "r_358",
            "target": "s_1319"
        },
        {
            "source": "r_359",
            "target": "s_321"
        },
        {
            "source": "r_359",
            "target": "s_1443"
        },
        {
            "source": "r_359",
            "target": "s_584"
        },
        {
            "source": "r_359",
            "target": "s_103"
        },
        {
            "source": "r_360",
            "target": "s_1310"
        },
        {
            "source": "r_360",
            "target": "s_1115"
        },
        {
            "source": "r_360",
            "target": "s_715"
        },
        {
            "source": "r_360",
            "target": "s_100"
        },
        {
            "source": "r_361",
            "target": "s_789"
        },
        {
            "source": "r_361",
            "target": "s_195"
        },
        {
            "source": "r_361",
            "target": "s_1247"
        },
        {
            "source": "r_361",
            "target": "s_162"
        },
        {
            "source": "r_361",
            "target": "s_12"
        },
        {
            "source": "r_362",
            "target": "s_741"
        },
        {
            "source": "r_362",
            "target": "s_1266"
        },
        {
            "source": "r_362",
            "target": "s_167"
        },
        {
            "source": "r_363",
            "target": "s_1280"
        },
        {
            "source": "r_363",
            "target": "s_399"
        },
        {
            "source": "r_363",
            "target": "s_458"
        },
        {
            "source": "r_363",
            "target": "s_314"
        },
        {
            "source": "r_363",
            "target": "s_132"
        },
        {
            "source": "r_364",
            "target": "s_1183"
        },
        {
            "source": "r_364",
            "target": "s_1495"
        },
        {
            "source": "r_364",
            "target": "s_1363"
        },
        {
            "source": "r_364",
            "target": "s_393"
        },
        {
            "source": "r_364",
            "target": "s_360"
        },
        {
            "source": "r_365",
            "target": "s_1222"
        },
        {
            "source": "r_365",
            "target": "s_737"
        },
        {
            "source": "r_365",
            "target": "s_763"
        },
        {
            "source": "r_365",
            "target": "s_233"
        },
        {
            "source": "r_366",
            "target": "s_485"
        },
        {
            "source": "r_366",
            "target": "s_1399"
        },
        {
            "source": "r_366",
            "target": "s_253"
        },
        {
            "source": "r_366",
            "target": "s_1285"
        },
        {
            "source": "r_367",
            "target": "s_1205"
        },
        {
            "source": "r_367",
            "target": "s_934"
        },
        {
            "source": "r_367",
            "target": "s_595"
        },
        {
            "source": "r_367",
            "target": "s_1399"
        },
        {
            "source": "r_367",
            "target": "s_1244"
        },
        {
            "source": "r_367",
            "target": "s_231"
        },
        {
            "source": "r_368",
            "target": "s_291"
        },
        {
            "source": "r_368",
            "target": "s_1473"
        },
        {
            "source": "r_368",
            "target": "s_90"
        },
        {
            "source": "r_368",
            "target": "s_1399"
        },
        {
            "source": "r_368",
            "target": "s_1183"
        },
        {
            "source": "r_369",
            "target": "s_144"
        },
        {
            "source": "r_369",
            "target": "s_1388"
        },
        {
            "source": "r_369",
            "target": "s_79"
        },
        {
            "source": "r_369",
            "target": "s_578"
        },
        {
            "source": "r_369",
            "target": "s_25"
        },
        {
            "source": "r_369",
            "target": "s_1466"
        },
        {
            "source": "r_369",
            "target": "s_531"
        },
        {
            "source": "r_370",
            "target": "s_878"
        },
        {
            "source": "r_370",
            "target": "s_1299"
        },
        {
            "source": "r_370",
            "target": "s_422"
        },
        {
            "source": "r_370",
            "target": "s_1357"
        },
        {
            "source": "r_370",
            "target": "s_107"
        },
        {
            "source": "r_370",
            "target": "s_1142"
        },
        {
            "source": "r_370",
            "target": "s_955"
        },
        {
            "source": "r_371",
            "target": "s_275"
        },
        {
            "source": "r_371",
            "target": "s_1311"
        },
        {
            "source": "r_371",
            "target": "s_91"
        },
        {
            "source": "r_372",
            "target": "s_721"
        },
        {
            "source": "r_372",
            "target": "s_464"
        },
        {
            "source": "r_372",
            "target": "s_605"
        },
        {
            "source": "r_372",
            "target": "s_1101"
        },
        {
            "source": "r_372",
            "target": "s_580"
        },
        {
            "source": "r_372",
            "target": "s_952"
        },
        {
            "source": "r_372",
            "target": "s_54"
        },
        {
            "source": "r_372",
            "target": "s_1435"
        },
        {
            "source": "r_372",
            "target": "s_1141"
        },
        {
            "source": "r_372",
            "target": "s_1130"
        },
        {
            "source": "r_373",
            "target": "s_604"
        },
        {
            "source": "r_373",
            "target": "s_441"
        },
        {
            "source": "r_373",
            "target": "s_27"
        },
        {
            "source": "r_373",
            "target": "s_65"
        },
        {
            "source": "r_373",
            "target": "s_559"
        },
        {
            "source": "r_373",
            "target": "s_1293"
        },
        {
            "source": "r_373",
            "target": "s_150"
        },
        {
            "source": "r_373",
            "target": "s_1517"
        },
        {
            "source": "r_373",
            "target": "s_333"
        },
        {
            "source": "r_374",
            "target": "s_703"
        },
        {
            "source": "r_374",
            "target": "s_1264"
        },
        {
            "source": "r_374",
            "target": "s_1159"
        },
        {
            "source": "r_374",
            "target": "s_331"
        },
        {
            "source": "r_375",
            "target": "s_389"
        },
        {
            "source": "r_375",
            "target": "s_1394"
        },
        {
            "source": "r_375",
            "target": "s_1032"
        },
        {
            "source": "r_375",
            "target": "s_952"
        },
        {
            "source": "r_376",
            "target": "s_373"
        },
        {
            "source": "r_376",
            "target": "s_318"
        },
        {
            "source": "r_376",
            "target": "s_327"
        },
        {
            "source": "r_376",
            "target": "s_1274"
        },
        {
            "source": "r_377",
            "target": "s_119"
        },
        {
            "source": "r_377",
            "target": "s_39"
        },
        {
            "source": "r_377",
            "target": "s_1402"
        },
        {
            "source": "r_377",
            "target": "s_763"
        },
        {
            "source": "r_378",
            "target": "s_31"
        },
        {
            "source": "r_378",
            "target": "s_1361"
        },
        {
            "source": "r_378",
            "target": "s_1517"
        },
        {
            "source": "r_378",
            "target": "s_100"
        },
        {
            "source": "r_379",
            "target": "s_1329"
        },
        {
            "source": "r_379",
            "target": "s_1385"
        },
        {
            "source": "r_379",
            "target": "s_254"
        },
        {
            "source": "r_380",
            "target": "s_865"
        },
        {
            "source": "r_380",
            "target": "s_480"
        },
        {
            "source": "r_380",
            "target": "s_1517"
        },
        {
            "source": "r_380",
            "target": "s_684"
        },
        {
            "source": "r_381",
            "target": "s_1367"
        },
        {
            "source": "r_381",
            "target": "s_1191"
        },
        {
            "source": "r_381",
            "target": "s_288"
        },
        {
            "source": "r_381",
            "target": "s_330"
        },
        {
            "source": "r_381",
            "target": "s_673"
        },
        {
            "source": "r_381",
            "target": "s_1176"
        },
        {
            "source": "r_381",
            "target": "s_1487"
        },
        {
            "source": "r_381",
            "target": "s_1006"
        },
        {
            "source": "r_382",
            "target": "s_918"
        },
        {
            "source": "r_382",
            "target": "s_1383"
        },
        {
            "source": "r_382",
            "target": "s_1521"
        },
        {
            "source": "r_382",
            "target": "s_701"
        },
        {
            "source": "r_382",
            "target": "s_646"
        },
        {
            "source": "r_383",
            "target": "s_958"
        },
        {
            "source": "r_383",
            "target": "s_1192"
        },
        {
            "source": "r_383",
            "target": "s_1428"
        },
        {
            "source": "r_383",
            "target": "s_205"
        },
        {
            "source": "r_384",
            "target": "s_1384"
        },
        {
            "source": "r_384",
            "target": "s_970"
        },
        {
            "source": "r_384",
            "target": "s_797"
        },
        {
            "source": "r_384",
            "target": "s_1479"
        },
        {
            "source": "r_385",
            "target": "s_1320"
        },
        {
            "source": "r_385",
            "target": "s_1177"
        },
        {
            "source": "r_386",
            "target": "s_1179"
        },
        {
            "source": "r_386",
            "target": "s_797"
        },
        {
            "source": "r_386",
            "target": "s_970"
        },
        {
            "source": "r_387",
            "target": "s_873"
        },
        {
            "source": "r_387",
            "target": "s_113"
        },
        {
            "source": "r_387",
            "target": "s_631"
        },
        {
            "source": "r_387",
            "target": "s_541"
        },
        {
            "source": "r_387",
            "target": "s_459"
        },
        {
            "source": "r_388",
            "target": "s_129"
        },
        {
            "source": "r_388",
            "target": "s_678"
        },
        {
            "source": "r_388",
            "target": "s_1132"
        },
        {
            "source": "r_388",
            "target": "s_363"
        },
        {
            "source": "r_388",
            "target": "s_1351"
        },
        {
            "source": "r_389",
            "target": "s_1219"
        },
        {
            "source": "r_389",
            "target": "s_1144"
        },
        {
            "source": "r_389",
            "target": "s_1002"
        },
        {
            "source": "r_389",
            "target": "s_1393"
        },
        {
            "source": "r_390",
            "target": "s_317"
        },
        {
            "source": "r_390",
            "target": "s_1061"
        },
        {
            "source": "r_391",
            "target": "s_305"
        },
        {
            "source": "r_391",
            "target": "s_1077"
        },
        {
            "source": "r_391",
            "target": "s_304"
        },
        {
            "source": "r_391",
            "target": "s_331"
        },
        {
            "source": "r_391",
            "target": "s_1368"
        },
        {
            "source": "r_391",
            "target": "s_233"
        },
        {
            "source": "r_392",
            "target": "s_352"
        },
        {
            "source": "r_392",
            "target": "s_528"
        },
        {
            "source": "r_392",
            "target": "s_213"
        },
        {
            "source": "r_392",
            "target": "s_699"
        },
        {
            "source": "r_393",
            "target": "s_607"
        },
        {
            "source": "r_393",
            "target": "s_687"
        },
        {
            "source": "r_393",
            "target": "s_1493"
        },
        {
            "source": "r_393",
            "target": "s_978"
        },
        {
            "source": "r_393",
            "target": "s_531"
        },
        {
            "source": "r_393",
            "target": "s_927"
        },
        {
            "source": "r_394",
            "target": "s_1065"
        },
        {
            "source": "r_394",
            "target": "s_1023"
        },
        {
            "source": "r_394",
            "target": "s_1137"
        },
        {
            "source": "r_394",
            "target": "s_763"
        },
        {
            "source": "r_394",
            "target": "s_975"
        },
        {
            "source": "r_395",
            "target": "s_309"
        },
        {
            "source": "r_396",
            "target": "s_1137"
        },
        {
            "source": "r_396",
            "target": "s_816"
        },
        {
            "source": "r_396",
            "target": "s_626"
        },
        {
            "source": "r_396",
            "target": "s_763"
        },
        {
            "source": "r_396",
            "target": "s_834"
        },
        {
            "source": "r_397",
            "target": "s_1004"
        },
        {
            "source": "r_397",
            "target": "s_1057"
        },
        {
            "source": "r_397",
            "target": "s_1153"
        },
        {
            "source": "r_397",
            "target": "s_131"
        },
        {
            "source": "r_397",
            "target": "s_1470"
        },
        {
            "source": "r_398",
            "target": "s_442"
        },
        {
            "source": "r_398",
            "target": "s_743"
        },
        {
            "source": "r_398",
            "target": "s_1394"
        },
        {
            "source": "r_398",
            "target": "s_942"
        },
        {
            "source": "r_398",
            "target": "s_952"
        },
        {
            "source": "r_399",
            "target": "s_1121"
        },
        {
            "source": "r_399",
            "target": "s_309"
        },
        {
            "source": "r_400",
            "target": "s_490"
        },
        {
            "source": "r_400",
            "target": "s_119"
        },
        {
            "source": "r_400",
            "target": "s_801"
        },
        {
            "source": "r_400",
            "target": "s_511"
        },
        {
            "source": "r_400",
            "target": "s_1409"
        },
        {
            "source": "r_400",
            "target": "s_547"
        },
        {
            "source": "r_401",
            "target": "s_78"
        },
        {
            "source": "r_401",
            "target": "s_1442"
        },
        {
            "source": "r_401",
            "target": "s_172"
        },
        {
            "source": "r_401",
            "target": "s_1124"
        },
        {
            "source": "r_401",
            "target": "s_489"
        },
        {
            "source": "r_401",
            "target": "s_345"
        },
        {
            "source": "r_401",
            "target": "s_427"
        },
        {
            "source": "r_402",
            "target": "s_1225"
        },
        {
            "source": "r_402",
            "target": "s_1457"
        },
        {
            "source": "r_402",
            "target": "s_309"
        },
        {
            "source": "r_403",
            "target": "s_1115"
        },
        {
            "source": "r_403",
            "target": "s_253"
        },
        {
            "source": "r_403",
            "target": "s_108"
        },
        {
            "source": "r_403",
            "target": "s_555"
        },
        {
            "source": "r_404",
            "target": "s_721"
        },
        {
            "source": "r_404",
            "target": "s_206"
        },
        {
            "source": "r_404",
            "target": "s_1452"
        },
        {
            "source": "r_404",
            "target": "s_1268"
        },
        {
            "source": "r_404",
            "target": "s_936"
        },
        {
            "source": "r_404",
            "target": "s_1435"
        },
        {
            "source": "r_404",
            "target": "s_54"
        },
        {
            "source": "r_404",
            "target": "s_1141"
        },
        {
            "source": "r_404",
            "target": "s_1224"
        },
        {
            "source": "r_404",
            "target": "s_952"
        },
        {
            "source": "r_405",
            "target": "s_1503"
        },
        {
            "source": "r_405",
            "target": "s_629"
        },
        {
            "source": "r_405",
            "target": "s_763"
        },
        {
            "source": "r_405",
            "target": "s_233"
        },
        {
            "source": "r_405",
            "target": "s_608"
        },
        {
            "source": "r_406",
            "target": "s_1109"
        },
        {
            "source": "r_406",
            "target": "s_998"
        },
        {
            "source": "r_407",
            "target": "s_533"
        },
        {
            "source": "r_407",
            "target": "s_993"
        },
        {
            "source": "r_407",
            "target": "s_605"
        },
        {
            "source": "r_407",
            "target": "s_952"
        },
        {
            "source": "r_407",
            "target": "s_1094"
        },
        {
            "source": "r_407",
            "target": "s_891"
        },
        {
            "source": "r_407",
            "target": "s_806"
        },
        {
            "source": "r_408",
            "target": "s_557"
        },
        {
            "source": "r_408",
            "target": "s_758"
        },
        {
            "source": "r_408",
            "target": "s_104"
        },
        {
            "source": "r_408",
            "target": "s_1220"
        },
        {
            "source": "r_408",
            "target": "s_1382"
        },
        {
            "source": "r_408",
            "target": "s_1461"
        },
        {
            "source": "r_408",
            "target": "s_1191"
        },
        {
            "source": "r_409",
            "target": "s_1287"
        },
        {
            "source": "r_409",
            "target": "s_1348"
        },
        {
            "source": "r_409",
            "target": "s_246"
        },
        {
            "source": "r_409",
            "target": "s_471"
        },
        {
            "source": "r_409",
            "target": "s_836"
        },
        {
            "source": "r_409",
            "target": "s_1081"
        },
        {
            "source": "r_409",
            "target": "s_1204"
        },
        {
            "source": "r_410",
            "target": "s_11"
        },
        {
            "source": "r_410",
            "target": "s_1195"
        },
        {
            "source": "r_410",
            "target": "s_329"
        },
        {
            "source": "r_410",
            "target": "s_166"
        },
        {
            "source": "r_411",
            "target": "s_765"
        },
        {
            "source": "r_411",
            "target": "s_397"
        },
        {
            "source": "r_411",
            "target": "s_254"
        },
        {
            "source": "r_412",
            "target": "s_1077"
        },
        {
            "source": "r_412",
            "target": "s_1087"
        },
        {
            "source": "r_412",
            "target": "s_1326"
        },
        {
            "source": "r_412",
            "target": "s_233"
        },
        {
            "source": "r_412",
            "target": "s_305"
        },
        {
            "source": "r_413",
            "target": "s_1259"
        },
        {
            "source": "r_413",
            "target": "s_1106"
        },
        {
            "source": "r_413",
            "target": "s_1015"
        },
        {
            "source": "r_413",
            "target": "s_1276"
        },
        {
            "source": "r_413",
            "target": "s_666"
        },
        {
            "source": "r_413",
            "target": "s_699"
        },
        {
            "source": "r_414",
            "target": "s_709"
        },
        {
            "source": "r_414",
            "target": "s_840"
        },
        {
            "source": "r_414",
            "target": "s_990"
        },
        {
            "source": "r_414",
            "target": "s_1312"
        },
        {
            "source": "r_414",
            "target": "s_378"
        },
        {
            "source": "r_415",
            "target": "s_1423"
        },
        {
            "source": "r_415",
            "target": "s_202"
        },
        {
            "source": "r_415",
            "target": "s_852"
        },
        {
            "source": "r_415",
            "target": "s_1170"
        },
        {
            "source": "r_416",
            "target": "s_1003"
        },
        {
            "source": "r_416",
            "target": "s_1010"
        },
        {
            "source": "r_416",
            "target": "s_131"
        },
        {
            "source": "r_416",
            "target": "s_1319"
        },
        {
            "source": "r_416",
            "target": "s_531"
        },
        {
            "source": "r_416",
            "target": "s_270"
        },
        {
            "source": "r_417",
            "target": "s_415"
        },
        {
            "source": "r_417",
            "target": "s_1102"
        },
        {
            "source": "r_417",
            "target": "s_1327"
        },
        {
            "source": "r_417",
            "target": "s_349"
        },
        {
            "source": "r_418",
            "target": "s_421"
        },
        {
            "source": "r_418",
            "target": "s_197"
        },
        {
            "source": "r_418",
            "target": "s_980"
        },
        {
            "source": "r_418",
            "target": "s_1025"
        },
        {
            "source": "r_418",
            "target": "s_1403"
        },
        {
            "source": "r_419",
            "target": "s_668"
        },
        {
            "source": "r_419",
            "target": "s_429"
        },
        {
            "source": "r_419",
            "target": "s_956"
        },
        {
            "source": "r_419",
            "target": "s_1330"
        },
        {
            "source": "r_419",
            "target": "s_284"
        },
        {
            "source": "r_419",
            "target": "s_796"
        },
        {
            "source": "r_420",
            "target": "s_327"
        },
        {
            "source": "r_420",
            "target": "s_11"
        },
        {
            "source": "r_420",
            "target": "s_1195"
        },
        {
            "source": "r_420",
            "target": "s_408"
        },
        {
            "source": "r_420",
            "target": "s_1274"
        },
        {
            "source": "r_421",
            "target": "s_1143"
        },
        {
            "source": "r_421",
            "target": "s_1529"
        },
        {
            "source": "r_421",
            "target": "s_436"
        },
        {
            "source": "r_421",
            "target": "s_1153"
        },
        {
            "source": "r_421",
            "target": "s_1012"
        },
        {
            "source": "r_421",
            "target": "s_739"
        },
        {
            "source": "r_421",
            "target": "s_664"
        },
        {
            "source": "r_421",
            "target": "s_131"
        },
        {
            "source": "r_422",
            "target": "s_1350"
        },
        {
            "source": "r_422",
            "target": "s_560"
        },
        {
            "source": "r_422",
            "target": "s_102"
        },
        {
            "source": "r_422",
            "target": "s_704"
        },
        {
            "source": "r_423",
            "target": "s_221"
        },
        {
            "source": "r_423",
            "target": "s_947"
        },
        {
            "source": "r_423",
            "target": "s_920"
        },
        {
            "source": "r_423",
            "target": "s_1354"
        },
        {
            "source": "r_423",
            "target": "s_131"
        },
        {
            "source": "r_424",
            "target": "s_622"
        },
        {
            "source": "r_424",
            "target": "s_732"
        },
        {
            "source": "r_424",
            "target": "s_282"
        },
        {
            "source": "r_424",
            "target": "s_175"
        },
        {
            "source": "r_424",
            "target": "s_994"
        },
        {
            "source": "r_424",
            "target": "s_531"
        },
        {
            "source": "r_424",
            "target": "s_840"
        },
        {
            "source": "r_425",
            "target": "s_1026"
        },
        {
            "source": "r_425",
            "target": "s_266"
        },
        {
            "source": "r_425",
            "target": "s_1291"
        },
        {
            "source": "r_425",
            "target": "s_755"
        },
        {
            "source": "r_425",
            "target": "s_918"
        },
        {
            "source": "r_426",
            "target": "s_1209"
        },
        {
            "source": "r_426",
            "target": "s_847"
        },
        {
            "source": "r_426",
            "target": "s_1431"
        },
        {
            "source": "r_426",
            "target": "s_799"
        },
        {
            "source": "r_427",
            "target": "s_599"
        },
        {
            "source": "r_427",
            "target": "s_398"
        },
        {
            "source": "r_427",
            "target": "s_531"
        },
        {
            "source": "r_427",
            "target": "s_857"
        },
        {
            "source": "r_427",
            "target": "s_20"
        },
        {
            "source": "r_428",
            "target": "s_487"
        },
        {
            "source": "r_428",
            "target": "s_1481"
        },
        {
            "source": "r_428",
            "target": "s_116"
        },
        {
            "source": "r_428",
            "target": "s_135"
        },
        {
            "source": "r_428",
            "target": "s_864"
        },
        {
            "source": "r_428",
            "target": "s_363"
        },
        {
            "source": "r_429",
            "target": "s_300"
        },
        {
            "source": "r_429",
            "target": "s_782"
        },
        {
            "source": "r_429",
            "target": "s_1465"
        },
        {
            "source": "r_429",
            "target": "s_1500"
        },
        {
            "source": "r_429",
            "target": "s_311"
        },
        {
            "source": "r_430",
            "target": "s_1035"
        },
        {
            "source": "r_430",
            "target": "s_506"
        },
        {
            "source": "r_430",
            "target": "s_1206"
        },
        {
            "source": "r_431",
            "target": "s_1196"
        },
        {
            "source": "r_431",
            "target": "s_225"
        },
        {
            "source": "r_431",
            "target": "s_771"
        },
        {
            "source": "r_431",
            "target": "s_145"
        },
        {
            "source": "r_431",
            "target": "s_151"
        },
        {
            "source": "r_432",
            "target": "s_938"
        },
        {
            "source": "r_432",
            "target": "s_805"
        },
        {
            "source": "r_432",
            "target": "s_254"
        },
        {
            "source": "r_433",
            "target": "s_37"
        },
        {
            "source": "r_433",
            "target": "s_637"
        },
        {
            "source": "r_433",
            "target": "s_1038"
        },
        {
            "source": "r_433",
            "target": "s_548"
        },
        {
            "source": "r_433",
            "target": "s_951"
        },
        {
            "source": "r_434",
            "target": "s_264"
        },
        {
            "source": "r_434",
            "target": "s_114"
        },
        {
            "source": "r_435",
            "target": "s_136"
        },
        {
            "source": "r_435",
            "target": "s_447"
        },
        {
            "source": "r_436",
            "target": "s_226"
        },
        {
            "source": "r_436",
            "target": "s_1198"
        },
        {
            "source": "r_436",
            "target": "s_112"
        },
        {
            "source": "r_437",
            "target": "s_850"
        },
        {
            "source": "r_437",
            "target": "s_223"
        },
        {
            "source": "r_437",
            "target": "s_1341"
        },
        {
            "source": "r_437",
            "target": "s_1186"
        },
        {
            "source": "r_437",
            "target": "s_1149"
        },
        {
            "source": "r_438",
            "target": "s_1001"
        },
        {
            "source": "r_438",
            "target": "s_30"
        },
        {
            "source": "r_438",
            "target": "s_1036"
        },
        {
            "source": "r_438",
            "target": "s_669"
        },
        {
            "source": "r_438",
            "target": "s_483"
        },
        {
            "source": "r_438",
            "target": "s_497"
        },
        {
            "source": "r_438",
            "target": "s_249"
        },
        {
            "source": "r_438",
            "target": "s_814"
        },
        {
            "source": "r_438",
            "target": "s_393"
        },
        {
            "source": "r_439",
            "target": "s_521"
        },
        {
            "source": "r_439",
            "target": "s_75"
        },
        {
            "source": "r_439",
            "target": "s_593"
        },
        {
            "source": "r_439",
            "target": "s_1376"
        },
        {
            "source": "r_439",
            "target": "s_361"
        },
        {
            "source": "r_440",
            "target": "s_286"
        },
        {
            "source": "r_440",
            "target": "s_510"
        },
        {
            "source": "r_440",
            "target": "s_259"
        },
        {
            "source": "r_440",
            "target": "s_1319"
        },
        {
            "source": "r_440",
            "target": "s_1065"
        },
        {
            "source": "r_440",
            "target": "s_257"
        },
        {
            "source": "r_440",
            "target": "s_1329"
        },
        {
            "source": "r_440",
            "target": "s_511"
        },
        {
            "source": "r_441",
            "target": "s_39"
        },
        {
            "source": "r_441",
            "target": "s_558"
        },
        {
            "source": "r_441",
            "target": "s_547"
        },
        {
            "source": "r_441",
            "target": "s_1038"
        },
        {
            "source": "r_441",
            "target": "s_511"
        },
        {
            "source": "r_442",
            "target": "s_550"
        },
        {
            "source": "r_442",
            "target": "s_173"
        },
        {
            "source": "r_442",
            "target": "s_90"
        },
        {
            "source": "r_443",
            "target": "s_1117"
        },
        {
            "source": "r_443",
            "target": "s_546"
        },
        {
            "source": "r_444",
            "target": "s_1205"
        },
        {
            "source": "r_444",
            "target": "s_1399"
        },
        {
            "source": "r_444",
            "target": "s_595"
        },
        {
            "source": "r_444",
            "target": "s_934"
        },
        {
            "source": "r_444",
            "target": "s_1244"
        },
        {
            "source": "r_444",
            "target": "s_231"
        },
        {
            "source": "r_445",
            "target": "s_459"
        },
        {
            "source": "r_445",
            "target": "s_113"
        },
        {
            "source": "r_445",
            "target": "s_495"
        },
        {
            "source": "r_445",
            "target": "s_873"
        },
        {
            "source": "r_446",
            "target": "s_1045"
        },
        {
            "source": "r_446",
            "target": "s_62"
        },
        {
            "source": "r_446",
            "target": "s_1248"
        },
        {
            "source": "r_446",
            "target": "s_724"
        },
        {
            "source": "r_446",
            "target": "s_97"
        },
        {
            "source": "r_446",
            "target": "s_957"
        },
        {
            "source": "r_446",
            "target": "s_131"
        },
        {
            "source": "r_447",
            "target": "s_1219"
        },
        {
            "source": "r_447",
            "target": "s_360"
        },
        {
            "source": "r_447",
            "target": "s_1133"
        },
        {
            "source": "r_447",
            "target": "s_204"
        },
        {
            "source": "r_447",
            "target": "s_1238"
        },
        {
            "source": "r_447",
            "target": "s_393"
        },
        {
            "source": "r_448",
            "target": "s_818"
        },
        {
            "source": "r_448",
            "target": "s_149"
        },
        {
            "source": "r_448",
            "target": "s_750"
        },
        {
            "source": "r_449",
            "target": "s_1455"
        },
        {
            "source": "r_449",
            "target": "s_995"
        },
        {
            "source": "r_449",
            "target": "s_1253"
        },
        {
            "source": "r_449",
            "target": "s_1084"
        },
        {
            "source": "r_450",
            "target": "s_1058"
        },
        {
            "source": "r_450",
            "target": "s_1122"
        },
        {
            "source": "r_450",
            "target": "s_38"
        },
        {
            "source": "r_450",
            "target": "s_614"
        },
        {
            "source": "r_450",
            "target": "s_716"
        },
        {
            "source": "r_450",
            "target": "s_1464"
        },
        {
            "source": "r_450",
            "target": "s_66"
        },
        {
            "source": "r_450",
            "target": "s_1357"
        },
        {
            "source": "r_450",
            "target": "s_1319"
        },
        {
            "source": "r_451",
            "target": "s_1259"
        },
        {
            "source": "r_451",
            "target": "s_1354"
        },
        {
            "source": "r_451",
            "target": "s_482"
        },
        {
            "source": "r_451",
            "target": "s_699"
        },
        {
            "source": "r_452",
            "target": "s_586"
        },
        {
            "source": "r_452",
            "target": "s_96"
        },
        {
            "source": "r_452",
            "target": "s_898"
        },
        {
            "source": "r_452",
            "target": "s_15"
        },
        {
            "source": "r_452",
            "target": "s_202"
        },
        {
            "source": "r_453",
            "target": "s_850"
        },
        {
            "source": "r_453",
            "target": "s_223"
        },
        {
            "source": "r_453",
            "target": "s_1149"
        },
        {
            "source": "r_453",
            "target": "s_1186"
        },
        {
            "source": "r_454",
            "target": "s_1193"
        },
        {
            "source": "r_454",
            "target": "s_537"
        },
        {
            "source": "r_454",
            "target": "s_620"
        },
        {
            "source": "r_454",
            "target": "s_963"
        },
        {
            "source": "r_454",
            "target": "s_757"
        },
        {
            "source": "r_454",
            "target": "s_78"
        },
        {
            "source": "r_454",
            "target": "s_1397"
        },
        {
            "source": "r_454",
            "target": "s_1124"
        },
        {
            "source": "r_455",
            "target": "s_1368"
        },
        {
            "source": "r_455",
            "target": "s_305"
        },
        {
            "source": "r_455",
            "target": "s_658"
        },
        {
            "source": "r_455",
            "target": "s_613"
        },
        {
            "source": "r_455",
            "target": "s_233"
        },
        {
            "source": "r_456",
            "target": "s_1496"
        },
        {
            "source": "r_456",
            "target": "s_1377"
        },
        {
            "source": "r_456",
            "target": "s_1266"
        },
        {
            "source": "r_457",
            "target": "s_1189"
        },
        {
            "source": "r_457",
            "target": "s_275"
        },
        {
            "source": "r_458",
            "target": "s_657"
        },
        {
            "source": "r_458",
            "target": "s_1115"
        },
        {
            "source": "r_458",
            "target": "s_753"
        },
        {
            "source": "r_458",
            "target": "s_1073"
        },
        {
            "source": "r_458",
            "target": "s_756"
        },
        {
            "source": "r_459",
            "target": "s_764"
        },
        {
            "source": "r_459",
            "target": "s_1477"
        },
        {
            "source": "r_459",
            "target": "s_307"
        },
        {
            "source": "r_459",
            "target": "s_819"
        },
        {
            "source": "r_460",
            "target": "s_991"
        },
        {
            "source": "r_460",
            "target": "s_699"
        },
        {
            "source": "r_461",
            "target": "s_1110"
        },
        {
            "source": "r_461",
            "target": "s_1206"
        },
        {
            "source": "r_461",
            "target": "s_63"
        },
        {
            "source": "r_462",
            "target": "s_586"
        },
        {
            "source": "r_462",
            "target": "s_202"
        },
        {
            "source": "r_463",
            "target": "s_523"
        },
        {
            "source": "r_463",
            "target": "s_975"
        },
        {
            "source": "r_463",
            "target": "s_1319"
        },
        {
            "source": "r_463",
            "target": "s_706"
        },
        {
            "source": "r_463",
            "target": "s_44"
        },
        {
            "source": "r_463",
            "target": "s_1513"
        },
        {
            "source": "r_463",
            "target": "s_794"
        },
        {
            "source": "r_463",
            "target": "s_997"
        },
        {
            "source": "r_463",
            "target": "s_67"
        },
        {
            "source": "r_464",
            "target": "s_702"
        },
        {
            "source": "r_464",
            "target": "s_1227"
        },
        {
            "source": "r_464",
            "target": "s_1067"
        },
        {
            "source": "r_464",
            "target": "s_918"
        },
        {
            "source": "r_465",
            "target": "s_459"
        },
        {
            "source": "r_465",
            "target": "s_631"
        },
        {
            "source": "r_465",
            "target": "s_113"
        },
        {
            "source": "r_465",
            "target": "s_541"
        },
        {
            "source": "r_465",
            "target": "s_873"
        },
        {
            "source": "r_466",
            "target": "s_1239"
        },
        {
            "source": "r_466",
            "target": "s_1032"
        },
        {
            "source": "r_466",
            "target": "s_1164"
        },
        {
            "source": "r_466",
            "target": "s_945"
        },
        {
            "source": "r_466",
            "target": "s_1368"
        },
        {
            "source": "r_466",
            "target": "s_448"
        },
        {
            "source": "r_466",
            "target": "s_1191"
        },
        {
            "source": "r_466",
            "target": "s_659"
        },
        {
            "source": "r_466",
            "target": "s_1158"
        },
        {
            "source": "r_466",
            "target": "s_1075"
        },
        {
            "source": "r_466",
            "target": "s_929"
        },
        {
            "source": "r_466",
            "target": "s_862"
        },
        {
            "source": "r_466",
            "target": "s_233"
        },
        {
            "source": "r_466",
            "target": "s_952"
        },
        {
            "source": "r_467",
            "target": "s_745"
        },
        {
            "source": "r_467",
            "target": "s_710"
        },
        {
            "source": "r_467",
            "target": "s_1154"
        },
        {
            "source": "r_467",
            "target": "s_1517"
        },
        {
            "source": "r_467",
            "target": "s_1096"
        },
        {
            "source": "r_468",
            "target": "s_1105"
        },
        {
            "source": "r_468",
            "target": "s_547"
        },
        {
            "source": "r_468",
            "target": "s_983"
        },
        {
            "source": "r_468",
            "target": "s_278"
        },
        {
            "source": "r_468",
            "target": "s_536"
        },
        {
            "source": "r_468",
            "target": "s_157"
        },
        {
            "source": "r_468",
            "target": "s_1139"
        },
        {
            "source": "r_468",
            "target": "s_597"
        },
        {
            "source": "r_468",
            "target": "s_1038"
        },
        {
            "source": "r_469",
            "target": "s_508"
        },
        {
            "source": "r_469",
            "target": "s_733"
        },
        {
            "source": "r_469",
            "target": "s_287"
        },
        {
            "source": "r_470",
            "target": "s_247"
        },
        {
            "source": "r_470",
            "target": "s_777"
        },
        {
            "source": "r_470",
            "target": "s_601"
        },
        {
            "source": "r_470",
            "target": "s_750"
        },
        {
            "source": "r_471",
            "target": "s_19"
        },
        {
            "source": "r_471",
            "target": "s_172"
        },
        {
            "source": "r_471",
            "target": "s_1124"
        },
        {
            "source": "r_471",
            "target": "s_49"
        },
        {
            "source": "r_471",
            "target": "s_1031"
        },
        {
            "source": "r_472",
            "target": "s_1156"
        },
        {
            "source": "r_472",
            "target": "s_871"
        },
        {
            "source": "r_472",
            "target": "s_889"
        },
        {
            "source": "r_472",
            "target": "s_1165"
        },
        {
            "source": "r_472",
            "target": "s_241"
        },
        {
            "source": "r_473",
            "target": "s_120"
        },
        {
            "source": "r_473",
            "target": "s_333"
        },
        {
            "source": "r_473",
            "target": "s_403"
        },
        {
            "source": "r_474",
            "target": "s_290"
        },
        {
            "source": "r_474",
            "target": "s_693"
        },
        {
            "source": "r_474",
            "target": "s_1442"
        },
        {
            "source": "r_474",
            "target": "s_275"
        },
        {
            "source": "r_475",
            "target": "s_602"
        },
        {
            "source": "r_475",
            "target": "s_899"
        },
        {
            "source": "r_476",
            "target": "s_231"
        },
        {
            "source": "r_476",
            "target": "s_202"
        },
        {
            "source": "r_476",
            "target": "s_1517"
        },
        {
            "source": "r_476",
            "target": "s_1244"
        },
        {
            "source": "r_476",
            "target": "s_635"
        },
        {
            "source": "r_476",
            "target": "s_297"
        },
        {
            "source": "r_476",
            "target": "s_1403"
        },
        {
            "source": "r_477",
            "target": "s_309"
        },
        {
            "source": "r_477",
            "target": "s_577"
        },
        {
            "source": "r_477",
            "target": "s_907"
        },
        {
            "source": "r_477",
            "target": "s_8"
        },
        {
            "source": "r_477",
            "target": "s_717"
        },
        {
            "source": "r_477",
            "target": "s_546"
        },
        {
            "source": "r_478",
            "target": "s_1313"
        },
        {
            "source": "r_478",
            "target": "s_851"
        },
        {
            "source": "r_478",
            "target": "s_404"
        },
        {
            "source": "r_478",
            "target": "s_314"
        },
        {
            "source": "r_479",
            "target": "s_1374"
        },
        {
            "source": "r_479",
            "target": "s_407"
        },
        {
            "source": "r_479",
            "target": "s_39"
        },
        {
            "source": "r_479",
            "target": "s_1038"
        },
        {
            "source": "r_479",
            "target": "s_511"
        },
        {
            "source": "r_480",
            "target": "s_174"
        },
        {
            "source": "r_480",
            "target": "s_260"
        },
        {
            "source": "r_480",
            "target": "s_651"
        },
        {
            "source": "r_480",
            "target": "s_1354"
        },
        {
            "source": "r_480",
            "target": "s_47"
        },
        {
            "source": "r_480",
            "target": "s_939"
        },
        {
            "source": "r_481",
            "target": "s_1131"
        },
        {
            "source": "r_481",
            "target": "s_802"
        },
        {
            "source": "r_481",
            "target": "s_344"
        },
        {
            "source": "r_481",
            "target": "s_112"
        },
        {
            "source": "r_482",
            "target": "s_1310"
        },
        {
            "source": "r_482",
            "target": "s_1003"
        },
        {
            "source": "r_482",
            "target": "s_1010"
        },
        {
            "source": "r_482",
            "target": "s_131"
        },
        {
            "source": "r_483",
            "target": "s_1164"
        },
        {
            "source": "r_483",
            "target": "s_1319"
        },
        {
            "source": "r_483",
            "target": "s_1129"
        },
        {
            "source": "r_483",
            "target": "s_912"
        },
        {
            "source": "r_483",
            "target": "s_315"
        },
        {
            "source": "r_483",
            "target": "s_952"
        },
        {
            "source": "r_484",
            "target": "s_301"
        },
        {
            "source": "r_484",
            "target": "s_353"
        },
        {
            "source": "r_484",
            "target": "s_32"
        },
        {
            "source": "r_484",
            "target": "s_1286"
        },
        {
            "source": "r_484",
            "target": "s_1502"
        },
        {
            "source": "r_484",
            "target": "s_881"
        },
        {
            "source": "r_485",
            "target": "s_1178"
        },
        {
            "source": "r_485",
            "target": "s_1071"
        },
        {
            "source": "r_485",
            "target": "s_809"
        },
        {
            "source": "r_485",
            "target": "s_379"
        },
        {
            "source": "r_486",
            "target": "s_943"
        },
        {
            "source": "r_486",
            "target": "s_309"
        },
        {
            "source": "r_487",
            "target": "s_90"
        },
        {
            "source": "r_487",
            "target": "s_395"
        },
        {
            "source": "r_487",
            "target": "s_995"
        },
        {
            "source": "r_487",
            "target": "s_1126"
        },
        {
            "source": "r_488",
            "target": "s_421"
        },
        {
            "source": "r_488",
            "target": "s_1025"
        },
        {
            "source": "r_488",
            "target": "s_684"
        },
        {
            "source": "r_488",
            "target": "s_368"
        },
        {
            "source": "r_488",
            "target": "s_980"
        },
        {
            "source": "r_488",
            "target": "s_231"
        },
        {
            "source": "r_489",
            "target": "s_1220"
        },
        {
            "source": "r_489",
            "target": "s_928"
        },
        {
            "source": "r_489",
            "target": "s_284"
        },
        {
            "source": "r_489",
            "target": "s_796"
        },
        {
            "source": "r_490",
            "target": "s_371"
        },
        {
            "source": "r_490",
            "target": "s_403"
        },
        {
            "source": "r_491",
            "target": "s_218"
        },
        {
            "source": "r_491",
            "target": "s_841"
        },
        {
            "source": "r_491",
            "target": "s_778"
        },
        {
            "source": "r_492",
            "target": "s_789"
        },
        {
            "source": "r_492",
            "target": "s_195"
        },
        {
            "source": "r_492",
            "target": "s_984"
        },
        {
            "source": "r_492",
            "target": "s_73"
        },
        {
            "source": "r_492",
            "target": "s_1247"
        },
        {
            "source": "r_493",
            "target": "s_1184"
        },
        {
            "source": "r_493",
            "target": "s_16"
        },
        {
            "source": "r_493",
            "target": "s_399"
        },
        {
            "source": "r_493",
            "target": "s_132"
        },
        {
            "source": "r_494",
            "target": "s_273"
        },
        {
            "source": "r_494",
            "target": "s_1075"
        },
        {
            "source": "r_494",
            "target": "s_1448"
        },
        {
            "source": "r_494",
            "target": "s_1407"
        },
        {
            "source": "r_495",
            "target": "s_1008"
        },
        {
            "source": "r_495",
            "target": "s_959"
        },
        {
            "source": "r_495",
            "target": "s_433"
        },
        {
            "source": "r_495",
            "target": "s_1375"
        },
        {
            "source": "r_495",
            "target": "s_473"
        },
        {
            "source": "r_496",
            "target": "s_1021"
        },
        {
            "source": "r_496",
            "target": "s_1270"
        },
        {
            "source": "r_496",
            "target": "s_391"
        },
        {
            "source": "r_496",
            "target": "s_130"
        },
        {
            "source": "r_496",
            "target": "s_251"
        },
        {
            "source": "r_497",
            "target": "s_1471"
        },
        {
            "source": "r_497",
            "target": "s_449"
        },
        {
            "source": "r_497",
            "target": "s_82"
        },
        {
            "source": "r_497",
            "target": "s_1262"
        },
        {
            "source": "r_497",
            "target": "s_1274"
        },
        {
            "source": "r_498",
            "target": "s_305"
        },
        {
            "source": "r_498",
            "target": "s_855"
        },
        {
            "source": "r_498",
            "target": "s_120"
        },
        {
            "source": "r_498",
            "target": "s_596"
        },
        {
            "source": "r_498",
            "target": "s_431"
        },
        {
            "source": "r_498",
            "target": "s_233"
        },
        {
            "source": "r_498",
            "target": "s_1368"
        },
        {
            "source": "r_498",
            "target": "s_333"
        },
        {
            "source": "r_499",
            "target": "s_1297"
        },
        {
            "source": "r_499",
            "target": "s_712"
        },
        {
            "source": "r_499",
            "target": "s_756"
        },
        {
            "source": "r_499",
            "target": "s_171"
        },
        {
            "source": "r_500",
            "target": "s_1446"
        },
        {
            "source": "r_500",
            "target": "s_214"
        },
        {
            "source": "r_500",
            "target": "s_314"
        },
        {
            "source": "r_501",
            "target": "s_268"
        },
        {
            "source": "r_501",
            "target": "s_1018"
        },
        {
            "source": "r_501",
            "target": "s_252"
        },
        {
            "source": "r_501",
            "target": "s_322"
        },
        {
            "source": "r_501",
            "target": "s_480"
        },
        {
            "source": "r_501",
            "target": "s_924"
        },
        {
            "source": "r_501",
            "target": "s_792"
        },
        {
            "source": "r_501",
            "target": "s_163"
        },
        {
            "source": "r_501",
            "target": "s_518"
        },
        {
            "source": "r_502",
            "target": "s_430"
        },
        {
            "source": "r_502",
            "target": "s_1345"
        },
        {
            "source": "r_502",
            "target": "s_486"
        },
        {
            "source": "r_502",
            "target": "s_131"
        },
        {
            "source": "r_502",
            "target": "s_273"
        },
        {
            "source": "r_503",
            "target": "s_547"
        },
        {
            "source": "r_503",
            "target": "s_511"
        },
        {
            "source": "r_503",
            "target": "s_1409"
        },
        {
            "source": "r_504",
            "target": "s_240"
        },
        {
            "source": "r_504",
            "target": "s_1378"
        },
        {
            "source": "r_504",
            "target": "s_1453"
        },
        {
            "source": "r_504",
            "target": "s_1151"
        },
        {
            "source": "r_504",
            "target": "s_254"
        },
        {
            "source": "r_505",
            "target": "s_1310"
        },
        {
            "source": "r_505",
            "target": "s_469"
        },
        {
            "source": "r_505",
            "target": "s_100"
        },
        {
            "source": "r_506",
            "target": "s_1423"
        },
        {
            "source": "r_506",
            "target": "s_1016"
        },
        {
            "source": "r_506",
            "target": "s_827"
        },
        {
            "source": "r_506",
            "target": "s_1492"
        },
        {
            "source": "r_507",
            "target": "s_1243"
        },
        {
            "source": "r_507",
            "target": "s_698"
        },
        {
            "source": "r_507",
            "target": "s_128"
        },
        {
            "source": "r_507",
            "target": "s_1062"
        },
        {
            "source": "r_507",
            "target": "s_840"
        },
        {
            "source": "r_507",
            "target": "s_628"
        },
        {
            "source": "r_507",
            "target": "s_1480"
        },
        {
            "source": "r_508",
            "target": "s_1439"
        },
        {
            "source": "r_508",
            "target": "s_581"
        },
        {
            "source": "r_508",
            "target": "s_822"
        },
        {
            "source": "r_508",
            "target": "s_124"
        },
        {
            "source": "r_508",
            "target": "s_158"
        },
        {
            "source": "r_508",
            "target": "s_927"
        },
        {
            "source": "r_508",
            "target": "s_1232"
        },
        {
            "source": "r_508",
            "target": "s_531"
        },
        {
            "source": "r_509",
            "target": "s_547"
        },
        {
            "source": "r_509",
            "target": "s_326"
        },
        {
            "source": "r_509",
            "target": "s_511"
        },
        {
            "source": "r_509",
            "target": "s_1038"
        },
        {
            "source": "r_509",
            "target": "s_85"
        },
        {
            "source": "r_510",
            "target": "s_443"
        },
        {
            "source": "r_510",
            "target": "s_848"
        },
        {
            "source": "r_510",
            "target": "s_1108"
        },
        {
            "source": "r_511",
            "target": "s_37"
        },
        {
            "source": "r_511",
            "target": "s_676"
        },
        {
            "source": "r_511",
            "target": "s_1336"
        },
        {
            "source": "r_511",
            "target": "s_1038"
        },
        {
            "source": "r_512",
            "target": "s_1205"
        },
        {
            "source": "r_512",
            "target": "s_1244"
        },
        {
            "source": "r_512",
            "target": "s_231"
        },
        {
            "source": "r_512",
            "target": "s_1399"
        },
        {
            "source": "r_513",
            "target": "s_214"
        },
        {
            "source": "r_513",
            "target": "s_1102"
        },
        {
            "source": "r_513",
            "target": "s_831"
        },
        {
            "source": "r_513",
            "target": "s_937"
        },
        {
            "source": "r_513",
            "target": "s_751"
        },
        {
            "source": "r_513",
            "target": "s_914"
        },
        {
            "source": "r_513",
            "target": "s_1304"
        },
        {
            "source": "r_513",
            "target": "s_404"
        },
        {
            "source": "r_514",
            "target": "s_1517"
        },
        {
            "source": "r_514",
            "target": "s_1492"
        },
        {
            "source": "r_514",
            "target": "s_1030"
        },
        {
            "source": "r_514",
            "target": "s_202"
        },
        {
            "source": "r_515",
            "target": "s_1195"
        },
        {
            "source": "r_515",
            "target": "s_11"
        },
        {
            "source": "r_515",
            "target": "s_1013"
        },
        {
            "source": "r_516",
            "target": "s_573"
        },
        {
            "source": "r_516",
            "target": "s_1504"
        },
        {
            "source": "r_516",
            "target": "s_248"
        },
        {
            "source": "r_516",
            "target": "s_334"
        },
        {
            "source": "r_516",
            "target": "s_1038"
        },
        {
            "source": "r_517",
            "target": "s_1161"
        },
        {
            "source": "r_517",
            "target": "s_217"
        },
        {
            "source": "r_517",
            "target": "s_794"
        },
        {
            "source": "r_517",
            "target": "s_639"
        },
        {
            "source": "r_517",
            "target": "s_625"
        },
        {
            "source": "r_517",
            "target": "s_856"
        },
        {
            "source": "r_517",
            "target": "s_1394"
        },
        {
            "source": "r_517",
            "target": "s_589"
        },
        {
            "source": "r_517",
            "target": "s_952"
        },
        {
            "source": "r_518",
            "target": "s_542"
        },
        {
            "source": "r_518",
            "target": "s_1228"
        },
        {
            "source": "r_518",
            "target": "s_1497"
        },
        {
            "source": "r_518",
            "target": "s_964"
        },
        {
            "source": "r_518",
            "target": "s_699"
        },
        {
            "source": "r_519",
            "target": "s_473"
        },
        {
            "source": "r_519",
            "target": "s_1516"
        },
        {
            "source": "r_519",
            "target": "s_884"
        },
        {
            "source": "r_519",
            "target": "s_34"
        },
        {
            "source": "r_519",
            "target": "s_833"
        },
        {
            "source": "r_519",
            "target": "s_357"
        },
        {
            "source": "r_519",
            "target": "s_444"
        },
        {
            "source": "r_519",
            "target": "s_695"
        },
        {
            "source": "r_519",
            "target": "s_675"
        },
        {
            "source": "r_519",
            "target": "s_1337"
        },
        {
            "source": "r_519",
            "target": "s_1309"
        },
        {
            "source": "r_519",
            "target": "s_511"
        },
        {
            "source": "r_519",
            "target": "s_904"
        },
        {
            "source": "r_519",
            "target": "s_1145"
        },
        {
            "source": "r_519",
            "target": "s_58"
        },
        {
            "source": "r_519",
            "target": "s_944"
        },
        {
            "source": "r_519",
            "target": "s_1038"
        },
        {
            "source": "r_520",
            "target": "s_1043"
        },
        {
            "source": "r_520",
            "target": "s_969"
        },
        {
            "source": "r_520",
            "target": "s_223"
        },
        {
            "source": "r_520",
            "target": "s_11"
        },
        {
            "source": "r_520",
            "target": "s_1195"
        },
        {
            "source": "r_521",
            "target": "s_272"
        },
        {
            "source": "r_521",
            "target": "s_45"
        },
        {
            "source": "r_521",
            "target": "s_333"
        },
        {
            "source": "r_522",
            "target": "s_392"
        },
        {
            "source": "r_522",
            "target": "s_1370"
        },
        {
            "source": "r_522",
            "target": "s_1399"
        },
        {
            "source": "r_522",
            "target": "s_219"
        },
        {
            "source": "r_523",
            "target": "s_211"
        },
        {
            "source": "r_523",
            "target": "s_653"
        },
        {
            "source": "r_523",
            "target": "s_768"
        },
        {
            "source": "r_523",
            "target": "s_1169"
        },
        {
            "source": "r_523",
            "target": "s_1038"
        },
        {
            "source": "r_523",
            "target": "s_846"
        },
        {
            "source": "r_524",
            "target": "s_657"
        },
        {
            "source": "r_524",
            "target": "s_1115"
        },
        {
            "source": "r_524",
            "target": "s_753"
        },
        {
            "source": "r_524",
            "target": "s_179"
        },
        {
            "source": "r_524",
            "target": "s_756"
        },
        {
            "source": "r_525",
            "target": "s_691"
        },
        {
            "source": "r_525",
            "target": "s_60"
        },
        {
            "source": "r_525",
            "target": "s_1486"
        },
        {
            "source": "r_525",
            "target": "s_138"
        },
        {
            "source": "r_525",
            "target": "s_172"
        },
        {
            "source": "r_526",
            "target": "s_389"
        },
        {
            "source": "r_526",
            "target": "s_1394"
        },
        {
            "source": "r_526",
            "target": "s_952"
        },
        {
            "source": "r_526",
            "target": "s_72"
        },
        {
            "source": "r_527",
            "target": "s_149"
        },
        {
            "source": "r_527",
            "target": "s_566"
        },
        {
            "source": "r_527",
            "target": "s_750"
        },
        {
            "source": "r_527",
            "target": "s_623"
        },
        {
            "source": "r_528",
            "target": "s_1007"
        },
        {
            "source": "r_528",
            "target": "s_1041"
        },
        {
            "source": "r_528",
            "target": "s_699"
        },
        {
            "source": "r_529",
            "target": "s_84"
        },
        {
            "source": "r_529",
            "target": "s_514"
        },
        {
            "source": "r_530",
            "target": "s_627"
        },
        {
            "source": "r_530",
            "target": "s_1047"
        },
        {
            "source": "r_531",
            "target": "s_511"
        },
        {
            "source": "r_531",
            "target": "s_1234"
        },
        {
            "source": "r_531",
            "target": "s_906"
        },
        {
            "source": "r_531",
            "target": "s_893"
        },
        {
            "source": "r_531",
            "target": "s_99"
        },
        {
            "source": "r_531",
            "target": "s_241"
        },
        {
            "source": "r_531",
            "target": "s_609"
        },
        {
            "source": "r_532",
            "target": "s_596"
        },
        {
            "source": "r_532",
            "target": "s_1467"
        },
        {
            "source": "r_532",
            "target": "s_233"
        },
        {
            "source": "r_532",
            "target": "s_333"
        },
        {
            "source": "r_533",
            "target": "s_120"
        },
        {
            "source": "r_533",
            "target": "s_305"
        },
        {
            "source": "r_533",
            "target": "s_403"
        },
        {
            "source": "r_533",
            "target": "s_233"
        },
        {
            "source": "r_533",
            "target": "s_1368"
        },
        {
            "source": "r_534",
            "target": "s_627"
        },
        {
            "source": "r_534",
            "target": "s_721"
        },
        {
            "source": "r_534",
            "target": "s_416"
        },
        {
            "source": "r_535",
            "target": "s_883"
        },
        {
            "source": "r_535",
            "target": "s_1007"
        },
        {
            "source": "r_535",
            "target": "s_1041"
        },
        {
            "source": "r_535",
            "target": "s_1033"
        },
        {
            "source": "r_535",
            "target": "s_494"
        },
        {
            "source": "r_535",
            "target": "s_109"
        },
        {
            "source": "r_535",
            "target": "s_699"
        },
        {
            "source": "r_536",
            "target": "s_707"
        },
        {
            "source": "r_536",
            "target": "s_786"
        },
        {
            "source": "r_536",
            "target": "s_668"
        },
        {
            "source": "r_536",
            "target": "s_366"
        },
        {
            "source": "r_536",
            "target": "s_796"
        },
        {
            "source": "r_537",
            "target": "s_754"
        },
        {
            "source": "r_537",
            "target": "s_63"
        },
        {
            "source": "r_538",
            "target": "s_747"
        },
        {
            "source": "r_538",
            "target": "s_94"
        },
        {
            "source": "r_538",
            "target": "s_450"
        },
        {
            "source": "r_539",
            "target": "s_485"
        },
        {
            "source": "r_539",
            "target": "s_145"
        },
        {
            "source": "r_539",
            "target": "s_151"
        },
        {
            "source": "r_540",
            "target": "s_183"
        },
        {
            "source": "r_540",
            "target": "s_42"
        },
        {
            "source": "r_540",
            "target": "s_401"
        },
        {
            "source": "r_540",
            "target": "s_940"
        },
        {
            "source": "r_540",
            "target": "s_721"
        },
        {
            "source": "r_541",
            "target": "s_1032"
        },
        {
            "source": "r_541",
            "target": "s_1394"
        },
        {
            "source": "r_541",
            "target": "s_442"
        },
        {
            "source": "r_541",
            "target": "s_795"
        },
        {
            "source": "r_541",
            "target": "s_979"
        },
        {
            "source": "r_541",
            "target": "s_952"
        },
        {
            "source": "r_542",
            "target": "s_1163"
        },
        {
            "source": "r_542",
            "target": "s_1274"
        },
        {
            "source": "r_542",
            "target": "s_82"
        },
        {
            "source": "r_542",
            "target": "s_685"
        },
        {
            "source": "r_542",
            "target": "s_327"
        },
        {
            "source": "r_542",
            "target": "s_1262"
        },
        {
            "source": "r_542",
            "target": "s_1528"
        },
        {
            "source": "r_542",
            "target": "s_277"
        },
        {
            "source": "r_542",
            "target": "s_1060"
        },
        {
            "source": "r_543",
            "target": "s_839"
        },
        {
            "source": "r_543",
            "target": "s_537"
        },
        {
            "source": "r_543",
            "target": "s_1193"
        },
        {
            "source": "r_543",
            "target": "s_757"
        },
        {
            "source": "r_544",
            "target": "s_622"
        },
        {
            "source": "r_544",
            "target": "s_282"
        },
        {
            "source": "r_544",
            "target": "s_732"
        },
        {
            "source": "r_544",
            "target": "s_175"
        },
        {
            "source": "r_544",
            "target": "s_994"
        },
        {
            "source": "r_544",
            "target": "s_531"
        },
        {
            "source": "r_544",
            "target": "s_840"
        },
        {
            "source": "r_545",
            "target": "s_684"
        },
        {
            "source": "r_545",
            "target": "s_1244"
        },
        {
            "source": "r_545",
            "target": "s_231"
        },
        {
            "source": "r_546",
            "target": "s_492"
        },
        {
            "source": "r_546",
            "target": "s_1332"
        },
        {
            "source": "r_546",
            "target": "s_131"
        },
        {
            "source": "r_547",
            "target": "s_1240"
        },
        {
            "source": "r_547",
            "target": "s_1126"
        },
        {
            "source": "r_547",
            "target": "s_212"
        },
        {
            "source": "r_548",
            "target": "s_778"
        },
        {
            "source": "r_548",
            "target": "s_865"
        },
        {
            "source": "r_548",
            "target": "s_855"
        },
        {
            "source": "r_548",
            "target": "s_333"
        },
        {
            "source": "r_549",
            "target": "s_59"
        },
        {
            "source": "r_549",
            "target": "s_586"
        },
        {
            "source": "r_549",
            "target": "s_476"
        },
        {
            "source": "r_550",
            "target": "s_275"
        },
        {
            "source": "r_550",
            "target": "s_1311"
        },
        {
            "source": "r_551",
            "target": "s_596"
        },
        {
            "source": "r_551",
            "target": "s_224"
        },
        {
            "source": "r_551",
            "target": "s_658"
        },
        {
            "source": "r_551",
            "target": "s_233"
        },
        {
            "source": "r_551",
            "target": "s_333"
        },
        {
            "source": "r_552",
            "target": "s_211"
        },
        {
            "source": "r_552",
            "target": "s_168"
        },
        {
            "source": "r_552",
            "target": "s_1531"
        },
        {
            "source": "r_552",
            "target": "s_1038"
        },
        {
            "source": "r_552",
            "target": "s_1169"
        },
        {
            "source": "r_552",
            "target": "s_768"
        },
        {
            "source": "r_552",
            "target": "s_647"
        },
        {
            "source": "r_552",
            "target": "s_1461"
        },
        {
            "source": "r_553",
            "target": "s_99"
        },
        {
            "source": "r_553",
            "target": "s_85"
        },
        {
            "source": "r_554",
            "target": "s_1017"
        },
        {
            "source": "r_554",
            "target": "s_1049"
        },
        {
            "source": "r_554",
            "target": "s_363"
        },
        {
            "source": "r_554",
            "target": "s_760"
        },
        {
            "source": "r_555",
            "target": "s_1328"
        },
        {
            "source": "r_555",
            "target": "s_1506"
        },
        {
            "source": "r_555",
            "target": "s_770"
        },
        {
            "source": "r_555",
            "target": "s_1136"
        },
        {
            "source": "r_556",
            "target": "s_1177"
        },
        {
            "source": "r_557",
            "target": "s_754"
        },
        {
            "source": "r_557",
            "target": "s_63"
        },
        {
            "source": "r_558",
            "target": "s_815"
        },
        {
            "source": "r_558",
            "target": "s_793"
        },
        {
            "source": "r_558",
            "target": "s_1203"
        },
        {
            "source": "r_558",
            "target": "s_63"
        },
        {
            "source": "r_558",
            "target": "s_905"
        },
        {
            "source": "r_559",
            "target": "s_1015"
        },
        {
            "source": "r_559",
            "target": "s_1498"
        },
        {
            "source": "r_559",
            "target": "s_1506"
        },
        {
            "source": "r_559",
            "target": "s_666"
        },
        {
            "source": "r_560",
            "target": "s_853"
        },
        {
            "source": "r_560",
            "target": "s_178"
        },
        {
            "source": "r_560",
            "target": "s_670"
        },
        {
            "source": "r_560",
            "target": "s_1360"
        },
        {
            "source": "r_560",
            "target": "s_10"
        },
        {
            "source": "r_561",
            "target": "s_116"
        },
        {
            "source": "r_561",
            "target": "s_1405"
        },
        {
            "source": "r_561",
            "target": "s_457"
        },
        {
            "source": "r_561",
            "target": "s_35"
        },
        {
            "source": "r_562",
            "target": "s_694"
        },
        {
            "source": "r_562",
            "target": "s_1200"
        },
        {
            "source": "r_562",
            "target": "s_1194"
        },
        {
            "source": "r_563",
            "target": "s_211"
        },
        {
            "source": "r_563",
            "target": "s_168"
        },
        {
            "source": "r_563",
            "target": "s_890"
        },
        {
            "source": "r_563",
            "target": "s_653"
        },
        {
            "source": "r_563",
            "target": "s_1038"
        },
        {
            "source": "r_563",
            "target": "s_1169"
        },
        {
            "source": "r_563",
            "target": "s_615"
        },
        {
            "source": "r_563",
            "target": "s_332"
        },
        {
            "source": "r_563",
            "target": "s_610"
        },
        {
            "source": "r_563",
            "target": "s_989"
        },
        {
            "source": "r_563",
            "target": "s_1190"
        },
        {
            "source": "r_564",
            "target": "s_655"
        },
        {
            "source": "r_564",
            "target": "s_1206"
        },
        {
            "source": "r_565",
            "target": "s_216"
        },
        {
            "source": "r_565",
            "target": "s_1208"
        },
        {
            "source": "r_565",
            "target": "s_1112"
        },
        {
            "source": "r_565",
            "target": "s_551"
        },
        {
            "source": "r_566",
            "target": "s_1492"
        },
        {
            "source": "r_566",
            "target": "s_1438"
        },
        {
            "source": "r_566",
            "target": "s_283"
        },
        {
            "source": "r_566",
            "target": "s_1461"
        },
        {
            "source": "r_566",
            "target": "s_202"
        },
        {
            "source": "r_567",
            "target": "s_388"
        },
        {
            "source": "r_567",
            "target": "s_979"
        },
        {
            "source": "r_567",
            "target": "s_148"
        },
        {
            "source": "r_568",
            "target": "s_453"
        },
        {
            "source": "r_568",
            "target": "s_1532"
        },
        {
            "source": "r_568",
            "target": "s_184"
        },
        {
            "source": "r_568",
            "target": "s_417"
        },
        {
            "source": "r_568",
            "target": "s_13"
        },
        {
            "source": "r_568",
            "target": "s_813"
        },
        {
            "source": "r_568",
            "target": "s_1245"
        },
        {
            "source": "r_569",
            "target": "s_53"
        },
        {
            "source": "r_569",
            "target": "s_308"
        },
        {
            "source": "r_569",
            "target": "s_93"
        },
        {
            "source": "r_570",
            "target": "s_46"
        },
        {
            "source": "r_570",
            "target": "s_514"
        },
        {
            "source": "r_570",
            "target": "s_1126"
        },
        {
            "source": "r_571",
            "target": "s_1218"
        },
        {
            "source": "r_571",
            "target": "s_326"
        },
        {
            "source": "r_571",
            "target": "s_85"
        },
        {
            "source": "r_572",
            "target": "s_893"
        },
        {
            "source": "r_572",
            "target": "s_241"
        },
        {
            "source": "r_572",
            "target": "s_889"
        },
        {
            "source": "r_572",
            "target": "s_511"
        },
        {
            "source": "r_573",
            "target": "s_1482"
        },
        {
            "source": "r_573",
            "target": "s_498"
        },
        {
            "source": "r_573",
            "target": "s_1216"
        },
        {
            "source": "r_573",
            "target": "s_309"
        },
        {
            "source": "r_574",
            "target": "s_443"
        },
        {
            "source": "r_574",
            "target": "s_848"
        },
        {
            "source": "r_574",
            "target": "s_650"
        },
        {
            "source": "r_574",
            "target": "s_1108"
        },
        {
            "source": "r_575",
            "target": "s_193"
        },
        {
            "source": "r_575",
            "target": "s_900"
        },
        {
            "source": "r_575",
            "target": "s_109"
        },
        {
            "source": "r_575",
            "target": "s_1281"
        },
        {
            "source": "r_575",
            "target": "s_699"
        },
        {
            "source": "r_576",
            "target": "s_596"
        },
        {
            "source": "r_576",
            "target": "s_627"
        },
        {
            "source": "r_576",
            "target": "s_233"
        },
        {
            "source": "r_576",
            "target": "s_333"
        },
        {
            "source": "r_577",
            "target": "s_234"
        },
        {
            "source": "r_577",
            "target": "s_1473"
        },
        {
            "source": "r_578",
            "target": "s_11"
        },
        {
            "source": "r_578",
            "target": "s_1195"
        },
        {
            "source": "r_579",
            "target": "s_730"
        },
        {
            "source": "r_579",
            "target": "s_682"
        },
        {
            "source": "r_579",
            "target": "s_1353"
        },
        {
            "source": "r_579",
            "target": "s_1152"
        },
        {
            "source": "r_579",
            "target": "s_588"
        },
        {
            "source": "r_579",
            "target": "s_840"
        },
        {
            "source": "r_579",
            "target": "s_990"
        },
        {
            "source": "r_580",
            "target": "s_1340"
        },
        {
            "source": "r_580",
            "target": "s_1109"
        },
        {
            "source": "r_580",
            "target": "s_907"
        },
        {
            "source": "r_580",
            "target": "s_1331"
        },
        {
            "source": "r_580",
            "target": "s_901"
        },
        {
            "source": "r_580",
            "target": "s_309"
        },
        {
            "source": "r_580",
            "target": "s_998"
        },
        {
            "source": "r_580",
            "target": "s_161"
        },
        {
            "source": "r_580",
            "target": "s_1342"
        },
        {
            "source": "r_580",
            "target": "s_546"
        },
        {
            "source": "r_581",
            "target": "s_1051"
        },
        {
            "source": "r_581",
            "target": "s_549"
        },
        {
            "source": "r_581",
            "target": "s_902"
        },
        {
            "source": "r_581",
            "target": "s_1290"
        },
        {
            "source": "r_581",
            "target": "s_638"
        },
        {
            "source": "r_582",
            "target": "s_92"
        },
        {
            "source": "r_582",
            "target": "s_691"
        },
        {
            "source": "r_582",
            "target": "s_310"
        },
        {
            "source": "r_582",
            "target": "s_22"
        },
        {
            "source": "r_582",
            "target": "s_1204"
        },
        {
            "source": "r_582",
            "target": "s_531"
        },
        {
            "source": "r_583",
            "target": "s_1462"
        },
        {
            "source": "r_583",
            "target": "s_925"
        },
        {
            "source": "r_583",
            "target": "s_1079"
        },
        {
            "source": "r_583",
            "target": "s_503"
        },
        {
            "source": "r_583",
            "target": "s_254"
        },
        {
            "source": "r_584",
            "target": "s_302"
        },
        {
            "source": "r_584",
            "target": "s_164"
        },
        {
            "source": "r_584",
            "target": "s_1047"
        },
        {
            "source": "r_585",
            "target": "s_838"
        },
        {
            "source": "r_585",
            "target": "s_333"
        },
        {
            "source": "r_586",
            "target": "s_1324"
        },
        {
            "source": "r_586",
            "target": "s_1126"
        },
        {
            "source": "r_587",
            "target": "s_1398"
        },
        {
            "source": "r_587",
            "target": "s_1093"
        },
        {
            "source": "r_587",
            "target": "s_190"
        },
        {
            "source": "r_587",
            "target": "s_1445"
        },
        {
            "source": "r_588",
            "target": "s_965"
        },
        {
            "source": "r_588",
            "target": "s_657"
        },
        {
            "source": "r_588",
            "target": "s_1115"
        },
        {
            "source": "r_588",
            "target": "s_756"
        },
        {
            "source": "r_589",
            "target": "s_120"
        },
        {
            "source": "r_589",
            "target": "s_305"
        },
        {
            "source": "r_589",
            "target": "s_403"
        },
        {
            "source": "r_589",
            "target": "s_233"
        },
        {
            "source": "r_589",
            "target": "s_1368"
        },
        {
            "source": "r_590",
            "target": "s_865"
        },
        {
            "source": "r_590",
            "target": "s_347"
        },
        {
            "source": "r_590",
            "target": "s_333"
        },
        {
            "source": "r_591",
            "target": "s_1225"
        },
        {
            "source": "r_591",
            "target": "s_1457"
        },
        {
            "source": "r_592",
            "target": "s_26"
        },
        {
            "source": "r_592",
            "target": "s_309"
        },
        {
            "source": "r_593",
            "target": "s_331"
        },
        {
            "source": "r_593",
            "target": "s_493"
        },
        {
            "source": "r_593",
            "target": "s_468"
        },
        {
            "source": "r_593",
            "target": "s_462"
        },
        {
            "source": "r_594",
            "target": "s_941"
        },
        {
            "source": "r_594",
            "target": "s_879"
        },
        {
            "source": "r_594",
            "target": "s_105"
        },
        {
            "source": "r_594",
            "target": "s_420"
        },
        {
            "source": "r_594",
            "target": "s_972"
        },
        {
            "source": "r_594",
            "target": "s_209"
        },
        {
            "source": "r_594",
            "target": "s_616"
        },
        {
            "source": "r_594",
            "target": "s_579"
        },
        {
            "source": "r_594",
            "target": "s_308"
        },
        {
            "source": "r_594",
            "target": "s_504"
        },
        {
            "source": "r_594",
            "target": "s_552"
        },
        {
            "source": "r_594",
            "target": "s_769"
        },
        {
            "source": "r_594",
            "target": "s_93"
        },
        {
            "source": "r_595",
            "target": "s_637"
        },
        {
            "source": "r_595",
            "target": "s_1337"
        },
        {
            "source": "r_595",
            "target": "s_1167"
        },
        {
            "source": "r_595",
            "target": "s_1346"
        },
        {
            "source": "r_595",
            "target": "s_1303"
        },
        {
            "source": "r_595",
            "target": "s_278"
        },
        {
            "source": "r_595",
            "target": "s_630"
        },
        {
            "source": "r_595",
            "target": "s_1038"
        },
        {
            "source": "r_596",
            "target": "s_1126"
        },
        {
            "source": "r_596",
            "target": "s_1411"
        },
        {
            "source": "r_596",
            "target": "s_156"
        },
        {
            "source": "r_596",
            "target": "s_514"
        },
        {
            "source": "r_596",
            "target": "s_1469"
        },
        {
            "source": "r_597",
            "target": "s_1244"
        },
        {
            "source": "r_598",
            "target": "s_565"
        },
        {
            "source": "r_598",
            "target": "s_1519"
        },
        {
            "source": "r_598",
            "target": "s_1070"
        },
        {
            "source": "r_598",
            "target": "s_515"
        },
        {
            "source": "r_599",
            "target": "s_903"
        },
        {
            "source": "r_599",
            "target": "s_1445"
        },
        {
            "source": "r_599",
            "target": "s_258"
        },
        {
            "source": "r_599",
            "target": "s_1415"
        },
        {
            "source": "r_599",
            "target": "s_1398"
        },
        {
            "source": "r_599",
            "target": "s_516"
        },
        {
            "source": "r_600",
            "target": "s_1519"
        },
        {
            "source": "r_600",
            "target": "s_333"
        },
        {
            "source": "r_600",
            "target": "s_68"
        },
        {
            "source": "r_600",
            "target": "s_233"
        },
        {
            "source": "r_601",
            "target": "s_975"
        },
        {
            "source": "r_601",
            "target": "s_1023"
        },
        {
            "source": "r_601",
            "target": "s_1526"
        },
        {
            "source": "r_602",
            "target": "s_427"
        },
        {
            "source": "r_603",
            "target": "s_296"
        },
        {
            "source": "r_603",
            "target": "s_126"
        },
        {
            "source": "r_603",
            "target": "s_1410"
        },
        {
            "source": "r_603",
            "target": "s_663"
        },
        {
            "source": "r_603",
            "target": "s_396"
        },
        {
            "source": "r_603",
            "target": "s_866"
        },
        {
            "source": "r_603",
            "target": "s_1047"
        },
        {
            "source": "r_604",
            "target": "s_1109"
        },
        {
            "source": "r_604",
            "target": "s_513"
        },
        {
            "source": "r_604",
            "target": "s_901"
        },
        {
            "source": "r_604",
            "target": "s_907"
        },
        {
            "source": "r_604",
            "target": "s_998"
        },
        {
            "source": "r_604",
            "target": "s_546"
        },
        {
            "source": "r_604",
            "target": "s_1342"
        },
        {
            "source": "r_605",
            "target": "s_707"
        },
        {
            "source": "r_605",
            "target": "s_786"
        },
        {
            "source": "r_605",
            "target": "s_1220"
        },
        {
            "source": "r_605",
            "target": "s_1468"
        },
        {
            "source": "r_605",
            "target": "s_477"
        },
        {
            "source": "r_605",
            "target": "s_796"
        },
        {
            "source": "r_606",
            "target": "s_1403"
        },
        {
            "source": "r_606",
            "target": "s_1038"
        },
        {
            "source": "r_606",
            "target": "s_83"
        },
        {
            "source": "r_606",
            "target": "s_95"
        },
        {
            "source": "r_607",
            "target": "s_126"
        },
        {
            "source": "r_607",
            "target": "s_396"
        },
        {
            "source": "r_607",
            "target": "s_866"
        },
        {
            "source": "r_607",
            "target": "s_296"
        },
        {
            "source": "r_607",
            "target": "s_1410"
        },
        {
            "source": "r_607",
            "target": "s_663"
        },
        {
            "source": "r_607",
            "target": "s_1047"
        },
        {
            "source": "r_607",
            "target": "s_551"
        },
        {
            "source": "r_608",
            "target": "s_296"
        },
        {
            "source": "r_608",
            "target": "s_126"
        },
        {
            "source": "r_608",
            "target": "s_1410"
        },
        {
            "source": "r_608",
            "target": "s_663"
        },
        {
            "source": "r_608",
            "target": "s_396"
        },
        {
            "source": "r_608",
            "target": "s_866"
        },
        {
            "source": "r_609",
            "target": "s_1109"
        },
        {
            "source": "r_609",
            "target": "s_513"
        },
        {
            "source": "r_609",
            "target": "s_901"
        },
        {
            "source": "r_609",
            "target": "s_907"
        },
        {
            "source": "r_610",
            "target": "s_707"
        },
        {
            "source": "r_610",
            "target": "s_786"
        },
        {
            "source": "r_610",
            "target": "s_796"
        }
    ]
}