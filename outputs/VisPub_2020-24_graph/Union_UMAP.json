{
    "directed": false,
    "multigraph": false,
    "graph": {
        "name": "VisPub_2020-24_graph"
    },
    "nodes": [
        {
            "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization",
            "data": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030418",
            "id": "r_0",
            "s_ids": [
                "s_1297",
                "s_1205",
                "s_729",
                "s_26",
                "s_244",
                "s_1104",
                "s_1554",
                "s_665"
            ],
            "type": "rich",
            "x": 10.032611846923828,
            "y": 10.507582664489746
        },
        {
            "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays",
            "data": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030460",
            "id": "r_1",
            "s_ids": [
                "s_257",
                "s_755",
                "s_1384"
            ],
            "type": "rich",
            "x": -5.766855239868164,
            "y": 18.480419158935547
        },
        {
            "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment",
            "data": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030450",
            "id": "r_2",
            "s_ids": [
                "s_1478",
                "s_1434",
                "s_1321",
                "s_269",
                "s_55",
                "s_210"
            ],
            "type": "rich",
            "x": -7.417187213897705,
            "y": 17.90383529663086
        },
        {
            "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet",
            "data": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030403",
            "id": "r_3",
            "s_ids": [
                "s_455",
                "s_1082",
                "s_226",
                "s_184",
                "s_989"
            ],
            "type": "rich",
            "x": -9.334024429321289,
            "y": 19.743906021118164
        },
        {
            "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries",
            "data": "Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030378",
            "id": "r_4",
            "s_ids": [
                "s_1027",
                "s_27",
                "s_850"
            ],
            "type": "rich",
            "x": -4.684976577758789,
            "y": 15.461275100708008
        },
        {
            "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
            "data": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209479",
            "id": "r_5",
            "s_ids": [
                "s_996",
                "s_1236",
                "s_851",
                "s_251",
                "s_689",
                "s_1486",
                "s_514"
            ],
            "type": "rich",
            "x": -8.649679183959961,
            "y": 16.446565628051758
        },
        {
            "title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation",
            "data": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114863",
            "id": "r_6",
            "s_ids": [
                "s_709",
                "s_1210",
                "s_353",
                "s_1605",
                "s_156"
            ],
            "type": "rich",
            "x": -9.94491958618164,
            "y": 18.606401443481445
        },
        {
            "title": "Preserving Minority Structures in Graph Sampling",
            "data": "Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030428",
            "id": "r_7",
            "s_ids": [
                "s_1429",
                "s_361",
                "s_482",
                "s_1162",
                "s_1049",
                "s_1152",
                "s_348",
                "s_1121",
                "s_308",
                "s_134"
            ],
            "type": "rich",
            "x": -9.25388240814209,
            "y": 17.559770584106445
        },
        {
            "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content",
            "data": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114770",
            "id": "r_8",
            "s_ids": [
                "s_297",
                "s_920"
            ],
            "type": "rich",
            "x": -4.3232574462890625,
            "y": 16.060781478881836
        },
        {
            "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization",
            "data": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030392",
            "id": "r_9",
            "s_ids": [
                "s_626",
                "s_1589",
                "s_152",
                "s_753",
                "s_237",
                "s_1249",
                "s_423",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.432251930236816,
            "y": 17.387325286865234
        },
        {
            "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions",
            "data": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030335",
            "id": "r_10",
            "s_ids": [
                "s_875",
                "s_1031",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.312108278274536,
            "y": 16.771453857421875
        },
        {
            "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics",
            "data": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030334",
            "id": "r_11",
            "s_ids": [
                "s_1021",
                "s_100",
                "s_269",
                "s_420",
                "s_427",
                "s_83",
                "s_1011",
                "s_1168",
                "s_1099",
                "s_210"
            ],
            "type": "rich",
            "x": -7.465552806854248,
            "y": 17.83583641052246
        },
        {
            "title": "Composition and Configuration Patterns in Multiple-View Visualizations",
            "data": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030338",
            "id": "r_12",
            "s_ids": [
                "s_574",
                "s_1438",
                "s_596",
                "s_569",
                "s_1533",
                "s_366"
            ],
            "type": "rich",
            "x": -4.877254009246826,
            "y": 15.964829444885254
        },
        {
            "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics",
            "data": "In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called \u201cstacked generalization\u201d) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030352",
            "id": "r_13",
            "s_ids": [
                "s_700",
                "s_1156",
                "s_529",
                "s_603"
            ],
            "type": "rich",
            "x": -3.950421094894409,
            "y": 19.762981414794922
        },
        {
            "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models",
            "data": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030342",
            "id": "r_14",
            "s_ids": [
                "s_897",
                "s_1511",
                "s_156"
            ],
            "type": "rich",
            "x": -10.065874099731445,
            "y": 18.57805633544922
        },
        {
            "title": "Dashboard Design Patterns",
            "data": "This paper introduces design patterns for dashboards to inform dashboard design processes. Despite a growing number of public examples, case studies, and general guidelines there is surprisingly little design guidance for dashboards. Such guidance is necessary to inspire designs and discuss tradeoffs in, e.g., screenspace, interaction, or information shown. Based on a systematic review of 144 dashboards, we report on eight groups of design patterns that provide common solutions in dashboard design. We discuss combinations of these patterns in \u201cdashboard genres\u201d such as narrative, analytical, or embedded dashboard. We ran a 2-week dashboard design workshop with 23 participants of varying expertise working on their own data and dashboards. We discuss the application of patterns for the dashboard design processes, as well as general design tradeoffs and common challenges. Our work complements previous surveys and aims to support dashboard designers and researchers in co-creation, structured design decisions, as well as future user evaluations about dashboard design guidelines. Detailed pattern descriptions and workshop material can be found online: https://dashboarddesignpatterns.github.io",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209448",
            "id": "r_15",
            "s_ids": [
                "s_1221",
                "s_663",
                "s_1490",
                "s_96",
                "s_1542",
                "s_724",
                "s_298"
            ],
            "type": "rich",
            "x": -7.0077409744262695,
            "y": 17.002302169799805
        },
        {
            "title": "Towards Better Bus Networks: A Visual Analytics Approach",
            "data": "Bus routes are typically updated every 3\u20135 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real-world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges, namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates, and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision-making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real-world data and received positive feedback from the experts. Index Terms-Bus route planning, spatial decision-making, urban data visual analytics",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030458",
            "id": "r_16",
            "s_ids": [
                "s_735",
                "s_758",
                "s_1308",
                "s_627",
                "s_1189",
                "s_105",
                "s_79",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.380578994750977,
            "y": 17.628076553344727
        },
        {
            "title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
            "data": "Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114861",
            "id": "r_17",
            "s_ids": [
                "s_152",
                "s_1574",
                "s_626",
                "s_336",
                "s_939",
                "s_1167",
                "s_1589",
                "s_647",
                "s_1415"
            ],
            "type": "rich",
            "x": -9.984320640563965,
            "y": 17.261417388916016
        },
        {
            "title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
            "data": "Traffic light detection is crucial for environment perception and decision-making in autonomous driving. State-of-the-art detectors are built upon deep Convolutional Neural Networks (CNNs) and have exhibited promising performance. However, one looming concern with CNN based detectors is how to thoroughly evaluate the performance of accuracy and robustness before they can be deployed to autonomous vehicles. In this work, we propose a visual analytics system, VATLD, equipped with a disentangled representation learning and semantic adversarial learning, to assess, understand, and improve the accuracy and robustness of traffic light detectors in autonomous driving applications. The disentangled representation learning extracts data semantics to augment human cognition with human-friendly visual summarization, and the semantic adversarial learning efficiently exposes interpretable robustness risks and enables minimal human interaction for actionable insights. We also demonstrate the effectiveness of various performance improvement strategies derived from actionable insights with our visual analytics system, VATLD, and illustrate some practical implications for safety-critical applications in autonomous driving.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030350",
            "id": "r_18",
            "s_ids": [
                "s_141",
                "s_699",
                "s_1043",
                "s_254",
                "s_1180",
                "s_1572",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.40147626399993896,
            "y": 17.5732479095459
        },
        {
            "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
            "data": "Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2 Lens, to visualize and explain multimodal models for sentiment analysis. M2 Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114794",
            "id": "r_19",
            "s_ids": [
                "s_1235",
                "s_538",
                "s_1272",
                "s_449",
                "s_1210",
                "s_156"
            ],
            "type": "rich",
            "x": -9.905343055725098,
            "y": 18.464176177978516
        },
        {
            "title": "Augmenting Sports Videos with VisCommentator",
            "data": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114806",
            "id": "r_20",
            "s_ids": [
                "s_1589",
                "s_626",
                "s_152",
                "s_1108",
                "s_647",
                "s_156",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.151012420654297,
            "y": 18.009902954101562
        },
        {
            "title": "PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes",
            "data": "In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players' passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team's employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team's passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030359",
            "id": "r_21",
            "s_ids": [
                "s_1574",
                "s_887",
                "s_495",
                "s_1341",
                "s_409",
                "s_647",
                "s_1385",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.14438533782959,
            "y": 17.640615463256836
        },
        {
            "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality",
            "data": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030435",
            "id": "r_22",
            "s_ids": [
                "s_1478",
                "s_84",
                "s_274",
                "s_677",
                "s_1126",
                "s_210"
            ],
            "type": "rich",
            "x": -7.546168804168701,
            "y": 17.929973602294922
        },
        {
            "title": "Revisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An Empirical Study",
            "data": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users' subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114694",
            "id": "r_23",
            "s_ids": [
                "s_308",
                "s_570",
                "s_1284",
                "s_1123",
                "s_1146",
                "s_348"
            ],
            "type": "rich",
            "x": -8.54440689086914,
            "y": 18.25780487060547
        },
        {
            "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing",
            "data": "Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators' feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209435",
            "id": "r_24",
            "s_ids": [
                "s_470",
                "s_1002",
                "s_783",
                "s_1042",
                "s_393"
            ],
            "type": "rich",
            "x": -8.430948257446289,
            "y": 17.1000919342041
        },
        {
            "title": "Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization",
            "data": "The combination of diverse data types and analysis tasks in genomics has resulted in the development of a wide range of visualization techniques and tools. However, most existing tools are tailored to a specific problem or data type and offer limited customization, making it challenging to optimize visualizations for new analysis tasks or datasets. To address this challenge, we designed Gosling-a grammar for interactive and scalable genomics data visualization. Gosling balances expressiveness for comprehensive multi-scale genomics data visualizations with accessibility for domain scientists. Our accompanying JavaScript toolkit called Gosling.js provides scalable and interactive rendering. Gosling.js is built on top of an existing platform for web-based genomics data visualization to further simplify the visualization of common genomics data formats. We demonstrate the expressiveness of the grammar through a variety of real-world examples. Furthermore, we show how Gosling supports the design of novel genomics visualizations. An online editor and examples of Gosling.js, its source code, and documentation are available at <uri>https://gosling.js.org</uri>.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114876",
            "id": "r_25",
            "s_ids": [
                "s_1266",
                "s_470",
                "s_1344",
                "s_393"
            ],
            "type": "rich",
            "x": -8.437018394470215,
            "y": 17.04268455505371
        },
        {
            "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?",
            "data": "Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants' response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030427",
            "id": "r_26",
            "s_ids": [
                "s_41",
                "s_1321",
                "s_689",
                "s_210",
                "s_1305",
                "s_1486"
            ],
            "type": "rich",
            "x": -7.825517177581787,
            "y": 17.579397201538086
        },
        {
            "title": "Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments",
            "data": "Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users' cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114846",
            "id": "r_27",
            "s_ids": [
                "s_1325",
                "s_163",
                "s_508",
                "s_493",
                "s_173"
            ],
            "type": "rich",
            "x": -3.3894450664520264,
            "y": 16.446468353271484
        },
        {
            "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning",
            "data": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030467",
            "id": "r_28",
            "s_ids": [
                "s_456",
                "s_62",
                "s_911",
                "s_311",
                "s_1380",
                "s_446",
                "s_1280",
                "s_932",
                "s_346",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.168953895568848,
            "y": 17.272502899169922
        },
        {
            "title": "Boba: Authoring and Visualizing Multiverse Analyses",
            "data": "Multiverse analysis is an approach to data analysis in which all \u201creasonable\u201d analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028985",
            "id": "r_29",
            "s_ids": [
                "s_1288",
                "s_875",
                "s_72",
                "s_1386"
            ],
            "type": "rich",
            "x": -3.6028330326080322,
            "y": 17.011445999145508
        },
        {
            "title": "Kori: Interactive Synthesis of Text and Charts in Data Documents",
            "data": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114802",
            "id": "r_30",
            "s_ids": [
                "s_14",
                "s_817",
                "s_1335",
                "s_429",
                "s_1401"
            ],
            "type": "rich",
            "x": -10.72455883026123,
            "y": 17.352869033813477
        },
        {
            "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics",
            "data": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030461",
            "id": "r_31",
            "s_ids": [
                "s_183",
                "s_883",
                "s_405",
                "s_1449",
                "s_58",
                "s_997"
            ],
            "type": "rich",
            "x": 7.535855770111084,
            "y": 5.239401817321777
        },
        {
            "title": "Compass: Towards Better Causal Analysis of Urban Time Series",
            "data": "The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time-varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long-time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in-depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time-varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi-dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real-world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114875",
            "id": "r_32",
            "s_ids": [
                "s_1308",
                "s_735",
                "s_1574",
                "s_1189",
                "s_105",
                "s_79",
                "s_1385",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.009161949157715,
            "y": 17.435535430908203
        },
        {
            "title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
            "data": "Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent's reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts' feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114832",
            "id": "r_33",
            "s_ids": [
                "s_1087",
                "s_1160",
                "s_270",
                "s_246",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.315354347229004,
            "y": 17.189992904663086
        },
        {
            "title": "Affective Learning Objectives for Communicative Visualizations",
            "data": "When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents\u2013those that seek to influence or leverage the audience's opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived \u2018neutrality\u2019 or are \u2018political,\u2019 designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions\u2013such as advocacy visualizations or persuasive cartography\u2013we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209500",
            "id": "r_34",
            "s_ids": [
                "s_1132",
                "s_477"
            ],
            "type": "rich",
            "x": -3.170497179031372,
            "y": 16.877840042114258
        },
        {
            "title": "VizLinter: A Linter and Fixer Framework for Data Visualization",
            "data": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114804",
            "id": "r_35",
            "s_ids": [
                "s_293",
                "s_226",
                "s_1082",
                "s_1299",
                "s_351",
                "s_989"
            ],
            "type": "rich",
            "x": -9.334222793579102,
            "y": 19.70109748840332
        },
        {
            "title": "What Makes a Data-GIF Understandable?",
            "data": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030396",
            "id": "r_36",
            "s_ids": [
                "s_1516",
                "s_877",
                "s_765",
                "s_1221",
                "s_1415",
                "s_156"
            ],
            "type": "rich",
            "x": -10.34140682220459,
            "y": 17.387149810791016
        },
        {
            "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines",
            "data": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030361",
            "id": "r_37",
            "s_ids": [
                "s_101",
                "s_959",
                "s_1347",
                "s_533",
                "s_728",
                "s_428"
            ],
            "type": "rich",
            "x": -4.037666320800781,
            "y": 20.140613555908203
        },
        {
            "title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation",
            "data": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114826",
            "id": "r_38",
            "s_ids": [
                "s_877",
                "s_737",
                "s_367",
                "s_1055",
                "s_404",
                "s_156",
                "s_155"
            ],
            "type": "rich",
            "x": -10.626473426818848,
            "y": 18.319528579711914
        },
        {
            "title": "Visual Causality Analysis of Event Sequence Data",
            "data": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030465",
            "id": "r_39",
            "s_ids": [
                "s_721",
                "s_459",
                "s_1387",
                "s_157",
                "s_976",
                "s_989"
            ],
            "type": "rich",
            "x": -9.080272674560547,
            "y": 19.73349380493164
        },
        {
            "title": "Cartographic Relief Shading with Neural Networks",
            "data": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030456",
            "id": "r_40",
            "s_ids": [
                "s_55",
                "s_1008",
                "s_1389",
                "s_589",
                "s_830",
                "s_960"
            ],
            "type": "rich",
            "x": -7.294429302215576,
            "y": 17.98094367980957
        },
        {
            "title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics",
            "data": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030448",
            "id": "r_41",
            "s_ids": [
                "s_517",
                "s_1439",
                "s_1010",
                "s_1382",
                "s_404",
                "s_155"
            ],
            "type": "rich",
            "x": -10.849642753601074,
            "y": 18.25053596496582
        },
        {
            "title": "Evaluation of Sampling Methods for Scatterplots",
            "data": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030432",
            "id": "r_42",
            "s_ids": [
                "s_421",
                "s_425",
                "s_308",
                "s_1280",
                "s_348"
            ],
            "type": "rich",
            "x": -9.384607315063477,
            "y": 17.465473175048828
        },
        {
            "title": "QualDash: Adaptable Generation of Visualisation Dashboards for Healthcare Quality Improvement",
            "data": "Adapting dashboard design to different contexts of use is an open question in visualisation research. Dashboard designers often seek to strike a balance between dashboard adaptability and ease-of-use, and in hospitals challenges arise from the vast diversity of key metrics, data models and users involved at different organizational levels. In this design study, we present QualDash, a dashboard generation engine that allows for the dynamic configuration and deployment of visualisation dashboards for healthcare quality improvement (QI). We present a rigorous task analysis based on interviews with healthcare professionals, a co-design workshop and a series of one-on-one meetings with front line analysts. From these activities we define a metric card metaphor as a unit of visual analysis in healthcare QI, using this concept as a building block for generating highly adaptable dashboards, and leading to the design of a Metric Specification Structure (MSS). Each MSS is a JSON structure which enables dashboard authors to concisely configure unit-specific variants of a metric card, while offloading common patterns that are shared across cards to be preset by the engine. We reflect on deploying and iterating the design of OualDash in cardiology wards and pediatric intensive care units of five NHS hospitals. Finally, we report evaluation results that demonstrate the adaptability, ease-of-use and usefulness of QualDash in a real-world scenario.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030424",
            "id": "r_43",
            "s_ids": [
                "s_368",
                "s_1239",
                "s_12",
                "s_747",
                "s_176",
                "s_1114",
                "s_1535"
            ],
            "type": "rich",
            "x": -6.464885711669922,
            "y": 16.533206939697266
        },
        {
            "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction",
            "data": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028889",
            "id": "r_44",
            "s_ids": [
                "s_1317",
                "s_1310",
                "s_634",
                "s_586",
                "s_1467",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7858415246009827,
            "y": 17.9387149810791
        },
        {
            "title": "KD-Box: Line-segment-based KD-tree for Interactive Exploration of Large-scale Time-Series Data",
            "data": "Time-series data-usually presented in the form of lines-plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114865",
            "id": "r_45",
            "s_ids": [
                "s_1393",
                "s_1146",
                "s_1441",
                "s_1479",
                "s_93",
                "s_646"
            ],
            "type": "rich",
            "x": -8.051802635192871,
            "y": 18.424768447875977
        },
        {
            "title": "A Design Space for Applying the Freytag's Pyramid Structure to Data Stories",
            "data": "Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag's Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag's Pyramid to data stories, little systematic and practical guidance is available on how to use Freytag's Pyramid for creating structured data stories. To bridge this gap, we examined how existing practices apply Freytag's Pyramid by analyzing stories extracted from 103 data videos. Based on our findings, we proposed a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through story creation. We evaluated the proposed design space through a workshop with 25 participants. Results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag's Pyramid.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114774",
            "id": "r_46",
            "s_ids": [
                "s_1311",
                "s_1115",
                "s_750",
                "s_1358",
                "s_459",
                "s_184",
                "s_156",
                "s_989"
            ],
            "type": "rich",
            "x": -9.46800708770752,
            "y": 19.37660026550293
        },
        {
            "title": "Gemini: A Grammar and Recommender System for Animated Transitions in Statistical Graphics",
            "data": "Animated transitions help viewers follow changes between related visualizations. Specifying effective animations demands significant effort: authors must select the elements and properties to animate, provide transition parameters, and coordinate the timing of stages. To facilitate this process, we present Gemini, a declarative grammar and recommendation system for animated transitions between single-view statistical graphics. Gemini specifications define transition \u201csteps\u201d in terms of high-level visual components (marks, axes, legends) and composition rules to synchronize and concatenate steps. With this grammar, Gemini can recommend animation designs to augment and accelerate designers' work. Gemini enumerates staged animation designs for given start and end states, and ranks those designs using a cost function informed by prior perceptual studies. To evaluate Gemini, we conduct both a formative study on Mechanical Turk to assess and tune our ranking function, and a summative study in which 8 experienced visualization developers implement animations in D3 that we then compare to Gemini's suggestions. We find that most designs (9/11) are exactly replicable in Gemini, with many (8/11) achievable via edits to suggestions, and that Gemini suggestions avoid multiple participant errors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030360",
            "id": "r_47",
            "s_ids": [
                "s_858",
                "s_1386"
            ],
            "type": "rich",
            "x": -3.7683827877044678,
            "y": 17.041162490844727
        },
        {
            "title": "Untidy Data: The Unreasonable Effectiveness of Tables",
            "data": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets \u2014 the quintessential table tool \u2014 remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for <i>data workers</i> <xref ref-type=\"bibr\" rid=\"ref61\">[61]</xref>, people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and \u201cget their hands on\u201d the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114830",
            "id": "r_48",
            "s_ids": [
                "s_842",
                "s_1340",
                "s_1331"
            ],
            "type": "rich",
            "x": -4.245246410369873,
            "y": 16.0910587310791
        },
        {
            "title": "Context-aware Sampling of Large Networks via Graph Representation Learning",
            "data": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030440",
            "id": "r_49",
            "s_ids": [
                "s_1121",
                "s_912",
                "s_1520",
                "s_142",
                "s_849",
                "s_800",
                "s_1429",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.27978801727295,
            "y": 17.720273971557617
        },
        {
            "title": "Rainbows Revisited: Modeling Effective Colormap Design for Graphical Inference",
            "data": "Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are, particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric-color name variation-impacts people's ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants' performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow, which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap's performance and suggest alternative guidelines for designing new quantitative colormaps to support inference. The data and materials for this paper are available at: https://osf.io/tck2r/",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030439",
            "id": "r_50",
            "s_ids": [
                "s_1080",
                "s_781"
            ],
            "type": "rich",
            "x": -2.1509599685668945,
            "y": 18.05047607421875
        },
        {
            "title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
            "data": "Time-series data is widely studied in various scenarios, like weather forecast, stock market, customer behavior analysis. To comprehensively learn about the dynamic environments, it is necessary to comprehend features from multiple data sources. This paper proposes a novel visual analysis approach for detecting and analyzing concept drifts from multi-sourced time-series. We propose a visual detection scheme for discovering concept drifts from multiple sourced time-series based on prediction models. We design a drift level index to depict the dynamics, and a consistency judgment model to justify whether the concept drifts from various sources are consistent. Our integrated visual interface, ConceptExplorer, facilitates visual exploration, extraction, understanding, and comparison of concepts and concept drifts from multi-source time-series data. We conduct three case studies and expert interviews to verify the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00006",
            "id": "r_51",
            "s_ids": [
                "s_1531",
                "s_1385",
                "s_308",
                "s_1451",
                "s_287",
                "s_1202",
                "s_79",
                "s_578"
            ],
            "type": "rich",
            "x": -9.226168632507324,
            "y": 18.125951766967773
        },
        {
            "title": "Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models",
            "data": "The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114837",
            "id": "r_52",
            "s_ids": [
                "s_1612",
                "s_1204",
                "s_616",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.3598942458629608,
            "y": 17.433584213256836
        },
        {
            "title": "VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models",
            "data": "Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that <i>connect the dots</i> between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114836",
            "id": "r_53",
            "s_ids": [
                "s_897",
                "s_1160",
                "s_1226",
                "s_596",
                "s_1273",
                "s_61",
                "s_156",
                "s_824"
            ],
            "type": "rich",
            "x": -10.1483736038208,
            "y": 18.5379638671875
        },
        {
            "title": "Interactive Dimensionality Reduction for Comparative Analysis",
            "data": "Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114807",
            "id": "r_54",
            "s_ids": [
                "s_1317",
                "s_670",
                "s_1425",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7597665786743164,
            "y": 18.063989639282227
        },
        {
            "title": "Kineticharts: Augmenting Affective Expressiveness of Charts in Data Stories with Animation Design",
            "data": "Data stories often seek to elicit affective feelings from viewers. However, how to design affective data stories remains under-explored. In this work, we investigate one specific design factor, animation, and present Kineticharts, an animation design scheme for creating charts that express five positive affects: joy, amusement, surprise, tenderness, and excitement. These five affects were found to be frequently communicated through animation in data stories. Regarding each affect, we designed varied kinetic motions represented by bar charts, line charts, and pie charts, resulting in 60 animated charts for the five affects. We designed Kineticharts by first conducting a need-finding study with professional practitioners from data journalism and then analyzing a corpus of affective motion graphics to identify salient kinetic patterns. We evaluated Kineticharts through two user studies. The results suggest that Kineticharts can accurately convey affects, and improve the expressiveness of data stories, as well as enhance user engagement without hindering data comprehension compared to the animation design from DataClips, an authoring tool for data videos.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114775",
            "id": "r_55",
            "s_ids": [
                "s_750",
                "s_184",
                "s_1131",
                "s_1088",
                "s_989"
            ],
            "type": "rich",
            "x": -9.340970039367676,
            "y": 19.71561050415039
        },
        {
            "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics",
            "data": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030410",
            "id": "r_56",
            "s_ids": [
                "s_1438",
                "s_974",
                "s_461",
                "s_676",
                "s_308",
                "s_96",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.481925010681152,
            "y": 17.854761123657227
        },
        {
            "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data",
            "data": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030374",
            "id": "r_57",
            "s_ids": [
                "s_753",
                "s_431",
                "s_667",
                "s_948",
                "s_779",
                "s_938",
                "s_673",
                "s_1181"
            ],
            "type": "rich",
            "x": -10.91034984588623,
            "y": 16.9282283782959
        },
        {
            "title": "Design Patterns for Situated Visualization in Augmented Reality",
            "data": "Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327398",
            "id": "r_58",
            "s_ids": [
                "s_1478",
                "s_899",
                "s_1515"
            ],
            "type": "rich",
            "x": -8.325372695922852,
            "y": 18.69918441772461
        },
        {
            "title": "MetroSets: Visualizing Sets as Metro Maps",
            "data": "We propose MetroSets, a new, flexible online tool for visualizing set systems using the metro map metaphor. We model a given set system as a hypergraph $\\mathcal{H}=(V,\\ \\mathcal{S})$, consisting of a set $V$ of vertices and a set $\\mathcal{S}$, which contains subsets of $V$ called hyperedges. Our system then computes a metro map representation of $\\mathcal{H}$, where each hyperedge $E$ in $\\mathcal{S}$ corresponds to a metro line and each vertex corresponds to a metro station. Vertices that appear in two or more hyperedges are drawn as interchanges in the metro map, connecting the different sets. MetroSets is based on a modular 4-step pipeline which constructs and optimizes a path-based hypergraph support, which is then drawn and schematized using metro map layout algorithms. We propose and implement multiple algorithms for each step of the MetroSet pipeline and provide a functional prototype with easy-to-use preset configurations. Furthermore, using several real-world datasets, we perform an extensive quantitative evaluation of the impact of different pipeline stages on desirable properties of the generated maps, such as octolinearity, monotonicity, and edge uniformity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030475",
            "id": "r_59",
            "s_ids": [
                "s_601",
                "s_262",
                "s_1459",
                "s_124"
            ],
            "type": "rich",
            "x": -5.580721855163574,
            "y": 15.103619575500488
        },
        {
            "title": "Scalability of Network Visualisation from a Cognitive Load Perspective",
            "data": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030459",
            "id": "r_60",
            "s_ids": [
                "s_587",
                "s_41",
                "s_210",
                "s_600",
                "s_1475",
                "s_1305"
            ],
            "type": "rich",
            "x": -7.545261859893799,
            "y": 17.86939239501953
        },
        {
            "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction",
            "data": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030447",
            "id": "r_61",
            "s_ids": [
                "s_1072",
                "s_1385",
                "s_658",
                "s_941",
                "s_410",
                "s_1539"
            ],
            "type": "rich",
            "x": -9.561711311340332,
            "y": 17.957963943481445
        },
        {
            "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework",
            "data": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030423",
            "id": "r_62",
            "s_ids": [
                "s_877",
                "s_961",
                "s_210",
                "s_274",
                "s_1124",
                "s_156"
            ],
            "type": "rich",
            "x": -7.420721054077148,
            "y": 18.048152923583984
        },
        {
            "title": "PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation",
            "data": "Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327168",
            "id": "r_63",
            "s_ids": [
                "s_91",
                "s_1235",
                "s_1362",
                "s_1098",
                "s_224",
                "s_1072",
                "s_654",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.486387252807617,
            "y": 18.07782745361328
        },
        {
            "title": "Towards Natural Language-Based Visualization Authoring",
            "data": "A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users' visualization editing intents, called editing actions, based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209357",
            "id": "r_64",
            "s_ids": [
                "s_737",
                "s_714",
                "s_457",
                "s_1111",
                "s_419",
                "s_636",
                "s_404",
                "s_155"
            ],
            "type": "rich",
            "x": -10.847156524658203,
            "y": 18.23579216003418
        },
        {
            "title": "Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects",
            "data": "Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose <i>VASS</i>, a <i><u>V</u></i>isual <i><u>A</u></i>nalytics approach to diagnosing and improving the accuracy and robustness of <i><u>S</u></i>emantic <i><u>S</u></i>egmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models' performance. We then use it to guide the generation of adversarial examples to evaluate models' spatial robustness and obtain actionable insights. We demonstrate the effectiveness of <i>VASS</i> via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models' performance with actionable insights obtained from <i>VASS</i>.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114855",
            "id": "r_65",
            "s_ids": [
                "s_1095",
                "s_699",
                "s_1180",
                "s_141",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.3910301923751831,
            "y": 17.55990219116211
        },
        {
            "title": "Topology Density Map for Urban Data Visualization and Analysis",
            "data": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030469",
            "id": "r_66",
            "s_ids": [
                "s_623",
                "s_709",
                "s_1438",
                "s_825",
                "s_156"
            ],
            "type": "rich",
            "x": -9.9420804977417,
            "y": 18.128610610961914
        },
        {
            "title": "The Quest for Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences",
            "data": "Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes' positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans' game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans' in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans' in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209353",
            "id": "r_67",
            "s_ids": [
                "s_416",
                "s_1589",
                "s_41",
                "s_1298",
                "s_689",
                "s_1486"
            ],
            "type": "rich",
            "x": -8.837972640991211,
            "y": 16.548809051513672
        },
        {
            "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
            "data": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&amp;M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&amp;M experts\u2014all of them blind\u2014to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible\u2014using sonification and auralization. However, our experts recommended supporting a combination of senses\u2014sound and touch\u2014to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114829",
            "id": "r_68",
            "s_ids": [
                "s_125",
                "s_187",
                "s_524",
                "s_791",
                "s_793",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.0067896842956543,
            "y": 16.035276412963867
        },
        {
            "title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms",
            "data": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114814",
            "id": "r_69",
            "s_ids": [
                "s_1400",
                "s_1274",
                "s_1226",
                "s_69",
                "s_618",
                "s_802",
                "s_110",
                "s_1025"
            ],
            "type": "rich",
            "x": -4.135924816131592,
            "y": 16.86663818359375
        },
        {
            "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children",
            "data": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030464",
            "id": "r_70",
            "s_ids": [
                "s_312",
                "s_1571",
                "s_702",
                "s_198"
            ],
            "type": "rich",
            "x": -6.616936206817627,
            "y": 16.789398193359375
        },
        {
            "title": "Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making",
            "data": "Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts - who often have no expertise in ML or data science - are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, Sibyl, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114864",
            "id": "r_71",
            "s_ids": [
                "s_1273",
                "s_1160",
                "s_756",
                "s_824"
            ],
            "type": "rich",
            "x": -10.238370895385742,
            "y": 18.03723907470703
        },
        {
            "title": "Modeling in the Time of COVID-19: Statistical and Rule-based Mesoscale Models",
            "data": "We present a new technique for the rapid modeling and construction of scientifically accurate mesoscale biological models. The resulting 3D models are based on a few 2D microscopy scans and the latest knowledge available about the biological entity, represented as a set of geometric relationships. Our new visual-programming technique is based on statistical and rule-based modeling approaches that are rapid to author, fast to construct, and easy to revise. From a few 2D microscopy scans, we determine the statistical properties of various structural aspects, such as the outer membrane shape, the spatial properties, and the distribution characteristics of the macromolecular elements on the membrane. This information is utilized in the construction of the 3D model. Once all the imaging evidence is incorporated into the model, additional information can be incorporated by interactively defining the rules that spatially characterize the rest of the biological entity, such as mutual interactions among macromolecules, and their distances and orientations relative to other structures. These rules are defined through an intuitive 3D interactive visualization as a visual-programming feedback loop. We demonstrate the applicability of our approach on a use case of the modeling procedure of the SARS-CoV-2 virion ultrastructure. This atomistic model, which we present here, can steer biological research to new promising directions in our efforts to fight the spread of the virus.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030415",
            "id": "r_72",
            "s_ids": [
                "s_371",
                "s_694",
                "s_520",
                "s_723",
                "s_222",
                "s_629",
                "s_185",
                "s_1558",
                "s_355",
                "s_584",
                "s_510"
            ],
            "type": "rich",
            "x": -6.225412368774414,
            "y": 19.003862380981445
        },
        {
            "title": "Lyra 2: Designing Interactive Visualizations by Demonstration",
            "data": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030367",
            "id": "r_73",
            "s_ids": [
                "s_801",
                "s_1294",
                "s_313",
                "s_920"
            ],
            "type": "rich",
            "x": -4.2969970703125,
            "y": 16.08725357055664
        },
        {
            "title": "A Visual Analytics Approach for Exploratory Causal Analysis: Exploration, Validation, and Applications",
            "data": "Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028957",
            "id": "r_74",
            "s_ids": [
                "s_1574",
                "s_1226",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.390738487243652,
            "y": 17.40200424194336
        },
        {
            "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation",
            "data": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028947",
            "id": "r_75",
            "s_ids": [
                "s_389",
                "s_695",
                "s_1349"
            ],
            "type": "rich",
            "x": -3.5072460174560547,
            "y": 19.7928409576416
        },
        {
            "title": "A Critical Reflection on Visualization Research: Where Do Decision Making Tasks Hide?",
            "data": "It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user <i>task</i>, we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114813",
            "id": "r_76",
            "s_ids": [
                "s_203",
                "s_850"
            ],
            "type": "rich",
            "x": -4.447474479675293,
            "y": 15.262223243713379
        },
        {
            "title": "Communicative Visualizations as a Learning Problem",
            "data": "Significant research has provided robust task and evaluation languages for the analysis of exploratory visualizations. Unfortunately, these taxonomies fail when applied to communicative visualizations. Instead, designers often resort to evaluating communicative visualizations from the cognitive efficiency perspective: \u201ccan the recipient accurately decode my message/insight?\u201d However, designers are unlikely to be satisfied if the message went \u2018in one ear and out the other.\u2019 The consequence of this inconsistency is that it is difficult to design or select between competing options in a principled way. The problem we address is the fundamental mismatch between how designers want to describe their intent, and the language they have. We argue that visualization designers can address this limitation through a learning lens: that the recipient is a student and the designer a teacher. By using learning objectives, designers can better define, assess, and compare communicative visualizations. We illustrate how the learning-based approach provides a framework for understanding a wide array of communicative goals. To understand how the framework can be applied (and its limitations), we surveyed and interviewed members of the Data Visualization Society using their own visualizations as a probe. Through this study we identified the broad range of objectives in communicative visualizations and the prevalence of certain objective types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030375",
            "id": "r_77",
            "s_ids": [
                "s_477",
                "s_75"
            ],
            "type": "rich",
            "x": -3.1493146419525146,
            "y": 16.86516571044922
        },
        {
            "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes",
            "data": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028888",
            "id": "r_78",
            "s_ids": [
                "s_48",
                "s_880",
                "s_1232",
                "s_821",
                "s_815"
            ],
            "type": "rich",
            "x": -4.251132488250732,
            "y": 21.07118797302246
        },
        {
            "title": "Visual Analysis of Discrimination in Machine Learning",
            "data": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030471",
            "id": "r_79",
            "s_ids": [
                "s_470",
                "s_1045",
                "s_1589",
                "s_1210",
                "s_348",
                "s_156"
            ],
            "type": "rich",
            "x": -9.141372680664062,
            "y": 17.373796463012695
        },
        {
            "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions",
            "data": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030419",
            "id": "r_80",
            "s_ids": [
                "s_1266",
                "s_1442",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.3789105415344238,
            "y": 18.056167602539062
        },
        {
            "title": "Palettailor: Discriminable Colorization for Categorical Data",
            "data": "We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030406",
            "id": "r_81",
            "s_ids": [
                "s_862",
                "s_809",
                "s_363",
                "s_899",
                "s_1345",
                "s_805",
                "s_53",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.212104797363281,
            "y": 18.74017906188965
        },
        {
            "title": "Diagnosing Concept Drift with Visual Analytics",
            "data": "Concept drift is a phenomenon in which the distribution of a data stream changes over time in unforeseen ways, causing prediction models built on historical data to become inaccurate. While a variety of automated methods have been developed to identify when concept drift occurs, there is limited support for analysts who need to understand and correct their models when drift is detected. In this paper, we present a visual analytics method, DriftVis, to support model builders and analysts in the identification and correction of concept drift in streaming data. DriftVis combines a distribution-based drift detection method with a streaming scatterplot to support the analysis of drift caused by the distribution changes of data streams and to explore the impact of these changes on the model\u2019s accuracy. A quantitative experiment and two case studies on weather prediction and text classification have been conducted to demonstrate our proposed tool and illustrate how visual analytics can be used to support the detection, examination, and correction of concept drift.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00007",
            "id": "r_82",
            "s_ids": [
                "s_148",
                "s_943",
                "s_548",
                "s_894",
                "s_726",
                "s_815",
                "s_348"
            ],
            "type": "rich",
            "x": -9.237787246704102,
            "y": 17.269519805908203
        },
        {
            "title": "ggdist: Visualizations of Distributions and Uncertainty in the Grammar of Graphics",
            "data": "The grammar of graphics is ubiquitous, providing the foundation for a variety of popular visualization tools and toolkits. Yet support for uncertainty visualization in the grammar graphics\u2014beyond simple variations of error bars, uncertainty bands, and density plots\u2014remains rudimentary. Research in uncertainty visualization has developed a rich variety of improved uncertainty visualizations, most of which are difficult to create in existing grammar of graphics implementations. ggdist, an extension to the popular ggplot2 grammar of graphics toolkit, is an attempt to rectify this situation. ggdist unifies a variety of uncertainty visualization types through the lens of distributional visualization, allowing functions of distributions to be mapped to directly to visual channels (aesthetics), making it straightforward to express a variety of (sometimes weird!) uncertainty visualization types. This distributional lens also offers a way to unify Bayesian and frequentist uncertainty visualization by formalizing the latter with the help of confidence distributions. In this paper, I offer a description of this uncertainty visualization paradigm and lessons learned from its development and adoption: ggdist has existed in some form for about six years (originally as part of the tidybayes R package for post-processing Bayesian models), and it has evolved substantially over that time, with several rewrites and API re-organizations as it changed in response to user feedback and expanded to cover increasing varieties of uncertainty visualization types. Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experience with ggdist will provide a catalyst for further improvements to formalizations and implementations of uncertainty visualization in grammar of graphics ecosystems. A free copy of this paper is available at https://osf.io/2gsz6. All supplemental materials are available at https://github.com/mjskay/ggdist-paper and are archived on Zenodo at doi:10.5281/zenodo.7770984.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327195",
            "id": "r_83",
            "s_ids": [
                "s_1031"
            ],
            "type": "rich",
            "x": -3.2106516361236572,
            "y": 16.611797332763672
        },
        {
            "title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
            "data": "In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114797",
            "id": "r_84",
            "s_ids": [
                "s_294",
                "s_23",
                "s_42",
                "s_899",
                "s_103",
                "s_1509",
                "s_578",
                "s_990"
            ],
            "type": "rich",
            "x": -8.619284629821777,
            "y": 18.616809844970703
        },
        {
            "title": "Sequence Braiding: Visual Overviews of Temporal Event Sequences and Attributes",
            "data": "Temporal event sequence alignment has been used in many domains to visualize nuanced changes and interactions over time. Existing approaches align one or two sentinel events. Overview tasks require examining all alignments of interest using interaction and time or juxtaposition of many visualizations. Furthermore, any event attribute overviews are not closely tied to sequence visualizations. We present Sequence Braiding, a novel overview visualization for temporal event sequences and attributes using a layered directed acyclic network. Sequence Braiding visually aligns many temporal events and attribute groups simultaneously and supports arbitrary ordering, absence, and duplication of events. In a controlled experiment we compare Sequence Braiding and IDMVis on user task completion time, correctness, error, and confidence. Our results provide good evidence that users of Sequence Braiding can understand high-level patterns and trends faster and with similar error. A full version of this paper with all appendices; the evaluation stimuli, data, and analysis code; and source code are available at $\\text{osf.io}/\\mathrm{mq}2\\mathrm{wt}$.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030442",
            "id": "r_85",
            "s_ids": [
                "s_286",
                "s_879",
                "s_572",
                "s_1006"
            ],
            "type": "rich",
            "x": -6.727741718292236,
            "y": 21.1700382232666
        },
        {
            "title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
            "data": "In this work, we propose a generic visual analytics framework to support tactic analysis based on data collected from racquet sports (such as tennis and badminton). The proposed approach models each rally in a game as a sequence of hits (i.e., events) until one athlete scores a point. Each hit can be described with a set of attributes, such as the positions of the ball and the techniques used to hit the ball (such as drive and volley in tennis). Thus, the mentioned sequence of hits can be viewed as a multivariate event sequence. By detecting and analyzing the multivariate subsequences that frequently occur in the rallies (namely, tactical patterns), athletes can gain insights into the playing styles adopted by their opponents, and therefore help them identify systematic weaknesses of the opponents and develop counter strategies in matches. To support such analysis effectively, we propose a steerable multivariate sequential pattern mining algorithm with adjustable weights over event attributes, such that the domain expert can obtain frequent tactical patterns according to the attributes specified by himself. We also propose a re-configurable glyph design to help users simultaneously analyze multiple attributes of the hits. The framework further supports comparative analysis of the tactical patterns, e.g., for different athletes or the same athlete playing under different conditions. By applying the framework on two datasets collected in tennis and badminton matches, we demonstrate that the system is generic and effective for tactic analysis in sports and can help identify signature techniques used by individual athletes. Finally, we discuss the strengths and limitations of the proposed approach based on the feedback from the domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00009",
            "id": "r_86",
            "s_ids": [
                "s_1087",
                "s_270",
                "s_56",
                "s_246",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.171313285827637,
            "y": 17.41390037536621
        },
        {
            "title": "Perception! Immersion! Empowerment! Superpowers as Inspiration for Visualization",
            "data": "We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations' ability to \u201cmake the invisible visible\u201d and to \u201cenhance cognitive abilities.\u201d Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of \u201cvisualization superpowers\u201d and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114844",
            "id": "r_87",
            "s_ids": [
                "s_392",
                "s_1388",
                "s_978",
                "s_245",
                "s_1487",
                "s_326",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.344207286834717,
            "y": 18.250524520874023
        },
        {
            "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
            "data": "This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance [104] and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the $L$2-Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks [110] for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach - with barycenter computations in the orders of minutes for the largest examples - as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114839",
            "id": "r_88",
            "s_ids": [
                "s_1462",
                "s_635",
                "s_1067",
                "s_707"
            ],
            "type": "rich",
            "x": -4.0712890625,
            "y": 20.731643676757812
        },
        {
            "title": "Examining Effort in 1D Uncertainty Communication Using Individual Differences in Working Memory and NASA-TLX",
            "data": "As uncertainty visualizations for general audiences become increasingly common, designers must understand the full impact of uncertainty communication techniques on viewers' decision processes. Prior work demonstrates mixed performance outcomes with respect to how individuals make decisions using various visual and textual depictions of uncertainty. Part of the inconsistency across findings may be due to an over-reliance on task accuracy, which cannot, on its own, provide a comprehensive understanding of how uncertainty visualization techniques support reasoning processes. In this work, we advance the debate surrounding the efficacy of modern 1D uncertainty visualizations by conducting converging quantitative and qualitative analyses of both the effort and strategies used by individuals when provided with quantile dotplots, density plots, interval plots, mean plots, and textual descriptions of uncertainty. We utilize two approaches for examining effort across uncertainty communication techniques: a measure of individual differences in working-memory capacity known as an operation span (OSPAN) task and self-reports of perceived workload via the NASA-TLX. The results reveal that both visualization methods and working-memory capacity impact participants' decisions. Specifically, quantile dotplots and density plots (i.e., distributional annotations) result in more accurate judgments than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). Additionally, participants' open-ended responses suggest that individuals viewing distributional annotations are more likely to employ a strategy that explicitly incorporates uncertainty into their judgments than those viewing summary annotations. When comparing quantile dotplots to density plots, this work finds that both methods are equally effective for low-working-memory individuals. However, for individuals with high-working-memory capacity, quantile dotplots evoke more accurate responses with less perceived effort. Given these results, we advocate for the inclusion of converging behavioral and subjective workload metrics in addition to accuracy performance to further disambiguate meaningful differences among visualization techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114803",
            "id": "r_89",
            "s_ids": [
                "s_927",
                "s_453",
                "s_822",
                "s_229"
            ],
            "type": "rich",
            "x": -3.4222471714019775,
            "y": 17.53603744506836
        },
        {
            "title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
            "data": "Pathogen outbreaks (i.e., outbreaks of bacteria and viruses) in hospitals can cause high mortality rates and increase costs for hospitals significantly. An outbreak is generally noticed when the number of infected patients rises above an endemic level or the usual prevalence of a pathogen in a defined population. Reconstructing transmission pathways back to the source of an outbreak - the patient zero or index patient - requires the analysis of microbiological data and patient contacts. This is often manually completed by infection control experts. We present a novel visual analytics approach to support the analysis of transmission pathways, patient contacts, the progression of the outbreak, and patient timelines during hospitalization. Infection control experts applied our solution to a real outbreak of Klebsiella pneumoniae in a large German hospital. Using our system, our experts were able to scale the analysis of transmission pathways to longer time intervals (i.e., several years of data instead of days) and across a larger number of wards. Also, the system is able to reduce the analysis time from days to hours. In our final study, feedback from twenty-five experts from seven German hospitals provides evidence that our solution brings significant benefits for analyzing outbreaks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030437",
            "id": "r_90",
            "s_ids": [
                "s_256",
                "s_379",
                "s_1032",
                "s_95",
                "s_464",
                "s_1546",
                "s_221",
                "s_471",
                "s_1506",
                "s_472",
                "s_819",
                "s_1150",
                "s_267"
            ],
            "type": "rich",
            "x": -5.803048610687256,
            "y": 15.345170974731445
        },
        {
            "title": "Data Comics for Reporting Controlled User Studies in Human-Computer Interaction",
            "data": "Inspired by data comics, this paper introduces a novel format for reporting controlled studies in the domain of human-computer interaction (HCI). While many studies in HCI follow similar steps in explaining hypotheses, laying out a study design, and reporting results, many of these decisions are buried in blocks of dense scientific text. We propose leveraging data comics as study reports to provide an open and glanceable view of studies by tightly integrating text and images, illustrating design decisions and key insights visually, resulting in visual narratives that can be compelling to non-scientists and researchers alike. Use cases of data comics study reports range from illustrations for non-scientific audiences to graphical abstracts, study summaries, technical talks, textbooks, teaching, blogs, supplementary submission material, and inclusion in scientific articles. This paper provides examples of data comics study reports alongside a graphical repertoire of examples, embedded in a framework of guidelines for creating comics reports which was iterated upon and evaluated through a series of collaborative design sessions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030433",
            "id": "r_91",
            "s_ids": [
                "s_214",
                "s_1",
                "s_123",
                "s_198",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.72831916809082,
            "y": 16.95513153076172
        },
        {
            "title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs",
            "data": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114877",
            "id": "r_92",
            "s_ids": [
                "s_763",
                "s_456",
                "s_1431",
                "s_1267",
                "s_1574",
                "s_1280",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.073988914489746,
            "y": 17.1561222076416
        },
        {
            "title": "Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases",
            "data": "Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental \u201cshortcuts\u201d). In this work, we explore how visualizing a user's interaction history (i.e., which data points and attributes a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one's analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowd-sourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where things like gender bias or political party bias may drive one's analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114862",
            "id": "r_93",
            "s_ids": [
                "s_6",
                "s_1027",
                "s_1590",
                "s_655",
                "s_1599"
            ],
            "type": "rich",
            "x": -4.743325233459473,
            "y": 15.570289611816406
        },
        {
            "title": "Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways",
            "data": "Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the \u201cright\u201d takeaway.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114823",
            "id": "r_94",
            "s_ids": [
                "s_1215",
                "s_536",
                "s_1221",
                "s_110",
                "s_687",
                "s_942"
            ],
            "type": "rich",
            "x": -4.154066562652588,
            "y": 16.393550872802734
        },
        {
            "title": "Improving Visualization Interpretation Using Counterfactuals",
            "data": "Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes <i>counterfactual subsets</i> to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114779",
            "id": "r_95",
            "s_ids": [
                "s_552",
                "s_504",
                "s_989",
                "s_976"
            ],
            "type": "rich",
            "x": -9.20560073852539,
            "y": 19.756027221679688
        },
        {
            "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
            "data": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030370",
            "id": "r_96",
            "s_ids": [
                "s_1313",
                "s_1362",
                "s_1210",
                "s_473",
                "s_309",
                "s_156",
                "s_823"
            ],
            "type": "rich",
            "x": -9.943920135498047,
            "y": 18.474443435668945
        },
        {
            "title": "Chartem: Reviving Chart Images with Data Embedding",
            "data": "In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030351",
            "id": "r_97",
            "s_ids": [
                "s_913",
                "s_535",
                "s_1010",
                "s_177",
                "s_737",
                "s_404",
                "s_636",
                "s_992",
                "s_155",
                "s_1251"
            ],
            "type": "rich",
            "x": -10.91963005065918,
            "y": 18.260923385620117
        },
        {
            "title": "A Design Space of Vision Science Methods for Visualization Research",
            "data": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3029413",
            "id": "r_98",
            "s_ids": [
                "s_615",
                "s_547",
                "s_1215",
                "s_781"
            ],
            "type": "rich",
            "x": -2.231674909591675,
            "y": 17.99751091003418
        },
        {
            "title": "Bayesian-Assisted Inference from Visualized Data",
            "data": "A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter's value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user's subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people's Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people's updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people's proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028984",
            "id": "r_99",
            "s_ids": [
                "s_173",
                "s_984",
                "s_1190",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.379232406616211,
            "y": 16.58677101135254
        },
        {
            "title": "Affective Visualization Design: Leveraging the Emotional Impact of Data",
            "data": "In recent years, more and more researchers have reflected on the undervaluation of emotion in data visualization and highlighted the importance of considering human emotion in visualization design. Meanwhile, an increasing number of studies have been conducted to explore emotion-related factors. However, so far, this research area is still in its early stages and faces a set of challenges, such as the unclear definition of key concepts, the insufficient justification of why emotion is important in visualization design, and the lack of characterization of the design space of affective visualization design. To address these challenges, first, we conducted a literature review and identified three research lines that examined both emotion and data visualization. We clarified the differences between these research lines and kept 109 papers that studied or discussed how data visualization communicates and influences emotion. Then, we coded the 109 papers in terms of how they justified the legitimacy of considering emotion in visualization design (i.e., why emotion is important) and identified five argumentative perspectives. Based on these papers, we also identified 61 projects that practiced affective visualization design. We coded these design projects in three dimensions, including design fields (where), design tasks (what), and design methods (how), to explore the design space of affective visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327385",
            "id": "r_100",
            "s_ids": [
                "s_750",
                "s_1131",
                "s_989"
            ],
            "type": "rich",
            "x": -9.326383590698242,
            "y": 19.735633850097656
        },
        {
            "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
            "data": "With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209484",
            "id": "r_101",
            "s_ids": [
                "s_593",
                "s_277"
            ],
            "type": "rich",
            "x": -9.2594575881958,
            "y": 18.782257080078125
        },
        {
            "title": "SliceTeller: A Data Slice-Driven Approach for Machine Learning Model Validation",
            "data": "Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as \u201cData Slices\u201d. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller, a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting, to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development, to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209465",
            "id": "r_102",
            "s_ids": [
                "s_1007",
                "s_101",
                "s_712",
                "s_141",
                "s_789",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.5924453139305115,
            "y": 17.77898597717285
        },
        {
            "title": "Data Hunches: Incorporating Personal Knowledge into Visualizations",
            "data": "The trouble with data is that it frequently provides only an imperfect representation of a phenomenon of interest. Experts who are familiar with their datasets will often make implicit, mental corrections when analyzing a dataset, or will be cautious not to be overly confident about their findings if caveats are present. However, personal knowledge about the caveats of a dataset is typically not incorporated in a structured way, which is problematic if others who lack that knowledge interpret the data. In this work, we define such analysts' knowledge about datasets as data hunches. We differentiate data hunches from uncertainty and discuss types of hunches. We then explore ways of recording data hunches, and, based on a prototypical design, develop recommendations for designing visualizations that support data hunches. We conclude by discussing various challenges associated with data hunches, including the potential for harm and challenges for trust and privacy. We envision that data hunches will empower analysts to externalize their knowledge, facilitate collaboration and communication, and support the ability to learn from others' data hunches.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209451",
            "id": "r_103",
            "s_ids": [
                "s_1527",
                "s_719",
                "s_1517",
                "s_215"
            ],
            "type": "rich",
            "x": -2.2495174407958984,
            "y": 15.395811080932617
        },
        {
            "title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization",
            "data": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209447",
            "id": "r_104",
            "s_ids": [
                "s_763",
                "s_1516",
                "s_1341",
                "s_1158",
                "s_456",
                "s_1280",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.38817024230957,
            "y": 17.251184463500977
        },
        {
            "title": "In Defence of Visual Analytics Systems: Replies to Critics",
            "data": "The last decade has witnessed many visual analytics (VA) systems that make successful applications to wide-ranging domains like urban analytics and explainable AI. However, their research rigor and contributions have been extensively challenged within the visualization community. We come in defence of VA systems by contributing two interview studies for gathering critics and responses to those criticisms. First, we interview 24 researchers to collect criticisms the review comments on their VA work. Through an iterative coding and refinement process, the interview feedback is summarized into a list of 36 common criticisms. Second, we interview 17 researchers to validate our list and collect their responses, thereby discussing implications for defending and improving the scientific values and rigor of VA systems. We highlight that the presented knowledge is deep, extensive, but also imperfect, provocative, and controversial, and thus recommend reading with an inclusive and critical eye. We hope our work can provide thoughts and foundations for conducting VA research and spark discussions to promote the research field forward more rigorously and vibrantly.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209360",
            "id": "r_105",
            "s_ids": [
                "s_877",
                "s_1341",
                "s_897",
                "s_1415",
                "s_348",
                "s_156"
            ],
            "type": "rich",
            "x": -10.075447082519531,
            "y": 17.597408294677734
        },
        {
            "title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
            "data": "Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114781",
            "id": "r_106",
            "s_ids": [
                "s_456",
                "s_525",
                "s_1415",
                "s_1280",
                "s_1140"
            ],
            "type": "rich",
            "x": -10.153021812438965,
            "y": 17.280071258544922
        },
        {
            "title": "Visual Analytics for Temporal Hypergraph Model Exploration",
            "data": "Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute Hyper-Matrix, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030408",
            "id": "r_107",
            "s_ids": [
                "s_953",
                "s_1593",
                "s_434",
                "s_272",
                "s_1100",
                "s_1254"
            ],
            "type": "rich",
            "x": -7.013009548187256,
            "y": 20.8903751373291
        },
        {
            "title": "Efficient and Flexible Hierarchical Data Layouts for a Unified Encoding of Scalar Field Precision and Resolution",
            "data": "To address the problem of ever-growing scientific data sizes making data movement a major hindrance to analysis, we introduce a novel encoding for scalar fields: a unified tree of resolution and precision, specifically constructed so that valid cuts correspond to sensible approximations of the original field in the precision-resolution space. Furthermore, we introduce a highly flexible encoding of such trees that forms a parameterized family of data hierarchies. We discuss how different parameter choices lead to different trade-offs in practice, and show how specific choices result in known data representation schemes such as zfp [52], idx [58], and jpeg2000 [76]. Finally, we provide system-level details and empirical evidence on how such hierarchies facilitate common approximate queries with minimal data movement and time, using real-world data sets ranging from a few gigabytes to nearly a terabyte in size. Experiments suggest that our new strategy of combining reductions in resolution and precision is competitive with state-of-the-art compression techniques with respect to data quality, while being significantly more flexible and orders of magnitude faster, and requiring significantly reduced resources.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030381",
            "id": "r_108",
            "s_ids": [
                "s_671",
                "s_444",
                "s_218",
                "s_1106",
                "s_94",
                "s_708",
                "s_506",
                "s_343"
            ],
            "type": "rich",
            "x": -15.0811185836792,
            "y": 7.093166828155518
        },
        {
            "title": "A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling",
            "data": "Small multiples are miniature representations of visual information used generically across many domains. Handling large numbers of small multiples imposes challenges on many analytic tasks like inspection, comparison, navigation, or annotation. To address these challenges, we developed a framework and implemented a library called PILlNG.JS for designing interactive piling interfaces. Based on the piling metaphor, such interfaces afford flexible organization, exploration, and comparison of large numbers of small multiples by interactively aggregating visual objects into piles. Based on a systematic analysis of previous work, we present a structured design space to guide the design of visual piling interfaces. To enable designers to efficiently build their own visual piling interfaces, PILlNG.JS provides a declarative interface to avoid having to write low-level code and implements common aspects of the design space. An accompanying GUI additionally supports the dynamic configuration of the piling interface. We demonstrate the expressiveness of PILlNG.JS with examples from machine learning, immunofluorescence microscopy, genomics, and public health.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028948",
            "id": "r_109",
            "s_ids": [
                "s_1344",
                "s_631",
                "s_1385",
                "s_393",
                "s_1221",
                "s_1486"
            ],
            "type": "rich",
            "x": -8.786530494689941,
            "y": 16.914173126220703
        },
        {
            "title": "Challenges and Opportunities in Data Visualization Education: A Call to Action",
            "data": "This paper is a call to action for research and discussion on data visualization education. As visualization evolves and spreads through our professional and personal lives, we need to understand how to support and empower a broad and diverse community of learners in visualization. Data Visualization is a diverse and dynamic discipline that combines knowledge from different fields, is tailored to suit diverse audiences and contexts, and frequently incorporates tacit knowledge. This complex nature leads to a series of interrelated challenges for data visualization education. Driven by a lack of consolidated knowledge, overview, and orientation for visualization education, the 21 authors of this paper\u2014educators and researchers in data visualization\u2014identify and describe 19 challenges informed by our collective practical experience. We organize these challenges around seven themes People, Goals & Assessment, Environment, Motivation, Methods, Materials, and Change. Across these themes, we formulate 43 research questions to address these challenges. As part of our call to action, we then conclude with 5 cross-cutting opportunities and respective action items: embrace DIVERSITY+INCLUSION, build COMMUNITIES, conduct RESEARCH, act AGILE, and relish RESPONSIBILITY. We aim to inspire researchers, educators and learners to drive visualization education forward and discuss why, how, who and where we educate, as we learn to use visualization to address challenges across many scales and many domains in a rapidly changing world: viseducationchallenges.github.io.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327378",
            "id": "r_110",
            "s_ids": [
                "s_1221",
                "s_1079",
                "s_1348",
                "s_1016",
                "s_1371",
                "s_582",
                "s_642",
                "s_1159",
                "s_867",
                "s_985",
                "s_386",
                "s_684",
                "s_546",
                "s_917",
                "s_1220",
                "s_732",
                "s_777",
                "s_1101",
                "s_1547",
                "s_1303",
                "s_978"
            ],
            "type": "rich",
            "x": -6.3232831954956055,
            "y": 17.297468185424805
        },
        {
            "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals",
            "data": "A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices. Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209469",
            "id": "r_111",
            "s_ids": [
                "s_1429",
                "s_57",
                "s_1049",
                "s_442",
                "s_116",
                "s_774",
                "s_579",
                "s_742",
                "s_134"
            ],
            "type": "rich",
            "x": -9.217333793640137,
            "y": 17.68059730529785
        },
        {
            "title": "Attribute-based Explanation of Non-Linear Embeddings of High-Dimensional Data",
            "data": "Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114870",
            "id": "r_112",
            "s_ids": [
                "s_196",
                "s_1540",
                "s_8",
                "s_1613",
                "s_40"
            ],
            "type": "rich",
            "x": -3.784461498260498,
            "y": 20.917457580566406
        },
        {
            "title": "Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data",
            "data": "Vision-based deep learning (DL) methods have made great progress in learning autonomous driving models from large-scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. In this paper, we develop a geo-context aware visualization system for the study of Autonomous Driving Model (ADM) predictions together with large-scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining DL model performance with geospatial visualization techniques. Model performance measures can be studied together with a set of geospatial attributes over map views. Users can also discover and compare prediction behaviors of multiple DL models in both city-wide and street-level analysis, together with road images and video contents. Therefore, the system provides a new visual exploration platform for DL model designers in autonomous driving. Use cases and domain expert evaluation show the utility and effectiveness of the visualization system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114853",
            "id": "r_113",
            "s_ids": [
                "s_1615",
                "s_1053",
                "s_630",
                "s_242"
            ],
            "type": "rich",
            "x": -0.22270642220973969,
            "y": 17.618955612182617
        },
        {
            "title": "Rethinking the Ranks of Visual Channels",
            "data": "Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or \u2018wind map\u2019 (angle). With a Bayesian multilevel modeling approach, we show how the rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new probabilistic ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for reliably ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114684",
            "id": "r_114",
            "s_ids": [
                "s_1209",
                "s_607",
                "s_126",
                "s_942"
            ],
            "type": "rich",
            "x": -3.1547093391418457,
            "y": 16.56633949279785
        },
        {
            "title": "Data-Driven Space-Filling Curves",
            "data": "Abstract-We propose a data-driven space-filling curve method for 2D and 3D visualization. Our flexible curve traverses the data elements in the spatial domain in a way that the resulting linearization better preserves features in space compared to existing methods. We achieve such data coherency by calculating a Hamiltonian path that approximately minimizes an objective function that describes the similarity of data values and location coherency in a neighborhood. Our extended variant even supports multiscale data via quadtrees and octrees. Our method is useful in many areas of visualization including multivariate or comparative visualization ensemble visualization of 2D and 3D data on regular grids or multiscale visual analysis of particle simulations. The effectiveness of our method is evaluated with numerical comparisons to existing techniques and through examples of ensemble and multivariate datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030473",
            "id": "r_115",
            "s_ids": [
                "s_219",
                "s_395",
                "s_157"
            ],
            "type": "rich",
            "x": -8.506171226501465,
            "y": 19.322471618652344
        },
        {
            "title": "Localized Topological Simplification of Scalar Data",
            "data": "This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field $f$ and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to $f$ and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed-from a combinatorial perspective-to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to $\\times 36$. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030353",
            "id": "r_116",
            "s_ids": [
                "s_35",
                "s_1068",
                "s_815",
                "s_707"
            ],
            "type": "rich",
            "x": -4.166536331176758,
            "y": 20.959911346435547
        },
        {
            "title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering",
            "data": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030347",
            "id": "r_117",
            "s_ids": [
                "s_318",
                "s_1560",
                "s_107",
                "s_1480",
                "s_1233"
            ],
            "type": "rich",
            "x": -6.514970779418945,
            "y": 17.085498809814453
        },
        {
            "title": "SilkViser: A Visual Explorer of Blockchain-based Cryptocurrency Transaction Data",
            "data": "Many blockchain-based cryptocurrencies provide users with online blockchain explorers for viewing online transaction data. However, traditional blockchain explorers mostly present transaction information in textual and tabular forms. Such forms make understanding cryptocurrency transaction mechanisms difficult for novice users (NUsers). They are also insufficiently informative for experienced users (EUsers) to recognize advanced transaction information. This study introduces a new online cryptocurrency transaction data viewing tool called SilkViser. Guided by detailed scenario and requirement analyses, we create a series of appreciating visualization designs, such as paper ledger-inspired block and blockchain visualizations and ancient copper coin-inspired transaction visualizations, to help users understand cryptocurrency transaction mechanisms and recognize advanced transaction information. We also provide a set of lightweight interactions to facilitate easy and free data exploration. Moreover, a controlled user study is conducted to quantitatively evaluate the usability and effectiveness of SilkViser. Results indicate that SilkViser can satisfy the requirements of NUsers and EUsers. Our visualization designs can compensate for the inexperience of NUsers in data viewing and attract potential users to participate in cryptocurrency transactions.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00014",
            "id": "r_118",
            "s_ids": [
                "s_979",
                "s_1367",
                "s_466",
                "s_1429",
                "s_134",
                "s_1203",
                "s_1035"
            ],
            "type": "rich",
            "x": -9.253543853759766,
            "y": 17.730852127075195
        },
        {
            "title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
            "data": "How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose <i>ACSeeker</i>, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of <i>ACSeeker</i>, we report two case studies and interviews with a social scientist and general researchers.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114790",
            "id": "r_119",
            "s_ids": [
                "s_592",
                "s_46",
                "s_770",
                "s_60",
                "s_1574",
                "s_156",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.232556343078613,
            "y": 17.491436004638672
        },
        {
            "title": "STRATISFIMAL LAYOUT: A modular optimization model for laying out layered node-link network visualizations",
            "data": "Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing <i>layered</i> layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a <i>layout optimization model</i> that prioritizes <i>optimality</i> - as compared to <i>scalability</i> - because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing S<small>tratisfimal</small> L<small>ayout</small>, a <i>modular integer-linear-programming formulation</i> that can consider several important readability criteria <i>simultaneously</i> \u2014 crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that we believe cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at osf.io/qdyt9 with live examples at <uri>https://visdunneright.github.io/stratisfimal/</uri>.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114756",
            "id": "r_120",
            "s_ids": [
                "s_286",
                "s_10",
                "s_1242",
                "s_1006"
            ],
            "type": "rich",
            "x": -6.746462345123291,
            "y": 21.14967155456543
        },
        {
            "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs",
            "data": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030443",
            "id": "r_121",
            "s_ids": [
                "s_690",
                "s_489",
                "s_24",
                "s_76",
                "s_638",
                "s_1360",
                "s_179",
                "s_1599",
                "s_366"
            ],
            "type": "rich",
            "x": -4.786947250366211,
            "y": 15.757040977478027
        },
        {
            "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter Optimization of Deep Neural Networks",
            "data": "To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030380",
            "id": "r_122",
            "s_ids": [
                "s_792",
                "s_1329",
                "s_1218",
                "s_364"
            ],
            "type": "rich",
            "x": -1.676479697227478,
            "y": 16.446977615356445
        },
        {
            "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology",
            "data": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030365",
            "id": "r_123",
            "s_ids": [
                "s_962",
                "s_542"
            ],
            "type": "rich",
            "x": -1.9174931049346924,
            "y": 17.959209442138672
        },
        {
            "title": "Cultivating Visualization Literacy for Children Through Curiosity and Play",
            "data": "Fostering data visualization literacy (DVL) as part of childhood education could lead to a more data literate society. However, most work in DVL for children relies on a more formal educational context (i.e., a teacher-led approach) that limits children's engagement with data to classroom-based environments and, consequently, children's ability to ask questions about and explore data on topics they find personally meaningful. We explore how a curiosity-driven, child-led approach can provide more agency to children when they are authoring data visualizations. This paper explores how informal learning with crafting physicalizations through play and curiosity may foster increased literacy and engagement with data. Employing a constructionist approach, we designed a do-it-yourself toolkit made out of everyday materials (e.g., paper, cardboard, mirrors) that enables children to create, customize, and personalize three different interactive visualizations (bar, line, pie). We used the toolkit as a design probe in a series of in-person workshops with 5 children (6 to 11-year-olds) and interviews with 5 educators. Our observations reveal that the toolkit helped children creatively engage and interact with visualizations. Children with prior knowledge of data visualization reported the toolkit serving as more of an authoring tool that they envision using in their daily lives, while children with little to no experience found the toolkit as an engaging introduction to data visualization. Our study demonstrates the potential of using the constructionist approach to cultivate children's DVL through curiosity and play.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209442",
            "id": "r_124",
            "s_ids": [
                "s_1330",
                "s_1607",
                "s_319",
                "s_999",
                "s_213",
                "s_82",
                "s_781"
            ],
            "type": "rich",
            "x": -2.145326852798462,
            "y": 18.049043655395508
        },
        {
            "title": "ConceptExplainer: Interactive Explanation for Deep Neural Networks from a Concept Perspective",
            "data": "Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network's latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209384",
            "id": "r_125",
            "s_ids": [
                "s_302",
                "s_111",
                "s_1292",
                "s_394"
            ],
            "type": "rich",
            "x": -3.2538654804229736,
            "y": 17.612823486328125
        },
        {
            "title": "Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts",
            "data": "While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts containing text with varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways referring to statistics or relational comparisons than text describing elemental or encoded components. Finally, we find different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. We compile these results into four chart design guidelines and discuss future implications for the combination of text and charts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209383",
            "id": "r_126",
            "s_ids": [
                "s_1161",
                "s_536",
                "s_1225",
                "s_920",
                "s_1488"
            ],
            "type": "rich",
            "x": -4.243634223937988,
            "y": 16.132274627685547
        },
        {
            "title": "Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics",
            "data": "We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language's existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel. Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209369",
            "id": "r_127",
            "s_ids": [
                "s_801",
                "s_624",
                "s_130",
                "s_920"
            ],
            "type": "rich",
            "x": -4.328786849975586,
            "y": 16.061872482299805
        },
        {
            "title": "Modeling Just Noticeable Differences in Charts",
            "data": "One of the fundamental tasks in visualization is to compare two or more visual elements. However, it is often difficult to visually differentiate graphical elements encoding a small difference in value, such as the heights of similar bars in bar chart or angles of similar sections in pie chart. Perceptual laws can be used in order to model when and how we perceive this difference. In this work, we model the perception of Just Noticeable Differences (JNDs), the minimum difference in visual attributes that allow faithfully comparing similar elements, in charts. Specifically, we explore the relation between JNDs and two major visual variables: the intensity of visual elements and the distance between them, and study it in three charts: bar chart, pie chart and bubble chart. Through an empirical study, we identify main effects on JND for distance in bar charts, intensity in pie charts, and both distance and intensity in bubble charts. By fitting a linear mixed effects model, we model JND and find that JND grows as the exponential function of variables. We highlight several usage scenarios that make use of the JND modeling in which elements below the fitted JND are detected and enhanced with secondary visual cues for better discrimination.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114874",
            "id": "r_128",
            "s_ids": [
                "s_1557",
                "s_230",
                "s_331",
                "s_407",
                "s_1194",
                "s_1345",
                "s_1076"
            ],
            "type": "rich",
            "x": -8.042977333068848,
            "y": 18.68854522705078
        },
        {
            "title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks",
            "data": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114858",
            "id": "r_129",
            "s_ids": [
                "s_26",
                "s_244",
                "s_966",
                "s_350",
                "s_729",
                "s_1104",
                "s_665"
            ],
            "type": "rich",
            "x": 10.010871887207031,
            "y": 10.521193504333496
        },
        {
            "title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics",
            "data": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114820",
            "id": "r_130",
            "s_ids": [
                "s_1027",
                "s_384",
                "s_1148",
                "s_6"
            ],
            "type": "rich",
            "x": -4.739893913269043,
            "y": 15.450584411621094
        },
        {
            "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty",
            "data": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030394",
            "id": "r_131",
            "s_ids": [
                "s_1617",
                "s_1017",
                "s_930",
                "s_395",
                "s_181"
            ],
            "type": "rich",
            "x": -1.8635698556900024,
            "y": 17.93963050842285
        },
        {
            "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis",
            "data": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028891",
            "id": "r_132",
            "s_ids": [
                "s_1025",
                "s_616"
            ],
            "type": "rich",
            "x": -0.35877835750579834,
            "y": 17.14664649963379
        },
        {
            "title": "Sporthesia: Augmenting Sports Videos Using Natural Language",
            "data": "Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural language, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we propose a three-step approach \u2013 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video \u2013 and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia's applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209497",
            "id": "r_133",
            "s_ids": [
                "s_1589",
                "s_18",
                "s_1574",
                "s_689",
                "s_1108",
                "s_1415",
                "s_1486"
            ],
            "type": "rich",
            "x": -9.300018310546875,
            "y": 16.747400283813477
        },
        {
            "title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning",
            "data": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209468",
            "id": "r_134",
            "s_ids": [
                "s_1341",
                "s_877",
                "s_156",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.310097694396973,
            "y": 17.43646240234375
        },
        {
            "title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
            "data": "The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey's graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114878",
            "id": "r_135",
            "s_ids": [
                "s_765",
                "s_1611",
                "s_456",
                "s_735",
                "s_981",
                "s_1280",
                "s_204",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.172351837158203,
            "y": 17.233488082885742
        },
        {
            "title": "Interactive Data Comics",
            "data": "This paper investigates how to make data comics interactive. Data comics are an effective and versatile means for visual communication, leveraging the power of sequential narration and combined textual and visual content, while providing an overview of the storyline through panels assembled in expressive layouts. While a powerful static storytelling medium that works well on paper support, adding interactivity to data comics can enable non-linear storytelling, personalization, levels of details, explanations, and potentially enriched user experiences. This paper introduces a set of operations tailored to support data comics narrative goals that go beyond the traditional linear, immutable storyline curated by a story author. The goals and operations include adding and removing panels into pre-defined layouts to support branching, change of perspective, or access to detail-on-demand, as well as providing and modifying data, and interacting with data representation, to support personalization and reader-defined data focus. We propose a lightweight specification language, COMICSCRIPT, for designers to add such interactivity to static comics. To assess the viability of our authoring process, we recruited six professional illustrators, designers and data comics enthusiasts and asked them to craft an interactive comic, allowing us to understand authoring workflow and potential of our approach. We present examples of interactive comics in a gallery. This initial step towards understanding the design space of interactive comics can inform the design of creation tools and experiences for interactive storytelling.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114849",
            "id": "r_136",
            "s_ids": [
                "s_214",
                "s_432",
                "s_198",
                "s_970",
                "s_558",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.747092247009277,
            "y": 16.972440719604492
        },
        {
            "title": "Causal Support: Modeling Causal Inferences with Visualizations",
            "data": "Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual \u201cinsights\u201d. We formally evaluate the quality of causal inferences from visualizations by adopting <i>causal support</i>\u2014a Bayesian cognition model that learns the probability of alternative causal explanations given some data\u2014as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users' causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114824",
            "id": "r_137",
            "s_ids": [
                "s_875",
                "s_408",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.4207770824432373,
            "y": 16.88893699645996
        },
        {
            "title": "CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces",
            "data": "Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114822",
            "id": "r_138",
            "s_ids": [
                "s_1423",
                "s_1127",
                "s_285",
                "s_1425"
            ],
            "type": "rich",
            "x": -0.6080064177513123,
            "y": 18.150659561157227
        },
        {
            "title": "Visual Evaluation for Autonomous Driving",
            "data": "Autonomous driving technologies often use state-of-the-art artificial intelligence algorithms to understand the relationship between the vehicle and the external environment, to predict the changes of the environment, and then to plan and control the behaviors of the vehicle accordingly. The complexity of such technologies makes it challenging to evaluate the performance of autonomous driving systems and to find ways to improve them. The current approaches to evaluating such autonomous driving systems largely use a single score to indicate the overall performance of a system, but domain experts have difficulties in understanding how individual components or algorithms in an autonomous driving system may contribute to the score. To address this problem, we collaborate with domain experts on autonomous driving algorithms, and propose a visual evaluation method for autonomous driving. Our method considers the data generated in all components during the whole process of autonomous driving, including perception results, planning routes, prediction of obstacles, various controlling parameters, and evaluation of comfort. We develop a visual analytics workflow to integrate an evaluation mathematical model with adjustable parameters, support the evaluation of the system from the level of the overall performance to the level of detailed measures of individual components, and to show both evaluation scores and their contributing factors. Our implemented visual analytics system provides an overview evaluation score at the beginning and shows the animation of the dynamic change of the scores at each period. Experts can interactively explore the specific component at different time periods and identify related factors. With our method, domain experts not only learn about the performance of an autonomous driving system, but also identify and access the problematic parts of each component. Our visual evaluation system can be applied to the autonomous driving simulation system and used for various evaluation cases. The results of using our system in some simulation cases and the feedback from involved domain experts confirm the usefulness and efficiency of our method in helping people gain in-depth insight into autonomous driving systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114777",
            "id": "r_139",
            "s_ids": [
                "s_1217",
                "s_778",
                "s_136",
                "s_345",
                "s_518",
                "s_109",
                "s_512",
                "s_360"
            ],
            "type": "rich",
            "x": -10.153386116027832,
            "y": 19.612293243408203
        },
        {
            "title": "Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets",
            "data": "We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114765",
            "id": "r_140",
            "s_ids": [
                "s_1248",
                "s_1366",
                "s_1442",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.131513595581055,
            "y": 18.56564712524414
        },
        {
            "title": "From Jam Session to Recital: Synchronous Communication and Collaboration Around Data in Organizations",
            "data": "Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to <i>jam session</i>, while more structured presentations can range from <i>semi-improvisational performances</i> among peers to formal <i>recitals</i> given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience's view, and the coordination of a presenter's video with interactive visualization. Our distillation of interviewees' responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114760",
            "id": "r_141",
            "s_ids": [
                "s_12",
                "s_80"
            ],
            "type": "rich",
            "x": -6.465084075927734,
            "y": 16.527671813964844
        },
        {
            "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and Editing Multivariate Graphs",
            "data": "Matrix visualizations are a useful tool to provide a general overview of a graph's structure. For multivariate graphs, a remaining challenge is to cope with the attributes that are associated with nodes and edges. Addressing this challenge, we propose responsive matrix cells as a focus+context approach for embedding additional interactive views into a matrix. Responsive matrix cells are local zoomable regions of interest that provide auxiliary data exploration and editing facilities for multivariate graphs. They behave responsively by adapting their visual contents to the cell location, the available display space, and the user task. Responsive matrix cells enable users to reveal details about the graph, compare node and edge attributes, and edit data values directly in a matrix without resorting to external views or tools. We report the general design considerations for responsive matrix cells covering the visual and interactive means necessary to support a seamless data exploration and editing. Responsive matrix cells have been implemented in a web-based prototype based on which we demonstrate the utility of our approach. We describe a walk-through for the use case of analyzing a graph of soccer players and report on insights from a preliminary user feedback session.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030371",
            "id": "r_142",
            "s_ids": [
                "s_1005",
                "s_1061",
                "s_854",
                "s_1384",
                "s_424"
            ],
            "type": "rich",
            "x": -5.73802375793457,
            "y": 18.45638084411621
        },
        {
            "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design",
            "data": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030337",
            "id": "r_143",
            "s_ids": [
                "s_1075",
                "s_1119",
                "s_1210",
                "s_156",
                "s_1556"
            ],
            "type": "rich",
            "x": -10.43816089630127,
            "y": 18.768911361694336
        },
        {
            "title": "A Bayesian cognition approach for belief updating of correlation judgement through uncertainty visualizations",
            "data": "Understanding correlation judgement is important to designing effective visualizations of bivariate data. Prior work on correlation perception has not considered how factors including prior beliefs and uncertainty representation impact such judgements. The present work focuses on the impact of uncertainty communication when judging bivariate visualizations. Specifically, we model how users update their beliefs about variable relationships after seeing a scatterplot with and without uncertainty representation. To model and evaluate the belief updating, we present three studies. Study 1 focuses on a proposed \u201cLine + Cone\u201d visual elicitation method for capturing users' beliefs in an accurate and intuitive fashion. The findings reveal that our proposed method of belief solicitation reduces complexity and accurately captures the users' uncertainty about a range of bivariate relationships. Study 2 leverages the \u201cLine + Cone\u201d elicitation method to measure belief updating on the relationship between different sets of variables when seeing correlation visualization with and without uncertainty representation. We compare changes in users beliefs to the predictions of Bayesian cognitive models which provide normative benchmarks for how users should update their prior beliefs about a relationship in light of observed data. The findings from Study 2 revealed that one of the visualization conditions with uncertainty communication led to users being slightly more confident about their judgement compared to visualization without uncertainty information. Study 3 builds on findings from Study 2 and explores differences in belief update when the bivariate visualization is congruent or incongruent with users' prior belief. Our results highlight the effects of incorporating uncertainty representation, and the potential of measuring belief updating on correlation judgement with Bayesian cognitive models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3029412",
            "id": "r_144",
            "s_ids": [
                "s_384",
                "s_1030",
                "s_1148",
                "s_335"
            ],
            "type": "rich",
            "x": -4.702700138092041,
            "y": 15.390769004821777
        },
        {
            "title": "Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators",
            "data": "During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. Prior work on COVID-19 visualizations has primarily focused on the design and evaluation of specific visualization systems from technology-centered perspectives. However, little is known about what occurs behind the scenes during the visualization creation processes, given the complex sociotechnical contexts in which they are embedded. Yet, such ecological knowledge is necessary to help characterize the nuances and trajectories of visualization design practices in the wild, as well as generate insights into how creators come to understand and approach visualization design on their own terms and for their own situated purposes. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices\u2014from creation to expansion, maintenance, and termination\u2014that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209493",
            "id": "r_145",
            "s_ids": [
                "s_879",
                "s_892",
                "s_720",
                "s_505",
                "s_359",
                "s_122"
            ],
            "type": "rich",
            "x": -6.718688011169434,
            "y": 21.182985305786133
        },
        {
            "title": "Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction",
            "data": "We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209423",
            "id": "r_146",
            "s_ids": [
                "s_308",
                "s_1197",
                "s_1009",
                "s_19",
                "s_1523",
                "s_1123",
                "s_1429",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.338164329528809,
            "y": 17.884363174438477
        },
        {
            "title": "Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation",
            "data": "When an analyst or scientist has a belief about how the world works, their thinking can be biased in favor of that belief. Therefore, one bedrock principle of science is to minimize that bias by testing the predictions of one's belief against objective data. But interpreting visualized data is a complex perceptual and cognitive process. Through two crowdsourced experiments, we demonstrate that supposedly objective assessments of the strength of a correlational relationship can be influenced by how strongly a viewer believes in the existence of that relationship. Participants viewed scatterplots depicting a relationship between meaningful variable pairs (e.g., number of environmental regulations and air quality) and estimated their correlations. They also estimated the correlation of the same scatterplots labeled instead with generic \u2018X\u2019 and \u2018Y\u2019 axes. In a separate section, they also reported how strongly they believed there to be a correlation between the meaningful variable pairs. Participants estimated correlations more accurately when they viewed scatterplots labeled with generic axes compared to scatterplots labeled with meaningful variable pairs. Furthermore, when viewers believed that two variables should have a strong relationship, they overestimated correlations between those variables by an r-value of about 0.1. When they believed that the variables should be unrelated, they underestimated the correlations by an r-value of about 0.1. While data visualizations are typically thought to present objective truths to the viewer, these results suggest that existing personal beliefs can bias even objective statistical values people extract from data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209405",
            "id": "r_147",
            "s_ids": [
                "s_1215",
                "s_1161",
                "s_173",
                "s_942"
            ],
            "type": "rich",
            "x": -3.402642011642456,
            "y": 16.50166130065918
        },
        {
            "title": "Exploring Interactions with Printed Data Visualizations in Augmented Reality",
            "data": "This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops ($\\mathrm{N}=20$) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study ($\\mathrm{N}=12$, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement \u201cpoint\u201d for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209386",
            "id": "r_148",
            "s_ids": [
                "s_961",
                "s_1589",
                "s_1075",
                "s_1495",
                "s_1169",
                "s_1221",
                "s_156"
            ],
            "type": "rich",
            "x": -10.031769752502441,
            "y": 18.252065658569336
        },
        {
            "title": "A Mixed-Initiative Approach to Reusing Infographic Charts",
            "data": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114856",
            "id": "r_149",
            "s_ids": [
                "s_1010",
                "s_1144",
                "s_636",
                "s_737",
                "s_1397",
                "s_404",
                "s_155"
            ],
            "type": "rich",
            "x": -10.821074485778809,
            "y": 18.278976440429688
        },
        {
            "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows",
            "data": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030454",
            "id": "r_150",
            "s_ids": [
                "s_1003",
                "s_1250",
                "s_689",
                "s_977",
                "s_1486",
                "s_1496",
                "s_388"
            ],
            "type": "rich",
            "x": -8.806673049926758,
            "y": 16.48382568359375
        },
        {
            "title": "TopoMap: A 0-dimensional Homology Preserving Projection of High-Dimensional Data",
            "data": "Multidimensional Projection is a fundamental tool for high-dimensional data analytics and visualization. With very few exceptions, projection techniques are designed to map data from a high-dimensional space to a visual space so as to preserve some dissimilarity (similarity) measure, such as the Euclidean distance for example. In fact, although adopting distinct mathematical formulations designed to favor different aspects of the data, most multidimensional projection methods strive to preserve dissimilarity measures that encapsulate geometric properties such as distances or the proximity relation between data objects. However, geometric relations are not the only interesting property to be preserved in a projection. For instance, the analysis of particular structures such as clusters and outliers could be more reliably performed if the mapping process gives some guarantee as to topological invariants such as connected components and loops. This paper introduces TopoMap, a novel projection technique which provides topological guarantees during the mapping process. In particular, the proposed method performs the mapping from a high-dimensional space to a visual space, while preserving the 0-dimensional persistence diagram of the Rips filtration of the high-dimensional data, ensuring that the filtrations generate the same connected components when applied to the original as well as projected data. The presented case studies show that the topological guarantee provided by TopoMap not only brings confidence to the visual analytic process but also can be used to assist in the assessment of other projection methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030441",
            "id": "r_151",
            "s_ids": [
                "s_28",
                "s_707",
                "s_231",
                "s_402",
                "s_428"
            ],
            "type": "rich",
            "x": -4.040421485900879,
            "y": 20.222017288208008
        },
        {
            "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality",
            "data": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations-causal graphs and Hasse diagrams-with and without an associated textual narrative. Finally, we describe Causeworks, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate Causeworks through interviews with experts who used the system for understanding complex events.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030358",
            "id": "r_152",
            "s_ids": [
                "s_637",
                "s_855",
                "s_125",
                "s_1275",
                "s_1457",
                "s_1549",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.0050201416015625,
            "y": 16.02499008178711
        },
        {
            "title": "Visual cohort comparison for spatial single-cell omics-data",
            "data": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030336",
            "id": "r_153",
            "s_ids": [
                "s_1351",
                "s_1416",
                "s_271",
                "s_526",
                "s_944",
                "s_1142",
                "s_1253"
            ],
            "type": "rich",
            "x": -6.791979789733887,
            "y": 14.298264503479004
        },
        {
            "title": "Effects of View Layout on Situated Analytics for Multiple-View Representations in Immersive Visualization",
            "data": "Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209475",
            "id": "r_154",
            "s_ids": [
                "s_485",
                "s_1438",
                "s_1377",
                "s_828",
                "s_79",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.57453727722168,
            "y": 17.887718200683594
        },
        {
            "title": "Multiple Forecast Visualizations (MFVs): Trade-offs in Trust and Performance in Multiple COVID-19 Forecast Visualizations",
            "data": "The prevalence of inadequate SARS-COV-2 (COVID-19) responses may indicate a lack of trust in forecasts and risk communication. However, no work has empirically tested how multiple forecast visualization choices impact trust and task-based performance. The three studies presented in this paper ($N=1299$) examine how visualization choices impact trust in COVID-19 mortality forecasts and how they influence performance in a trend prediction task. These studies focus on line charts populated with real-time COVID-19 data that varied the number and color encoding of the forecasts and the presence of best/worst-case forecasts. The studies reveal that trust in COVID-19 forecast visualizations initially increases with the number of forecasts and then plateaus after 6\u20139 forecasts. However, participants were most trusting of visualizations that showed less visual information, including a 95% confidence interval, single forecast, and grayscale encoded forecasts. Participants maintained high trust in intervals labeled with 50% and 25% and did not proportionally scale their trust to the indicated interval size. Despite the high trust, the 95% CI condition was the most likely to evoke predictions that did not correspond with the actual COVID-19 trend. Qualitative analysis of participants' strategies confirmed that many participants trusted both the simplistic visualizations and those with numerous forecasts. This work provides practical guides for how COVID-19 forecast visualizations influence trust, including recommendations for identifying the range where forecasts balance trade-offs between trust and task-based performance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209457",
            "id": "r_155",
            "s_ids": [
                "s_229",
                "s_605",
                "s_927",
                "s_533"
            ],
            "type": "rich",
            "x": -3.400148391723633,
            "y": 17.58595848083496
        },
        {
            "title": "DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps",
            "data": "In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209425",
            "id": "r_156",
            "s_ids": [
                "s_1264",
                "s_1012",
                "s_273",
                "s_352",
                "s_253",
                "s_67",
                "s_1554"
            ],
            "type": "rich",
            "x": 9.958935737609863,
            "y": 10.581907272338867
        },
        {
            "title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball",
            "data": "In a basketball play, players who are not in possession of the ball (i.e., off-ball players) can still effectively contribute to the team's offense, such as making a sudden move to create scoring opportunities. Analyzing the movements of off-ball players can thus facilitate the development of effective strategies for coaches. However, common basketball statistics (e.g., points and assists) primarily focus on what happens around the ball and are mostly result-oriented, making it challenging to objectively assess and fully understand the contributions of off-ball movements. To address these challenges, we collaborate closely with domain experts and summarize the multi-level requirements for off-ball movement analysis in basketball. We first establish an assessment model to quantitatively evaluate the offensive contribution of an off-ball movement considering both the position of players and the team cooperation. Based on the model, we design and develop a visual analytics system called OBTracker to support the multifaceted analysis of off-ball movements. OBTracker enables users to identify the frequency and effectiveness of off-ball movement patterns and learn the performance of different off-ball players. A tailored visualization based on the Voronoi diagram is proposed to help users interpret the contribution of off-ball movements from a temporal perspective. We conduct two case studies based on the tracking data from NBA games and demonstrate the effectiveness and usability of OBTracker through expert feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209373",
            "id": "r_157",
            "s_ids": [
                "s_682",
                "s_1341",
                "s_1574",
                "s_144",
                "s_740",
                "s_225",
                "s_647",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.363659858703613,
            "y": 17.4772891998291
        },
        {
            "title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
            "data": "Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114850",
            "id": "r_158",
            "s_ids": [
                "s_202",
                "s_48",
                "s_310",
                "s_441",
                "s_815"
            ],
            "type": "rich",
            "x": -4.241588115692139,
            "y": 21.063413619995117
        },
        {
            "title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
            "data": "Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030414",
            "id": "r_159",
            "s_ids": [
                "s_947",
                "s_952",
                "s_649",
                "s_829",
                "s_1534",
                "s_1198",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.3724403381347656,
            "y": 18.075664520263672
        },
        {
            "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams",
            "data": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030411",
            "id": "r_160",
            "s_ids": [
                "s_360",
                "s_11",
                "s_1243",
                "s_451",
                "s_1338"
            ],
            "type": "rich",
            "x": -10.266351699829102,
            "y": 19.638986587524414
        },
        {
            "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams",
            "data": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030393",
            "id": "r_161",
            "s_ids": [
                "s_766",
                "s_1385",
                "s_1327",
                "s_784",
                "s_1438",
                "s_1072",
                "s_1223",
                "s_237",
                "s_1415"
            ],
            "type": "rich",
            "x": -9.670419692993164,
            "y": 17.978158950805664
        },
        {
            "title": "Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay",
            "data": "Data visualizations and narratives are often integrated to convey data stories effectively. Among various data storytelling formats, data videos have been garnering increasing attention. These videos provide an intuitive interpretation of data charts while vividly articulating the underlying data insights. However, the production of data videos demands a diverse set of professional skills and considerable manual labor, including understanding narratives, linking visual elements with narration segments, designing and crafting animations, recording audio narrations, and synchronizing audio with visual animations. To simplify this process, our paper introduces a novel method, referred to as Data Player, capable of automatically generating dynamic data videos with narration-animation interplay. This approach lowers the technical barriers associated with creating data videos rich in narration. To enable narration-animation interplay, Data Player constructs references between visualizations and text input. Specifically, it first extracts data into tables from the visualizations. Subsequently, it utilizes large language models to form semantic connections between text and visuals. Finally, Data Player encodes animation design knowledge as computational low-level constraints, allowing for the recommendation of suitable animation presets that align with the audio narration produced by text-to-speech technologies. We assessed Data Player's efficacy through an example gallery, a user study, and expert interviews. The evaluation results demonstrated that Data Player can generate high-quality data videos that are comparable to human-composed ones.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327197",
            "id": "r_162",
            "s_ids": [
                "s_457",
                "s_900",
                "s_404",
                "s_737"
            ],
            "type": "rich",
            "x": -10.863585472106934,
            "y": 18.269775390625
        },
        {
            "title": "ThreadStates: State-based Visual Analysis of Disease Progression",
            "data": "A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStates is to identify the states of disease progression by learning from observation data in a human-in-the-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114840",
            "id": "r_163",
            "s_ids": [
                "s_470",
                "s_843",
                "s_987",
                "s_4",
                "s_393"
            ],
            "type": "rich",
            "x": -8.398113250732422,
            "y": 17.102235794067383
        },
        {
            "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections",
            "data": "We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114833",
            "id": "r_164",
            "s_ids": [
                "s_649",
                "s_261",
                "s_1442",
                "s_947",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.3767904043197632,
            "y": 18.06041717529297
        },
        {
            "title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
            "data": "We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called \u201cmining.\u201d Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114821",
            "id": "r_165",
            "s_ids": [
                "s_337",
                "s_252",
                "s_1545",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.340062141418457,
            "y": 18.336097717285156
        },
        {
            "title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
            "data": "Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114800",
            "id": "r_166",
            "s_ids": [
                "s_1380",
                "s_446",
                "s_456",
                "s_1385",
                "s_1415",
                "s_348",
                "s_346"
            ],
            "type": "rich",
            "x": -9.739123344421387,
            "y": 17.2515811920166
        },
        {
            "title": "Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data",
            "data": "Inspection of tissues using a light microscope is the primary method of diagnosing many diseases, notably cancer. Highly multiplexed tissue imaging builds on this foundation, enabling the collection of up to 60 channels of molecular information plus cell and tissue morphology using antibody staining. This provides unique insight into disease biology and promises to help with the design of patient-specific therapies. However, a substantial gap remains with respect to visualizing the resulting multivariate image data and effectively supporting pathology workflows in digital environments on screen. We, therefore, developed Scope2Screen, a scalable software system for focus+context exploration and annotation of whole-slide, high-plex, tissue images. Our approach scales to analyzing 100GB images of 10<sup>9</sup> or more pixels per channel, containing millions of individual cells. A multidisciplinary team of visualization experts, microscopists, and pathologists identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest (ROIs) in an intuitive and cohesive manner. Building on a scope-to-screen metaphor, we present interactive lensing techniques that operate at single-cell and tissue levels. Lenses are equipped with task-specific functionality and descriptive statistics, making it possible to analyze image features, cell types, and spatial arrangements (neighborhoods) across image channels and scales. A fast sliding-window search guides users to regions similar to those under the lens; these regions can be analyzed and considered either separately or as part of a larger image collection. A novel snapshot method enables linked lens configurations and image statistics to be saved, restored, and shared with these regions. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancers to discover cancer-relevant image features.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114786",
            "id": "r_167",
            "s_ids": [
                "s_1492",
                "s_1186",
                "s_501",
                "s_521",
                "s_1216",
                "s_1240",
                "s_604",
                "s_738",
                "s_399",
                "s_68",
                "s_1421",
                "s_507",
                "s_1486"
            ],
            "type": "rich",
            "x": -8.673368453979492,
            "y": 16.5445613861084
        },
        {
            "title": "Truth or Square: Aspect Ratio Biases Recall of Position Encodings",
            "data": "Bar charts are among the most frequently used visualizations, in part because their position encoding leads them to convey data values precisely. Yet reproductions of single bars or groups of bars within a graph can be biased. Curiously, some previous work found that this bias resulted in an overestimation of reproduced data values, while other work found an underestimation. Across three empirical studies, we offer an explanation for these conflicting findings: this discrepancy is a consequence of the differing aspect ratios of the tested bar marks. Viewers are biased to remember a bar mark as being more similar to a prototypical square, leading to an overestimation of bars with a wide aspect ratio, and an underestimation of bars with a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of the bar marks indeed influenced the direction of this bias. Experiment 3 confirmed that this pattern of misestimation bias was present for reproductions from memory, suggesting that this bias may arise when comparing values across sequential displays or views. We describe additional visualization designs that might be prone to this bias beyond bar charts (e.g., Mekko charts and treemaps), and speculate that other visual channels might hold similar biases toward prototypical values.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030422",
            "id": "r_168",
            "s_ids": [
                "s_1328",
                "s_1209",
                "s_1215",
                "s_1026"
            ],
            "type": "rich",
            "x": -3.5574004650115967,
            "y": 16.562118530273438
        },
        {
            "title": "Interactive Visualization of Atmospheric Effects for Celestial Bodies",
            "data": "We present an atmospheric model tailored for the interactive visualization of planetary surfaces. As the exploration of the solar system is progressing with increasingly accurate missions and instruments, the faithful visualization of planetary environments is gaining increasing interest in space research, mission planning, and science communication and education. Atmospheric effects are crucial in data analysis and to provide contextual information for planetary data. Our model correctly accounts for the non-linear path of the light inside the atmosphere (in Earth's case), the light absorption effects by molecules and dust particles, such as the ozone layer and the Martian dust, and a wavelength-dependent phase function for Mie scattering. The mode focuses on interactivity, versatility, and customization, and a comprehensive set of interactive controls make it possible to adapt its appearance dynamically. We demonstrate our results using Earth and Mars as examples. However, it can be readily adapted for the exploration of other atmospheres found on, for example, of exoplanets. For Earth's atmosphere, we visually compare our results with pictures taken from the International Space Station and against the CIE clear sky model. The Martian atmosphere is reproduced based on available scientific data, feedback from domain experts, and is compared to images taken by the Curiosity rover. The work presented here has been implemented in the OpenSpace system, which enables interactive parameter setting and real-time feedback visualization targeting presentations in a wide range of environments, from immersive dome theaters to virtual reality headsets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030333",
            "id": "r_169",
            "s_ids": [
                "s_1586",
                "s_1363",
                "s_1555",
                "s_741",
                "s_292",
                "s_428"
            ],
            "type": "rich",
            "x": -4.0728864669799805,
            "y": 20.105228424072266
        },
        {
            "title": "A Simple Pipeline for Coherent Grid Maps",
            "data": "Grid maps are spatial arrangements of simple tiles (often squares or hexagons), each of which represents a spatial element. They are an established, effective way to show complex data per spatial element, using visual encodings within each tile ranging from simple coloring to nested small-multiples visualizations. An effective grid map is coherent with the underlying geographic space: the tiles maintain the contiguity, neighborhoods and identifiability of the corresponding spatial elements, while the grid map as a whole maintains the global shape of the input. Of particular importance are salient local features of the global shape which need to be represented by tiles assigned to the appropriate spatial elements. State-of-the-art techniques can adequately deal only with simple cases, such as close-to-uniform spatial distributions or global shapes that have few characteristic features. We introduce a simple fully-automated 3-step pipeline for computing coherent grid maps. Each step is a well-studied problem: shape decomposition based on salient features, tile-based Mosaic Cartograms, and point-set matching. Our pipeline is a seamless composition of existing techniques for these problems and results in high-quality grid maps. We provide an implementation, demonstrate the efficacy of our approach on various complex datasets, and compare it to the state-of-the-art.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028953",
            "id": "r_170",
            "s_ids": [
                "s_545",
                "s_1081",
                "s_347"
            ],
            "type": "rich",
            "x": -5.949309349060059,
            "y": 15.689262390136719
        },
        {
            "title": "SMAP: A Joint Dimensionality Reduction Scheme for Secure Multi-Party Visualization",
            "data": "Nowadays, as data becomes increasingly complex and distributed, data analyses often involve several related datasets that are stored on different servers and probably owned by different stakeholders. While there is an emerging need to provide these stakeholders with a full picture of their data under a global context, conventional visual analytical methods, such as dimensionality reduction, could expose data privacy when multi-party datasets are fused into a single site to build point-level relationships. In this paper, we reformulate the conventional t-SNE method from the single-site mode into a secure distributed infrastructure. We present a secure multi-party scheme for joint t-SNE computation, which can minimize the risk of data leakage. Aggregated visualization can be optionally employed to hide disclosure of point-level relationships. We build a prototype system based on our method, SMAP, to support the organization, computation, and exploration of secure joint embedding. We demonstrate the effectiveness of our approach with three case studies, one of which is based on the deployment of our system in real-world applications.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00015",
            "id": "r_171",
            "s_ids": [
                "s_308",
                "s_577",
                "s_284",
                "s_1385",
                "s_1123",
                "s_283",
                "s_1395",
                "s_578"
            ],
            "type": "rich",
            "x": -9.253605842590332,
            "y": 18.347179412841797
        },
        {
            "title": "ARGUS: Visualization of AI-Assisted Task Guidance in AR",
            "data": "The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327396",
            "id": "r_172",
            "s_ids": [
                "s_959",
                "s_1562",
                "s_304",
                "s_240",
                "s_1405",
                "s_1518",
                "s_406",
                "s_1347",
                "s_158",
                "s_3",
                "s_113",
                "s_1370",
                "s_97",
                "s_904",
                "s_1575",
                "s_1286",
                "s_1037",
                "s_428"
            ],
            "type": "rich",
            "x": -3.993169069290161,
            "y": 20.062915802001953
        },
        {
            "title": "Data Navigator: An Accessibility-Centered Data Navigation Toolkit",
            "data": "Making data visualizations accessible for people with disabilities remains a significant challenge in current practitioner efforts. Existing visualizations often lack an underlying navigable structure, fail to engage necessary input modalities, and rely heavily on visual-only rendering practices. These limitations exclude people with disabilities, especially users of assistive technologies. To address these challenges, we present Data Navigator: a system built on a dynamic graph structure, enabling developers to construct navigable lists, trees, graphs, and flows as well as spatial, diagrammatic, and geographic relations. Data Navigator supports a wide range of input modalities: screen reader, keyboard, speech, gesture detection, and even fabricated assistive devices. We present 3 case examples with Data Navigator, demonstrating we can provide accessible navigation structures on top of raster images, integrate with existing toolkits at scale, and rapidly develop novel prototypes. Data Navigator is a step towards making accessible data visualizations easier to design and implement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327393",
            "id": "r_173",
            "s_ids": [
                "s_934",
                "s_656",
                "s_646"
            ],
            "type": "rich",
            "x": -3.8942291736602783,
            "y": 17.41757583618164
        },
        {
            "title": "Unraveling the Design Space of Immersive Analytics: A Systematic Review",
            "data": "Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions:  Academic Theory and Contribution,  Immersive Technology,  Data,  Spatial Presentation, and  Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327368",
            "id": "r_174",
            "s_ids": [
                "s_1187",
                "s_286",
                "s_1077",
                "s_705",
                "s_390",
                "s_377",
                "s_1006"
            ],
            "type": "rich",
            "x": -6.754899978637695,
            "y": 21.1390380859375
        },
        {
            "title": "Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer",
            "data": "Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209486",
            "id": "r_175",
            "s_ids": [
                "s_184",
                "s_602",
                "s_1374",
                "s_1477",
                "s_989"
            ],
            "type": "rich",
            "x": -9.287257194519043,
            "y": 19.72150230407715
        },
        {
            "title": "Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale",
            "data": "Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics). Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code. Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209466",
            "id": "r_176",
            "s_ids": [
                "s_1103",
                "s_1095",
                "s_1180",
                "s_141",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.3981315493583679,
            "y": 17.54228973388672
        },
        {
            "title": "ChartWalk: Navigating large collections of text notes in electronic health records for clinical chart review",
            "data": "Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient's electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk, an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209444",
            "id": "r_177",
            "s_ids": [
                "s_814",
                "s_1594",
                "s_847",
                "s_198"
            ],
            "type": "rich",
            "x": -6.597766876220703,
            "y": 16.810270309448242
        },
        {
            "title": "Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations",
            "data": "This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study ($\\mathrm{n}=150$), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209427",
            "id": "r_178",
            "s_ids": [
                "s_1551",
                "s_45",
                "s_840",
                "s_1596",
                "s_899",
                "s_157"
            ],
            "type": "rich",
            "x": -8.499821662902832,
            "y": 19.119516372680664
        },
        {
            "title": "Lotse: A Practical Framework for Guidance in Visual Analytics",
            "data": "Co-adaptive guidance aims to enable efficient human-machine collaboration in visual analytics, as proposed by multiple theoretical frameworks. This paper bridges the gap between such conceptual frameworks and practical implementation by introducing an accessible model of guidance and an accompanying guidance library, mapping theory into practice. We contribute a model of system-provided guidance based on design templates and derived strategies. We instantiate the model in a library called Lotse that allows specifying guidance strategies in definition files and generates running code from them. Lotse is the first guidance library using such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrate its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse. Further, we analyze our framework's cognitive dimensions to evaluate its expressiveness and outline a summary of open research questions for aligning guidance practice with its intricate theory.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209393",
            "id": "r_179",
            "s_ids": [
                "s_1258",
                "s_898",
                "s_891"
            ],
            "type": "rich",
            "x": -5.131680488586426,
            "y": 15.102641105651855
        },
        {
            "title": "Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training",
            "data": "Conventional racket sports training highly relies on coaches' knowledge and experience, leading to biases in the guidance. To solve this problem, smart wearable devices based on Internet of Things technology (IoT) have been extensively investigated to support data-driven training. Considerable studies introduced methods to extract valuable information from the sensor data collected by IoT devices. However, the information cannot provide actionable insights for coaches due to the large data volume and high data dimensions. We proposed an IoT + VA framework, Tac-Trainer, to integrate the sensor data, the information, and coaches' knowledge to facilitate racket sports training. Tac-Trainer consists of four components: device configuration, data interpretation, training optimization, and result visualization. These components collect trainees' kinematic data through IoT devices, transform the data into attributes and indicators, generate training suggestions, and provide an interactive visualization interface for exploration, respectively. We further discuss new research opportunities and challenges inspired by our work from two perspectives, VA for IoT and IoT for VA.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209352",
            "id": "r_180",
            "s_ids": [
                "s_887",
                "s_811",
                "s_439",
                "s_817",
                "s_647",
                "s_1574",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.620598793029785,
            "y": 17.314029693603516
        },
        {
            "title": "HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning",
            "data": "Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209347",
            "id": "r_181",
            "s_ids": [
                "s_1531",
                "s_1385",
                "s_308",
                "s_485",
                "s_190",
                "s_578"
            ],
            "type": "rich",
            "x": -9.203997611999512,
            "y": 18.024459838867188
        },
        {
            "title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
            "data": "Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include control-flow graphs in programs and semantic scene graphs in images. Identifying subgraph patterns in graphs is an important approach to understand their structural properties. We propose a visual analytics system GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. To support fast, interactive queries, we use graph neural networks (GNNs) to encode a graph as fixed-length latent vector representation, and perform subgraph matching in the latent space. Due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondences in the matching results that are crucial for visualization and interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19%-29% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100\u00d7 speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness for both usage scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114857",
            "id": "r_182",
            "s_ids": [
                "s_712",
                "s_1073",
                "s_1204",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.43224862217903137,
            "y": 17.61040687561035
        },
        {
            "title": "Propagating Visual Designs to Numerous Plots and Dashboards",
            "data": "In the process of developing an infrastructure for providing visualization and visual analytics (VIS) tools to epidemiologists and modeling scientists, we encountered a technical challenge for applying a number of visual designs to numerous datasets rapidly and reliably with limited development resources. In this paper, we present a technical solution to address this challenge. Operationally, we separate the tasks of data management, visual designs, and plots and dashboard deployment in order to streamline the development workflow. Technically, we utilize: an ontology to bring datasets, visual designs, and deployable plots and dashboards under the same management framework; multi-criteria search and ranking algorithms for discovering potential datasets that match a visual design; and a purposely-design user interface for propagating each visual design to appropriate datasets (often in tens and hundreds) and quality-assuring the propagation before the deployment. This technical solution has been used in the development of the RAMPVIS infrastructure for supporting a consortium of epidemiologists and modeling scientists through visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114828",
            "id": "r_183",
            "s_ids": [
                "s_1542",
                "s_65",
                "s_1490",
                "s_1221",
                "s_298",
                "s_663",
                "s_96"
            ],
            "type": "rich",
            "x": -6.977016448974609,
            "y": 17.018417358398438
        },
        {
            "title": "Lumos: Increasing Awareness of Analytic Behavior during Visual Data Analysis",
            "data": "Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionalities. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users' data exploration and decision-making processes. We found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114827",
            "id": "r_184",
            "s_ids": [
                "s_1027",
                "s_1590",
                "s_6",
                "s_1599"
            ],
            "type": "rich",
            "x": -4.766509532928467,
            "y": 15.534274101257324
        },
        {
            "title": "Professional Differences: A Comparative Study of Visualization Task Performance and Spatial Ability Across Disciplines",
            "data": "Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual's personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: <i>Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities?</i> This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants' confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114805",
            "id": "r_185",
            "s_ids": [
                "s_496",
                "s_1381",
                "s_182",
                "s_781",
                "s_1609"
            ],
            "type": "rich",
            "x": -2.335725784301758,
            "y": 17.975940704345703
        },
        {
            "title": "SightBi: Exploring Cross-View Data Relationships with Biclusters",
            "data": "Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search for and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking), which may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, and then observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the use of cross-view data relationships. SightBi formalizes cross-view data relationships as biclusters, computes them from a dataset, and uses a bi-context design that highlights creating stand-alone relationship-views. This helps preserve existing views and offers an overview of cross-view data relationships to guide user exploration. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114801",
            "id": "r_186",
            "s_ids": [
                "s_127",
                "s_609",
                "s_785",
                "s_1425"
            ],
            "type": "rich",
            "x": -0.6240852475166321,
            "y": 18.1379451751709
        },
        {
            "title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
            "data": "Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, Edge-Path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-Path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-Path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of Edge-Path bundling over other techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114795",
            "id": "r_187",
            "s_ids": [
                "s_262",
                "s_464",
                "s_1510",
                "s_124",
                "s_588"
            ],
            "type": "rich",
            "x": -5.738165378570557,
            "y": 15.210325241088867
        },
        {
            "title": "Gender in 30 Years of IEEE Visualization",
            "data": "We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference over the last 30 years. Our goal is to provide descriptive data on which diversity discussions and efforts in the community can build. We look in particular at the gender of VIS authors as a proxy for the community at large. We consider measures of overall gender representation among authors, differences in careers, positions in author lists, and collaborations. We found that the proportion of female authors has increased from 9% in the first five years to 22% in the last five years of the conference. Over the years, we found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position, but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community. All non-gender related data is available on https://osf.io/ydfj4/ and the gender-author matching can be accessed through https://nyu.databrary.org/volume/1301.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114787",
            "id": "r_188",
            "s_ids": [
                "s_337",
                "s_245",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.3429460525512695,
            "y": 18.38355827331543
        },
        {
            "title": "The Weighted Average Illusion: Biases in Perceived Mean Position in Scatterplots",
            "data": "Scatterplots can encode a third dimension by using additional channels like size or color (e.g. bubble charts). We explore a potential misinterpretation of trivariate scatterplots, which we call the <i>weighted average illusion</i>, where locations of larger and darker points are given more weight toward x- and y-mean estimates. This systematic bias is sensitive to a designer's choice of size or lightness ranges mapped onto the data. In this paper, we quantify this bias against varying size/lightness ranges and data correlations. We discuss possible explanations for its cause by measuring attention given to individual data points using a vision science technique called the centroid method. Our work illustrates how ensemble processing mechanisms and mental shortcuts can significantly distort visual summaries of data, and can lead to misjudgments like the demonstrated weighted average illusion.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114783",
            "id": "r_189",
            "s_ids": [
                "s_1056",
                "s_820",
                "s_781"
            ],
            "type": "rich",
            "x": -2.174001932144165,
            "y": 18.068321228027344
        },
        {
            "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
            "data": "The collection and visual analysis of large-scale data from complex systems, such as electronic health records or clickstream data, has become increasingly common across a wide range of industries. This type of retrospective visual analysis, however, is prone to a variety of selection bias effects, especially for high-dimensional data where only a subset of dimensions is visualized at any given time. The risk of selection bias is even higher when analysts dynamically apply filters or perform grouping operations during ad hoc analyses. These bias effects threaten the validity and generalizability of insights discovered during visual analysis as the basis for decision making. Past work has focused on bias transparency, helping users understand when selection bias may have occurred. However, countering the effects of selection bias via bias mitigation is typically left for the user to accomplish as a separate process. Dynamic reweighting (DR) is a novel computational approach to selection bias mitigation that helps users craft bias-corrected visualizations. This paper describes the DR workflow, introduces key DR visualization designs, and presents statistical methods that support the DR process. Use cases from the medical domain, as well as findings from domain expert user interviews, are also reported.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030455",
            "id": "r_190",
            "s_ids": [
                "s_504",
                "s_139",
                "s_552",
                "s_976"
            ],
            "type": "rich",
            "x": -9.026718139648438,
            "y": 19.786354064941406
        },
        {
            "title": "The Effectiveness of Interactive Visualization Techniques for Time Navigation of Dynamic Graphs on Large Displays",
            "data": "Dynamic networks can be challenging to analyze visually, especially if they span a large time range during which new nodes and edges can appear and disappear. Although it is straightforward to provide interfaces for visualization that represent multiple states of the network (i.e., multiple timeslices) either simultaneously (e.g., through small multiples) or interactively (e.g., through interactive animation), these interfaces might not support tasks in which disjoint timeslices need to be compared. Since these tasks are key for understanding the dynamic aspects of the network, understanding which interactive visualizations best support these tasks is important. We present the results of a series of laboratory experiments comparing two traditional approaches (small multiples and interactive animation), with a more recent approach based on interactive timeslicing. The tasks were performed on a large display through a touch interface. Participants completed 24 trials of three tasks with all techniques. The results show that interactive timeslicing brings benefit when comparing distant points in time, but less benefits when analyzing contiguous intervals of time.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030446",
            "id": "r_191",
            "s_ids": [
                "s_1595",
                "s_464",
                "s_782"
            ],
            "type": "rich",
            "x": -5.843934535980225,
            "y": 15.350255012512207
        },
        {
            "title": "Revealing Perceptual Proxies with Adversarial Examples",
            "data": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer-the series with the larger arithmetic mean or range-was pitted against an \u201cadversarial\u201d series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030429",
            "id": "r_192",
            "s_ids": [
                "s_362",
                "s_607",
                "s_1031",
                "s_1237",
                "s_942"
            ],
            "type": "rich",
            "x": -2.191175699234009,
            "y": 16.12715721130371
        },
        {
            "title": "Insights From Experiments With Rigor in an EvoBio Design Study",
            "data": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030405",
            "id": "r_193",
            "s_ids": [
                "s_98",
                "s_1356",
                "s_1301",
                "s_215",
                "s_1517"
            ],
            "type": "rich",
            "x": -2.2673590183258057,
            "y": 15.39924144744873
        },
        {
            "title": "SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects",
            "data": "In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030404",
            "id": "r_194",
            "s_ids": [
                "s_9",
                "s_138",
                "s_470",
                "s_1441",
                "s_899",
                "s_1345",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.279236793518066,
            "y": 18.68036460876465
        },
        {
            "title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements",
            "data": "Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030363",
            "id": "r_195",
            "s_ids": [
                "s_387",
                "s_708",
                "s_121",
                "s_1602",
                "s_606",
                "s_342",
                "s_620",
                "s_343"
            ],
            "type": "rich",
            "x": -15.026847839355469,
            "y": 7.147484302520752
        },
        {
            "title": "Visual Abstraction of Geographical Point Data with Spatial Autocorrelations",
            "data": "Scatterplots are always employed to visualize geographical point datasets, which often suffer from an overdraw problem due to the increase of data sizes. A variety of sampling strategies have been proposed to reduce overdraw and visual clutter with the spatial densities of points taken into account. However, informative attributes associated with the points also play significant roles in the exploration of geographical datasets. In this paper, we propose an attribute-based abstraction method to simplify the cluttered visualization of large-scale geographical points. Spatial autocorrelations are utilized to measure the attribute relationships of points in local areas, and a novel attribute-based sampling model is designed to generate a subset of points to preserve both density and attribute characteristics of original geographical points. A set of visual designs and user-friendly interactions are implemented, enabling users to capture the spatial distribution of geographical points and get deeper insights into the attribute features across local areas. Case studies and quantitative comparisons based on the real-world datasets further demonstrate the effectiveness of our method in the abstraction and exploration of large-scale geographical point datasets.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00011",
            "id": "r_196",
            "s_ids": [
                "s_1121",
                "s_1192",
                "s_37",
                "s_704",
                "s_800",
                "s_1553",
                "s_78",
                "s_1429",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.423011779785156,
            "y": 17.802234649658203
        },
        {
            "title": "A Visual Analytics Framework for Contrastive Network Analysis",
            "data": "A common network analysis task is comparison of two networks to identify unique characteristics in one network with respect to the other. For example, when comparing protein interaction networks derived from normal and cancer tissues, one essential task is to discover protein-protein interactions unique to cancer tissues. However, this task is challenging when the networks contain complex structural (and semantic) relations. To address this problem, we design ContraNA, a visual analytics framework leveraging both the power of machine learning for uncovering unique characteristics in networks and also the effectiveness of visualization for understanding such uniqueness. The basis of ContraNA is cNRL, which integrates two machine learning schemes, network representation learning (NRL) and contrastive learning (CL), to generate a low-dimensional embedding that reveals the uniqueness of one network when compared to another. ContraNA provides an interactive visualization interface to help analyze the uniqueness by relating embedding results and network structures as well as explaining the learned features by cNRL. We demonstrate the usefulness of ContraNA with two case studies using real-world datasets. We also evaluate ContraNA through a controlled user study with 12 participants on network comparison tasks. The results show that participants were able to both effectively identify unique characteristics from complex networks and interpret the results obtained from cNRL.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00010",
            "id": "r_197",
            "s_ids": [
                "s_1317",
                "s_1425",
                "s_539",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7346718907356262,
            "y": 17.990816116333008
        },
        {
            "title": "Knowledge Graphs in Practice: Characterizing their Users, Challenges, and Visualization Opportunities",
            "data": "This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners \u2013 KG Builders, Analysts, and Consumers \u2013 each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, technologies, and collaborative workflows. From the analysis of our interviews, we distill several visualization research directions to improve KG usability, including knowledge cards that balance digestibility and discoverability, timeline views to track temporal changes, interfaces that support organic discovery, and semantic explanations for AI and machine learning predictions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326904",
            "id": "r_198",
            "s_ids": [
                "s_43",
                "s_99",
                "s_197",
                "s_366",
                "s_523"
            ],
            "type": "rich",
            "x": -4.785314559936523,
            "y": 15.947704315185547
        },
        {
            "title": "Calibrate: Interactive Analysis of Probabilistic Model Output",
            "data": "Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier's predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209489",
            "id": "r_199",
            "s_ids": [
                "s_1093",
                "s_1562",
                "s_402",
                "s_748",
                "s_428"
            ],
            "type": "rich",
            "x": -4.0175089836120605,
            "y": 20.092098236083984
        },
        {
            "title": "CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data",
            "data": "In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209483",
            "id": "r_200",
            "s_ids": [
                "s_1112",
                "s_746",
                "s_1531",
                "s_834",
                "s_190",
                "s_1141",
                "s_249",
                "s_1603",
                "s_156",
                "s_360",
                "s_1385"
            ],
            "type": "rich",
            "x": -10.021349906921387,
            "y": 19.01624298095703
        },
        {
            "title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
            "data": "Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support \u201cwhat-if\u201d analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209440",
            "id": "r_201",
            "s_ids": [
                "s_965",
                "s_1460",
                "s_597",
                "s_1171",
                "s_1133",
                "s_1212",
                "s_1004",
                "s_1556",
                "s_549"
            ],
            "type": "rich",
            "x": -10.986954689025879,
            "y": 19.25214958190918
        },
        {
            "title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
            "data": "Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209391",
            "id": "r_202",
            "s_ids": [
                "s_641",
                "s_1531",
                "s_746",
                "s_192",
                "s_1307",
                "s_51",
                "s_1028",
                "s_162",
                "s_156",
                "s_1069",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.726114273071289,
            "y": 18.11345100402832
        },
        {
            "title": "AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing",
            "data": "We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114784",
            "id": "r_203",
            "s_ids": [
                "s_1548",
                "s_886",
                "s_542"
            ],
            "type": "rich",
            "x": -1.880584478378296,
            "y": 17.956995010375977
        },
        {
            "title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization",
            "data": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114782",
            "id": "r_204",
            "s_ids": [
                "s_64",
                "s_1440",
                "s_940",
                "s_646",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.5737667083740234,
            "y": 17.098773956298828
        },
        {
            "title": "Visualizing Uncertainty in Probabilistic Graphs with Network Hypothetical Outcome Plots (NetHOPs)",
            "data": "Probabilistic graphs are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it difficult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution defined by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability for uncertainty estimation. We present a community matching algorithm to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants' estimates fell, on average, within 11% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these findings, we synthesize design recommendations for developing and using animated visualizations for probabilistic networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114679",
            "id": "r_205",
            "s_ids": [
                "s_159",
                "s_477",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.3125836849212646,
            "y": 16.827856063842773
        },
        {
            "title": "StructGraphics: Flexible Visualization Design through Data-Agnostic and Reusable Graphical Structures",
            "data": "Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a concrete dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030476",
            "id": "r_206",
            "s_ids": [
                "s_426"
            ],
            "type": "rich",
            "x": -6.552950382232666,
            "y": 16.692943572998047
        },
        {
            "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics",
            "data": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030453",
            "id": "r_207",
            "s_ids": [
                "s_1063",
                "s_789"
            ],
            "type": "rich",
            "x": -0.76425701379776,
            "y": 18.001251220703125
        },
        {
            "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
            "data": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030438",
            "id": "r_208",
            "s_ids": [
                "s_888",
                "s_484",
                "s_1120",
                "s_1001",
                "s_1170",
                "s_541",
                "s_717",
                "s_713",
                "s_1354"
            ],
            "type": "rich",
            "x": -5.803529739379883,
            "y": 19.448266983032227
        },
        {
            "title": "Staged Animation Strategies for Online Dynamic Networks",
            "data": "Dynamic networks-networks that change over time-can be categorized into two types: offline dynamic networks, where all states of the network are known, and online dynamic networks, where only the past states of the network are known. Research on staging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into account past and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balance between timeliness for monitoring tasks-so that the animations do not lag too far behind the events-and clarity for comprehension tasks-to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements, we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approach that we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy in representing low- and high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks. We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization. Our findings show that animation staging strategies that emphasize comprehension do better for participant response times and accuracy. However, the notion of \u201ccomprehension\u201d is not always clear when it comes to complex changes in highly dynamic networks, requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancing event-based and time-based parameters for our hybrid approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030385",
            "id": "r_209",
            "s_ids": [
                "s_1077",
                "s_1310",
                "s_208",
                "s_789"
            ],
            "type": "rich",
            "x": -0.734056830406189,
            "y": 17.951095581054688
        },
        {
            "title": "II-20: Intelligent and pragmatic analytic categorization of image collections",
            "data": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030383",
            "id": "r_210",
            "s_ids": [
                "s_1175",
                "s_1254",
                "s_74"
            ],
            "type": "rich",
            "x": -6.988070011138916,
            "y": 20.916547775268555
        },
        {
            "title": "CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering",
            "data": "Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327201",
            "id": "r_211",
            "s_ids": [
                "s_649",
                "s_962",
                "s_1563",
                "s_542",
                "s_781",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.976939082145691,
            "y": 18.02499008178711
        },
        {
            "title": "AttentionViz: A Global View of Transformer Attention",
            "data": "Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327163",
            "id": "r_212",
            "s_ids": [
                "s_1290",
                "s_955",
                "s_877",
                "s_1372",
                "s_1337",
                "s_180"
            ],
            "type": "rich",
            "x": -10.429803848266602,
            "y": 17.977886199951172
        },
        {
            "title": "VIRD: Immersive Match Video Analysis for High-Performance Badminton Coaching",
            "data": "Badminton is a fast-paced sport that requires a strategic combination of spatial, temporal, and technical tactics. To gain a competitive edge at high-level competitions, badminton professionals frequently analyze match videos to gain insights and develop game strategies. However, the current process for analyzing matches is time-consuming and relies heavily on manual note-taking, due to the lack of automatic data collection and appropriate visualization tools. As a result, there is a gap in effectively analyzing matches and communicating insights among badminton coaches and players. This work proposes an end-to-end immersive match analysis pipeline designed in close collaboration with badminton professionals, including Olympic and national coaches and players. We present VIRD, a VR Bird (i.e., shuttle) immersive analysis tool, that supports interactive badminton game analysis in an immersive environment based on 3D reconstructed game views of the match video. We propose a top-down analytic workflow that allows users to seamlessly move from a high-level match overview to a detailed game view of individual rallies and shots, using situated 3D visualizations and video. We collect 3D spatial and dynamic shot data and player poses with computer vision models and visualize them in VR. Through immersive visualizations, coaches can interactively analyze situated spatial data (player positions, poses, and shot trajectories) with flexible viewpoints while navigating between shots and rallies effectively with embodied interaction. We evaluated the usefulness of VIRD with Olympic and national-level coaches and players in real matches. Results show that immersive analytics supports effective badminton match analysis with reduced context-switching costs and enhances spatial understanding with a high sense of presence.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327161",
            "id": "r_213",
            "s_ids": [
                "s_416",
                "s_1426",
                "s_1589",
                "s_689",
                "s_1486",
                "s_852"
            ],
            "type": "rich",
            "x": -8.850021362304688,
            "y": 16.663352966308594
        },
        {
            "title": "MeTACAST: Target- and Context-Aware Spatial Selection in VR",
            "data": "We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters\u2014with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration\u2014with simple and approximate 3D pointing, brushing, or drawing input\u2014using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326517",
            "id": "r_214",
            "s_ids": [
                "s_1289",
                "s_1323",
                "s_1543",
                "s_865",
                "s_1280"
            ],
            "type": "rich",
            "x": -6.31501579284668,
            "y": 18.83547592163086
        },
        {
            "title": "FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks",
            "data": "Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet\u2014a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user's focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user's field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system's rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209498",
            "id": "r_215",
            "s_ids": [
                "s_895",
                "s_869",
                "s_789"
            ],
            "type": "rich",
            "x": -0.726744532585144,
            "y": 17.990955352783203
        },
        {
            "title": "Understanding how Designers Find and Use Data Visualization Examples",
            "data": "Examples are useful for inspiring ideas and facilitating implementation in visualization design. However, there is little understanding of how visualization designers use examples, and how computational tools may support such activities. In this paper, we contribute an exploratory study of current practices in incorporating visualization examples. We conducted semi-structured interviews with 15 university students and 15 professional designers. Our analysis focus on two core design activities: searching for examples and utilizing examples. We characterize observed strategies and tools for performing these activities, as well as major challenges that hinder designers' current workflows. In addition, we identify themes that cut across these two activities: criteria for determining example usefulness, curation practices, and design fixation. Given our findings, we discuss the implications for visualization design and authoring tools and highlight critical areas for future research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209490",
            "id": "r_216",
            "s_ids": [
                "s_462",
                "s_1458",
                "s_1025",
                "s_1185"
            ],
            "type": "rich",
            "x": -4.28496789932251,
            "y": 16.952566146850586
        },
        {
            "title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning",
            "data": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209461",
            "id": "r_217",
            "s_ids": [
                "s_47",
                "s_519",
                "s_184",
                "s_293",
                "s_989",
                "s_360"
            ],
            "type": "rich",
            "x": -9.729653358459473,
            "y": 19.658164978027344
        },
        {
            "title": "No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization",
            "data": "There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209460",
            "id": "r_218",
            "s_ids": [
                "s_1293"
            ],
            "type": "rich",
            "x": -3.718989133834839,
            "y": 17.17158317565918
        },
        {
            "title": "Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design",
            "data": "The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209456",
            "id": "r_219",
            "s_ids": [
                "s_833",
                "s_536",
                "s_27",
                "s_69",
                "s_1215"
            ],
            "type": "rich",
            "x": -3.9136135578155518,
            "y": 16.514129638671875
        },
        {
            "title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition",
            "data": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209421",
            "id": "r_220",
            "s_ids": [
                "s_522",
                "s_27",
                "s_536"
            ],
            "type": "rich",
            "x": -4.064199924468994,
            "y": 16.26487922668457
        },
        {
            "title": "Rigel: Transforming Tabular Data by Declarative Mapping",
            "data": "We present Rigel, an interactive system for rapid transformation of tabular data. Rigel implements a new declarative mapping approach that formulates the data transformation procedure as direct mappings from data to the row, column, and cell channels of the target table. To construct such mappings, Rigel allows users to directly drag data attributes from input data to these three channels and indirectly drag or type data values in a spreadsheet, and possible mappings that do not contradict these interactions are recommended to achieve efficient and straightforward data transformation. The recommended mappings are generated by enumerating and composing data variables based on the row, column, and cell channels, thereby revealing the possibility of alternative tabular forms and facilitating open-ended exploration in many data transformation scenarios, such as designing tables for presentation. In contrast to existing systems that transform data by composing operations (like transposing and pivoting), Rigel requires less prior knowledge on these operations, and constructing tables from the channels is more efficient and results in less ambiguity than generating operation sequences as done by the traditional by-example approaches. User study results demonstrated that Rigel is significantly less demanding in terms of time and interactions and suits more scenarios compared to the state-of-the-art by-example approach. A gallery of diverse transformation cases is also presented to show the potential of Rigel's expressiveness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209385",
            "id": "r_221",
            "s_ids": [
                "s_1285",
                "s_735",
                "s_1211",
                "s_1516",
                "s_1585",
                "s_300",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.436847686767578,
            "y": 17.15751838684082
        },
        {
            "title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data",
            "data": "New highly-multiplexed imaging technologies have enabled the study of tissues in unprecedented detail. These methods are increasingly being applied to understand how cancer cells and immune response change during tumor development, progression, and metastasis, as well as following treatment. Yet, existing analysis approaches focus on investigating small tissue samples on a per-cell basis, not taking into account the spatial proximity of cells, which indicates cell-cell interaction and specific biological processes in the larger cancer microenvironment. We present Visinity, a scalable visual analytics system to analyze cell interaction patterns across cohorts of whole-slide multiplexed tissue images. Our approach is based on a fast regional neighborhood computation, leveraging unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. These neighborhoods can be visually analyzed in an exploratory and confirmatory workflow. Users can explore spatial patterns present across tissues through a scalable image viewer and coordinated views highlighting the neighborhood composition and spatial arrangements of cells. To verify or refine existing hypotheses, users can query for specific patterns to determine their presence and statistical significance. Findings can be interactively annotated, ranked, and compared in the form of small multiples. In two case studies with biomedical experts, we demonstrate that Visinity can identify common biological processes within a human tonsil and uncover novel white-blood cell networks and immune-tumor interactions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209378",
            "id": "r_222",
            "s_ids": [
                "s_501",
                "s_1186",
                "s_320",
                "s_604",
                "s_1492",
                "s_1240",
                "s_521",
                "s_1216",
                "s_767",
                "s_382",
                "s_1421",
                "s_507",
                "s_1486"
            ],
            "type": "rich",
            "x": -8.727982521057129,
            "y": 16.477399826049805
        },
        {
            "title": "A Memory Efficient Encoding for Ray Tracing Large Unstructured Data",
            "data": "In theory, efficient and high-quality rendering of unstructured data should greatly benefit from modern GPUs, but in practice, GPUs are often limited by the large amount of memory that large meshes require for element representation and for sample reconstruction acceleration structures. We describe a memory-optimized encoding for large unstructured meshes that efficiently encodes both the unstructured mesh and corresponding sample reconstruction acceleration structure, while still allowing for fast random-access sampling as required for rendering. We demonstrate that for large data our encoding allows for rendering even the 2.9 billion element Mars Lander on a single off-the-shelf GPU-and the largest 6.3 billion version on a pair of such GPUs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114869",
            "id": "r_223",
            "s_ids": [
                "s_1566",
                "s_121",
                "s_949"
            ],
            "type": "rich",
            "x": -15.019854545593262,
            "y": 7.154460906982422
        },
        {
            "title": "GenNI: Human-AI Collaboration for Data-Backed Text Generation",
            "data": "Table2Text systems generate textual output based on structured data utilizing machine learning. These systems are essential for fluent natural language interfaces in tools such as virtual assistants; however, left to generate freely these ML systems often produce misleading or unexpected outputs. GenNI (Generation Negotiation Interface) is an interactive visual system for high-level human-AI collaboration in producing descriptive text. The tool utilizes a deep learning model designed with explicit control states. These controls allow users to globally constrain model generations, without sacrificing the representation power of the deep learning models. The visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable. We report multiple use cases on two experiments that improve over uncontrolled generation approaches, while at the same time providing fine-grained control. A demo and source code are available at https://genni.vizhub.ai.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114845",
            "id": "r_224",
            "s_ids": [
                "s_996",
                "s_480",
                "s_1186",
                "s_689",
                "s_1486",
                "s_514"
            ],
            "type": "rich",
            "x": -8.74992847442627,
            "y": 16.485837936401367
        },
        {
            "title": "SPEULER: Semantics-preserving Euler Diagrams",
            "data": "Creating comprehensible visualizations of highly overlapping set-typed data is a challenging task due to its complexity. To facilitate insights into set connectivity and to leverage semantic relations between intersections, we propose a fast two-step layout technique for Euler diagrams that are both well-matched and well-formed. Our method conforms to established form guidelines for Euler diagrams regarding semantics, aesthetics, and readability. First, we establish an initial ordering of the data, which we then use to incrementally create a planar, connected, and monotone dual graph representation. In the next step, the graph is transformed into a circular layout that maintains the semantics and yields simple Euler diagrams with smooth curves. When the data cannot be represented by simple diagrams, our algorithm always falls back to a solution that is not well-formed but still well-matched, whereas previous methods often fail to produce expected results. We show the usefulness of our method for visualizing set-typed data using examples from text analysis and infographics. Furthermore, we discuss the characteristics of our approach and evaluate our method against state-of-the-art methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114834",
            "id": "r_225",
            "s_ids": [
                "s_1522",
                "s_247",
                "s_1146",
                "s_1345"
            ],
            "type": "rich",
            "x": -8.12211799621582,
            "y": 18.645017623901367
        },
        {
            "title": "An Efficient Dual-Hierarchy t-SNE Minimization",
            "data": "t-distributed Stochastic Neighbour Embedding (t-SNE) has become a standard for exploratory data analysis, as it is capable of revealing clusters even in complex data while requiring minimal user input. While its run-time complexity limited it to small datasets in the past, recent efforts improved upon the expensive similarity computations and the previously quadratic minimization. Nevertheless, t-SNE still has high runtime and memory costs when operating on millions of points. We present a novel method for executing the t-SNE minimization. While our method overall retains a linear runtime complexity, we obtain a significant performance increase in the most expensive part of the minimization. We achieve a significant improvement without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation and evaluate its performance against state-of-the-art methods on a variety of datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114817",
            "id": "r_226",
            "s_ids": [
                "s_443",
                "s_227",
                "s_167"
            ],
            "type": "rich",
            "x": -6.869783401489258,
            "y": 14.349102973937988
        },
        {
            "title": "Simultaneous Matrix Orderings for Graph Collections",
            "data": "Undirected graphs are frequently used to model phenomena that deal with interacting objects, such as social networks, brain activity and communication networks. The topology of an undirected graph <inline-formula><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-1-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula> can be captured by an adjacency matrix; this matrix in turn can be visualized directly to give insight into the graph structure. Which visual patterns appear in such a matrix visualization crucially depends on the <i>ordering</i> of its rows and columns. Formally defining the quality of an ordering and then automatically computing a high-quality ordering are both challenging problems; however, effective heuristics exist and are used in practice. <p>Often, graphs do not exist in isolation but as part of a collection of graphs on the same set of vertices, for example, brain scans over time or of different people. To visualize such graph collections, we need a <i>single</i> ordering that works well for all matrices <i>simultaneously</i>. The current state-of-the-art solves this problem by taking a (weighted) union over all graphs and applying existing heuristics. However, this union leads to a loss of information, specifically in those parts of the graphs which are different. We propose a <i>collection-aware</i> approach to avoid this loss of information and apply it to two popular heuristic methods: leaf order and barycenter.</p> <p>The de-facto standard computational quality metrics for matrix ordering capture only block-diagonal patterns (cliques). Instead, we propose to use <i>Moran's</i> <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-2-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>, a spatial auto-correlation metric, which captures the full range of established patterns. Moran's <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-3-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula> refines previously proposed stress measures. Furthermore, the popular leaf order method heuristically optimizes a similar measure which further supports the use of Moran's <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-4-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula> in this context. An ordering that maximizes Moran's <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-5-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula> can be computed via solutions to the Traveling Salesperson Problem (TSP); orderings that approximate the optimal ordering can be computed more efficiently, using any of the approximation algorithms for metric TSP.</p> <p>We evaluated our methods for simultaneous orderings on real-world datasets using Moran's <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-6-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula> as the quality metric. Our results show that our collection-aware approach matches or improves performance compared to the union approach, depending on the similarity of the graphs in the collection. Specifically, our Moran's <inline-formula><tex-math notation=\"LaTeX\">$I$</tex-math><alternatives><graphic orientation=\"portrait\" position=\"float\" xlink:href=\"28tvcg01-beusekom-3114773-eqinline-7-small.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>-based collection-aware leaf order implementation consistently outperforms other implementations. Our collection-aware implementations carry no significant additional computational costs.</p>",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114773",
            "id": "r_227",
            "s_ids": [
                "s_1083",
                "s_545",
                "s_347"
            ],
            "type": "rich",
            "x": -5.928497314453125,
            "y": 15.635523796081543
        },
        {
            "title": "Ray Tracing Structured AMR Data Using ExaBricks",
            "data": "Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt the domain resolution to save computation and storage, and has become one of the dominant data representations used by scientific simulations; however, efficiently rendering such data remains a challenge. We present an efficient approach for volume- and iso-surface ray tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different data structures. Together, these data structures allow a ray tracing based renderer to quickly determine which segments along the ray need to be integrated and at what frequency, while also providing quick access to all data values required for a smooth sample reconstruction kernel. Our method makes use of the RTX ray tracing hardware for surface rendering, ray marching, space skipping, and adaptive sampling; and allows for interactive changes to the transfer function and implicit iso-surfacing thresholds. We demonstrate that our method achieves high performance with little memory overhead, enabling interactive high quality rendering of complex AMR data sets on individual GPU workstations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030470",
            "id": "r_228",
            "s_ids": [
                "s_1566",
                "s_949",
                "s_708",
                "s_121",
                "s_1113",
                "s_343"
            ],
            "type": "rich",
            "x": -15.037880897521973,
            "y": 7.136486530303955
        },
        {
            "title": "Visual Neural Decomposition to Explain Multivariate Data Sets",
            "data": "Investigating relationships between variables in multi-dimensional data sets is a common task for data analysts and engineers. More specifically, it is often valuable to understand which ranges of which input variables lead to particular values of a given target variable. Unfortunately, with an increasing number of independent variables, this process may become cumbersome and time-consuming due to the many possible combinations that have to be explored. In this paper, we propose a novel approach to visualize correlations between input variables and a target output variable that scales to hundreds of variables. We developed a visual model based on neural networks that can be explored in a guided way to help analysts find and understand such correlations. First, we train a neural network to predict the target from the input variables. Then, we visualize the inner workings of the resulting model to help understand relations within the data set. We further introduce a new regularization term for the backpropagation algorithm that encourages the neural network to learn representations that are easier to interpret visually. We apply our method to artificial and real-world data sets to show its utility.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030420",
            "id": "r_229",
            "s_ids": [
                "s_1380",
                "s_963",
                "s_446",
                "s_346"
            ],
            "type": "rich",
            "x": -9.927846908569336,
            "y": 16.997947692871094
        },
        {
            "title": "Zoomless Maps: External Labeling Methods for the Interactive Exploration of Dense Point Sets at a Fixed Map Scale",
            "data": "Visualizing spatial data on small-screen devices such as smartphones and smartwatches poses new challenges in computational cartography. The current interfaces for map exploration require their users to zoom in and out frequently. Indeed, zooming and panning are tools suitable for choosing the map extent corresponding to an area of interest. They are not as suitable, however, for resolving the graphical clutter caused by a high feature density since zooming in to a large map scale leads to a loss of context. Therefore, in this paper, we present new external labeling methods that allow a user to navigate through dense sets of points of interest while keeping the current map extent fixed. We provide a unified model, in which labels are placed at the boundary of the map and visually associated with the corresponding features via connecting lines, which are called leaders. Since the screen space is limited, labeling all features at the same time is impractical. Therefore, at any time, we label a subset of the features. We offer interaction techniques to change the current selection of features systematically and, thus, give the user access to all features. We distinguish three methods, which allow the user either to slide the labels along the bottom side of the map or to browse the labels based on pages or stacks. We present a generic algorithmic framework that provides us with the possibility of expressing the different variants of interaction techniques as optimization problems in a unified way. We propose both exact algorithms and fast and simple heuristics that solve the optimization problems taking into account different criteria such as the ranking of the labels, the total leader length as well as the distance between leaders. In experiments on real-world data we evaluate these algorithms and discuss the three variants with respect to their strengths and weaknesses proving the flexibility of the presented algorithmic framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030399",
            "id": "r_230",
            "s_ids": [
                "s_89",
                "s_1138",
                "s_1279",
                "s_715"
            ],
            "type": "rich",
            "x": -5.611421585083008,
            "y": 15.068652153015137
        },
        {
            "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs",
            "data": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030398",
            "id": "r_231",
            "s_ids": [
                "s_1446",
                "s_450",
                "s_42",
                "s_1100",
                "s_578"
            ],
            "type": "rich",
            "x": -7.160260200500488,
            "y": 20.711206436157227
        },
        {
            "title": "Uncertainty-Oriented Ensemble Data Visualization and Exploration using Variable Spatial Spreading",
            "data": "As an important method of handling potential uncertainties in numerical simulations, ensemble simulation has been widely applied in many disciplines. Visualization is a promising and powerful ensemble simulation analysis method. However, conventional visualization methods mainly aim at data simplification and highlighting important information based on domain expertise instead of providing a flexible data exploration and intervention mechanism. Trial-and-error procedures have to be repeatedly conducted by such approaches. To resolve this issue, we propose a new perspective of ensemble data analysis using the attribute variable dimension as the primary analysis dimension. Particularly, we propose a variable uncertainty calculation method based on variable spatial spreading. Based on this method, we design an interactive ensemble analysis framework that provides a flexible interactive exploration of the ensemble data. Particularly, the proposed spreading curve view, the region stability heat map view, and the temporal analysis view, together with the commonly used 2D map view, jointly support uncertainty distribution perception, region selection, and temporal analysis, as well as other analysis requirements. We verify our approach by analyzing a real-world ensemble simulation dataset. Feedback collected from domain experts confirms the efficacy of our framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030377",
            "id": "r_232",
            "s_ids": [
                "s_39",
                "s_1047",
                "s_549",
                "s_1338",
                "s_1361"
            ],
            "type": "rich",
            "x": -10.918835639953613,
            "y": 19.388221740722656
        },
        {
            "title": "VisConnect: Distributed Event Synchronization for Collaborative Visualization",
            "data": "Tools and interfaces are increasingly expected to be synchronous and distributed to accommodate remote collaboration. Yet, adoption of these techniques for data visualization is low partly because development is difficult: existing collaboration software systems either do not support simultaneous interaction or require expensive redevelopment of existing visualizations. We contribute VisConnect: a web-based synchronous distributed collaborative visualization system that supports most web-based SVG data visualizations, balances system safety with responsiveness, and supports simultaneous interaction from many collaborators. VisConnect works with existing visualization implementations with little-to-no code changes by synchronizing low-level JavaScript events across clients such that visualization updates proceed transparently across clients. This is accomplished via a peer-to-peer system that establishes consensus among clients on the per-element sequence of events, and uses a lock service to grant access over elements to clients. We contribute collaborative extensions of traditional visualization interaction techniques, such as drag, brush, and lasso, and discuss different strategies for collaborative visualization interactions. To demonstrate the utility of VisConnect, we present novel examples of collaborative visualizations in the healthcare domain, remote collaboration with annotation, and show in an education case study for e-learning with 22 participants that students found the ability to remotely collaborate on class activities helpful and enjoyable for understanding concepts. A free copy of this paper and source code are available on OSF at osf.io/ut7e6 and at visconnect.us.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030366",
            "id": "r_233",
            "s_ids": [
                "s_1071",
                "s_1187",
                "s_879",
                "s_1463",
                "s_1139",
                "s_1493",
                "s_1006",
                "s_1163"
            ],
            "type": "rich",
            "x": -6.715527534484863,
            "y": 21.183147430419922
        },
        {
            "title": "Dead or Alive: Continuous Data Profiling for Interactive Data Science",
            "data": "Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively (\u201clive\u201d) and the other updates only on demand (\u201cdead\u201d). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327367",
            "id": "r_234",
            "s_ids": [
                "s_1149",
                "s_863",
                "s_646",
                "s_1229"
            ],
            "type": "rich",
            "x": -3.847445487976074,
            "y": 17.40815544128418
        },
        {
            "title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback",
            "data": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327363",
            "id": "r_235",
            "s_ids": [
                "s_1405",
                "s_459",
                "s_69",
                "s_1268",
                "s_1440",
                "s_110"
            ],
            "type": "rich",
            "x": -4.21651029586792,
            "y": 19.876596450805664
        },
        {
            "title": "How Do Viewers Synthesize Conflicting Information from Data Visualizations?",
            "data": "Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments ($\\mathrm{N}=1166$) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants' synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209467",
            "id": "r_236",
            "s_ids": [
                "s_956",
                "s_1339",
                "s_731",
                "s_1215"
            ],
            "type": "rich",
            "x": -3.707030773162842,
            "y": 16.520618438720703
        },
        {
            "title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization",
            "data": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209407",
            "id": "r_237",
            "s_ids": [
                "s_522",
                "s_1266",
                "s_470",
                "s_1163",
                "s_393"
            ],
            "type": "rich",
            "x": -8.392085075378418,
            "y": 17.142841339111328
        },
        {
            "title": "BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations",
            "data": "We developed and validated a rating scale to assess the aesthetic pleasure (or beauty) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, \u201cenjoyable,\u201d \u201clikable,\u201d \u201cpleasing,\u201d \u201cnice,\u201d and \u201cappealing.\u201d Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209390",
            "id": "r_238",
            "s_ids": [
                "s_1588",
                "s_1124",
                "s_1384",
                "s_1323"
            ],
            "type": "rich",
            "x": -6.123239517211914,
            "y": 18.5308837890625
        },
        {
            "title": "Traveler: Navigating Task Parallel Traces for Performance Analysis",
            "data": "Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace\u2014a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler, an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209375",
            "id": "r_239",
            "s_ids": [
                "s_358",
                "s_1038",
                "s_1117",
                "s_87",
                "s_616",
                "s_901",
                "s_223",
                "s_769",
                "s_564",
                "s_174"
            ],
            "type": "rich",
            "x": -0.2404078245162964,
            "y": 17.140819549560547
        },
        {
            "title": "HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data",
            "data": "Tabular visualization techniques integrate visual representations with tabular data to avoid additional cognitive load caused by splitting users' attention. However, most of the existing studies focus on simple flat tables instead of hierarchical tables, whose complex structure limits the expressiveness of visualization results and affects users' efficiency in visualization construction. We present HiTailor, a technique for presenting and exploring hierarchical tables. HiTailor constructs an abstract model, which defines row/column headings as biclustering and hierarchical structures. Based on our abstract model, we identify three pairs of operators, Swap/Transpose, ToStacked/ToLinear, Fold/Unfold, for transformations of hierarchical tables to support users' comprehensive explorations. After transformation, users can specify a cell or block of interest in hierarchical tables as a TableUnit for visualization, and HiTailor recommends other related TableUnits according to the abstract model using different mechanisms. We demonstrate the usability of the HiTailor system through a comparative study and a case study with domain experts, showing that HiTailor can present and explore hierarchical tables from different viewpoints. HiTailor is available at https://github.com/bitvis2021/HiTailor.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209354",
            "id": "r_240",
            "s_ids": [
                "s_385",
                "s_77",
                "s_915",
                "s_745",
                "s_1557",
                "s_645"
            ],
            "type": "rich",
            "x": -7.85701322555542,
            "y": 18.886768341064453
        },
        {
            "title": "Evaluating the Use of Uncertainty Visualisations for Imputations of Data Missing At Random in Scatterplots",
            "data": "Most real-world datasets contain missing values yet most exploratory data analysis (EDA) systems only support visualising data points with complete cases. This omission may potentially lead the user to biased analyses and insights. Imputation techniques can help estimate the value of a missing data point, but introduces additional uncertainty. In this work, we investigate the effects of visualising imputed values in charts using different ways of representing data imputations and imputation uncertainty\u2014no imputation, mean, 95% confidence intervals, probability density plots, gradient intervals, and hypothetical outcome plots. We focus on scatterplots, which is a commonly used chart type, and conduct a crowdsourced study with 202 participants. We measure users' bias and precision in performing two tasks\u2014estimating average and detecting trend\u2014and their self-reported confidence in performing these tasks. Our results suggest that, when estimating averages, uncertainty representations may reduce bias but at the cost of decreasing precision. When estimating trend, only hypothetical outcome plots may lead to a small probability of reducing bias while increasing precision. Participants in every uncertainty representation were less certain about their response when compared to the baseline. The findings point towards potential trade-offs in using uncertainty encodings for datasets with a large number of missing values. This paper and the associated analysis materials are available at: https://osf.io/q4y5r/",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209348",
            "id": "r_241",
            "s_ids": [
                "s_940",
                "s_459",
                "s_69",
                "s_1440",
                "s_1226",
                "s_110",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.661759853363037,
            "y": 16.6816463470459
        },
        {
            "title": "Visual Analysis of Hyperproperties for Understanding Model Checking Results",
            "data": "Model checkers provide algorithms for proving that a mathematical model of a system satisfies a given specification. In case of a violation, a counterexample that shows the erroneous behavior is returned. Understanding these counterexamples is challenging, especially for hyperproperty specifications, i.e., specifications that relate multiple executions of a system to each other. We aim to facilitate the visual analysis of such counterexamples through our H<small>yper</small>V<small>is</small> tool, which provides interactive visualizations of the given model, specification, and counterexample. Within an iterative and interdisciplinary design process, we developed visualization solutions that can effectively communicate the core aspects of the model checking result. Specifically, we introduce graphical representations of binary values for improving pattern recognition, color encoding for better indicating related aspects, visually enhanced textual descriptions, as well as extensive cross-view highlighting mechanisms. Further, through an underlying causal analysis of the counterexample, we are also able to identify values that contributed to the violation and use this knowledge for both improved encoding and highlighting. Finally, the analyst can modify both the specification of the hyperproperty and the system directly within H<small>yper</small>V<small>is</small> and initiate the model checking of the new version. In combination, these features notably support the analyst in understanding the error leading to the counterexample as well as iterating the provided system and specification. We ran multiple case studies with H<small>yper</small>V<small>is</small> and tested it with domain experts in qualitative feedback sessions. The participants' positive feedback confirms the considerable improvement over the manual, text-based status quo and the value of the tool for explaining hyperproperties.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114866",
            "id": "r_242",
            "s_ids": [
                "s_1005",
                "s_806",
                "s_1105",
                "s_276",
                "s_755",
                "s_1312",
                "s_401",
                "s_1427",
                "s_1384"
            ],
            "type": "rich",
            "x": -5.773529052734375,
            "y": 18.50848388671875
        },
        {
            "title": "Semantic Snapping for Guided Multi-View Visualization Design",
            "data": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \u201caligned\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114860",
            "id": "r_243",
            "s_ids": [
                "s_1137",
                "s_66",
                "s_1013"
            ],
            "type": "rich",
            "x": -5.913261413574219,
            "y": 19.263906478881836
        },
        {
            "title": "Loon: Using Exemplars to Visualize Large-Scale Microscopy Data",
            "data": "Which drug is most promising for a cancer patient? A new microscopy-based approach for measuring the mass of individual cancer cells treated with different drugs promises to answer this question in only a few hours. However, the analysis pipeline for extracting data from these images is still far from complete automation: human intervention is necessary for quality control for preprocessing steps such as segmentation, adjusting filters, removing noise, and analyzing the result. To address this workflow, we developed Loon, a visualization tool for analyzing drug screening data based on quantitative phase microscopy imaging. Loon visualizes both derived data such as growth rates and imaging data. Since the images are collected automatically at a large scale, manual inspection of images and segmentations is infeasible. However, reviewing representative samples of cells is essential, both for quality control and for data analysis. We introduce a new approach for choosing and visualizing representative exemplar cells that retain a close connection to the low-level data. By tightly integrating the derived data visualization capabilities with the novel exemplar visualization and providing selection and filtering capabilities, Loon is well suited for making decisions about which drugs are suitable for a specific patient.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114766",
            "id": "r_244",
            "s_ids": [
                "s_160",
                "s_1096",
                "s_945",
                "s_612",
                "s_215"
            ],
            "type": "rich",
            "x": -2.243487596511841,
            "y": 15.40510082244873
        },
        {
            "title": "Effect of uncertainty visualizations on myopic loss aversion and the equity premium puzzle in retirement investment decisions",
            "data": "For many households, investing for retirement is one of the most significant decisions and is fraught with uncertainty. In a classic study in behavioral economics, Benartzi and Thaler (1999) found evidence using bar charts that investors exhibit myopic loss aversion in retirement decisions: Investors overly focus on the potential for short-term losses, leading them to invest less in riskier assets and miss out on higher long-term returns. Recently, advances in uncertainty visualizations have shown improvements in decision-making under uncertainty in a variety of tasks. In this paper, we conduct a controlled and incentivized crowdsourced experiment replicating Benartzi and Thaler (1999) and extending it to measure the effect of different uncertainty representations on myopic loss aversion. Consistent with the original study, we find evidence of myopic loss aversion with bar charts and find that participants make better investment decisions with longer evaluation periods. We also find that common uncertainty representations such as interval plots and bar charts achieve the highest mean expected returns while other uncertainty visualizations lead to poorer long-term performance and strong effects on the equity premium. Qualitative feedback further suggests that different uncertainty representations lead to visual reasoning heuristics that can either mitigate or encourage a focus on potential short-term losses. We discuss implications of our results on using uncertainty visualizations for retirement decisions in practice and possible extensions for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114692",
            "id": "r_245",
            "s_ids": [
                "s_1148",
                "s_384",
                "s_1030",
                "s_335"
            ],
            "type": "rich",
            "x": -4.718637943267822,
            "y": 15.404156684875488
        },
        {
            "title": "Sea of Genes: A Reflection on Visualising Metagenomic Data for Museums",
            "data": "We examine the process of designing an exhibit to communicate scientific findings from a complex dataset and unfamiliar domain to the public in a science museum. Our exhibit sought to communicate new lessons based on scientific findings from the domain of metagenomics. This multi-user exhibit had three goals: (1) to inform the public about microbial communities and their daily cycles; (2) to link microbes' activity to the concept of gene expression; (3) and to highlight scientists' use of gene expression data to understand the role of microbes. To address these three goals, we derived visualization designs with three corresponding stories, each corresponding to a goal. We present three successive rounds of design and evaluation of our attempts to convey these goals. We could successfully present one story but had limited success with our second and third goals. This work presents a detailed account of an attempt to explain tightly coupled relationships through storytelling and animation in a multi-user, informal learning environment to a public with varying prior knowledge on the domain and identify lessons for future design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030412",
            "id": "r_246",
            "s_ids": [
                "s_1091",
                "s_789",
                "s_696",
                "s_1291"
            ],
            "type": "rich",
            "x": -0.7278158664703369,
            "y": 17.996524810791016
        },
        {
            "title": "Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes",
            "data": "Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030400",
            "id": "r_247",
            "s_ids": [
                "s_1450",
                "s_840",
                "s_716",
                "s_550",
                "s_899",
                "s_598"
            ],
            "type": "rich",
            "x": -8.40527057647705,
            "y": 18.829267501831055
        },
        {
            "title": "Kyrix-S: Authoring Scalable Scatterplot Visualizations of Big Data",
            "data": "Static scatterplots often suffer from the overdraw problem on big datasets where object overlap causes undesirable visual clutter. The use of zooming in scatterplots can help alleviate this problem. With multiple zoom levels, more screen real estate is available, allowing objects to be placed in a less crowded way. We call this type of visualization scalable scatterplot visualizations, or SSV for short. Despite the potential of SSVs, existing systems and toolkits fall short in supporting the authoring of SSVs due to three limitations. First, many systems have limited scalability, assuming that data fits in the memory of one computer. Second, too much developer work, e.g., using custom code to generate mark layouts or render objects, is required. Third, many systems focus on only a small subset of the SSV design space (e.g. supporting a specific type of visual marks). To address these limitations, we have developed Kyrix-S, a system for easy authoring of SSVs at scale. Kyrix-S derives a declarative grammar that enables specification of a variety of SSVs in a few tens of lines of code, based on an existing survey of scatterplot tasks and designs. The declarative grammar is supported by a distributed layout algorithm which automatically places visual marks onto zoom levels. We store data in a multi-node database and use multi-node spatial indexes to achieve interactive browsing of large SSVs. Extensive experiments show that 1) Kyrix-S enables interactive browsing of SSVs of billions of objects, with response times under 500ms and 2) Kyrix-S achieves 4X-9X reduction in specification compared to a state-of-the-art authoring system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030372",
            "id": "r_248",
            "s_ids": [
                "s_1309",
                "s_954",
                "s_1300",
                "s_1025",
                "s_366",
                "s_1591"
            ],
            "type": "rich",
            "x": -4.704930782318115,
            "y": 16.109821319580078
        },
        {
            "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems",
            "data": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030364",
            "id": "r_249",
            "s_ids": [
                "s_884",
                "s_210",
                "s_1059",
                "s_83",
                "s_1305"
            ],
            "type": "rich",
            "x": -7.509429931640625,
            "y": 17.864370346069336
        },
        {
            "title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning",
            "data": "Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant, a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326929",
            "id": "r_250",
            "s_ids": [
                "s_1287",
                "s_931",
                "s_610",
                "s_965",
                "s_897",
                "s_576",
                "s_933",
                "s_771",
                "s_549"
            ],
            "type": "rich",
            "x": -11.022322654724121,
            "y": 19.313064575195312
        },
        {
            "title": "The Urban Toolkit: A Grammar-Based Framework for Urban Visual Analytics",
            "data": "While cities around the world are looking for smart ways to use new advances in data collection, management, and analysis to address their problems, the complex nature of urban issues and the overwhelming amount of available data have posed significant challenges in translating these efforts into actionable insights. In the past few years, urban visual analytics tools have significantly helped tackle these challenges. When analyzing a feature of interest, an urban expert must transform, integrate, and visualize different thematic (e.g., sunlight access, demographic) and physical (e.g., buildings, street networks) data layers, oftentimes across multiple spatial and temporal scales. However, integrating and analyzing these layers require expertise in different fields, increasing development time and effort. This makes the entire visual data exploration and system implementation difficult for programmers and also sets a high entry barrier for urban experts outside of computer science. With this in mind, in this paper, we present the Urban Toolkit (UTK), a flexible and extensible visualization framework that enables the easy authoring of web-based visualizations through a new high-level grammar specifically built with common urban use cases in mind. In order to facilitate the integration and visualization of different urban data, we also propose the concept of knots to merge thematic and physical urban layers. We evaluate our approach through use cases and a series of interviews with experts and practitioners from different domains, including urban accessibility, urban planning, architecture, and climate science. UTK is available at urbantk.org.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326598",
            "id": "r_251",
            "s_ids": [
                "s_804",
                "s_22",
                "s_329",
                "s_1015",
                "s_744",
                "s_1320"
            ],
            "type": "rich",
            "x": -7.506605625152588,
            "y": -1.170562505722046
        },
        {
            "title": "VideoPro: A Visual Analytics Approach for Interactive Video Programming",
            "data": "Constructing supervised machine learning models for real-world video analysis require substantial labeled data, which is costly to acquire due to scarce domain expertise and laborious manual inspection. While data programming shows promise in generating labeled data at scale with user-defined labeling functions, the high dimensional and complex temporal information in videos poses additional challenges for effectively composing and evaluating labeling functions. In this paper, we propose VideoPro, a visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort. We first extract human-understandable events from videos using computer vision techniques and treat them as atomic components of labeling functions. We further propose a two-stage template mining algorithm that characterizes the sequential patterns of these events to serve as labeling function templates for efficient data labeling. The visual interface of VideoPro facilitates multifaceted exploration, examination, and application of the labeling templates, allowing for effective programming of video data at scale. Moreover, users can monitor the impact of programming on model performance and make informed adjustments during the iterative programming process. We demonstrate the efficiency and effectiveness of our approach with two case studies and expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326586",
            "id": "r_252",
            "s_ids": [
                "s_538",
                "s_1235",
                "s_1362",
                "s_412",
                "s_803",
                "s_369",
                "s_50",
                "s_1143",
                "s_156"
            ],
            "type": "rich",
            "x": -10.122471809387207,
            "y": 18.29790496826172
        },
        {
            "title": "Revealing the Semantics of Data Wrangling Scripts With Comantics",
            "data": "Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop Comantics, a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a \u201cslot filling\u201d strategy. We design experiments to evaluate the performance of Comantics. Further, we assess its flexibility using three example applications in different domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209470",
            "id": "r_253",
            "s_ids": [
                "s_1070",
                "s_668",
                "s_237",
                "s_467",
                "s_79",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.238783836364746,
            "y": 17.192853927612305
        },
        {
            "title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
            "data": "With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209462",
            "id": "r_254",
            "s_ids": [
                "s_239",
                "s_1563",
                "s_1461",
                "s_652",
                "s_81",
                "s_364",
                "s_1295"
            ],
            "type": "rich",
            "x": -1.7242685556411743,
            "y": 16.4134464263916
        },
        {
            "title": "Visual Comparison of Language Model Adaptation",
            "data": "Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209458",
            "id": "r_255",
            "s_ids": [
                "s_957",
                "s_1446",
                "s_1373",
                "s_288",
                "s_891"
            ],
            "type": "rich",
            "x": -5.0785746574401855,
            "y": 15.163722038269043
        },
        {
            "title": "VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing",
            "data": "Quantum computing has attracted considerable public attention due to its exponential speedup over classical computing. Despite its advantages, today's quantum computers intrinsically suffer from noise and are error-prone. To guarantee the high fidelity of the execution result of a quantum algorithm, it is crucial to inform users of the noises of the used quantum computer and the compiled physical circuits. However, an intuitive and systematic way to make users aware of the quantum computing noise is still missing. In this paper, we fill the gap by proposing a novel visualization approach to achieve noise-aware quantum computing. It provides a holistic picture of the noise of quantum computing through multiple interactively coordinated views: a Computer Evolution View with a circuit-like design overviews the temporal evolution of the noises of different quantum computers, a Circuit Filtering View facilitates quick filtering of multiple compiled physical circuits for the same quantum algorithm, and a Circuit Comparison View with a coupled bar chart enables detailed comparison of the filtered compiled circuits. We extensively evaluate the performance of VACSEN through two case studies on quantum algorithms of different scales and in-depth interviews with 12 quantum computing users. The results demonstrate the effectiveness and usability of VACSEN in achieving noise-aware quantum computing.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209455",
            "id": "r_256",
            "s_ids": [
                "s_32",
                "s_1210",
                "s_445",
                "s_133",
                "s_1404"
            ],
            "type": "rich",
            "x": -9.816133499145508,
            "y": 18.622865676879883
        },
        {
            "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
            "data": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data \u2014 which is often recorded as multivariate event sequences \u2014 to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209452",
            "id": "r_257",
            "s_ids": [
                "s_1087",
                "s_1160",
                "s_270",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.267300605773926,
            "y": 17.375003814697266
        },
        {
            "title": "A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms",
            "data": "Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users' visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209445",
            "id": "r_258",
            "s_ids": [
                "s_1524",
                "s_718",
                "s_278",
                "s_693",
                "s_976"
            ],
            "type": "rich",
            "x": -8.946663856506348,
            "y": 19.83131980895996
        },
        {
            "title": "Erato: Cooperative Data Story Editing via Fact Interpolation",
            "data": "As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209428",
            "id": "r_259",
            "s_ids": [
                "s_1477",
                "s_306",
                "s_1010",
                "s_1131",
                "s_184",
                "s_989"
            ],
            "type": "rich",
            "x": -9.430427551269531,
            "y": 19.61729621887207
        },
        {
            "title": "Studying Early Decision Making with Progressive Bar Charts",
            "data": "We conduct a user study to quantify and compare user performance for a value comparison task using four bar chart designs, where the bars show the mean values of data loaded progressively and updated every second (progressive bar charts). Progressive visualization divides different stages of the visualization pipeline\u2014data loading, processing, and visualization\u2014into iterative animated steps to limit the latency when loading large amounts of data. An animated visualization appearing quickly, unfolding, and getting more accurate with time, enables users to make early decisions. However, intermediate mean estimates are computed only on partial data and may not have time to converge to the true means, potentially misleading users and resulting in incorrect decisions. To address this issue, we propose two new designs visualizing the history of values in progressive bar charts, in addition to the use of confidence intervals. We comparatively study four progressive bar chart designs: with/without confidence intervals, and using near-history representation with/without confidence intervals, on three realistic data distributions. We evaluate user performance based on the percentage of correct answers (accuracy), response time, and user confidence. Our results show that, overall, users can make early and accurate decisions with 92% accuracy using only 18% of the data, regardless of the design. We find that our proposed bar chart design with only near-history is comparable to bar charts with only confidence intervals in performance, and the qualitative feedback we received indicates a preference for designs with history.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209426",
            "id": "r_260",
            "s_ids": [
                "s_590",
                "s_38",
                "s_881",
                "s_646",
                "s_107"
            ],
            "type": "rich",
            "x": -4.022096633911133,
            "y": 17.42189598083496
        },
        {
            "title": "Breaking the Fourth Wall of Data Stories through Interaction",
            "data": "Interaction is increasingly integrating into data stories to support data exploration and explanation. Interaction can also be combined with the narrative device, breaking the fourth wall (BTFW), to build a deeper connection between readers and data stories. BTFW interaction directly addresses readers by requiring their input. Such user input is then integrated into the narrative or visuals of data stories to encourage readers to inspect the stories more closely. In this work, we explore the design patterns of BTFW interaction commonly used in data stories. Six design patterns were identified through the analysis of 58 high-quality data stories collected from a range of online sources. Specifically, the data stories were categorized using a coding framework, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. To explore the benefits as well as concerns of using BTFW interaction, we conducted a three-session user study including the reading, interview, and recall sessions. The results of our user study suggested that BTFW interaction has a positive impact on self-story connection, user engagement, and information recall. We also discussed design implications to address the possible negative effects on the interactivity-comprehensibility balance, information privacy, and the learning curve of interaction brought by BTFW interaction.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209409",
            "id": "r_261",
            "s_ids": [
                "s_184",
                "s_1315",
                "s_1088",
                "s_989"
            ],
            "type": "rich",
            "x": -9.287636756896973,
            "y": 19.69170379638672
        },
        {
            "title": "Multi-View Design Patterns and Responsive Visualization for Genomics Data",
            "data": "A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as \u201cvertically long,\u201d \u201chorizontally wide,\u201d \u201ccircular,\u201d and \u201ccross-shaped\u201d compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209398",
            "id": "r_262",
            "s_ids": [
                "s_1266",
                "s_393"
            ],
            "type": "rich",
            "x": -8.424788475036621,
            "y": 17.020374298095703
        },
        {
            "title": "Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability",
            "data": "Embedding is a common technique for analyzing multi-dimensional data. However, the embedding projection cannot always form significant and interpretable visual structures that foreshadow underlying data patterns. We propose an approach that incorporates human knowledge into data embeddings to improve pattern significance and interpretability. The core idea is (1) externalizing tacit human knowledge as explicit sample labels and (2) adding a classification loss in the embedding network to encode samples' classes. The approach pulls samples of the same class with similar data features closer in the projection, leading to more compact (significant) and class-consistent (interpretable) visual structures. We give an embedding network with a customized classification loss to implement the idea and integrate the network into a visualization system to form a workflow that supports flexible class creation and pattern exploration. Patterns found on open datasets in case studies, subjects' performance in a user study, and quantitative experiment results illustrate the general usability and effectiveness of the approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209382",
            "id": "r_263",
            "s_ids": [
                "s_451",
                "s_1443"
            ],
            "type": "rich",
            "x": -10.222277641296387,
            "y": 19.650278091430664
        },
        {
            "title": "Dispersion vs Disparity: Hiding Variability Can Encourage Stereotyping When Visualizing Social Outcomes",
            "data": "Visualization research often focuses on perceptual accuracy or helping readers interpret key messages. However, we know very little about how chart designs might influence readers' perceptions of the people behind the data. Specifically, could designs interact with readers' social cognitive biases in ways that perpetuate harmful stereotypes? For example, when analyzing social inequality, bar charts are a popular choice to present outcome disparities between race, gender, or other groups. But bar charts may encourage deficit thinking, the perception that outcome disparities are caused by groups' personal strengths or deficiencies, rather than external factors. These faulty personal attributions can then reinforce stereotypes about the groups being visualized. We conducted four experiments examining design choices that influence attribution biases (and therefore deficit thinking). Crowdworkers viewed visualizations depicting social outcomes that either mask variability in data, such as bar charts or dot plots, or emphasize variability in data, such as jitter plots or prediction intervals. They reported their agreement with both personal and external explanations for the visualized disparities. Overall, when participants saw visualizations that hide within-group variability, they agreed more with personal explanations. When they saw visualizations that emphasize within-group variability, they agreed less with personal explanations. These results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping. Design choices can influence these biases: Hiding variability tends to increase stereotyping while emphasizing variability reduces it.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209377",
            "id": "r_264",
            "s_ids": [
                "s_1222",
                "s_1215"
            ],
            "type": "rich",
            "x": -3.7473700046539307,
            "y": 16.513717651367188
        },
        {
            "title": "Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization",
            "data": "We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our approach for exploring large data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114880",
            "id": "r_265",
            "s_ids": [
                "s_363",
                "s_1441",
                "s_1479",
                "s_107",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.096901893615723,
            "y": 18.59454917907715
        },
        {
            "title": "Exploring the Personal Informatics Analysis Gap: \"There's a Lot of Bacon\"",
            "data": "Personal informatics research helps people track personal data for the purposes of self-reflection and gaining self-knowledge. This field, however, has predominantly focused on the data collection and insight-generation elements of self-tracking, with less attention paid to flexible data analysis. As a result, this inattention has led to inflexible analytic pipelines that do not reflect or support the diverse ways people want to engage with their data. This paper contributes a review of personal informatics and visualization research literature to expose a gap in our knowledge for designing flexible tools that assist people engaging with and analyzing personal data in personal contexts, what we call the personal informatics analysis gap. We explore this gap through a multistage longitudinal study on how asthmatics engage with personal air quality data, and we report how participants: were motivated by broad and diverse goals; exhibited patterns in the way they explored their data; engaged with their data in playful ways; discovered new insights through serendipitous exploration; and were reluctant to use analysis tools on their own. These results present new opportunities for visual analysis research and suggest the need for fundamental shifts in how and what we design when supporting personal data analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114798",
            "id": "r_266",
            "s_ids": [
                "s_565",
                "s_958",
                "s_1601",
                "s_1517"
            ],
            "type": "rich",
            "x": -2.275679588317871,
            "y": 15.400981903076172
        },
        {
            "title": "Generative Design Inspiration for Glyphs with Diatoms",
            "data": "We introduce Diatoms, a technique that generates design inspiration for glyphs by sampling from palettes of mark shapes, encoding channels, and glyph scaffold shapes. Diatoms allows for a degree of randomness while respecting constraints imposed by columns in a data table: their data types and domains as well as semantic associations between columns as specified by the designer. We pair this generative design process with two forms of interactive design externalization that enable comparison and critique of the design alternatives. First, we incorporate a familiar <i>small multiples</i> configuration in which every data point is drawn according to a single glyph design, coupled with the ability to page between alternative glyph designs. Second, we propose a <i>small permutables</i> design gallery, in which a single data point is drawn according to each alternative glyph design, coupled with the ability to page between data points. We demonstrate an implementation of our technique as an extension to Tableau featuring three example palettes, and to better understand how Diatoms could fit into existing design workflows, we conducted interviews and chauffeured demos with 12 designers. Finally, we reflect on our process and the designers' reactions, discussing the potential of our technique in the context of visualization authoring systems. Ultimately, our approach to glyph design and comparison can kickstart and inspire visualization design, allowing for the serendipitous discovery of shape and channel combinations that would have otherwise been overlooked.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114792",
            "id": "r_267",
            "s_ids": [
                "s_12",
                "s_80",
                "s_1206"
            ],
            "type": "rich",
            "x": -6.481683254241943,
            "y": 16.54555892944336
        },
        {
            "title": "The Mixture Graph-A Data Structure for Compressing, Rendering, and Querying Segmentation Histograms",
            "data": "In this paper, we present a novel data structure, called the Mixture Graph. This data structure allows us to compress, render, and query segmentation histograms. Such histograms arise when building a mipmap of a volume containing segmentation IDs. Each voxel in the histogram mipmap contains a convex combination (mixture) of segmentation IDs. Each mixture represents the distribution of IDs in the respective voxel's children. Our method factorizes these mixtures into a series of linear interpolations between exactly two segmentation IDs. The result is represented as a directed acyclic graph (DAG) whose nodes are topologically ordered. Pruning replicate nodes in the tree followed by compression allows us to store the resulting data structure efficiently. During rendering, transfer functions are propagated from sources (leafs) through the DAG to allow for efficient, pre-filtered rendering at interactive frame rates. Assembly of histogram contributions across the footprint of a given volume allows us to efficiently query partial histograms, achieving up to 178 x speed-up over naive parallelized range queries. Additionally, we apply the Mixture Graph to compute correctly pre-filtered volume lighting and to interactively explore segments based on shape, geometry, and orientation using multi-dimensional transfer functions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030451",
            "id": "r_268",
            "s_ids": [
                "s_1060",
                "s_1503",
                "s_816"
            ],
            "type": "rich",
            "x": -8.921171188354492,
            "y": 16.143335342407227
        },
        {
            "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models",
            "data": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030449",
            "id": "r_269",
            "s_ids": [
                "s_470",
                "s_1408",
                "s_543",
                "s_156",
                "s_298"
            ],
            "type": "rich",
            "x": -8.4878568649292,
            "y": 17.398191452026367
        },
        {
            "title": "Visualization of Human Spine Biomechanics for Spinal Surgery",
            "data": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030388",
            "id": "r_270",
            "s_ids": [
                "s_583",
                "s_112",
                "s_1346",
                "s_1598"
            ],
            "type": "rich",
            "x": -3.551358222961426,
            "y": 19.82391929626465
        },
        {
            "title": "No mark is an island: Precision and category repulsion biases in data reproductions",
            "data": "Data visualization is powerful in large part because it facilitates visual extraction of values. Yet, existing measures of perceptual precision for data channels (e.g., position, length, orientation, etc.) are based largely on verbal reports of ratio judgments between two values (e.g., [7]). Verbal report conflates multiple sources of error beyond actual visual precision, introducing a ratio computation between these values and a requirement to translate that ratio to a verbal number. Here we observe raw measures of precision by eliminating both ratio computations and verbal reports; we simply ask participants to reproduce marks (a single bar or dot) to match a previously seen one. We manipulated whether the mark was initially presented (and later drawn) alone, paired with a reference (e.g. a second \u2018100%\u2019 bar also present at test, or a y-axis for the dot), or integrated with the reference (merging that reference bar into a stacked bar graph, or placing the dot directly on the axis). Reproductions of smaller values were overestimated, and larger values were underestimated, suggesting systematic memory biases. Average reproduction error was around 10% of the actual value, regardless of whether the reproduction was done on a common baseline with the original. In the reference and (especially) the integrated conditions, responses were repulsed from an implicit midpoint of the reference mark, such that values above 50% were overestimated, and values below 50% were underestimated. This reproduction paradigm may serve within a new suite of more fundamental measures of the precision of graphical perception.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030345",
            "id": "r_271",
            "s_ids": [
                "s_1209",
                "s_145",
                "s_809",
                "s_942"
            ],
            "type": "rich",
            "x": -3.1609930992126465,
            "y": 16.62378692626953
        },
        {
            "title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks",
            "data": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327170",
            "id": "r_272",
            "s_ids": [
                "s_596",
                "s_709",
                "s_1311",
                "s_877",
                "s_156"
            ],
            "type": "rich",
            "x": -10.14316177368164,
            "y": 18.21474838256836
        },
        {
            "title": "Visualizing Large-Scale Spatial Time Series with GeoChron",
            "data": "In geo-related fields such as urban informatics, atmospheric science, and geography, large-scale spatial time (ST) series (i.e., geo-referred time series) are collected for monitoring and understanding important spatiotemporal phenomena. ST series visualization is an effective means of understanding the data and reviewing spatiotemporal phenomena, which is a prerequisite for in-depth data analysis. However, visualizing these series is challenging due to their large scales, inherent dynamics, and spatiotemporal nature. In this study, we introduce the notion of patterns of evolution in ST series. Each evolution pattern is characterized by 1) a set of ST series that are close in space and 2) a time period when the trends of these ST series are correlated. We then leverage Storyline techniques by considering an analogy between evolution patterns and sessions, and finally design a novel visualization called GeoChron, which is capable of visualizing large-scale ST series in an evolution pattern-aware and narrative-preserving manner. GeoChron includes a mining framework to extract evolution patterns and two-level visualizations to enhance its visual scalability. We evaluate GeoChron with two case studies, an informal user study, an ablation study, parameter analysis, and running time analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327162",
            "id": "r_273",
            "s_ids": [
                "s_1308",
                "s_264",
                "s_578",
                "s_1341",
                "s_456",
                "s_79",
                "s_735",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.213462829589844,
            "y": 17.592164993286133
        },
        {
            "title": "Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model",
            "data": "Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to their predefined entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative models. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative models combined with an interactive interface for visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326913",
            "id": "r_274",
            "s_ids": [
                "s_798",
                "s_1213",
                "s_1481",
                "s_1193",
                "s_1438"
            ],
            "type": "rich",
            "x": -9.742119789123535,
            "y": 17.9200439453125
        },
        {
            "title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization",
            "data": "Data visualization has the power to revolutionize sports. For example, the rise of shot maps has changed basketball strategy by visually illustrating where \u201cgood/bad\u201d shots are taken from. As a result, professional basketball teams today take shots from very different positions on the court than they did 20 years ago. Although the shot map has transformed many facets of the game, there is still much room for improvement to support richer and more complex analytical tasks. More specifically, we believe that the lack of sufficient interactivity to support various analytical queries and the inability to visually compare differences across situations are significant limitations of current shot maps. To address these limitations and showcase new possibilities, we designed and developed HoopInSight, an interactive visualization system that centers around a novel spatial comparison visual technique, enhancing the capabilities of shot maps in basketball analytics. This article presents the system, with a focus on our proposed visual technique and its accompanying interactions, all designed to promote comparison of two different scenarios. Furthermore, we provide reflections on and a discussion of relevant issues, including considerations for designing spatial comparison techniques, the scalability and transferability of this approach, and the benefits and pitfalls of designing as domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326910",
            "id": "r_275",
            "s_ids": [
                "s_1122",
                "s_850"
            ],
            "type": "rich",
            "x": -4.531473636627197,
            "y": 15.365131378173828
        },
        {
            "title": "Transitioning to a Commercial Dashboarding System: Socio-Technical Observations and Opportunities",
            "data": "Many long-established, traditional manufacturing businesses are becoming more digital and data-driven to improve their production. These companies are embracing visual analytics in these transitions through their adoption of commercial dashboarding systems. Although a number of studies have looked at the technical challenges of adopting these systems, very few have focused on the socio-technical issues that arise. In this paper, we report on the results of an interview study with 17 participants working in a range of roles at a long-established, traditional manufacturing company as they adopted Microsoft Power BI. The results highlight a number of socio-technical challenges the employees faced, including difficulties in training, using and creating dashboards, and transitioning to a modern digital company. Based on these results, we propose a number of opportunities for both companies and visualization researchers to improve these difficult transitions, as well as opportunities for rethinking how we design dashboarding systems for real-world use.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326525",
            "id": "r_276",
            "s_ids": [
                "s_1094",
                "s_837",
                "s_580",
                "s_1517"
            ],
            "type": "rich",
            "x": -2.242964744567871,
            "y": 15.421961784362793
        },
        {
            "title": "Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches",
            "data": "This article reports on an in-depth study that investigates barriers to network exploration with visualizations. Network visualization tools are becoming increasingly popular, but little is known about how analysts plan and engage in the visual exploration of network data\u2014which exploration strategies they employ, and how they prepare their data, define questions, and decide on visual mappings. Our study involved a series of workshops, interaction logging, and observations from a 6-week network exploration course. Our findings shed light on the stages that define analysts' approaches to network visualization and barriers experienced by some analysts during their network visualization processes. These barriers mainly appear before using a specific tool and include defining exploration goals, identifying relevant network structures and abstractions, or creating appropriate visual mappings for their network data. Our findings inform future work in visualization education and analyst-centered network visualization tool design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209487",
            "id": "r_277",
            "s_ids": [
                "s_1159",
                "s_1322",
                "s_1538",
                "s_1480",
                "s_107",
                "s_1547",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.587298393249512,
            "y": 17.077775955200195
        },
        {
            "title": "A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics",
            "data": "Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. Lastly, we report on a series of interviews with four practitioners, and their feedback and suggestions for further work on the visualizations to support 3D spatiotemporal urban data analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209474",
            "id": "r_278",
            "s_ids": [
                "s_889",
                "s_744",
                "s_914",
                "s_786",
                "s_1015",
                "s_54",
                "s_1084",
                "s_893",
                "s_1320"
            ],
            "type": "rich",
            "x": -7.499454021453857,
            "y": -1.1777843236923218
        },
        {
            "title": "Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data",
            "data": "Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114788",
            "id": "r_279",
            "s_ids": [
                "s_15",
                "s_1003",
                "s_846",
                "s_1503",
                "s_388"
            ],
            "type": "rich",
            "x": -8.864974975585938,
            "y": 16.21597671508789
        },
        {
            "title": "Homomorphic-Encrypted Volume Rendering",
            "data": "Computationally demanding tasks are typically calculated in dedicated data centers, and real-time visualizations also follow this trend. Some rendering tasks, however, require the highest level of confidentiality so that no other party, besides the owner, can read or see the sensitive data. Here we present a direct volume rendering approach that performs volume rendering directly on encrypted volume data by using the homomorphic Paillier encryption algorithm. This approach ensures that the volume data and rendered image are uninterpretable to the rendering server. Our volume rendering pipeline introduces novel approaches for encrypted-data compositing, interpolation, and opacity modulation, as well as simple transfer function design, where each of these routines maintains the highest level of privacy. We present performance and memory overhead analysis that is associated with our privacy-preserving scheme. Our approach is open and secure by design, as opposed to secure through obscurity. Owners of the data only have to keep their secure key confidential to guarantee the privacy of their volume data and the rendered images. Our work is, to our knowledge, the first privacy-preserving remote volume-rendering approach that does not require that any server involved be trustworthy; even in cases when the server is compromised, no sensitive data will be leaked to a foreign party.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030436",
            "id": "r_280",
            "s_ids": [
                "s_926",
                "s_1051",
                "s_510"
            ],
            "type": "rich",
            "x": -6.225879669189453,
            "y": 18.981557846069336
        },
        {
            "title": "LineSmooth: An Analytical Framework for Evaluating the Effectiveness of Smoothing Techniques on Line Charts",
            "data": "We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing-rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as Gaussian filters and TOPOLOGY-based subsampling, perform well in general. Other methods, such as low-pass CUTOFF filters and Douglas-peucker subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used UNIFORM subsampling, produce low-quality results, and should, therefore, be avoided, if possible.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030421",
            "id": "r_281",
            "s_ids": [
                "s_542",
                "s_962"
            ],
            "type": "rich",
            "x": -1.86723792552948,
            "y": 17.961645126342773
        },
        {
            "title": "MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on Multiple Scales",
            "data": "Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030386",
            "id": "r_282",
            "s_ids": [
                "s_659",
                "s_1584",
                "s_272",
                "s_372",
                "s_1100"
            ],
            "type": "rich",
            "x": -7.029060363769531,
            "y": 20.875789642333984
        },
        {
            "title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
            "data": "Existing interactive visualization tools for deep learning are mostly applied to the training, debugging, and refinement of neural network models working on natural images. However, visual analytics tools are lacking for the specific application of x-ray image classification with multiple structural attributes. In this paper, we present an interactive system for domain scientists to visually study the multiple attributes learning models applied to x-ray scattering images. It allows domain scientists to interactively explore this important type of scientific images in embedded spaces that are defined on the model prediction output, the actual labels, and the discovered feature space of neural networks. Users are allowed to flexibly select instance images, their clusters, and compare them regarding the specified visual representation of attributes. The exploration is guided by the manifestation of model performance related to mutual relationships among attributes, which often affect the learning accuracy and effectiveness. The system thus supports domain scientists to improve the training dataset and model, find questionable attributes labels, and identify outlier images or spurious data clusters. Case studies and scientists feedback demonstrate its functionalities and usefulness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030384",
            "id": "r_283",
            "s_ids": [
                "s_630",
                "s_1615",
                "s_1053",
                "s_640",
                "s_234",
                "s_902",
                "s_794"
            ],
            "type": "rich",
            "x": -0.21793769299983978,
            "y": 17.632673263549805
        },
        {
            "title": "CcNav: Understanding Compiler Optimizations in Binary Code",
            "data": "Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030357",
            "id": "r_284",
            "s_ids": [
                "s_147",
                "s_686",
                "s_44",
                "s_433",
                "s_174"
            ],
            "type": "rich",
            "x": -0.2293265163898468,
            "y": 17.116474151611328
        },
        {
            "title": "A Testing Environment for Continuous Colormaps",
            "data": "Many computer science disciplines (e.g., combinatorial optimization, natural language processing, and information retrieval) use standard or established test suites for evaluating algorithms. In visualization, similar approaches have been adopted in some areas (e.g., volume visualization), while user testimonies and empirical studies have been the dominant means of evaluation in most other areas, such as designing colormaps. In this paper, we propose to establish a test suite for evaluating the design of colormaps. With such a suite, the users can observe the effects when different continuous colormaps are applied to planar scalar fields that may exhibit various characteristic features, such as jumps, local extrema, ridge or valley lines, different distributions of scalar values, different gradients, different signal frequencies, different levels of noise, and so on. The suite also includes an expansible collection of real-world data sets including the most popular data for colormap testing in the visualization literature. The test suite has been integrated into a web-based application for creating continuous colormaps (https://ccctool.com/), facilitating close inter-operation between design and evaluation processes. This new facility complements traditional evaluation methods such as user testimonies and empirical studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028955",
            "id": "r_285",
            "s_ids": [
                "s_1172",
                "s_298",
                "s_856",
                "s_1407",
                "s_968"
            ],
            "type": "rich",
            "x": -7.279052734375,
            "y": 16.9105224609375
        },
        {
            "title": "A Visual Analytics Approach to Debugging Cooperative, Autonomous Multi-Robot Systems\u2019 Worldviews",
            "data": "Autonomous multi-robot systems, where a team of robots shares information to perform tasks that are beyond an individual robot\u2019s abilities, hold great promise for a number of applications, such as planetary exploration missions. Each robot in a multi-robot system that uses the shared-world coordination paradigm autonomously schedules which robot should perform a given task, and when, using its worldview\u2013the robot\u2019s internal representation of its belief about both its own state, and other robots\u2019 states. A key problem for operators is that robots\u2019 worldviews can fall out of sync (often due to weak communication links), leading to desynchronization of the robots\u2019 scheduling decisions and inconsistent emergent behavior (e.g., tasks not performed, or performed by multiple robots). Operators face the time-consuming and difficult task of making sense of the robots\u2019 scheduling decisions, detecting de-synchronizations, and pinpointing the cause by comparing every robot\u2019s worldview. To address these challenges, we introduce MOSAIC Viewer, a visual analytics system that helps operators (i) make sense of the robots\u2019 schedules and (ii) detect and conduct a root cause analysis of the robots\u2019 desynchronized worldviews. Over a year-long partnership with roboticists at the NASA Jet Propulsion Laboratory, we conduct a formative study to identify the necessary system design requirements and a qualitative evaluation with 12 roboticists. We find that MOSAIC Viewer is faster- and easier-to-use than the users\u2019 current approaches, and it allows them to stitch low-level details to formulate a high-level understanding of the robots\u2019 schedules and detect and pinpoint the cause of the desynchronized worldviews.",
            "url": "http://dx.doi.org/10.1109/VAST50239.2020.00008",
            "id": "r_286",
            "s_ids": [
                "s_625",
                "s_1283",
                "s_1577",
                "s_1044",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7454751133918762,
            "y": 17.97185516357422
        },
        {
            "title": "Cluster-Aware Grid Layout",
            "data": "Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326934",
            "id": "r_287",
            "s_ids": [
                "s_1514",
                "s_148",
                "s_250",
                "s_803",
                "s_1498",
                "s_1048",
                "s_1280",
                "s_348"
            ],
            "type": "rich",
            "x": -9.613791465759277,
            "y": 17.203174591064453
        },
        {
            "title": "MosaicSets: Embedding Set Systems into Grid Graphs",
            "data": "Visualizing sets of elements and their relations is an important research area in information visualization. In this paper, we present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. As use case, we consider the research groups of a university faculty as elements, and the departments and joint research projects as sets. We aim at finding a suitable mapping between the research groups and the grid cells such that the department structure forms a base map layout. Our objectives are to optimize both the compactness of the entirety of all cells and of each set by itself. We show that computing the mapping is NP-hard. However, using integer linear programming we can solve real-world instances optimally within a few seconds. Moreover, we propose a relaxation of the contiguity requirement to visualize otherwise non-embeddable set systems. We present and discuss different rendering styles for the set overlays. Based on a case study with real-world data, our evaluation comprises quantitative measures as well as expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209485",
            "id": "r_288",
            "s_ids": [
                "s_916",
                "s_262",
                "s_1138",
                "s_89",
                "s_124",
                "s_715"
            ],
            "type": "rich",
            "x": -5.635190486907959,
            "y": 15.104708671569824
        },
        {
            "title": "Predicting User Preferences of Dimensionality Reduction Embedding Quality",
            "data": "A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select \u201cgood\u201d and \u201cmisleading\u201d views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209449",
            "id": "r_289",
            "s_ids": [
                "s_983",
                "s_1544",
                "s_790",
                "s_391",
                "s_899"
            ],
            "type": "rich",
            "x": -8.42553424835205,
            "y": 18.795394897460938
        },
        {
            "title": "RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics",
            "data": "Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user's fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209433",
            "id": "r_290",
            "s_ids": [
                "s_1219",
                "s_1099",
                "s_210",
                "s_1021",
                "s_73"
            ],
            "type": "rich",
            "x": -7.461250305175781,
            "y": 17.80843162536621
        },
        {
            "title": "Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations",
            "data": "The IEEE VIS Conference (VIS) recently rebranded itself as a unified conference and officially positioned itself within the discipline of Data Science. Driven by this movement, we investigated (1) who contributed to VIS, and (2) where VIS stands in the scientific world. We examined the authors and fields of study of 3,240 VIS publications in the past 32 years based on data collected from OpenAlex and IEEE Xplore, among other sources. We also examined the citation flows from referenced papers (i.e., those referenced in VIS) to VIS, and from VIS to citing papers (i.e., those citing VIS). We found that VIS has been becoming increasingly popular and collaborative. The number of publications, of unique authors, and of participating countries have been steadily growing. Both cross-country collaborations, and collaborations between educational and non-educational affiliations, namely \u201ccross-type collaborations\u201d, are increasing. The dominance of the US is decreasing, and authors from China are now an important part of VIS. In terms of author affiliation types, VIS is increasingly dominated by authors from universities. We found that the topics, inspirations, and influences of VIS research is limited such that (1) VIS, and their referenced and citing papers largely fall into the Computer Science domain, and (2) citations flow mostly between the same set of subfields within Computer Science. Our citation analyses showed that award-winning VIS papers had higher citations. Interactive visualizations, replication data, source code and supplementary material are available at https://32vis.hongtaoh.com and https://osf.io/zkvjm.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209422",
            "id": "r_291",
            "s_ids": [
                "s_241",
                "s_516",
                "s_354",
                "s_173"
            ],
            "type": "rich",
            "x": -3.4219229221343994,
            "y": 16.40859603881836
        },
        {
            "title": "sMolBoxes: Dataflow Model for Molecular Dynamics Exploration",
            "data": "We present sMolBoxes, a dataflow representation for the exploration and analysis of long molecular dynamics (MD) simulations. When MD simulations reach millions of snapshots, a frame-by-frame observation is not feasible anymore. Thus, biochemists rely to a large extent only on quantitative analysis of geometric and physico-chemical properties. However, the usage of abstract methods to study inherently spatial data hinders the exploration and poses a considerable workload. sMolBoxes link quantitative analysis of a user-defined set of properties with interactive 3D visualizations. They enable visual explanations of molecular behaviors, which lead to an efficient discovery of biochemically significant parts of the MD simulation. sMolBoxes follow a node-based model for flexible definition, combination, and immediate evaluation of properties to be investigated. Progressive analytics enable fluid switching between multiple properties, which facilitates hypothesis generation. Each sMolBox provides quick insight to an observed property or function, available in more detail in the bigBox View. The case studies illustrate that even with relatively few sMolBoxes, it is possible to express complex analytical tasks, and their use in exploratory analysis is perceived as more efficient than traditional scripting-based methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209411",
            "id": "r_292",
            "s_ids": [
                "s_484",
                "s_1507",
                "s_333",
                "s_924",
                "s_1499",
                "s_1354",
                "s_1001"
            ],
            "type": "rich",
            "x": -5.785519599914551,
            "y": 19.46434211730957
        },
        {
            "title": "Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles",
            "data": "Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the pairwise distance calculation as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn2N) to O(knN). Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209404",
            "id": "r_293",
            "s_ids": [
                "s_421",
                "s_548",
                "s_1174",
                "s_348"
            ],
            "type": "rich",
            "x": -9.281543731689453,
            "y": 17.262746810913086
        },
        {
            "title": "Computing a Stable Distance on Merge Trees",
            "data": "Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von K\u00e1rm\u00e1n vortex street.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209395",
            "id": "r_294",
            "s_ids": [
                "s_1247",
                "s_674",
                "s_1501"
            ],
            "type": "rich",
            "x": -4.022202968597412,
            "y": 20.732603073120117
        },
        {
            "title": "KiriPhys: Exploring New Data Physicalization Opportunities",
            "data": "We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while providing a pleasurable experience that stimulates curiosity and engagement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209365",
            "id": "r_295",
            "s_ids": [
                "s_608",
                "s_386",
                "s_978"
            ],
            "type": "rich",
            "x": -6.188132286071777,
            "y": 17.517118453979492
        },
        {
            "title": "NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis",
            "data": "The success of DL can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for DNN architectures making it possible for non-experts to work with DNNs. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate NNs. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop (HIL) One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics (VA) system aiming to solve three problems with One-Shot NAS; explainability, HIL design, and performance improvements compared to existing state-of-the-art (SOTA) techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other SOTA techniques. While adding Visual Analytics (VA) using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209361",
            "id": "r_296",
            "s_ids": [
                "s_265",
                "s_1395",
                "s_277"
            ],
            "type": "rich",
            "x": -9.265873908996582,
            "y": 18.787128448486328
        },
        {
            "title": "Photosensitive Accessibility for Interactive Data Visualizations",
            "data": "Accessibility guidelines place restrictions on the use of animations and interactivity on webpages to lessen the likelihood of webpages inadvertently producing sequences with flashes, patterns, or color changes that may trigger seizures for individuals with photosensitive epilepsy. Online data visualizations often incorporate elements of animation and interactivity to create a narrative, engage users, or encourage exploration. These design guidelines have been empirically validated by perceptual studies in visualization literature, but the impact of animation and interaction in visualizations on users with photosensitivity, who may experience seizures in response to certain visual stimuli, has not been considered. We systematically gathered and tested 1,132 interactive and animated visualizations for seizure-inducing risk using established methods and found that currently available methods for determining photosensitive risk are not reliable when evaluating interactive visualizations, as risk scores varied significantly based on the individual interacting with the visualization. To address this issue, we introduce a theoretical model defining the degree of control visualization designers have over three determinants of photosensitive risk in potentially seizure-inducing sequences: the size, frequency, and color of flashing content. Using an analysis of 375 visualizations hosted on bl.ocks.org, we created a theoretical model of photosensitive risk in visualizations by arranging the photosensitive risk determinants according to the degree of control visualization authors have over whether content exceeds photosensitive accessibility thresholds. We then use this model to propose a new method of testing for photosensitive risk that focuses on elements of visualizations that are subject to greater authorial control - and are therefore more robust to variations in the individual user - producing more reliable risk assessments than existing methods when applied to interactive visualizations. A full copy of this paper and all study materials are available at https://osf.io/8kzmg/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209359",
            "id": "r_297",
            "s_ids": [
                "s_705",
                "s_1163"
            ],
            "type": "rich",
            "x": -6.710111618041992,
            "y": 21.18351936340332
        },
        {
            "title": "Interactive Exploration of Physically-Observable Objective Vortices in Unsteady 2D Flow",
            "data": "State-of-the-art computation and visualization of vortices in unsteady fluid flow employ objective vortex criteria, which makes them independent of reference frames or observers. However, objectivity by itself, although crucial, is not sufficient to guarantee that one can identify physically-realizable observers that would perceive or detect the same vortices. Moreover, a significant challenge is that a single reference frame is often not sufficient to accurately observe multiple vortices that follow different motions. This paper presents a novel framework for the exploration and use of an interactively-chosen set of observers, of the resulting relative velocity fields, and of objective vortex structures. We show that our approach facilitates the objective detection and visualization of vortices relative to well-adapted reference frame motions, while at the same time guaranteeing that these observers are in fact physically realizable. In order to represent and manipulate observers efficiently, we make use of the low-dimensional vector space structure of the Lie algebra of physically-realizable observer motions. We illustrate that our framework facilitates the efficient choice and guided exploration of objective vortices in unsteady 2D flow, on planar as well as on spherical domains, using well-adapted reference frames.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3115565",
            "id": "r_298",
            "s_ids": [
                "s_1519",
                "s_388",
                "s_1496",
                "s_1003"
            ],
            "type": "rich",
            "x": -8.823071479797363,
            "y": 16.227825164794922
        },
        {
            "title": "Explanatory Journeys: Visualising to Understand and Explain Administrative Justice Paths of Redress",
            "data": "Administrative justice concerns the relationships between individuals and the state. It includes redress and complaints on decisions of a child's education, social care, licensing, planning, environment, housing and homelessness. However, if someone has a complaint or an issue, it is challenging for people to understand different possible redress paths and explore what path is suitable for their situation. Explanatory visualisation has the potential to display these paths of redress in a clear way, such that people can see, understand and explore their options. The visualisation challenge is further complicated because information is spread across many documents, laws, guidance and policies and requires judicial interpretation. Consequently, there is not a single database of paths of redress. In this work we present how we have co-designed a system to visualise administrative justice paths of redress. Simultaneously, we classify, collate and organise the underpinning data, from expert workshops, heuristic evaluation and expert critical reflection. We make four contributions: (i) an application design study of the explanatory visualisation tool (Artemus), (ii) coordinated and co-design approach to aggregating the data, (iii) two in-depth case studies in housing and education demonstrating explanatory paths of redress in administrative law, and (iv) reflections on the expert co-design process and expert data gathering and explanatory visualisation for administrative justice and law.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114818",
            "id": "r_299",
            "s_ids": [
                "s_1303",
                "s_49",
                "s_807",
                "s_998"
            ],
            "type": "rich",
            "x": -6.0250959396362305,
            "y": 17.330781936645508
        },
        {
            "title": "DIEL: Interactive Visualization Beyond the Here and Now",
            "data": "Interactive visualization design and research have primarily focused on local data and synchronous events. However, for more complex use cases-e.g., remote database access and streaming data sources-developers must grapple with distributed data and asynchronous events. Currently, constructing these use cases is difficult and time-consuming; developers are forced to operationally program low-level details like asynchronous database querying and reactive event handling. This approach is in stark contrast to modern methods for browser-based interactive visualization, which feature high-level declarative specifications. In response, we present DIEL, a declarative framework that supports asynchronous events over distributed data. As in many declarative languages, DIEL developers specify only what data they want, rather than procedural steps for how to assemble it. Uniquely, DIEL models asynchronous events (e.g., user interactions, server responses) as streams of data that are captured in event logs. To specify the state of a visualization at any time, developers write declarative queries over the data and event logs; DIEL compiles and optimizes a corresponding dataflow graph, and automatically generates necessary low-level distributed systems details. We demonstrate DIEL'S performance and expressivity through example interactive visualizations that make diverse use of remote data and asynchronous events. We further evaluate DIEL'S usability using the Cognitive Dimensions of Notations framework, revealing wins such as ease of change, and compromises such as premature commitments.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114796",
            "id": "r_300",
            "s_ids": [
                "s_1296",
                "s_366",
                "s_327",
                "s_920",
                "s_1157"
            ],
            "type": "rich",
            "x": -4.6706461906433105,
            "y": 15.991397857666016
        },
        {
            "title": "Knowledge Rocks: Adding Knowledge Assistance to Visualization Systems",
            "data": "We present Knowledge Rocks, an implementation strategy and guideline for augmenting visualization systems to knowledge-assisted visualization systems, as defined by the KAVA model. Visualization systems become more and more sophisticated. Hence, it is increasingly important to support users with an integrated knowledge base in making constructive choices and drawing the right conclusions. We support the effective reactivation of visualization software resources by augmenting them with knowledge-assistance. To provide a general and yet supportive implementation strategy, we propose an implementation process that bases on an application-agnostic architecture. This architecture is derived from existing knowledge-assisted visualization systems and the KAVA model. Its centerpiece is an ontology that is able to automatically analyze and classify input data, linked to a database to store classified instances. We discuss design decisions and advantages of the KR framework and illustrate its broad area of application in diverse integration possibilities of this architecture into an existing visualization system. In addition, we provide a detailed case study by augmenting an it-security system with knowledge-assistance facilities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114687",
            "id": "r_301",
            "s_ids": [
                "s_657",
                "s_1343",
                "s_40",
                "s_1068"
            ],
            "type": "rich",
            "x": -3.897082805633545,
            "y": 20.857725143432617
        },
        {
            "title": "Interactive Black-Hole Visualization",
            "data": "We present an efficient algorithm for visualizing the effect of black holes on its distant surroundings as seen from an observer nearby in orbit. Our solution is GPU-based and builds upon a two-step approach, where we first derive an adaptive grid to map the 360-view around the observer to the distorted celestial sky, which can be directly reused for different camera orientations. Using a grid, we can rapidly trace rays back to the observer through the distorted spacetime, avoiding the heavy workload of standard tracing solutions at real-time rates. By using a novel interpolation technique we can also simulate an observer path by smoothly transitioning between multiple grids. Our approach accepts real star catalogues and environment maps of the celestial sky and generates the resulting black-hole deformations in real time.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030452",
            "id": "r_302",
            "s_ids": [
                "s_1022",
                "s_167"
            ],
            "type": "rich",
            "x": -6.857983112335205,
            "y": 14.335586547851562
        },
        {
            "title": "Implicit Multidimensional Projection of Local Subspaces",
            "data": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030368",
            "id": "r_303",
            "s_ids": [
                "s_1145",
                "s_1257",
                "s_219",
                "s_1441",
                "s_1259",
                "s_157",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.326698303222656,
            "y": 19.072193145751953
        },
        {
            "title": "Guidelines For Pursuing and Revealing Data Abstractions",
            "data": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030355",
            "id": "r_304",
            "s_ids": [
                "s_1038",
                "s_1052",
                "s_174"
            ],
            "type": "rich",
            "x": -0.22428227961063385,
            "y": 17.114627838134766
        },
        {
            "title": "Advanced Rendering of Line Data with Ambient Occlusion and Transparency",
            "data": "3D Lines are a widespread rendering primitive for the visualization of data from research fields like fluid dynamics or fiber tractography. Global illumination effects and transparent rendering improve the perception of three-dimensional features and decrease occlusion within the data set, thus enabling better understanding of complex line data. We present an efficient approach for high quality GPU-based rendering of line data with ambient occlusion and transparency effects. Our approach builds on GPU-based raycasting of rounded cones, which are geometric primitives similar to truncated cones, but with spherical endcaps. Object space ambient occlusion is provided by an efficient voxel cone tracing approach. Our core contribution is a new fragment visibility sorting strategy that allows for interactive visualization of line data sets with millions of line segments. We improve performance further by exploiting hierarchical opacity maps.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028954",
            "id": "r_305",
            "s_ids": [
                "s_681",
                "s_1039"
            ],
            "type": "rich",
            "x": -5.794628620147705,
            "y": 18.50850486755371
        },
        {
            "title": "2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality",
            "data": "Relational information between different types of entities is often modelled by a multilayer network (MLN) \u2013 a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327402",
            "id": "r_306",
            "s_ids": [
                "s_301",
                "s_63",
                "s_1459",
                "s_475",
                "s_1037",
                "s_603",
                "s_632",
                "s_380",
                "s_870"
            ],
            "type": "rich",
            "x": -4.000612258911133,
            "y": 19.914953231811523
        },
        {
            "title": "Differentiable Design Galleries: A Differentiable Approach to Explore the Design Space of Transfer Functions",
            "data": "The transfer function is crucial for direct volume rendering (DVR) to create an informative visual representation of volumetric data. However, manually adjusting the transfer function to achieve the desired DVR result can be time-consuming and unintuitive. In this paper, we propose Differentiable Design Galleries, an image-based transfer function design approach to help users explore the design space of transfer functions by taking advantage of the recent advances in deep learning and differentiable rendering. Specifically, we leverage neural rendering to learn a latent design space, which is a continuous manifold representing various types of implicit transfer functions. We further provide a set of interactive tools to support intuitive query, navigation, and modification to obtain the target design, which is represented as a neural-rendered design exemplar. The explicit transfer function can be reconstructed from the target design with a differentiable direct volume renderer. Experimental results on real volumetric data demonstrate the effectiveness of our method.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327371",
            "id": "r_307",
            "s_ids": [
                "s_1452",
                "s_925",
                "s_373",
                "s_876",
                "s_1597",
                "s_1072",
                "s_857",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.449146270751953,
            "y": 18.038524627685547
        },
        {
            "title": "Swaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms",
            "data": "We conducted a longitudinal study during the 2022 U.S. midterm elections, investigating the real-world impacts of uncertainty visualizations. Using our forecast model of the governor elections in 33 states, we created a website and deployed four uncertainty visualizations for the election forecasts: single quantile dotplot (1-Dotplot), dual quantile dotplots (2-Dotplot), dual histogram intervals (2-Interval), and Plinko quantile dotplot (Plinko), an animated design with a physical and probabilistic analogy. Our online experiment ran from Oct. 18, 2022, to Nov. 23, 2022, involving 1,327 participants from 15 states. We use Bayesian multilevel modeling and post-stratification to produce demographically-representative estimates of people's emotions, trust in forecasts, and political participation intention. We find that election forecast visualizations can heighten emotions, increase trust, and slightly affect people's intentions to participate in elections. 2-Interval shows the strongest effects across all measures; 1-Dotplot increases trust the most after elections. Both visualizations create emotional and trust gaps between different partisan identities, especially when a Republican candidate is predicted to win. Our qualitative analysis uncovers the complex political and social contexts of election forecast visualizations, showcasing that visualizations may provoke polarization. This intriguing interplay between visualization types, partisanship, and trust exemplifies the fundamental challenge of disentangling visualization from its context, underscoring a need for deeper investigation into the real-world impacts of visualizations. Our preprint and supplements are available at https://doi.org/osf.io/ajq8f.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327356",
            "id": "r_308",
            "s_ids": [
                "s_607",
                "s_413",
                "s_193",
                "s_1494",
                "s_1191",
                "s_1411",
                "s_942",
                "s_1383",
                "s_131",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.235961675643921,
            "y": 16.657039642333984
        },
        {
            "title": "Residency Octree: A Hybrid Approach for Scalable Web-Based Multi-Volume Rendering",
            "data": "We present a hybrid multi-volume rendering approach based on a novel Residency Octree that combines the advantages of out-of-core volume rendering using page tables with those of standard octrees. Octree approaches work by performing hierarchical tree traversal. However, in octree volume rendering, tree traversal and the selection of data resolution are intrinsically coupled. This makes fine-grained empty-space skipping costly. Page tables, on the other hand, allow access to any cached brick from any resolution. However, they do not offer a clear and efficient strategy for substituting missing high-resolution data with lower-resolution data. We enable flexible mixed-resolution out-of-core multi-volume rendering by decoupling the cache residency of multi-resolution data from a resolution-independent spatial subdivision determined by the tree. Instead of one-to-one node-to-brick correspondences, each residency octree node is mapped to a set of bricks from different resolution levels. This makes it possible to efficiently and adaptively choose and mix resolutions, adapt sampling rates, and compensate for cache misses. At the same time, residency octrees support fine-grained empty-space skipping, independent of the data subdivision used for caching. Finally, to facilitate collaboration and outreach, and to eliminate local data storage, our implementation is a web-based, pure client-side renderer using WebGPU and WebAssembly. Our method is faster than prior approaches and efficient for many data channels with a flexible and adaptive choice of data resolution.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327193",
            "id": "r_309",
            "s_ids": [
                "s_808",
                "s_388",
                "s_1186",
                "s_507",
                "s_1486",
                "s_1033",
                "s_689"
            ],
            "type": "rich",
            "x": -8.737139701843262,
            "y": 16.43450355529785
        },
        {
            "title": "Mosaic: An Architecture for Scalable & Interoperable Data Views",
            "data": "Mosaic is an architecture for greater scalability, extensibility, and interoperability of interactive data views. Mosaic decouples data processing from specification logic: clients publish their data needs as declarative queries that are then managed and automatically optimized by a coordinator that proxies access to a scalable data store. Mosaic generalizes Vegalite's selection abstraction to enable rich integration and linking across visualizations and components such as menus, text search, and tables. We demonstrate Mosaic's expressiveness, extensibility, and interoperability through examples that compose diverse visualization, interaction, and optimization techniques\u2014many constructed using vgplot, a grammar of interactive graphics in which graphical marks act as Mosaic clients. To evaluate scalability, we present benchmark studies with order-of-magnitude performance improvements over existing web-based visualization systems\u2014enabling flexible, real-time visual exploration of billion+ record datasets. We conclude by discussing Mosaic's potential as an open platform that bridges visualization languages, scalable visualization, and interactive data systems more broadly.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327189",
            "id": "r_310",
            "s_ids": [
                "s_1386",
                "s_646"
            ],
            "type": "rich",
            "x": -3.8175151348114014,
            "y": 17.347108840942383
        },
        {
            "title": "VISGRADER: Automatic Grading of D3 Visualizations",
            "data": "Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VISGRADER, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students' learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327181",
            "id": "r_311",
            "s_ids": [
                "s_1570",
                "s_458",
                "s_463",
                "s_1136",
                "s_1419",
                "s_447",
                "s_905",
                "s_448",
                "s_1297",
                "s_661",
                "s_1078",
                "s_341"
            ],
            "type": "rich",
            "x": 10.281496047973633,
            "y": 10.265835762023926
        },
        {
            "title": "Heuristics for Supporting Cooperative Dashboard Design",
            "data": "Dashboards are no longer mere static displays of metrics; through functionality such as interaction and storytelling, they have evolved to support analytic and communicative goals like monitoring and reporting. Existing dashboard design guidelines, however, are often unable to account for this expanded scope as they largely focus on best practices for visual design. In contrast, we frame dashboard design as facilitating an analytical conversation: a cooperative, interactive experience where a user may interact with, reason about, or freely query the underlying data. By drawing on established principles of conversational flow and communication, we define the concept of a cooperative dashboard as one that enables a fruitful and productive analytical conversation, and derive a set of 39 dashboard design heuristics to support effective analytical conversations. To assess the utility of this framing, we asked 52 computer science and engineering graduate students to apply our heuristics to critique and design dashboards as part of an ungraded, opt-in homework assignment. Feedback from participants demonstrates that our heuristics surface new reasons dashboards may fail, and encourage a more fluid, supportive, and responsive style of dashboard design. Our approach suggests several compelling directions for future work, including dashboard authoring tools that better anticipate conversational turn-taking, repair, and refinement and extending cooperative principles to other analytical workflows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327158",
            "id": "r_312",
            "s_ids": [
                "s_536",
                "s_1340",
                "s_920",
                "s_1331"
            ],
            "type": "rich",
            "x": -4.224549293518066,
            "y": 16.150938034057617
        },
        {
            "title": "A Heuristic Approach for Dual Expert/End-User Evaluation of Guidance in Visual Analytics",
            "data": "Guidance can support users during the exploration and analysis of complex data. Previous research focused on characterizing the theoretical aspects of guidance in visual analytics and implementing guidance in different scenarios. However, the evaluation of guidance-enhanced visual analytics solutions remains an open research question. We tackle this question by introducing and validating a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and collect expert feedback on their validity. To facilitate actual evaluation studies, we derive two sets of heuristics. The first set targets heuristic evaluations conducted by expert evaluators. The second set facilitates end-user studies where participants actually use a guidance-enhanced system. By following such a dual approach, the different quality criteria of guidance can be examined from two different perspectives, enhancing the overall value of evaluation studies. To test the practical utility of our methodology, we employ it in two studies to gain insight into the quality of two guidance-enhanced visual analytics solutions, one being a work-in-progress research prototype, and the other being a publicly available visualization recommender system. Based on these two evaluations, we derive good practices for conducting evaluations of guidance in visual analytics and identify pitfalls to be avoided during such studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327152",
            "id": "r_313",
            "s_ids": [
                "s_898",
                "s_1609",
                "s_891",
                "s_1195",
                "s_424",
                "s_653"
            ],
            "type": "rich",
            "x": -5.254399299621582,
            "y": 15.037402153015137
        },
        {
            "title": "Reducing Ambiguities in Line-Based Density Plots by Image-Space Colorization",
            "data": "Line-based density plots are used to reduce visual clutter in line charts with a multitude of individual lines. However, these traditional density plots are often perceived ambiguously, which obstructs the user's identification of underlying trends in complex datasets. Thus, we propose a novel image space coloring method for line-based density plots that enhances their interpretability. Our method employs color not only to visually communicate data density but also to highlight similar regions in the plot, allowing users to identify and distinguish trends easily. We achieve this by performing hierarchical clustering based on the lines passing through each region and mapping the identified clusters to the hue circle using circular MDS. Additionally, we propose a heuristic approach to assign each line to the most probable cluster, enabling users to analyze density and individual lines. We motivate our method by conducting a small-scale user study, demonstrating the effectiveness of our method using synthetic and real-world datasets, and providing an interactive online tool for generating colored line-based density plots.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327149",
            "id": "r_314",
            "s_ids": [
                "s_1257",
                "s_1109",
                "s_1522",
                "s_314",
                "s_161",
                "s_1146",
                "s_1345"
            ],
            "type": "rich",
            "x": -8.045707702636719,
            "y": 18.632585525512695
        },
        {
            "title": "From Information to Choice: A Critical Inquiry Into Visualization Tools for Decision Making",
            "data": "In the face of complex decisions, people often engage in a three-stage process that spans from (1) exploring and analyzing pertinent information (intelligence); (2) generating and exploring alternative options (design); and ultimately culminating in (3) selecting the optimal decision by evaluating discerning criteria (choice). We can fairly assume that all good visualizations aid in the \u201cintelligence\u201d stage by enabling data exploration and analysis. Yet, to what degree and how do visualization systems currently support the other decision making stages, namely \u201cdesign\u201d and \u201cchoice\u201d? To further explore this question, we conducted a comprehensive review of decision-focused visualization tools by examining publications in major visualization journals and conferences, including VIS, EuroVis, and CHI, spanning all available years. We employed a deductive coding method and in-depth analysis to assess whether and how visualization tools support design and choice. Specifically, we examined each visualization tool by (i) its degree of visibility for displaying decision alternatives, criteria, and preferences, and (ii) its degree of flexibility for offering means to manipulate the decision alternatives, criteria, and preferences with interactions such as adding, modifying, changing mapping, and filtering. Our review highlights the opportunities and challenges that decision-focused visualization tools face in realizing their full potential to support all stages of the decision making process. It reveals a surprising scarcity of tools that support all stages, and while most tools excel in offering visibility for decision criteria and alternatives, the degree of flexibility to manipulate these elements is often limited, and the lack of tools that accommodate decision preferences and their elicitation is notable. Based on our findings, to better support the choice stage, future research could explore enhancing flexibility levels and variety, exploring novel visualization paradigms, increasing algorithmic support, and ensuring that this automation is user-controlled via the enhanced flexibility I evels. Our curated list of the 88 surveyed visualization tools is available in the OSF link (https://osf.io/nrasz/?view_only=b92a90a34ae241449b5f2cd33383bfcb).",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326593",
            "id": "r_315",
            "s_ids": [
                "s_1410",
                "s_207",
                "s_563",
                "s_1578",
                "s_203"
            ],
            "type": "rich",
            "x": -4.408379554748535,
            "y": 15.207061767578125
        },
        {
            "title": "Wizualization: A \u201cHard Magic\u201d Visualization System for Immersive and Ubiquitous Analytics",
            "data": "What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326580",
            "id": "r_316",
            "s_ids": [
                "s_775",
                "s_49",
                "s_988",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.0037267208099365,
            "y": 16.034936904907227
        },
        {
            "title": "Vistrust: a Multidimensional Framework and Empirical Study of Trust in Data Visualizations",
            "data": "Trust is an essential aspect of data visualization, as it plays a crucial role in the interpretation and decision-making processes of users. While research in social sciences outlines the multi-dimensional factors that can play a role in trust formation, most data visualization trust researchers employ a single-item scale to measure trust. We address this gap by proposing a comprehensive, multidimensional conceptualization and operationalization of trust in visualization. We do this by applying general theories of trust from social sciences, as well as synthesizing and extending earlier work and factors identified by studies in the visualization field. We apply a two-dimensional approach to trust in visualization, to distinguish between cognitive and affective elements, as well as between visualization and data-specific trust antecedents. We use our framework to design and run a large crowd-sourced study to quantify the role of visual complexity in establishing trust in science visualizations. Our study provides empirical evidence for several aspects of our proposed theoretical framework, most notably the impact of cognition, affective responses, and individual differences when establishing trust in visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326579",
            "id": "r_317",
            "s_ids": [
                "s_1548",
                "s_1040",
                "s_689",
                "s_1444",
                "s_1486",
                "s_1184",
                "s_1238"
            ],
            "type": "rich",
            "x": -8.62548542022705,
            "y": 16.519954681396484
        },
        {
            "title": "Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization",
            "data": "Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that are combinations of topic models and dimensionality reductions, and (3) quality metrics for quantifying the resulting layout. The corpora are given as document-term matrices, and each document is assigned to a thematic class. The chosen metrics quantify the preservation of local and global properties and the perceptual effectiveness of the two-dimensional scatter plots. By evaluating the benchmark on a computing cluster, we derived a multivariate dataset with over 45 000 individual layouts and corresponding quality metrics. Based on the results, we propose guidelines for the effective design of text spatializations that are based on topic models and dimensionality reductions. As a main result, we show that interpretable topic models are beneficial for capturing the structure of text corpora. We furthermore recommend the use of t-SNE as a subsequent dimensionality reduction.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326569",
            "id": "r_318",
            "s_ids": [
                "s_921",
                "s_1564",
                "s_1529",
                "s_486",
                "s_530",
                "s_1376",
                "s_578"
            ],
            "type": "rich",
            "x": -8.88625717163086,
            "y": 18.442312240600586
        },
        {
            "title": "EVM: Incorporating Model Checking into Exploratory Visual Analysis",
            "data": "Visual analytics (VA) tools support data exploration by helping analysts quickly and iteratively generate views of data which reveal interesting patterns. However, these tools seldom enable explicit checks of the resulting interpretations of data\u2014e.g., whether patterns can be accounted for by a model that implies a particular structure in the relationships between variables. We present EVM, a data exploration tool that enables users to express and check provisional interpretations of data in the form of statistical models. EVM integrates support for visualization-based model checks by rendering distributions of model predictions alongside user-generated views of data. In a user study with data scientists practicing in the private and public sector, we evaluate how model checks influence analysts' thinking during data exploration. Our analysis characterizes how participants use model checks to scrutinize expectations about data generating process and surfaces further opportunities to scaffold model exploration in VA tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326516",
            "id": "r_319",
            "s_ids": [
                "s_875",
                "s_270",
                "s_664",
                "s_1386",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.4817261695861816,
            "y": 16.938175201416016
        },
        {
            "title": "Polarizing Political Polls: How Visualization Design Choices Can Shape Public Opinion and Increase Political Polarization",
            "data": "While we typically focus on data visualization as a tool for facilitating cognitive tasks (e.g. learning facts, making decisions), we know relatively little about their second-order impacts on our opinions, attitudes, and values. For example, could design or framing choices interact with viewers' social cognitive biases in ways that promote political polarization? When reporting on U.S. attitudes toward public policies, it is popular to highlight the gap between Democrats and Republicans (e.g. with blue vs red connected dot plots). But these charts may encourage social-normative conformity, influencing viewers' attitudes to match the divided opinions shown in the visualization. We conducted three experiments examining visualization framing in the context of social conformity and polarization. Crowdworkers viewed charts showing simulated polling results for public policy proposals. We varied framing (aggregating data as non-partisan \u201cAll US Adults,\u201d or partisan \u201cDemocrat\u201d / \u201cRepublican\u201d) and the visualized groups' support levels. Participants then reported their own support for each policy. We found that participants' attitudes biased significantly toward the group attitudes shown in the stimuli and this can increase inter-party attitude divergence. These results demonstrate that data visualizations can induce social conformity and accelerate political polarization. Choosing to visualize partisan divisions can divide us further.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326512",
            "id": "r_320",
            "s_ids": [
                "s_1222",
                "s_1184"
            ],
            "type": "rich",
            "x": -3.989363670349121,
            "y": 16.641162872314453
        },
        {
            "title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants",
            "data": "Improving the efficiency of coal-fired power plants has numerous benefits. The control strategy is one of the major factors affecting such efficiency. However, due to the complex and dynamic environment inside the power plants, it is hard to extract and evaluate control strategies and their cascading impact across massive sensors. Existing manual and data-driven approaches cannot well support the analysis of control strategies because these approaches are time-consuming and do not scale with the complexity of the power plant systems. Three challenges were identified: a) interactive extraction of control strategies from large-scale dynamic sensor data, b) intuitive visual representation of cascading impact among the sensors in a complex power plant system, and c) time-lag-aware analysis of the impact of control strategies on electricity generation efficiency. By collaborating with energy domain experts, we addressed these challenges with ECoalVis, a novel interactive system for experts to visually analyze the control strategies of coal-fired power plants extracted from historical sensor data. The effectiveness of the proposed system is evaluated with two usage scenarios on a real-world historical dataset and received positive feedback from experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209430",
            "id": "r_321",
            "s_ids": [
                "s_311",
                "s_735",
                "s_1536",
                "s_1308",
                "s_1445",
                "s_1413",
                "s_1432",
                "s_268",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.339364051818848,
            "y": 17.141740798950195
        },
        {
            "title": "Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models",
            "data": "Visualization and analysis of multivariate data and their uncertainty are top research challenges in data visualization. Constructing fiber surfaces is a popular technique for multivariate data visualization that generalizes the idea of level-set visualization for univariate data to multivariate data. In this paper, we present a statistical framework to quantify positional probabilities of fibers extracted from uncertain bivariate fields. Specifically, we extend the state-of-the-art Gaussian models of uncertainty for bivariate data to other parametric distributions (e.g., uniform and Epanechnikov) and more general nonparametric probability distributions (e.g., histograms and kernel density estimation) and derive corresponding spatial probabilities of fibers. In our proposed framework, we leverage Green's theorem for closed-form computation of fiber probabilities when bivariate data are assumed to have independent parametric and nonparametric noise. Additionally, we present a nonparametric approach combined with numerical integration to study the positional probability of fibers when bivariate data are assumed to have correlated noise. For uncertainty analysis, we visualize the derived probability volumes for fibers via volume rendering and extracting level sets based on probability thresholds. We present the utility of our proposed techniques via experiments on synthetic and simulation datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209424",
            "id": "r_322",
            "s_ids": [
                "s_1617",
                "s_571",
                "s_502",
                "s_1399"
            ],
            "type": "rich",
            "x": -1.839240312576294,
            "y": 17.934268951416016
        },
        {
            "title": "Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids",
            "data": "We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6\u00d7 (1 frame per second (fps) and 1.0 M rays per second (rps) up to now 6 fps and 12.4 M rps, now including volumetric shadows) while simultaneously reducing memory consumption by 3\u00d7(33 GB down to 11 GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48 GB of an RTX 8000, we improve the performance of Lander by 17 \u00d7 (1 fps up to 17 fps, 1.0 M rps up to 35.6 M rps).",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209418",
            "id": "r_323",
            "s_ids": [
                "s_121",
                "s_20",
                "s_295",
                "s_1566",
                "s_343"
            ],
            "type": "rich",
            "x": -15.048036575317383,
            "y": 7.126321315765381
        },
        {
            "title": "The State of the Art in BGP Visualization Tools: A Mapping of Visualization Techniques to Cyberattack Types",
            "data": "Internet routing is largely dependent on Border Gateway Protocol (BGP). However, BGP does not have any inherent authentication or integrity mechanisms that help make it secure. Effective security is challenging or infeasible to implement due to high costs, policy employment in these distributed systems, and unique routing behavior. Visualization tools provide an attractive alternative in lieu of traditional security approaches. Several BGP security visualization tools have been developed as a stop-gap in the face of ever-present BGP attacks. Even though the target users, tasks, and domain remain largely consistent across such tools, many diverse visualization designs have been proposed. The purpose of this study is to provide an initial formalization of methods and visualization techniques for BGP cybersecurity analysis. Using PRISMA guidelines, we provide a systematic review and survey of 29 BGP visualization tools with their tasks, implementation techniques, and attacks and anomalies that they were intended for. We focused on BGP visualization tools as the main inclusion criteria to best capture the visualization techniques used in this domain while excluding solely algorithmic solutions and other detection tools that do not involve user interaction or interpretation. We take the unique approach of connecting (1) the actual BGP attacks and anomalies used to validate existing tools with (2) the techniques employed to detect them. In this way, we contribute an analysis of which techniques can be used for each attack type. Furthermore, we can see the evolution of visualization solutions in this domain as new attack types are discovered. This systematic review provides the groundwork for future designers and researchers building visualization tools for providing BGP cybersecurity, including an understanding of the state-of-the-art in this space and an analysis of what techniques are appropriate for each attack type. Our novel security visualization survey methodology\u2014connecting visualization techniques with appropriate attack types\u2014may also assist future researchers conducting systematic reviews of security visualizations. All supplemental materials are available at https://osf.io/tupz6/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209412",
            "id": "r_324",
            "s_ids": [
                "s_390",
                "s_1077",
                "s_286",
                "s_705",
                "s_1187",
                "s_1006"
            ],
            "type": "rich",
            "x": -6.767878532409668,
            "y": 21.127946853637695
        },
        {
            "title": "Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis",
            "data": "Reference-based cell-type annotation can significantly reduce time and effort in single-cell analysis by transferring labels from a previously-annotated dataset to a new dataset. However, label transfer by end-to-end computational methods is challenging due to the entanglement of technical (e.g., from different sequencing batches or techniques) and biological (e.g., from different cellular microenvironments) variations, only the first of which must be removed. To address this issue, we propose Polyphony, an interactive transfer learning (ITL) framework, to complement biologists' knowledge with advanced computational methods. Polyphony is motivated and guided by domain experts' needs for a controllable, interactive, and algorithm-assisted annotation process, identified through interviews with seven biologists. We introduce anchors, i.e., analogous cell populations across datasets, as a paradigm to explain the computational process and collect user feedback for model improvement. We further design a set of visualizations and interactions to empower users to add, delete, or modify anchors, resulting in refined cell type annotations. The effectiveness of this approach is demonstrated through quantitative experiments, two hypothetical use cases, and interviews with two biologists. The results show that our anchor-based ITL method takes advantage of both human and machine intelligence in annotating massive single-cell datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209408",
            "id": "r_325",
            "s_ids": [
                "s_897",
                "s_1414",
                "s_156",
                "s_393",
                "s_470"
            ],
            "type": "rich",
            "x": -10.052323341369629,
            "y": 18.349506378173828
        },
        {
            "title": "PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration",
            "data": "We present PuzzleFixer, an immersive interactive system for experts to rectify defective reassembled 3D objects. Reassembling the fragments of a broken object to restore its original state is the prerequisite of many analytical tasks such as cultural relics analysis and forensics reasoning. While existing computer-aided methods can automatically reassemble fragments, they often derive incorrect objects due to the complex and ambiguous fragment shapes. Thus, experts usually need to refine the object manually. Prior advances in immersive technologies provide benefits for realistic perception and direct interactions to visualize and interact with 3D fragments. However, few studies have investigated the reassembled object refinement. The specific challenges include: 1) the fragment combination set is too large to determine the correct matches, and 2) the geometry of the fragments is too complex to align them properly. To tackle the first challenge, PuzzleFixer leverages dimensionality reduction and clustering techniques, allowing users to review possible match categories, select the matches with reasonable shapes, and drill down to shapes to correct the corresponding faces. For the second challenge, PuzzleFixer embeds the object with node-link networks to augment the perception of match relations. Specifically, it instantly visualizes matches with graph edges and provides force feedback to facilitate the efficiency of alignment interactions. To demonstrate the effectiveness of PuzzleFixer, we conducted an expert evaluation based on two cases on real-world artifacts and collected feedback through post-study interviews. The results suggest that our system is suitable and efficient for experts to refine incorrect reassembled objects.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209388",
            "id": "r_326",
            "s_ids": [
                "s_626",
                "s_1589",
                "s_152",
                "s_1454",
                "s_611",
                "s_120",
                "s_325",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.30123519897461,
            "y": 17.28021812438965
        },
        {
            "title": "Taurus: Towards a Unified Force Representation and Universal Solver for Graph Layout",
            "data": "Over the past few decades, a large number of graph layout techniques have been proposed for visualizing graphs from various domains. In this paper, we present a general framework, Taurus, for unifying popular techniques such as the spring-electrical model, stress model, and maxent-stress model. It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. In addition, we introduce a universal augmented stochastic gradient descent (SGD) optimizer that efficiently finds proper solutions for all layout techniques. To demonstrate the power of our framework, we conduct a comprehensive evaluation of existing techniques on a large number of synthetic and real graphs. We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209371",
            "id": "r_327",
            "s_ids": [
                "s_195",
                "s_357",
                "s_509",
                "s_1210",
                "s_93",
                "s_1345",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.163533210754395,
            "y": 18.626859664916992
        },
        {
            "title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
            "data": "Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer, which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209351",
            "id": "r_328",
            "s_ids": [
                "s_1567",
                "s_1287",
                "s_200",
                "s_1600",
                "s_549"
            ],
            "type": "rich",
            "x": -11.021090507507324,
            "y": 19.295286178588867
        },
        {
            "title": "Towards replacing physical testing of granular materials with a Topology-based Model",
            "data": "In the study of packed granular materials, the performance of a sample (e.g., the detonation of a high-energy explosive) often correlates to measurements of a fluid flowing through it. The \u201ceffective surface area,\u201d the surface area accessible to the airflow, is typically measured using a permeametry apparatus that relates the flow conductance to the permeable surface area via the Carman-Kozeny equation. This equation allows calculating the flow rate of a fluid flowing through the granules packed in the sample for a given pressure drop. However, Carman-Kozeny makes inherent assumptions about tunnel shapes and flow paths that may not accurately hold in situations where the particles possess a wide distribution in shapes, sizes, and aspect ratios, as is true with many powdered systems of technological and commercial interest. To address this challenge, we replicate these measurements virtually on micro-CT images of the powdered material, introducing a new Pore Network Model based on the skeleton of the Morse-Smale complex. Pores are identified as basins of the complex, their incidence encodes adjacency, and the conductivity of the capillary between them is computed from the cross-section at their interface. We build and solve a resistive network to compute an approximate laminar fluid flow through the pore structure. We provide two means of estimating flow-permeable surface area: (i) by direct computation of conductivity, and (ii) by identifying dead-ends in the flow coupled with isosurface extraction and the application of the Carman-Kozeny equation, with the aim of establishing consistency over a range of particle shapes, sizes, porosity levels, and void distribution patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114819",
            "id": "r_329",
            "s_ids": [
                "s_1064",
                "s_1602",
                "s_994",
                "s_703",
                "s_575",
                "s_706",
                "s_506",
                "s_343"
            ],
            "type": "rich",
            "x": -15.041532516479492,
            "y": 7.132795333862305
        },
        {
            "title": "Rotate or Wrap? Interactive Visualisations of Cyclical Data on Cylindrical or Toroidal Topologies",
            "data": "In this paper, we report on a study of visual representations for cyclical data and the effect of interactively <i>wrapping</i> a bar chart \u2018around its boundaries\u2019. Compared to linear bar chart, polar (or radial) visualisations have the advantage that cyclical data can be presented continuously without mentally bridging the visual \u2018cut\u2019 across the left-and-right boundaries. To investigate this hypothesis and to assess the effect the cut has on analysis performance, this paper presents results from a crowdsourced, controlled experiment with 72 participants comparing new continuous panning technique to linear bar charts (<i>interactive wrapping</i>). Our results show that bar charts with interactive wrapping lead to less errors compared to standard bar charts or polar charts. Inspired by these results, we generalise the concept of interactive wrapping to other visualisations for cyclical or relational data. We describe a design space based on the concept of one-dimensional wrapping and two-dimensional wrapping, linked to two common 3D topologies; cylinder and torus that can be used to metaphorically explain one- and two-dimensional wrapping. This design space suggests that interactive wrapping is widely applicable to many different data types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114693",
            "id": "r_330",
            "s_ids": [
                "s_918",
                "s_210",
                "s_1221",
                "s_1305"
            ],
            "type": "rich",
            "x": -7.2184977531433105,
            "y": 17.50298309326172
        },
        {
            "title": "Multi-Perspective, Simultaneous Embedding",
            "data": "We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030373",
            "id": "r_331",
            "s_ids": [
                "s_186",
                "s_334",
                "s_1459",
                "s_1336"
            ],
            "type": "rich",
            "x": -3.9937338829040527,
            "y": 19.72168731689453
        },
        {
            "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
            "data": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3028958",
            "id": "r_332",
            "s_ids": [
                "s_202",
                "s_48",
                "s_441",
                "s_1029",
                "s_815"
            ],
            "type": "rich",
            "x": -4.271306991577148,
            "y": 21.08753776550293
        },
        {
            "title": "Interactive Design-of-Experiments: Optimizing a Cooling System",
            "data": "The optimization of cooling systems is important in many cases, for example for cabin and battery cooling in electric cars. Such an optimization is governed by multiple, conflicting objectives and it is performed across a multi-dimensional parameter space. The extent of the parameter space, the complexity of the non-linear model of the system, as well as the time needed per simulation run and factors that are not modeled in the simulation necessitate an iterative, semi-automatic approach. We present an interactive visual optimization approach, where the user works with a p-h diagram to steer an iterative, guided optimization process. A deep learning (DL) model provides estimates for parameters, given a target characterization of the system, while numerical simulation is used to compute system characteristics for an ensemble of parameter sets. Since the DL model only serves as an approximation of the inverse of the cooling system and since target characteristics can be chosen according to different, competing objectives, an iterative optimization process is realized, developing multiple sets of intermediate solutions, which are visually related to each other. The standard p-h diagram, integrated interactively in this approach, is complemented by a dual, also interactive visual representation of additional expressive measures representing the system characteristics. We show how the known four-points semantic of the p-h diagram meaningfully transfers to the dual data representation. When evaluating this approach in the automotive domain, we found that our solution helped with the overall comprehension of the cooling system and that it lead to a faster convergence during optimization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456356",
            "id": "r_333",
            "s_ids": [
                "s_796",
                "s_263",
                "s_1379",
                "s_1062",
                "s_418",
                "s_946"
            ],
            "type": "rich",
            "x": -6.503917217254639,
            "y": 13.798665046691895
        },
        {
            "title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations",
            "data": "Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327389",
            "id": "r_334",
            "s_ids": [
                "s_1485",
                "s_1559",
                "s_1193",
                "s_1438"
            ],
            "type": "rich",
            "x": -9.730464935302734,
            "y": 17.88077735900879
        },
        {
            "title": "ViMO - Visual Analysis of Neuronal Connectivity Motifs",
            "data": "Recent advances in high-resolution connectomics provide researchers with access to accurate petascale reconstructions of neuronal circuits and brain networks for the first time. Neuroscientists are analyzing these networks to better understand information processing in the brain. In particular, scientists are interested in identifying specific small network motifs, i.e., repeating subgraphs of the larger brain network that are believed to be neuronal building blocks. Although such motifs are typically small (e.g., 2\u20136 neurons), the vast data sizes and intricate data complexity present significant challenges to the search and analysis process. To analyze these motifs, it is crucial to review instances of a motif in the brain network and then map the graph structure to detailed 3D reconstructions of the involved neurons and synapses. We present Vimo, an interactive visual approach to analyze neuronal motifs and motif chains in large brain networks. Experts can sketch network motifs intuitively in a visual interface and specify structural properties of the involved neurons and synapses to query large connectomics datasets. Motif instances (MIs) can be explored in high-resolution 3D renderings. To simplify the analysis of MIs, we designed a continuous focus&context metaphor inspired by visual abstractions. This allows users to transition from a highly-detailed rendering of the anatomical structure to views that emphasize the underlying motif structure and synaptic connectivity. Furthermore, Vimo supports the identification of motif chains where a motif is used repeatedly (e.g., 2\u20134 times) to form a larger network structure. We evaluate Vimo in a user study and an in-depth case study with seven domain experts on motifs in a large connectome of the fruit fly, including more than 21,000 neurons and 20 million synapses. We find that Vimo enables hypothesis generation and confirmation through fast analysis iterations and connectivity highlighting.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327388",
            "id": "r_335",
            "s_ids": [
                "s_977",
                "s_501",
                "s_697",
                "s_859",
                "s_1228",
                "s_903",
                "s_296",
                "s_1476",
                "s_1129",
                "s_1486",
                "s_689"
            ],
            "type": "rich",
            "x": -8.715554237365723,
            "y": 16.494728088378906
        },
        {
            "title": "Handling Non-Visible Referents in Situated Visualizations",
            "data": "Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user's visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327361",
            "id": "r_336",
            "s_ids": [
                "s_1151",
                "s_269",
                "s_30",
                "s_245"
            ],
            "type": "rich",
            "x": -7.352967739105225,
            "y": 17.851285934448242
        },
        {
            "title": "GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation",
            "data": "Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selections, utilizes template-based text generation to summarize model outputs, and links with external knowledge repositories to provide annotations that help to explain the model results. As analysts explore the model results, all visualizations and annotations can be captured in an interactive report generation widget. We demonstrate our framework using a case study modeling the determinants of voting in the 2016 US Presidential Election.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327359",
            "id": "r_337",
            "s_ids": [
                "s_490",
                "s_48",
                "s_907",
                "s_1271",
                "s_531",
                "s_1230",
                "s_761",
                "s_815"
            ],
            "type": "rich",
            "x": -4.255157947540283,
            "y": 21.081512451171875
        },
        {
            "title": "Mystique: Deconstructing SVG Charts for Layout Reuse",
            "data": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327354",
            "id": "r_338",
            "s_ids": [
                "s_908",
                "s_274",
                "s_1146",
                "s_1276",
                "s_1185"
            ],
            "type": "rich",
            "x": -7.946536540985107,
            "y": 18.41827392578125
        },
        {
            "title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning",
            "data": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327200",
            "id": "r_339",
            "s_ids": [
                "s_184",
                "s_752",
                "s_365",
                "s_721",
                "s_25",
                "s_1088",
                "s_1315",
                "s_989"
            ],
            "type": "rich",
            "x": -9.292967796325684,
            "y": 19.804941177368164
        },
        {
            "title": "A Computational Design Pipeline to Fabricate Sensing Network Physicalizations",
            "data": "Interaction is critical for data analysis and sensemaking. However, designing interactive physicalizations is challenging as it requires cross-disciplinary knowledge in visualization, fabrication, and electronics. Interactive physicalizations are typically produced in an unstructured manner, resulting in unique solutions for a specific dataset, problem, or interaction that cannot be easily extended or adapted to new scenarios or future physicalizations. To mitigate these challenges, we introduce a computational design pipeline to 3D print network physicalizations with integrated sensing capabilities. Networks are ubiquitous, yet their complex geometry also requires significant engineering considerations to provide intuitive, effective interactions for exploration. Using our pipeline, designers can readily produce network physicalizations supporting selection\u2014the most critical atomic operation for interaction\u2014by touch through capacitive sensing and computational inference. Our computational design pipeline introduces a new design paradigm by concurrently considering the form and interactivity of a physicalization into one cohesive fabrication workflow. We evaluate our approach using (i) computational evaluations, (ii) three usage scenarios focusing on general visualization tasks, and (iii) expert interviews. The design paradigm introduced by our pipeline can lower barriers to physicalization research, creation, and adoption.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327198",
            "id": "r_340",
            "s_ids": [
                "s_1330",
                "s_1317",
                "s_292",
                "s_82",
                "s_613",
                "s_781"
            ],
            "type": "rich",
            "x": -2.1729040145874023,
            "y": 18.068710327148438
        },
        {
            "title": "Classes are Not Clusters: Improving Label-Based Evaluation of Dimensionality Reduction",
            "data": "A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures\u2014Label-Trustworthiness and Label-Continuity (Label-T&C)\u2014advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327187",
            "id": "r_341",
            "s_ids": [
                "s_649",
                "s_233",
                "s_835",
                "s_789",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.1501164436340332,
            "y": 18.039901733398438
        },
        {
            "title": "From Shock to Shift: Data Visualization for Constructive Climate Journalism",
            "data": "We present a multi-dimensional, multi-level, and multi-channel approach to data visualization for the purpose of constructive climate journalism. Data visualization has assumed a central role in environmental journalism and is often used in data stories to convey the dramatic consequences of climate change and other ecological crises. However, the emphasis on the catastrophic impacts of climate change tends to induce feelings of fear, anxiety, and apathy in readers. Climate mitigation, adaptation, and protection\u2014all highly urgent in the face of the climate crisis\u2014are at risk of being overlooked. These topics are more difficult to communicate as they are hard to convey on varying levels of locality, involve multiple interconnected sectors, and need to be mediated across various channels from the printed newspaper to social media platforms. So far, there has been little research on data visualization to enhance affective engagement with data about climate protection as part of solution-oriented reporting of climate change. With this research we characterize the unique challenges of constructive climate journalism for data visualization and share findings from a research and design study in collaboration with a national newspaper in Germany. Using the affordances and aesthetics of travel postcards, we present Klimakarten, a data journalism project on the progress of climate protection at multiple spatial scales (from national to local), across five key sectors (agriculture, buildings, energy, mobility, and waste), and for print and online use. The findings from quantitative and qualitative analysis of reader feedback confirm our overall approach and suggest implications for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327185",
            "id": "r_342",
            "s_ids": [
                "s_950",
                "s_291",
                "s_639",
                "s_188"
            ],
            "type": "rich",
            "x": -6.655655860900879,
            "y": 17.043533325195312
        },
        {
            "title": "Quantivine: A Visualization Approach for Large-Scale Quantum Circuit Representation and Analysis",
            "data": "Quantum computing is a rapidly evolving field that enables exponential speed-up over classical algorithms. At the heart of this revolutionary technology are quantum circuits, which serve as vital tools for implementing, analyzing, and optimizing quantum algorithms. Recent advancements in quantum computing and the increasing capability of quantum devices have led to the development of more complex quantum circuits. However, traditional quantum circuit diagrams suffer from scalability and readability issues, which limit the efficiency of analysis and optimization processes. In this research, we propose a novel visualization approach for large-scale quantum circuits by adopting semantic analysis to facilitate the comprehension of quantum circuits. We first exploit meta-data and semantic information extracted from the underlying code of quantum circuits to create component segmentations and pattern abstractions, allowing for easier wrangling of massive circuit diagrams. We then develop Quantivine, an interactive system for exploring and understanding quantum circuits. A series of novel circuit visualizations is designed to uncover contextual details such as qubit provenance, parallelism, and entanglement. The effectiveness of Quantivine is demonstrated through two usage scenarios of quantum circuits with up to 100 qubits and a formal user evaluation with quantum experts. A free copy of this paper and all supplemental materials are available at https://osf.io/2m9yh/?view_only=0aa1618c97244f5093cd7ce15f1431f9.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327148",
            "id": "r_343",
            "s_ids": [
                "s_485",
                "s_828",
                "s_1603",
                "s_599",
                "s_1072",
                "s_648",
                "s_872",
                "s_79",
                "s_1385"
            ],
            "type": "rich",
            "x": -9.408052444458008,
            "y": 18.016576766967773
        },
        {
            "title": "FSLens: A Visual Analytics Approach to Evaluating and Optimizing the Spatial Layout of Fire Stations",
            "data": "The provision of fire services plays a vital role in ensuring the safety of residents' lives and property. The spatial layout of fire stations is closely linked to the efficiency of fire rescue operations. Traditional approaches have primarily relied on mathematical planning models to generate appropriate layouts by summarizing relevant evaluation criteria. However, this optimization process presents significant challenges due to the extensive decision space, inherent conflicts among criteria, and decision-makers' preferences. To address these challenges, we propose FSLens, an interactive visual analytics system that enables in-depth evaluation and rational optimization of fire station layout. Our approach integrates fire records and correlation features to reveal fire occurrence patterns and influencing factors using spatiotemporal sequence forecasting. We design an interactive visualization method to explore areas within the city that are potentially under-resourced for fire service based on the fire distribution and existing fire station layout. Moreover, we develop a collaborative human-computer multi-criteria decision model that generates multiple candidate solutions for optimizing firefighting resources within these areas. We simulate and compare the impact of different solutions on the original layout through well-designed visualizations, providing decision-makers with the most satisfactory solution. We demonstrate the effectiveness of our approach through one case study with real-world datasets. The feedback from domain experts indicates that our system helps them to better identify and improve potential gaps in the current fire station layout.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327077",
            "id": "r_344",
            "s_ids": [
                "s_1567",
                "s_610",
                "s_1287",
                "s_1512",
                "s_1502",
                "s_549"
            ],
            "type": "rich",
            "x": -11.054966926574707,
            "y": 19.336000442504883
        },
        {
            "title": "LiberRoad: Probing into the Journey of Chinese Classics Through Visual Analytics",
            "data": "Books act as a crucial carrier of cultural dissemination in ancient times. This work involves joint efforts between visualization and humanities researchers, aiming at building a holistic view of the cultural exchange and integration between China and Japan brought about by the overseas circulation of Chinese classics. Book circulation data consist of uncertain spatiotemporal trajectories, with multiple dimensions, and movement across hierarchical spaces forms a compound network. LiberRoad visualizes the circulation of books collected in the Imperial Household Agency of Japan, and can be generalized to other book movement data. The LiberRoad system enables a smooth transition between three views (Location Graph, map, and timeline) according to the desired perspectives (spatial or temporal), as well as flexible filtering and selection. The Location Graph is a novel uncertainty-aware visualization method that employs improved circle packing to represent spatial hierarchy. The map view intuitively shows the overall circulation by clustering and allows zooming into single book trajectory with lenses magnifying local movements. The timeline view ranks dynamically in response to user interaction to facilitate the discovery of temporal events. The evaluation and feedback from the expert users demonstrate that LiberRoad is helpful in revealing movement patterns and comparing circulation characteristics of different times and spaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326944",
            "id": "r_345",
            "s_ids": [
                "s_497",
                "s_153",
                "s_813",
                "s_52",
                "s_1435",
                "s_1338"
            ],
            "type": "rich",
            "x": -10.409421920776367,
            "y": 19.64436149597168
        },
        {
            "title": "HealthPrism: A Visual Analytics System for Exploring Children's Physical and Mental Health Profiles with Multimodal Data",
            "data": "The correlation between children's personal and family characteristics (e.g., demographics and socioeconomic status) and their physical and mental health status has been extensively studied across various research domains, such as public health, medicine, and data science. Such studies can provide insights into the underlying factors affecting children's health and aid in the development of targeted interventions to improve their health outcomes. However, with the availability of multiple data sources, including context data (i.e., the background information of children) and motion data (i.e., sensor data measuring activities of children), new challenges have arisen due to the large-scale, heterogeneous, and multimodal nature of the data. Existing statistical hypothesis-based and learning model-based approaches have been inadequate for comprehensively analyzing the complex correlation between multimodal features and multi-dimensional health outcomes due to the limited information revealed. In this work, we first distill a set of design requirements from multiple levels through conducting a literature review and iteratively interviewing 11 experts from multiple domains (e.g., public health and medicine). Then, we propose HealthPrism, an interactive visual and analytics system for assisting researchers in exploring the importance and influence of various context and motion features on children's health status from multi-level perspectives. Within HealthPrism, a multimodal learning model with a gate mechanism is proposed for health profiling and cross-modality feature importance comparison. A set of visualization components is designed for experts to explore and understand multimodal data freely. We demonstrate the effectiveness and usability of HealthPrism through quantitative evaluation of the model performance, case studies, and expert interviews in associated domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326943",
            "id": "r_346",
            "s_ids": [
                "s_1107",
                "s_515",
                "s_1412",
                "s_764",
                "s_1173",
                "s_168",
                "s_1395",
                "s_592",
                "s_675"
            ],
            "type": "rich",
            "x": -9.339425086975098,
            "y": 18.886432647705078
        },
        {
            "title": "TactualPlot: Spatializing Data as Sound Using Sensory Substitution for Touchscreen Accessibility",
            "data": "Tactile graphics are one of the best ways for a blind person to perceive a chart using touch, but their fabrication is often costly, time-consuming, and does not lend itself to dynamic exploration. Refreshable haptic displays tend to be expensive and thus unavailable to most blind individuals. We propose TactualPlot, an approach to sensory substitution where touch interaction yields auditory (sonified) feedback. The technique relies on embodied cognition for spatial awareness\u2014i.e., individuals can perceive 2D touch locations of their fingers with reference to other 2D locations such as the relative locations of other fingers or chart characteristics that are visualized on touchscreens. Combining touch and sound in this way yields a scalable data exploration method for scatterplots where the data density under the user's fingertips is sampled. The sample regions can optionally be scaled based on how quickly the user moves their hand. Our development of TactualPlot was informed by formative design sessions with a blind collaborator, whose practice while using tactile scatterplots caused us to expand the technique for multiple fingers. We present results from an evaluation comparing our TactualPlot interaction technique to tactile graphics printed on swell touch paper.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326937",
            "id": "r_347",
            "s_ids": [
                "s_125",
                "s_524",
                "s_275",
                "s_793",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.0305216312408447,
            "y": 16.00535011291504
        },
        {
            "title": "A Comparative Visual Analytics Framework for Evaluating Evolutionary Processes in Multi-Objective Optimization",
            "data": "Evolutionary multi-objective optimization (EMO) algorithms have been demonstrated to be effective in solving multi-criteria decision-making problems. In real-world applications, analysts often employ several algorithms concurrently and compare their solution sets to gain insight into the characteristics of different algorithms and explore a broader range of feasible solutions. However, EMO algorithms are typically treated as black boxes, leading to difficulties in performing detailed analysis and comparisons between the internal evolutionary processes. Inspired by the successful application of visual analytics tools in explainable AI, we argue that interactive visualization can significantly enhance the comparative analysis between multiple EMO algorithms. In this paper, we present a visual analytics framework that enables the exploration and comparison of evolutionary processes in EMO algorithms. Guided by a literature review and expert interviews, the proposed framework addresses various analytical tasks and establishes a multi-faceted visualization design to support the comparative analysis of intermediate generations in the evolution as well as solution sets. We demonstrate the effectiveness of our framework through case studies on benchmarking and real-world multi-objective optimization problems to elucidate how analysts can leverage our framework to inspect and compare diverse algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326921",
            "id": "r_348",
            "s_ids": [
                "s_727",
                "s_995",
                "s_551",
                "s_48",
                "s_1110"
            ],
            "type": "rich",
            "x": -4.299012660980225,
            "y": 21.111059188842773
        },
        {
            "title": "TopoSZ: Preserving Topology in Error-Bounded Lossy Compression",
            "data": "Existing error-bounded lossy compression techniques control the pointwise error during compression to guarantee the integrity of the decompressed data. However, they typically do not explicitly preserve the topological features in data. When performing post hoc analysis with decompressed data using topological methods, preserving topology in the compression process to obtain topologically consistent and correct scientific insights is desirable. In this paper, we introduce TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. Specifically, we aim to preserve the types and locations of local extrema as well as the level set relations among critical points captured by contour trees in the decompressed data. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain, and incorporate such constraints with a customized error-controlled quantization strategy from the SZ compressor (version 1.4). Our method allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326920",
            "id": "r_349",
            "s_ids": [
                "s_1581",
                "s_1270",
                "s_986",
                "s_836"
            ],
            "type": "rich",
            "x": 7.405143737792969,
            "y": 5.369645595550537
        },
        {
            "title": "Vortex Lens: Interactive Vortex Core Line Extraction using Observed Line Integral Convolution",
            "data": "This paper describes a novel method for detecting and visualizing vortex structures in unsteady 2D fluid flows. The method is based on an interactive local reference frame estimation that minimizes the observed time derivative of the input flow field $\\mathrm{v}(x, t)$. A locally optimal reference frame $\\mathrm{w}(x, t)$ assists the user in the identification of physically observable vortex structures in Observed Line Integral Convolution (LIC) visualizations. The observed LIC visualizations are interactively computed and displayed in a user-steered vortex lens region, embedded in the context of a conventional LIC visualization outside the lens. The locally optimal reference frame is then used to detect observed critical points, where $\\mathrm{v}=\\mathrm{w}$, which are used to seed vortex core lines. Each vortex core line is computed as a solution of the ordinary differential equation (ODE) $\\dot{w}(t)=\\mathrm{w}(w(t), t)$, with an observed critical point as initial condition $(w(t_{0}), t_{0})$. During integration, we enforce a strict error bound on the difference between the extracted core line and the integration of a path line of the input vector field, i.e., a solution to the ODE $\\dot{v}(t)=\\mathrm{v}(v(t), t)$. We experimentally verify that this error depends on the step size of the core line integration. This ensures that our method extracts Lagrangian vortex core lines that are the simultaneous solution of both ODEs with a numerical error that is controllable by the integration step size. We show the usability of our method in the context of an interactive system using a lens metaphor, and evaluate the results in comparison to state-of-the-art vortex core line extraction methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326915",
            "id": "r_350",
            "s_ids": [
                "s_1003",
                "s_1519",
                "s_205",
                "s_1496",
                "s_388"
            ],
            "type": "rich",
            "x": -8.833763122558594,
            "y": 16.23857307434082
        },
        {
            "title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce",
            "data": "Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro, an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326911",
            "id": "r_351",
            "s_ids": [
                "s_931",
                "s_1134",
                "s_13",
                "s_1235",
                "s_739",
                "s_1447",
                "s_1004",
                "s_549"
            ],
            "type": "rich",
            "x": -11.011138916015625,
            "y": 19.303258895874023
        },
        {
            "title": "Merge Tree Geodesics and Barycenters with Path Mappings",
            "data": "Comparative visualization of scalar fields is often facilitated using similarity measures such as edit distances. In this paper, we describe a novel approach for similarity analysis of scalar fields that combines two recently introduced techniques: Wasserstein geodesics/barycenters as well as path mappings, a branch decomposition-independent edit distance. Effectively, we are able to leverage the reduced susceptibility of path mappings to small perturbations in the data when compared with the original Wasserstein distance. Our approach therefore exhibits superior performance and quality in typical tasks such as ensemble summarization, ensemble clustering, and temporal reduction of time series, while retaining practically feasible runtimes. Beyond studying theoretical properties of our approach and discussing implementation aspects, we describe a number of case studies that provide empirical insights into its utility for comparative visualization, and demonstrate the advantages of our method in both synthetic and real-world scenarios. We supply a C++ implementation that can be used to reproduce our results.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326601",
            "id": "r_352",
            "s_ids": [
                "s_279",
                "s_1462",
                "s_707",
                "s_1068"
            ],
            "type": "rich",
            "x": -4.033964157104492,
            "y": 20.704591751098633
        },
        {
            "title": "Class-Constrained t-SNE: Combining Data Features and Class Probabilities",
            "data": "Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation of classes \u2013 class landmarks. Furthermore, an interactive user-adjustable parameter balances these two components so that users can focus on the weighted perspectives of interest and also empowers a smooth visual transition between varying perspectives to preserve the mental map. We illustrate its application potential in model evaluation and visual-interactive labeling. A comparative analysis is performed to evaluate the DR results.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326600",
            "id": "r_353",
            "s_ids": [
                "s_1054",
                "s_1369",
                "s_1074",
                "s_478"
            ],
            "type": "rich",
            "x": -6.924692153930664,
            "y": 14.405339241027832
        },
        {
            "title": "Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting",
            "data": "We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a \u201cborn scalable\u201d query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as $\\mathbb{P}^{6}$: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326594",
            "id": "r_354",
            "s_ids": [
                "s_1103",
                "s_1237"
            ],
            "type": "rich",
            "x": -1.9804552793502808,
            "y": 16.02064323425293
        },
        {
            "title": "Average Estimates in Line Graphs Are Biased Toward Areas of Higher Variability",
            "data": "We investigate variability overweighting, a previously undocumented bias in line graphs, where estimates of average value are biased toward areas of higher variability in that line. We found this effect across two preregistered experiments with 140 and 420 participants. These experiments also show that the bias is reduced when using a dot encoding of the same series. We can model the bias with the average of the data series and the average of the points drawn along the line. This bias might arise because higher variability leads to stronger weighting in the average calculation, either due to the longer line segments (even though those segments contain the same number of data values) or line segments with higher variability being otherwise more visually salient. Understanding and predicting this bias is important for visualization design guidelines, recommendation systems, and tool builders, as the bias can adversely affect estimates of averages and trends.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326589",
            "id": "r_355",
            "s_ids": [
                "s_646",
                "s_229",
                "s_71",
                "s_1026"
            ],
            "type": "rich",
            "x": -3.745161294937134,
            "y": 17.45490264892578
        },
        {
            "title": "ManiVault: A Flexible and Extensible Visual Analytics Framework for High-Dimensional Data",
            "data": "Exploration and analysis of high-dimensional data are important tasks in many fields that produce large and complex data, like the financial sector, systems biology, or cultural heritage. Tailor-made visual analytics software is developed for each specific application, limiting their applicability in other fields. However, as diverse as these fields are, their characteristics and requirements for data analysis are conceptually similar. Many applications share abstract tasks and data types and are often constructed with similar building blocks. Developing such applications, even when based mostly on existing building blocks, requires significant engineering efforts. We developed ManiVault, a flexible and extensible open-source visual analytics framework for analyzing high-dimensional data. The primary objective of ManiVault is to facilitate rapid prototyping of visual analytics workflows for visualization software developers and practitioners alike. ManiVault is built using a plugin-based architecture that offers easy extensibility. While our architecture deliberately keeps plugins self-contained, to guarantee maximum flexibility and re-usability, we have designed and implemented a messaging API for tight integration and linking of modules to support common visual analytics design patterns. We provide several visualization and analytics plugins, and ManiVault's API makes the integration of new plugins easy for developers. ManiVault facilitates the distribution of visualization and analysis pipelines and results for practitioners through saving and reproducing complete application states. As such, ManiVault can be used as a communication tool among researchers to discuss workflows and results. A copy of this paper and all supplemental material is available at osf.io/9k6jw, and source code at github.com/ManiVaultStudio.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326582",
            "id": "r_356",
            "s_ids": [
                "s_1125",
                "s_1326",
                "s_211",
                "s_212",
                "s_1176",
                "s_1565",
                "s_167",
                "s_478",
                "s_1253",
                "s_1142"
            ],
            "type": "rich",
            "x": -6.851151943206787,
            "y": 14.34850788116455
        },
        {
            "title": "Character-Oriented Design for Visual Data Storytelling",
            "data": "When telling a data story, an author has an intention they seek to convey to an audience. This intention can be of many forms such as to persuade, to educate, to inform, or even to entertain. In addition to expressing their intention, the story plot must balance being consumable and enjoyable while preserving scientific integrity. In data stories, numerous methods have been identified for constructing and presenting a plot. However, there is an opportunity to expand how we think and create the visual elements that present the story. Stories are brought to life by characters; often they are what make a story captivating, enjoyable, memorable, and facilitate following the plot until the end. Through the analysis of 160 existing data stories, we systematically investigate and identify distinguishable features of characters in data stories, and we illustrate how they feed into the broader concept of \u201ccharacter-oriented design\u201d. We identify the roles and visual representations data characters assume as well as the types of relationships these roles have with one another. We identify characteristics of antagonists as well as define conflict in data stories. We find the need for an identifiable central character that the audience latches on to in order to follow the narrative and identify their visual representations. We then illustrate \u201ccharacter-oriented design\u201d by showing how to develop data characters with common data story plots. With this work, we present a framework for data characters derived from our analysis; we then offer our extension to the data storytelling process using character-oriented design. To access our supplemental materials please visit https://chaorientdesignds.github.io/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326578",
            "id": "r_357",
            "s_ids": [
                "s_1091",
                "s_233",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7592752575874329,
            "y": 17.921403884887695
        },
        {
            "title": "Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges",
            "data": "We introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa $m$ and exponent $e$ of a value $v=m\\cdot 10^{e}$. We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyze error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326576",
            "id": "r_358",
            "s_ids": [
                "s_1314",
                "s_1282",
                "s_1081",
                "s_267"
            ],
            "type": "rich",
            "x": -5.93712854385376,
            "y": 15.556697845458984
        },
        {
            "title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco",
            "data": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\u2014a framework to model visualization knowledge\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326527",
            "id": "r_359",
            "s_ids": [
                "s_1400",
                "s_1456",
                "s_646",
                "s_1386",
                "s_1025"
            ],
            "type": "rich",
            "x": -3.976733446121216,
            "y": 17.19133758544922
        },
        {
            "title": "TimeSplines: Sketch-Based Authoring of Flexible and Idiosyncratic Timelines",
            "data": "Timelines are essential for visually communicating chronological narratives and reflecting on the personal and cultural significance of historical events. Existing visualization tools tend to support conventional linear representations, but fail to capture personal idiosyncratic conceptualizations of time. In response, we built TimeSplines, a visualization authoring tool that allows people to sketch multiple free-form temporal axes and populate them with heterogeneous, time-oriented data via incremental and lazy data binding. Authors can bend, compress, and expand temporal axes to emphasize or de-emphasize intervals based on their personal importance; they can also annotate the axes with text and figurative elements to convey contextual information. The results of two user studies show how people appropriate the concepts in TimeSplines to express their own conceptualization of time, while our curated gallery of images demonstrates the expressive potential of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326520",
            "id": "r_360",
            "s_ids": [
                "s_143",
                "s_12",
                "s_198",
                "s_426"
            ],
            "type": "rich",
            "x": -6.540265083312988,
            "y": 16.67812728881836
        },
        {
            "title": "The Rational Agent Benchmark for Data Visualization",
            "data": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326513",
            "id": "r_361",
            "s_ids": [
                "s_408",
                "s_270",
                "s_481",
                "s_135",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.362769603729248,
            "y": 16.844820022583008
        },
        {
            "title": "Self-Supervised Color-Concept Association via Image Colorization",
            "data": "The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human's expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover's Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209481",
            "id": "r_362",
            "s_ids": [
                "s_175",
                "s_844",
                "s_314",
                "s_621",
                "s_1076"
            ],
            "type": "rich",
            "x": -7.981105327606201,
            "y": 18.753257751464844
        },
        {
            "title": "Geo-Storylines: Integrating Maps into Storyline Visualizations",
            "data": "Storyline visualizations are a powerful way to compactly visualize how the relationships between people evolve over time. Real-world relationships often also involve space, for example the cities that two political rivals visited together or alone over the years. By default, Storyline visualizations only show implicitly geospatial co-occurrence between people (drawn as lines), by bringing their lines together. Even the few designs that do explicitly show geographic locations only do so in abstract ways (e.g., annotations) and do not communicate geospatial information, such as the direction or extent of their political campains. We introduce Geo-Storylines, a collection of visualisation designs that integrate geospatial context into Storyline visualizations, using different strategies for compositing time and space. Our contribution is twofold. First, we present the results of a sketching workshop with 11 participants, that we used to derive a design space for integrating maps into Storylines. Second, by analyzing the strengths and weaknesses of the potential designs of the design space in terms of legibility and ability to scale to multiple relationships, we extract the three most promising: Time Glyphs, Coordinated Views, and Map Glyphs. We compare these three techniques first in a controlled study with 18 participants, under five different geospatial tasks and two maps of different complexity. We additionally collected informal feedback about their usefulness from domain experts in data journalism. Our results indicate that, as expected, detailed performance depends on the task. Nevertheless, Coordinated Views remain a highly effective and preferred technique across the board.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209480",
            "id": "r_363",
            "s_ids": [
                "s_628",
                "s_305",
                "s_182"
            ],
            "type": "rich",
            "x": -6.222087383270264,
            "y": 18.31991195678711
        },
        {
            "title": "Interactive Visual Analysis of Structure-borne Noise Data",
            "data": "Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise sources early in the design and production process is essential for reducing a product's development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise simulation data for an internal combustion engine.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209478",
            "id": "r_364",
            "s_ids": [
                "s_796",
                "s_1062",
                "s_1050",
                "s_1610",
                "s_585",
                "s_418",
                "s_946"
            ],
            "type": "rich",
            "x": -6.514283180236816,
            "y": 13.812209129333496
        },
        {
            "title": "Constrained Dynamic Mode Decomposition",
            "data": "Frequency-based decomposition of time series data is used in many visualization applications. Most of these decomposition methods (such as Fourier transform or singular spectrum analysis) only provide interaction via pre- and post-processing, but no means to influence the core algorithm. A method that also belongs to this class is Dynamic Mode Decomposition (DMD), a spectral decomposition method that extracts spatio-temporal patterns from data. In this paper, we incorporate frequency-based constraints into DMD for an adaptive decomposition that leads to user-controllable visualizations, allowing analysts to include their knowledge into the process. To accomplish this, we derive an equivalent reformulation of DMD that implicitly provides access to the eigenvalues (and therefore to the frequencies) identified by DMD. By utilizing a constrained minimization problem customized to DMD, we can guarantee the existence of desired frequencies by minimal changes to DMD. We complement this core approach by additional techniques for constrained DMD to facilitate explorative visualization and investigation of time series data. With several examples, we demonstrate the usefulness of constrained DMD and compare it to conventional frequency-based decomposition methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209437",
            "id": "r_365",
            "s_ids": [
                "s_1302",
                "s_344",
                "s_1587",
                "s_157"
            ],
            "type": "rich",
            "x": -8.546286582946777,
            "y": 19.32352638244629
        },
        {
            "title": "Uncertainty-Aware Multidimensional Scaling",
            "data": "We present an extension of multidimensional scaling (MDS) to uncertain data, facilitating uncertainty visualization of multidimensional data. Our approach uses local projection operators that map high-dimensional random vectors to low-dimensional space to formulate a generalized stress. In this way, our generic model supports arbitrary distributions and various stress types. We use our uncertainty-aware multidimensional scaling (UAMDS) concept to derive a formulation for the case of normally distributed random vectors and a squared stress. The resulting minimization problem is numerically solved via gradient descent. We complement UAMDS by additional visualization techniques that address the sensitivity and trustworthiness of dimensionality reduction under uncertainty. With several examples, we demonstrate the usefulness of our approach and the importance of uncertainty-aware techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209420",
            "id": "r_366",
            "s_ids": [
                "s_1406",
                "s_1302",
                "s_157"
            ],
            "type": "rich",
            "x": -8.55763053894043,
            "y": 19.3974609375
        },
        {
            "title": "Roboviz: A Game-Centered Project for Information Visualization Education",
            "data": "Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client\u2013real or simulated\u2013a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209402",
            "id": "r_367",
            "s_ids": [
                "s_477",
                "s_1132"
            ],
            "type": "rich",
            "x": -3.1689906120300293,
            "y": 16.875688552856445
        },
        {
            "title": "On-Tube Attribute Visualization for Multivariate Trajectory Data",
            "data": "Stylized tubes are an established visualization primitive for line data as encountered in many scientific fields, ranging from characteristic lines in flow fields, fiber tracks reconstructed from diffusion tensor imaging, to trajectories of moving objects as they arise from cyber-physical systems in many engineering disciplines. Typical challenges include large data set sizes demanding for efficient rendering techniques as well as a large number of attributes that cannot be mapped simultaneously to the basic visual attributes provided by a tube-based visualization. In this work, we tackle both challenges with a new on-tube visualization approach. We improve recent work on high-quality GPU ray casting of Hermite spline tubes supporting ambient occlusion and extend it by a new layered procedural texturing technique. In the proposed framework, a large number of data set attributes can be mapped simultaneously to a variety of glyphs and plots that are embedded in texture space and organized in layers. Efficient rendering with minimal data transfer is achieved by generating the glyphs procedurally and drawing them in a deferred shading pass. We integrated these techniques in a prototype visualization tool that facilitates flexible mapping of data set attributes to visual tube and glyph attributes. We studied our approach on a variety of example data from different fields and found it to provide a highly adaptable and extensible toolbox to quickly craft tailor-made tube-based trajectory visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209400",
            "id": "r_368",
            "s_ids": [
                "s_440",
                "s_681",
                "s_1384",
                "s_1039"
            ],
            "type": "rich",
            "x": -5.7671918869018555,
            "y": 18.471288681030273
        },
        {
            "title": "PMU Tracker: A Visualization Platform for Epicentric Event Propagation Analysis in the Power Grid",
            "data": "The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as \u2018smart\u2019 systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid's network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram, which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker's visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209380",
            "id": "r_369",
            "s_ids": [
                "s_236",
                "s_85",
                "s_1359",
                "s_394"
            ],
            "type": "rich",
            "x": -3.3055014610290527,
            "y": 17.615650177001953
        },
        {
            "title": "Visualizing Ensemble Predictions of Music Mood",
            "data": "Music mood classification has been a challenging problem in comparison with other music classification problems (e.g., genre, composer, or period). One solution for addressing this challenge is to use an ensemble of machine learning models. In this paper, we show that visualization techniques can effectively convey the popular prediction as well as uncertainty at different music sections along the temporal axis while enabling the analysis of individual ML models in conjunction with their application to different musical data. In addition to the traditional visual designs, such as stacked line graph, ThemeRiver, and pixel-based visualization, we introduce a new variant of ThemeRiver, called \u201cdual-flux ThemeRiver\u201d, which allows viewers to observe and measure the most popular prediction more easily than stacked line graph and ThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots can also assist in model-development workflows, in addition to annotating music using ensemble model predictions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209379",
            "id": "r_370",
            "s_ids": [
                "s_1469",
                "s_298"
            ],
            "type": "rich",
            "x": -7.306779384613037,
            "y": 16.93065643310547
        },
        {
            "title": "Probablement, Wahrscheinlich, Likely? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations",
            "data": "Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, $n=50$ participants per language both chose probability expressions \u2014 e.g. likely, probable \u2014 to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209367",
            "id": "r_371",
            "s_ids": [
                "s_1513",
                "s_594",
                "s_145"
            ],
            "type": "rich",
            "x": -3.078195810317993,
            "y": 16.500381469726562
        },
        {
            "title": "A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering",
            "data": "Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114872",
            "id": "r_372",
            "s_ids": [
                "s_370",
                "s_29",
                "s_102",
                "s_444"
            ],
            "type": "rich",
            "x": -15.089418411254883,
            "y": 7.0849385261535645
        },
        {
            "title": "Visualization Equilibrium",
            "data": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114842",
            "id": "r_373",
            "s_ids": [
                "s_984",
                "s_166",
                "s_135",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.317490339279175,
            "y": 16.806535720825195
        },
        {
            "title": "Learning Objectives, Insights, and Assessments: How Specification Formats Impact Design",
            "data": "Despite the ubiquity of communicative visualizations, specifying communicative intent during design is ad hoc. Whether we are selecting from a set of visualizations, commissioning someone to produce them, or creating them ourselves, an effective way of specifying intent can help guide this process. Ideally, we would have a concise and shared specification language. In previous work, we have argued that communicative intents can be viewed as a learning/assessment problem (i.e., what should the reader learn and what test should they do well on). Learning-based specification formats are linked (e.g., assessments are derived from objectives) but some may more effectively specify communicative intent. Through a large-scale experiment, we studied three specification types: learning objectives, insights, and assessments. Participants, guided by one of these specifications, rated their preferences for a set of visualization designs. Then, we evaluated the set of visualization designs to assess which specification led participants to prefer the most effective visualizations. We find that while all specification types have benefits over no-specification, each format has its own advantages. Our results show that learning objective-based specifications helped participants the most in visualization selection. We also identify situations in which specifications may be insufficient and assessments are vital.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114811",
            "id": "r_374",
            "s_ids": [
                "s_1132",
                "s_566",
                "s_477"
            ],
            "type": "rich",
            "x": -3.1418075561523438,
            "y": 16.858442306518555
        },
        {
            "title": "F2-Bubbles: Faithful Bubble Set Construction and Flexible Editing",
            "data": "In this paper, we propose F2-Bubbles, a set overlay visualization technique that addresses overlapping artifacts and supports interactive editing with intelligent suggestions. The core of our method is a new, efficient set overlay construction algorithm that approximates the optimal set overlay by considering set elements and their non-set neighbors. Thanks to the efficiency of the algorithm, interactive editing is achieved, and with intelligent suggestions, users can easily and flexibly edit visualizations through direct manipulations with local adaptations. A quantitative comparison with state-of-the-art set visualization techniques and case studies demonstrate the effectiveness of our method and suggests that F2-Bubbles is a helpful technique for set visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2021.3114761",
            "id": "r_375",
            "s_ids": [
                "s_1146",
                "s_194",
                "s_1261",
                "s_1441",
                "s_219",
                "s_1332",
                "s_1345"
            ],
            "type": "rich",
            "x": -8.105088233947754,
            "y": 18.673017501831055
        },
        {
            "title": "IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection",
            "data": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030389",
            "id": "r_376",
            "s_ids": [
                "s_581",
                "s_532",
                "s_418"
            ],
            "type": "rich",
            "x": -6.519144535064697,
            "y": 13.811857223510742
        },
        {
            "title": "Visilant: Visual Support for the Exploration and Analytical Process Tracking in Criminal Investigations",
            "data": "The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030356",
            "id": "r_377",
            "s_ids": [
                "s_356",
                "s_1352",
                "s_710",
                "s_679",
                "s_1422",
                "s_1354"
            ],
            "type": "rich",
            "x": -5.771312236785889,
            "y": 19.476715087890625
        },
        {
            "title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts",
            "data": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456398",
            "id": "r_378",
            "s_ids": [
                "s_827",
                "s_1608",
                "s_1160",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7537603974342346,
            "y": 17.97661018371582
        },
        {
            "title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration",
            "data": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456364",
            "id": "r_379",
            "s_ids": [
                "s_1583",
                "s_1521",
                "s_734",
                "s_1020",
                "s_470"
            ],
            "type": "rich",
            "x": -8.450043678283691,
            "y": 17.2716121673584
        },
        {
            "title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models",
            "data": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456320",
            "id": "r_380",
            "s_ids": [
                "s_1387",
                "s_662",
                "s_1525",
                "s_772",
                "s_1245"
            ],
            "type": "rich",
            "x": -8.972086906433105,
            "y": 19.911842346191406
        },
        {
            "title": "PUREsuggest: Citation-Based Literature Search and Visual Exploration with Keyword-Controlled Rankings",
            "data": "Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456199",
            "id": "r_381",
            "s_ids": [
                "s_429"
            ],
            "type": "rich",
            "x": -10.848739624023438,
            "y": 17.260053634643555
        },
        {
            "title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology",
            "data": "Science has long been viewed as a key driver of economic growth and rising standards of living. Knowledge about how scientific advances support marketplace inventions is therefore essential for understanding the role of science in propelling real-world applications and technological progress. The increasing availability of large-scale datasets tracing scientific publications and patented inventions and the complex interactions among them offers us new opportunities to explore the evolving dual frontiers of science and technology at an unprecedented level of scale and detail. However, we lack suitable visual analytics approaches to analyze such complex interactions effectively. Here we introduce InnovationInsights, an interactive visual analysis system for researchers, research institutions, and policymakers to explore the complex linkages between science and technology, and to identify critical innovations, inventors, and potential partners. The system first identifies important associations between scientific papers and patented inventions through a set of statistical measures introduced by our experts from the field of the Science of Science. A series of visualization views are then used to present these associations in the data context. In particular, we introduce the Interplay Graph to visualize patterns and insights derived from the data, helping users effectively navigate citation relationships between papers and patents. This visualization thereby helps them identify the origins of technical inventions and the impact of scientific research. We evaluate the system through two case studies with experts followed by expert interviews. We further engage a premier research institution to test-run the system, helping its institution leaders to extract new insights for innovation. Through both the case studies and the engagement project, we find that our system not only meets our original goals of design, allowing users to better identify the sources of technical inventions and to understand the broad impact of scientific research; it also goes beyond these purposes to enable an array of new applications for researchers and research institutions, ranging from identifying untapped innovation potential within an institution to forging new collaboration opportunities between science and industry.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327387",
            "id": "r_382",
            "s_ids": [
                "s_592",
                "s_528",
                "s_567",
                "s_989",
                "s_666"
            ],
            "type": "rich",
            "x": -9.363970756530762,
            "y": 19.622608184814453
        },
        {
            "title": "Attribute-Aware RBFs: Interactive Visualization of Time Series Particle Volumes Using RT Core Range Queries",
            "data": "Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327366",
            "id": "r_383",
            "s_ids": [
                "s_121",
                "s_949",
                "s_20",
                "s_339",
                "s_343"
            ],
            "type": "rich",
            "x": -15.037230491638184,
            "y": 7.13714599609375
        },
        {
            "title": "Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization",
            "data": "Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327194",
            "id": "r_384",
            "s_ids": [
                "s_1433",
                "s_322",
                "s_405",
                "s_986",
                "s_841"
            ],
            "type": "rich",
            "x": 7.503719329833984,
            "y": 5.2710723876953125
        },
        {
            "title": "My Model is Unfair, Do People Even Care? Visual Design Affects Trust and Perceived Bias in Machine Learning",
            "data": "Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer \u201cCan visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?\u201d Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327192",
            "id": "r_385",
            "s_ids": [
                "s_833",
                "s_1528",
                "s_812",
                "s_128",
                "s_496",
                "s_1324",
                "s_1184"
            ],
            "type": "rich",
            "x": -3.9782910346984863,
            "y": 16.655683517456055
        },
        {
            "title": "A General Framework for Progressive Data Compression and Retrieval",
            "data": "In scientific simulations, observations, and experiments, the transfer of data to and from disk and across networks has become a major bottleneck for data analysis and visualization. Compression techniques have been employed to tackle this challenge, but traditional lossy methods often demand conservative error tolerances to meet the numerical accuracy requirements of both anticipated and unknown data analysis tasks. Progressive data compression and retrieval has emerged as a promising solution, where each analysis task dictates its own accuracy needs. However, few analysis algorithms inherently support progressive data processing, and adapting compression techniques, file formats, client/server frameworks, and APIs to support progressivity can be challenging. This paper presents a framework that enables progressive-precision data queries for any data compressor or numerical representation. Our strategy hinges on a multi-component representation that successively reduces the error between the original and compressed field, allowing each field in the progressive sequence to be expressed as a partial sum of components. We have implemented this approach with four established scientific data compressors and assessed its effectiveness using real-world data sets from the SDRBench collection. The results show that our framework competes in accuracy with the standalone compressors it is based upon. Additionally, (de)compression time is proportional to the number of components requested by the user. Finally, our framework allows for fully lossless compression using lossy compressors when a sufficient number of components are employed.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327186",
            "id": "r_386",
            "s_ids": [
                "s_165",
                "s_1106"
            ],
            "type": "rich",
            "x": -15.094003677368164,
            "y": 7.080353260040283
        },
        {
            "title": "PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data",
            "data": "Although many deep-learning-based super-resolution approaches have been proposed in recent years, because no ground truth is available in the inference stage, few can quantify the errors and uncertainties of the super-resolved results. For scientific visualization applications, however, conveying uncertainties of the results to scientists is crucial to avoid generating misleading or incorrect information. In this paper, we propose PSRFlow, a novel normalizing flow-based generative model for scientific data super-resolution that incorporates uncertainty quantification into the super-resolution process. PSRFlow learns the conditional distribution of the high-resolution data based on the low-resolution counterpart. By sampling from a Gaussian latent space that captures the missing information in the high-resolution data, one can generate different plausible super-resolution outputs. The efficient sampling in the Gaussian latent space allows our model to perform uncertainty quantification for the super-resolved results. During model training, we augment the training data with samples across various scales to make the model adaptable to data of different scales, achieving flexible super-resolution for a given input. Our results demonstrate superior performance and robust uncertainty quantification compared with existing methods such as interpolation and GAN-based super-resolution networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327171",
            "id": "r_387",
            "s_ids": [
                "s_172",
                "s_405"
            ],
            "type": "rich",
            "x": 7.5425124168396,
            "y": 5.232537746429443
        },
        {
            "title": "Adaptive Assessment of Visualization Literacy",
            "data": "Visualization literacy is an essential skill for accurately interpreting data to inform critical decisions. Consequently, it is vital to understand the evolution of this ability and devise targeted interventions to enhance it, requiring concise and repeatable assessments of visualization literacy for individuals. However, current assessments, such as the Visualization Literacy Assessment Test (vlat), are time-consuming due to their fixed, lengthy format. To address this limitation, we develop two streamlined computerized adaptive tests (cats) for visualization literacy, a-vlat and a-calvi, which measure the same set of skills as their original versions in half the number of questions. Specifically, we (1) employ item response theory (IRT) and non-psychometric constraints to construct adaptive versions of the assessments, (2) finalize the configurations of adaptation through simulation, (3) refine the composition of test items of a-calvi via a qualitative study, and (4) demonstrate the test-retest reliability (ICC: 0.98 and 0.98) and convergent validity (correlation: 0.81 and 0.66) of both CATS via four online studies. We discuss practical recommendations for using our CATS and opportunities for further customization to leverage the full potential of adaptive assessments. All supplemental materials are available at https://osf.io/a6258/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327165",
            "id": "r_388",
            "s_ids": [
                "s_1058",
                "s_1482",
                "s_594",
                "s_607",
                "s_145",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.0818686485290527,
            "y": 16.514583587646484
        },
        {
            "title": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models",
            "data": "Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327153",
            "id": "r_389",
            "s_ids": [
                "s_1235",
                "s_1616",
                "s_1272",
                "s_1505",
                "s_156"
            ],
            "type": "rich",
            "x": -10.154744148254395,
            "y": 18.442806243896484
        },
        {
            "title": "EMPHASISCHECKER: A Tool for Guiding Chart and Caption Emphasis",
            "data": "Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text. Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors' intended messages. Motivated by the survey findings, we present EMPHASISCHECKER, an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis. The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data. This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions. A user study confirms that our tool is both useful and easy to use when authoring charts and captions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327150",
            "id": "r_390",
            "s_ids": [
                "s_7",
                "s_398",
                "s_1090",
                "s_536",
                "s_754"
            ],
            "type": "rich",
            "x": -4.0578694343566895,
            "y": 16.253864288330078
        },
        {
            "title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams",
            "data": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326925",
            "id": "r_391",
            "s_ids": [
                "s_293",
                "s_1387",
                "s_1182",
                "s_1405",
                "s_1177",
                "s_441",
                "s_989"
            ],
            "type": "rich",
            "x": -4.264146327972412,
            "y": 20.153764724731445
        },
        {
            "title": "Image or Information? Examining the Nature and Impact of Visualization Perceptual Classification",
            "data": "How do people internalize visualizations: as images or information? In this study, we investigate the nature of internalization for visualizations (i.e., how the mind encodes visualizations in memory) and how memory encoding affects its retrieval. This exploratory work examines the influence of various design elements on a user's perception of a chart. Specifically, which design elements lead to perceptions of visualization as an image (aims to provide visual references, evoke emotions, express creativity, and inspire philosophic thought) or as information (aims to present complex data, information, or ideas concisely and promote analytical thinking)? Understanding how design elements contribute to viewers perceiving a visualization more as an image or information will help designers decide which elements to include to achieve their communication goals. For this study, we annotated 500 visualizations and analyzed the responses of 250 online participants, who rated the visualizations on a bilinear scale as \u2018image\u2019 or \u2018information.\u2019 We then conducted an in-person study ($n = 101$) using a free recall task to examine how the image/information ratings and design elements impacted memory. The results revealed several interesting findings: Image-rated visualizations were perceived as more aesthetically \u2018appealing,\u2019 \u2018enjoyable,\u2019 and \u2018pleasing.\u2019 Information-rated visualizations were perceived as less \u2018difficult to understand\u2019 and more aesthetically \u2018likable\u2019 and \u2018nice,\u2019 though participants expressed higher \u2018positive\u2019 sentiment when viewing image-rated visualizations and felt less \u2018guided to a conclusion.\u2019 The presence of axes and text annotations heavily influenced the likelihood of participants rating the visualization as \u2018information.\u2019 We also found different patterns among participants that were older. Importantly, we show that visualizations internalized as \u2018images\u2019 are less effective in conveying trends and messages, though they elicit a more positive emotional judgment, while \u2018informative\u2019 visualizations exhibit annotation focused recall and elicit a more positive design judgment. We discuss the implications of this dissociation between aesthetic pleasure and perceived ease of use in visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326919",
            "id": "r_392",
            "s_ids": [
                "s_236",
                "s_229",
                "s_1201",
                "s_394"
            ],
            "type": "rich",
            "x": -3.330681085586548,
            "y": 17.613391876220703
        },
        {
            "title": "Enthusiastic and Grounded, Avoidant and Cautious: Understanding Public Receptivity to Data and Visualizations",
            "data": "Despite an abundance of open data initiatives aimed to inform and empower \u201cgeneral\u201d audiences, we still know little about the ways people outside of traditional data analysis communities experience and engage with public data and visualizations. To investigate this gap, we present results from an in-depth qualitative interview study with 19 participants from diverse ethnic, occupational, and demographic backgrounds. Our findings characterize a set of lived experiences with open data and visualizations in the domain of energy consumption, production, and transmission. This work exposes information receptivity \u2014 an individual's transient state of willingness or openness to receive information \u2014as a blind spot for the data visualization community, complementary to but distinct from previous notions of data visualization literacy and engagement. We observed four clusters of receptivity responses to data- and visualization-based rhetoric: Information-Avoidant, Data-Cautious, Data-Enthusiastic, and Domain-Grounded. Based on our findings, we highlight research opportunities for the visualization community. This exploratory work identifies the existence of diverse receptivity responses, highlighting the need to consider audiences with varying levels of openness to new information. Our findings also suggest new approaches for improving the accessibility and inclusivity of open data and visualization initiatives targeted at broad audiences. A free copy of this paper and all supplemental materials are available at https://OSF.IO/MPQ32.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326917",
            "id": "r_393",
            "s_ids": [
                "s_1234",
                "s_909",
                "s_375",
                "s_978",
                "s_392"
            ],
            "type": "rich",
            "x": -6.19083833694458,
            "y": 17.5593204498291
        },
        {
            "title": "OldVisOnline: Curating a Dataset of Historical Visualizations",
            "data": "With the increasing adoption of digitization, more and more historical visualizations created hundreds of years ago are accessible in digital libraries online. It provides a unique opportunity for visualization and history research. Meanwhile, there is no large-scale digital collection dedicated to historical visualizations. The visualizations are scattered in various collections, which hinders retrieval. In this study, we curate the first large-scale dataset dedicated to historical visualizations. Our dataset comprises 13K historical visualization images with corresponding processed metadata from seven digital libraries. In curating the dataset, we propose a workflow to scrape and process heterogeneous metadata. We develop a semi-automatic labeling approach to distinguish visualizations from other artifacts. Our dataset can be accessed with OldVisOnline, a system we have built to browse and label historical visualizations. We discuss our vision of usage scenarios and research opportunities with our dataset, such as textual criticism for historical visualizations. Drawing upon our experience, we summarize recommendations for future efforts to improve our dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326908",
            "id": "r_394",
            "s_ids": [
                "s_838",
                "s_118",
                "s_376",
                "s_332",
                "s_1472",
                "s_923",
                "s_360",
                "s_1338"
            ],
            "type": "rich",
            "x": -10.194145202636719,
            "y": 19.634979248046875
        },
        {
            "title": "TROPHY: A Topologically Robust Physics-Informed Tracking Framework for Tropical Cyclones",
            "data": "Tropical cyclones (TCs) are among the most destructive weather systems. Realistically and efficiently detecting and tracking TCs are critical for assessing their impacts and risks. In particular, the eye is a signature feature of a mature TC. Therefore, knowing the eyes' locations and movements is crucial for both operational weather forecasts and climate risk assessments. Recently, a multilevel robustness framework has been introduced to study the critical points of time-varying vector fields. The framework quantifies the robustness (i.e., structural stability) of critical points across varying neighborhoods. By relating the multilevel robustness with critical point tracking, the framework has demonstrated its potential in cyclone tracking. An advantage is that it identifies cyclonic features using only 2D wind vector fields, which is encouraging as most tracking algorithms require multiple dynamic and thermodynamic variables at different altitudes. A disadvantage is that the framework does not scale well computationally for datasets containing a large number of cyclones. This paper introduces a topologically robust physics-informed tracking framework (TROPHY) for TC tracking. The main idea is to integrate physical knowledge of TC to drastically improve the computational efficiency of multilevel robustness framework for large-scale climate datasets. First, during preprocessing, we propose a physics-informed feature selection strategy to filter 90% of critical points that are short-lived and have low stability, thus preserving good candidates for TC tracking. Second, during in-processing, we impose constraints during the multilevel robustness computation to focus only on physics-informed neighborhoods of TCs. We apply TROPHY to 30 years of 2D wind fields from reanalysis data in ERA5 and generate a number of TC tracks. In comparison with the observed tracks, we demonstrate that TROPHY can capture TC characteristics (e.g., frequency, intensity, duration, latitudes with maximum intensity, and genesis) that are comparable to and sometimes even better than a well-validated TC tracking algorithm that requires multiple dynamic and thermodynamic scalar fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326905",
            "id": "r_395",
            "s_ids": [
                "s_1581",
                "s_986",
                "s_1468",
                "s_836",
                "s_685"
            ],
            "type": "rich",
            "x": 7.382287979125977,
            "y": 5.392508029937744
        },
        {
            "title": "Are We Closing the Loop Yet? Gaps in the Generalizability of VIS4ML Research",
            "data": "Visualization for machine learning (VIS4ML) research aims to help experts apply their prior knowledge to develop, understand, and improve the performance of machine learning models. In conceiving VIS4ML systems, researchers characterize the nature of human knowledge to support human-in-the-loop tasks, design interactive visualizations to make ML components interpretable and elicit knowledge, and evaluate the effectiveness of human-model interchange. We survey recent VIS4ML papers to assess the generalizability of research contributions and claims in enabling human-in-the-loop ML. Our results show potential gaps between the current scope of VIS4ML research and aspirations for its use in practice. We find that while papers motivate that VIS4ML systems are applicable beyond the specific conditions studied, conclusions are often overfitted to non-representative scenarios, are based on interactions with a small set of ML experts and well-understood datasets, fail to acknowledge crucial dependencies, and hinge on decisions that lack justification. We discuss approaches to close the gap between aspirations and research claims and suggest documentation practices to report generality constraints that better acknowledge the exploratory nature of VIS4ML research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326591",
            "id": "r_396",
            "s_ids": [
                "s_1339",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.374890089035034,
            "y": 16.753334045410156
        },
        {
            "title": "A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision",
            "data": "Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection. In this paper, we develop an open-source visual analysis tool, Uni-Evaluator, to support a unified model evaluation for classification, object detection, and instance segmentation in computer vision. The key idea behind our method is to formulate both discrete and continuous predictions in different tasks as unified probability distributions. Based on these distributions, we develop 1) a matrix-based visualization to provide an overview of model performance; 2) a table visualization to identify the problematic data subsets where the model performs poorly; 3) a grid visualization to display the samples of interest. These visualizations work together to facilitate the model evaluation from a global overview to individual samples. Two case studies demonstrate the effectiveness of Uni-Evaluator in evaluating model performance and making informed improvements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326588",
            "id": "r_397",
            "s_ids": [
                "s_803",
                "s_1164",
                "s_1174",
                "s_906",
                "s_148",
                "s_465",
                "s_1523",
                "s_1014",
                "s_1486",
                "s_348"
            ],
            "type": "rich",
            "x": -9.004631042480469,
            "y": 16.72317886352539
        },
        {
            "title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations",
            "data": "Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge physio-chemical research. They provide critical insights into how a physical system evolves over time given a model of interatomic interactions. Understanding a system's evolution is key to selecting the best candidates for new drugs, materials for manufacturing, and countless other practical applications. With today's technology, these simulations can encompass millions of unit transitions between discrete molecular structures, spanning up to several milliseconds of real time. Attempting to perform a brute-force analysis with data-sets of this size is not only computationally impractical, but would not shed light on the physically-relevant features of the data. Moreover, there is a need to analyze simulation ensembles in order to compare similar processes in differing environments. These problems call for an approach that is analytically transparent, computationally efficient, and flexible enough to handle the variety found in materials-based research. In order to address these problems, we introduce MolSieve, a progressive visual analytics system that enables the comparison of multiple long-duration simulations. Using MolSieve, analysts are able to quickly identify and compare regions of interest within immense simulations through its combination of control charts, data-reduction techniques, and highly informative visual components. A simple programming interface is provided which allows experts to fit MolSieve to their needs. To demonstrate the efficacy of our approach, we present two case studies of MolSieve and report on findings from domain collaborators.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326584",
            "id": "r_398",
            "s_ids": [
                "s_871",
                "s_1097",
                "s_534",
                "s_1166",
                "s_815"
            ],
            "type": "rich",
            "x": -4.252939701080322,
            "y": 21.073904037475586
        },
        {
            "title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization",
            "data": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326583",
            "id": "r_399",
            "s_ids": [
                "s_64",
                "s_1440",
                "s_1411",
                "s_69"
            ],
            "type": "rich",
            "x": -3.4934964179992676,
            "y": 16.787626266479492
        },
        {
            "title": "RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios",
            "data": "Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them in an occlusion-free manner while keeping visual links legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-LABEL, a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-LABEL considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user's viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-LABEL in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-LABEL excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326568",
            "id": "r_400",
            "s_ids": [
                "s_1589",
                "s_1298",
                "s_416",
                "s_41",
                "s_689",
                "s_1486"
            ],
            "type": "rich",
            "x": -8.659092903137207,
            "y": 16.650089263916016
        },
        {
            "title": "Perception of Line Attributes for Visualization",
            "data": "Line attributes such as width and dashing are commonly used to encode information. However, many questions on the perception of line attributes remain, such as how many levels of attribute variation can be distinguished or which line attributes are the preferred choices for which tasks. We conducted three studies to develop guidelines for using stylized lines to encode scalar data. In our first study, participants drew stylized lines to encode uncertainty information. Uncertainty is usually visualized alongside other data. Therefore, alternative visual channels are important for the visualization of uncertainty. Additionally, uncertainty\u2014e.g., in weather forecasts\u2014is a familiar topic to most people. Thus, we picked it for our visualization scenarios in study 1. We used the results of our study to determine the most common line attributes for drawing uncertainty: Dashing, luminance, wave amplitude, and width. While those line attributes were especially common for drawing uncertainty, they are also commonly used in other areas. In studies 2 and 3, we investigated the discriminability of the line attributes determined in study 1. Studies 2 and 3 did not require specific application areas; thus, their results apply to visualizing any scalar data in line attributes. We evaluated the just-noticeable differences (JND) and derived recommendations for perceptually distinct line levels. We found that participants could discriminate considerably more levels for the line attribute width than for wave amplitude, dashing, or luminance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326523",
            "id": "r_401",
            "s_ids": [
                "s_1550",
                "s_1576",
                "s_1465",
                "s_1037",
                "s_722",
                "s_1598"
            ],
            "type": "rich",
            "x": -3.768678665161133,
            "y": 19.93783950805664
        },
        {
            "title": "NL2Color: Refining Color Palettes for Charts with Natural Language",
            "data": "Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to refine chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study ($\\mathrm{N}=71$) and a within-subjects user study ($\\mathrm{N}=12$). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326522",
            "id": "r_402",
            "s_ids": [
                "s_730",
                "s_1010",
                "s_1436",
                "s_758",
                "s_404",
                "s_672",
                "s_1556"
            ],
            "type": "rich",
            "x": -10.82747745513916,
            "y": 18.385696411132812
        },
        {
            "title": "Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study",
            "data": "Understanding one's audience is foundational to creating high impact visualization designs. However, individual differences and cognitive abilities influence interactions with information visualization. Different user needs and abilities suggest that an individual's background could influence cognitive performance and interactions with visuals in a systematic way. This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. We measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law & political science, and math & computer science. We conducted an online study with 90 participants using an established psychometric test to assess spatial visualization ability, and bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. Accuracy and response times varied with domain across chart types and task difficulty. We found that accuracy and time correlate with spatial visualization level, and education in math & computer science can indicate higher spatial visualization. Additionally, we found that motivational differences between domains could contribute to increased levels of accuracy. Our findings indicate discipline not only affects user needs and interactions with data visualization, but also cognitive traits. Our results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209491",
            "id": "r_403",
            "s_ids": [
                "s_171",
                "s_1490",
                "s_1282"
            ],
            "type": "rich",
            "x": -7.1711225509643555,
            "y": 16.854089736938477
        },
        {
            "title": "A Framework for Multiclass Contour Visualization",
            "data": "Multiclass contour visualization is often used to interpret complex data attributes in such fields as weather forecasting, computational fluid dynamics, and artificial intelligence. However, effective and accurate representations of underlying data patterns and correlations can be challenging in multiclass contour visualization, primarily due to the inevitable visual cluttering and occlusions when the number of classes is significant. To address this issue, visualization design must carefully choose design parameters to make visualization more comprehensible. With this goal in mind, we proposed a framework for multiclass contour visualization. The framework has two components: a set of four visualization design parameters, which are developed based on an extensive review of literature on contour visualization, and a declarative domain-specific language (DSL) for creating multiclass contour rendering, which enables a fast exploration of those design parameters. A task-oriented user study was conducted to assess how those design parameters affect users' interpretations of real-world data. The study results offered some suggestions on the value choices of design parameters in multiclass contour visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209482",
            "id": "r_404",
            "s_ids": [
                "s_349",
                "s_845",
                "s_1409",
                "s_1036",
                "s_518",
                "s_1338"
            ],
            "type": "rich",
            "x": -10.3424654006958,
            "y": 19.623945236206055
        },
        {
            "title": "Level Set Restricted Voronoi Tessellation for Large scale Spatial Statistical Analysis",
            "data": "Spatial statistical analysis of multivariate volumetric data can be challenging due to scale, complexity, and occlusion. Advances in topological segmentation, feature extraction, and statistical summarization have helped overcome the challenges. This work introduces a new spatial statistical decomposition method based on level sets, connected components, and a novel variation of the restricted centroidal Voronoi tessellation that is better suited for spatial statistical decomposition and parallel efficiency. The resulting data structures organize features into a coherent nested hierarchy to support flexible and efficient out-of-core region-of-interest extraction. Next, we provide an efficient parallel implementation. Finally, an interactive visualization system based on this approach is designed and then applied to turbulent combustion data. The combined approach enables an interactive spatial statistical analysis workflow for large-scale data with a top-down approach through multiple-levels-of-detail that links phase space statistics with spatial features.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209473",
            "id": "r_405",
            "s_ids": [
                "s_1398",
                "s_868",
                "s_725",
                "s_500",
                "s_1256",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7683596014976501,
            "y": 17.969942092895508
        },
        {
            "title": "A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data",
            "data": "Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a Scanner Deeply, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper's contribution.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209472",
            "id": "r_406",
            "s_ids": [
                "s_209",
                "s_556",
                "s_452",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.009779214859009,
            "y": 16.0128231048584
        },
        {
            "title": "Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modeling",
            "data": "Computational modeling is a commonly used technology in many scientific disciplines and has played a noticeable role in combating the COVID-19 pandemic. Modeling scientists conduct sensitivity analysis frequently to observe and monitor the behavior of a model during its development and deployment. The traditional algorithmic ranking of sensitivity of different parameters usually does not provide modeling scientists with sufficient information to understand the interactions between different parameters and model outputs, while modeling scientists need to observe a large number of model runs in order to gain actionable information for parameter optimization. To address the above challenge, we developed and compared two visual analytics approaches, namely: algorithm-centric and visualization-assisted, and visualization-centric and algorithm-assisted. We evaluated the two approaches based on a structured analysis of different tasks in visual sensitivity analysis as well as the feedback of domain experts. While the work was carried out in the context of epidemiological modeling, the two approaches developed in this work are directly applicable to a variety of modeling processes featuring time series outputs, and can be extended to work with models with other types of outputs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209464",
            "id": "r_407",
            "s_ids": [
                "s_1491",
                "s_1282",
                "s_1604",
                "s_1489",
                "s_1153",
                "s_937",
                "s_96",
                "s_298"
            ],
            "type": "rich",
            "x": -7.311141490936279,
            "y": 16.852439880371094
        },
        {
            "title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
            "data": "Projection and ranking are frequently used analysis techniques in multi-attribute data exploration. Both families of techniques help analysts with tasks such as identifying similarities between observations and determining ordered subgroups, and have shown good performances in multi-attribute data exploration. However, they often exhibit problems such as distorted projection layouts, obscure semantic interpretations, and non-intuitive effects produced by selecting a subset of (weighted) attributes. Moreover, few studies have attempted to combine projection and ranking into the same exploration space to complement each other's strengths and weaknesses. For this reason, we propose RankAxis, a visual analytics system that systematically combines projection and ranking to facilitate the mutual interpretation of these two techniques and jointly support multi-attribute data exploration. A real-world case study, expert feedback, and a user study demonstrate the efficacy of RankAxis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209463",
            "id": "r_408",
            "s_ids": [
                "s_1552",
                "s_1466",
                "s_1304",
                "s_115",
                "s_1556",
                "s_549"
            ],
            "type": "rich",
            "x": -11.019126892089844,
            "y": 19.284494400024414
        },
        {
            "title": "Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization",
            "data": "Large-scale scientific data, such as weather and climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, in particular logical AND operations, which is challenging for large data and many attributes. Many general data structures for this problem are built for and scale with a fixed number of attributes, and scalability of joint queries with arbitrary attribute subsets remains a significant problem. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size. This is particularly important for joint attribute queries, where the query output is usually much smaller than the whole data set. Additionally, our approach can split query evaluation between user interaction and rendering, achieving much better scalability for interactive visualization than the previous state of the art. Furthermore, even when a multi-resolution strategy is used for visualization, queries are jointly evaluated at the finest data granularity, because our framework does not limit query accuracy to a fixed spatial subdivision.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209439",
            "id": "r_409",
            "s_ids": [
                "s_33",
                "s_232",
                "s_315",
                "s_289",
                "s_1003",
                "s_388"
            ],
            "type": "rich",
            "x": -8.793378829956055,
            "y": 16.282915115356445
        },
        {
            "title": "IDLat: An Importance-Driven Latent Generation Method for Scientific Data",
            "data": "Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209419",
            "id": "r_410",
            "s_ids": [
                "s_172",
                "s_773",
                "s_1265",
                "s_910",
                "s_405"
            ],
            "type": "rich",
            "x": 7.5171217918396,
            "y": 5.257754325866699
        },
        {
            "title": "VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations",
            "data": "We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. Surrogate-enabled parameter space exploration allows domain scientists to preview simulation results without having to run a large number of computationally costly simulations. Limited by computational resources, however, existing surrogate models may not produce previews with sufficient resolution for visualization and analysis. To improve the efficient use of computational resources and support high-resolution exploration, we perform ray casting from different viewpoints to collect samples and produce compact latent representations. This latent encoding process reduces the cost of surrogate model training while maintaining the output quality. In the model training stage, we select viewpoints to cover the whole viewing sphere and train corresponding VDL-Surrogate models for the selected viewpoints. In the model inference stage, we predict the latent representations at previously selected viewpoints and decode the latent representations to data space. For any given viewpoint, we make interpolations over decoded data at selected viewpoints and generate visualizations with user-specified visual mappings. We show the effectiveness and efficiency of VDL-Surrogate in cosmological and ocean simulations with quantitative and qualitative evaluations. Source code is publicly available at https://github.com/trainsn/VDL-Surrogate.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209413",
            "id": "r_411",
            "s_ids": [
                "s_1437",
                "s_1265",
                "s_773",
                "s_986",
                "s_749",
                "s_405"
            ],
            "type": "rich",
            "x": 7.556617736816406,
            "y": 5.219920635223389
        },
        {
            "title": "GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models",
            "data": "The Gaussian mixture model (GMM) describes the distribution of random variables from several different populations. GMMs have widespread applications in probability theory, statistics, machine learning for unsupervised cluster analysis and topic modeling, as well as in deep learning pipelines. So far, few efforts have been made to explore the underlying point distribution in combination with the GMMs, in particular when the data becomes high-dimensional and when the GMMs are composed of many Gaussians. We present an analysis tool comprising various GPU-based visualization techniques to explore such complex GMMs. To facilitate the exploration of high-dimensional data, we provide a novel navigation system to analyze the underlying data. Instead of projecting the data to 2D, we utilize interactive 3D views to better support users in understanding the spatial arrangements of the Gaussian distributions. The interactive system is composed of two parts: (1) raycasting-based views that visualize cluster memberships, spatial arrangements, and support the discovery of new modes. (2) overview visualizations that enable the comparison of Gaussians with each other, as well as small multiples of different choices of basis vectors. Users are supported in their exploration with customization tools and smooth camera navigations. Our tool was developed and assessed by five domain experts, and its usefulness was evaluated with 23 participants. To demonstrate the effectiveness, we identify interesting features in several data sets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209374",
            "id": "r_412",
            "s_ids": [
                "s_1598",
                "s_936",
                "s_583",
                "s_499",
                "s_328",
                "s_1349"
            ],
            "type": "rich",
            "x": -3.5517501831054688,
            "y": 19.819477081298828
        },
        {
            "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop Analysis",
            "data": "Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of ancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022 Rosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in seeking signs of past life on Mars. Geologists measure and interpret 3D DOMs, create sedimentary logs and combine them in \u2018correlation panels\u2019 to map the extents of key geological horizons, and build a stratigraphic model to understand their position in the ancient landscape. Currently, the creation of correlation panels is completely manual and therefore time-consuming, and inflexible. With InCorr we present a visualization solution that encompasses a 3D logging tool and an interactive data-driven correlation panel that evolves with the stratigraphic analysis. For the creation of InCorr we closely cooperated with leading planetary geologists in the form of a design study. We verify our results by recreating an existing correlation analysis with InCorr and validate our correlation panel against a manually created illustration. Further, we conducted a user-study with a wider circle of geologists. Our evaluation shows that InCorr efficiently supports the domain experts in tackling their research questions and that it has the potential to significantly impact how geologists work with digital outcrop representations in general.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030409",
            "id": "r_413",
            "s_ids": [
                "s_108",
                "s_780",
                "s_340",
                "s_16",
                "s_1253",
                "s_614"
            ],
            "type": "rich",
            "x": -6.698816299438477,
            "y": 14.222657203674316
        },
        {
            "title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
            "data": "Modern automobiles have evolved from just being mechanical machines to having full-fledged electronics systems that enhance vehicle dynamics and driver experience. However, these complex hardware and software systems, if not properly designed, can experience failures that can compromise the safety of the vehicle, its occupants, and the surrounding environment. For example, a system to activate the brakes to avoid a collision saves lives when it functions properly, but could lead to tragic outcomes if the brakes were applied in a way that's inconsistent with the design. Broadly speaking, the analysis performed to minimize such risks falls into a systems engineering domain called Functional Safety. In this paper, we present SafetyLens, a visual data analysis tool to assist engineers and analysts in analyzing automotive Functional Safety datasets. SafetyLens combines techniques including network exploration and visual comparison to help analysts perform domain-specific tasks. This paper presents the design study with domain experts that resulted in the design guidelines, the tool, and user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2020.3030382",
            "id": "r_414",
            "s_ids": [
                "s_1027",
                "s_751",
                "s_1599"
            ],
            "type": "rich",
            "x": -4.767068862915039,
            "y": 15.57137680053711
        },
        {
            "title": "From Instruction to Insight: Exploring the Functional and Semantic Roles of Text in Interactive Dashboards",
            "data": "There is increased interest in understanding the interplay between text and visuals in the field of data visualization. However, this attention has predominantly been on the use of text in standalone visualizations (such as text annotation overlays) or augmenting text stories supported by a series of independent views. In this paper, we shift from the traditional focus on single-chart annotations to characterize the nuanced but crucial communication role of text in the complex environment of interactive dashboards. Through a survey and analysis of 190 dashboards in the wild, plus 13 expert interview sessions with experienced dashboard authors, we highlight the distinctive nature of text as an integral component of the dashboard experience, while delving into the categories, semantic levels, and functional roles of text, and exploring how these text elements are coalesced by dashboard authors to guide and inform dashboard users. Our contributions are threefold. First, we distill qualitative and quantitative findings from our studies to characterize current practices of text use in dashboards, including a categorization of text-based components and design patterns. Second, we leverage current practices and existing literature to propose, discuss, and validate recommended practices for text in dashboards, embodied as a set of 12 heuristics that underscore the semantic and functional role of text in offering navigational cues, contextualizing data insights, supporting reading order, among other concerns. Third, we reflect on our findings to identify gaps and propose opportunities for data visualization researchers to push the boundaries on text usage for dashboards, from authoring support and interactivity to text generation and content personalization. Our research underscores the significance of elevating text as a first-class citizen in data visualization, and the need to support the inclusion of textual components and their interactive affordances in dashboard design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456601",
            "id": "r_415",
            "s_ids": [
                "s_814",
                "s_536"
            ],
            "type": "rich",
            "x": -4.111392021179199,
            "y": 16.250934600830078
        },
        {
            "title": "Learnable and Expressive Visualization Authoring Through Blended Interfaces",
            "data": "A wide range of visualization authoring interfaces enable the creation of highly customized visualizations. However, prioritizing expressiveness often impedes the learnability of the authoring interface. The diversity of users, such as varying computational skills and prior experiences in user interfaces, makes it even more challenging for a single authoring interface to satisfy the needs of a broad audience. In this paper, we introduce a framework to balance learnability and expressivity in a visualization authoring system. Adopting insights from learnability studies, such as multimodal interaction and visualization literacy, we explore the design space of blending multiple visualization authoring interfaces for supporting authoring tasks in a complementary and flexible manner. To evaluate the effectiveness of blending interfaces, we implemented a proof-of-concept system, Blace, that combines four common visualization authoring interfaces\u2013template-based, shelf configuration, natural language, and code editor\u2013that are tightly linked to one another to help users easily relate unfamiliar interfaces to more familiar ones. Using the system, we conducted a user study with 12 domain experts who regularly visualize genomics data as part of their analysis workflow. Participants with varied visualization and programming backgrounds were able to successfully reproduce unfamiliar visualization examples without a guided tutorial in the study. Feedback from a post-study qualitative questionnaire further suggests that blending interfaces enabled participants to learn the system easily and assisted them in confidently editing unfamiliar visualization grammar in the code editor, enabling expressive customization. Reflecting on our study results and the design of our system, we discuss the different interaction patterns that we identified and design implications for blending visualization authoring interfaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456598",
            "id": "r_416",
            "s_ids": [
                "s_1266",
                "s_1402",
                "s_178",
                "s_260",
                "s_393"
            ],
            "type": "rich",
            "x": -8.413616180419922,
            "y": 17.021127700805664
        },
        {
            "title": "A Multi-Level Task Framework for Event Sequence Analysis",
            "data": "Despite the development of numerous visual analytics tools for event sequence data across various domains, including but not limited to healthcare, digital marketing, and user behavior analysis, comparing these domain-specific investigations and transferring the results to new datasets and problem areas remain challenging. Task abstractions can help us go beyond domain-specific details, but existing visualization task abstractions are insufficient for event sequence visual analytics because they primarily focus on multivariate datasets and often overlook automated analytical techniques. To address this gap, we propose a domain-agnostic multi-level task framework for event sequence analytics, derived from an analysis of 58 papers that present event sequence visualization systems. Our framework consists of four levels: objective, intent, strategy, and technique. Overall objectives identify the main goals of analysis. Intents comprises five high-level approaches adopted at each analysis step: augment data, simplify data, configure data, configure visualization, and manage provenance. Each intent is accomplished through a number of strategies, for instance, data simplification can be achieved through aggregation, summarization, or segmentation. Finally, each strategy can be implemented by a set of techniques depending on the input and output components. We further show that each technique can be expressed through a quartet of action-input-output-criteria. We demonstrate the framework's descriptive power through case studies and discuss its similarities and differences with previous event sequence task taxonomies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456510",
            "id": "r_417",
            "s_ids": [
                "s_1532",
                "s_1592",
                "s_411",
                "s_1185"
            ],
            "type": "rich",
            "x": -4.3561482429504395,
            "y": 17.012258529663086
        },
        {
            "title": "Unveiling How Examples Shape Visualization Design Outcomes",
            "data": "Visualization designers (e.g., journalists or data analysts) often rely on examples to explore the space of possible designs, yet we have little insight into how examples shape data visualization design outcomes. While the effects of examples have been studied in other disciplines, such as web design or engineering, the results are not readily applicable to visualization due to inconsistencies in findings and challenges unique to visualization design. Towards bridging this gap, we conduct an exploratory experiment involving 32 data visualization designers focusing on the influence of five factors (timing, quantity, diversity, data topic similarity, and data schema similarity) on objectively measurable design outcomes (e.g., numbers of designs and idea transfers). Our quantitative analysis shows that when examples are introduced after initial brainstorming, designers curate examples with topics less similar to the dataset they are working on and produce more designs with a high variation in visualization components. Also, designers copy more ideas from examples with higher data schema similarities. Our qualitative analysis of participants' thought processes provides insights into why designers incorporate examples into their designs, revealing potential factors that have not been previously investigated. Finally, we discuss how our results inform how designers may use examples during design ideation as well as future research on quantifying designs and supporting example-based visualization design. All supplemental materials are available in our OSF repo.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456407",
            "id": "r_418",
            "s_ids": [
                "s_462",
                "s_1458",
                "s_119",
                "s_617",
                "s_1025",
                "s_1185"
            ],
            "type": "rich",
            "x": -4.244604110717773,
            "y": 16.94487953186035
        },
        {
            "title": "Cell2Cell: Explorative Cell Interaction Analysis in Multi-Volumetric Tissue Data",
            "data": "We present Cell2Cell, a novel visual analytics approach for quantifying and visualizing networks of cell-cell interactions in three-dimensional (3D) multi-channel cancerous tissue data. By analyzing cellular interactions, biomedical experts can gain a more accurate understanding of the intricate relationships between cancer and immune cells. Recent methods have focused on inferring interaction based on the proximity of cells in low-resolution 2D multi-channel imaging data. By contrast, we analyze cell interactions by quantifying the presence and levels of specific proteins within a tissue sample (protein expressions) extracted from high-resolution 3D multi-channel volume data. Such analyses have a strong exploratory nature and require a tight integration of domain experts in the analysis loop to leverage their deep knowledge. We propose two complementary semi-automated approaches to cope with the increasing size and complexity of the data interactively: On the one hand, we interpret cell-to-cell interactions as edges in a cell graph and analyze the image signal (protein expressions) along those edges, using spatial as well as abstract visualizations. Complementary, we propose a cell-centered approach, enabling scientists to visually analyze polarized distributions of proteins in three dimensions, which also captures neighboring cells with biochemical and cell biological consequences. We evaluate our application in three case studies, where biologists and medical experts use Cell2Cell to investigate tumor micro-environments to identify and quantify T-cell activation in human tissue data. We confirmed that our tool can fully solve the use cases and enables a streamlined and detailed analysis of cell-cell interactions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456406",
            "id": "r_419",
            "s_ids": [
                "s_1444",
                "s_1130",
                "s_1246",
                "s_1573",
                "s_393",
                "s_507",
                "s_1486",
                "s_689",
                "s_1186"
            ],
            "type": "rich",
            "x": -8.627326011657715,
            "y": 16.636865615844727
        },
        {
            "title": "Motion-Based Visual Encoding Can Improve Performance on Perceptual Tasks with Dynamic Time Series",
            "data": "Dynamic data visualizations can convey large amounts of information over time, such as using motion to depict changes in data values for multiple entities. Such dynamic displays put a demand on our visual processing capacities, yet our perception of motion is limited. Several techniques have been shown to improve the processing of dynamic displays. Staging the animation to sequentially show steps in a transition and tracing object movement by displaying trajectory histories can improve processing by reducing the cognitive load. In this paper, We examine the effectiveness of staging and tracing in dynamic displays. We showed participants animated line charts depicting the movements of lines and asked them to identify the line with the highest mean and variance. We manipulated the animation to display the lines with or without staging, tracing and history, and compared the results to a static chart as a control. Results showed that tracing and staging are preferred by participants, and improve their performance in mean and variance tasks respectively. They also preferred display time 3 times shorter when staging is used. Also, encoding animation speed with mean and variance in congruent tasks is associated with higher accuracy. These findings help inform real-world best practices for building dynamic displays. The supplementary materials can be found at https://osf.io/8c95v/",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456405",
            "id": "r_420",
            "s_ids": [
                "s_1178",
                "s_762",
                "s_982",
                "s_1184"
            ],
            "type": "rich",
            "x": -3.9633841514587402,
            "y": 16.695096969604492
        },
        {
            "title": "DiffFit: Visually-Guided Differentiable Fitting of Molecule Structures to a Cryo-EM Map",
            "data": "We introduce DiffFit, a differentiable algorithm for fitting protein atomistic structures into an experimental reconstructed Cryo-Electron Microscopy (cryo-EM) volume map. In structural biology, this process is necessary to semi-automatically composite large mesoscale models of complex protein assemblies and complete cellular structures that are based on measured cryo-EM data. The current approaches require manual fitting in three dimensions to start, resulting in approximately aligned structures followed by an automated fine-tuning of the alignment. The DiffFit approach enables domain scientists to fit new structures automatically and visualize the results for inspection and interactive revision. The fitting begins with differentiable three-dimensional (3D) rigid transformations of the protein atom coordinates followed by sampling the density values at the atom coordinates from the target cryo-EM volume. To ensure a meaningful correlation between the sampled densities and the protein structure, we proposed a novel loss function based on a multi-resolution volume-array approach and the exploitation of the negative space. This loss function serves as a critical metric for assessing the fitting quality, ensuring the fitting accuracy and an improved visualization of the results. We assessed the placement quality of DiffFit with several large, realistic datasets and found it to be superior to that of previous methods. We further evaluated our method in two use cases: automating the integration of known composite structures into larger protein complexes and facilitating the fitting of predicted protein domains into volume densities to aid researchers in identifying unknown proteins. We implemented our algorithm as an open-source plugin (github.com/nanovis/DiffFit) in ChimeraX, a leading visualization software in the field. All supplemental materials are available at osf. io/5tx4q.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456404",
            "id": "r_421",
            "s_ids": [
                "s_723",
                "s_1207",
                "s_1606",
                "s_694",
                "s_1323",
                "s_510"
            ],
            "type": "rich",
            "x": -6.212699890136719,
            "y": 18.954517364501953
        },
        {
            "title": "The Effect of Visual Aids on Reading Numeric Data Tables",
            "data": "Data tables are one of the most common ways in which people encounter data. Although mostly built with text and numbers, data tables have a spatial layout and often exhibit visual elements meant to facilitate their reading. Surprisingly, there is an empirical knowledge gap on how people read tables and how different visual aids affect people's reading of tables. In this work, we seek to address this vacuum through a controlled study. We asked participants to repeatedly perform four different tasks with four table representation conditions (plain tables, tables with zebra striping, tables with cell background color encoding cell value, and tables with in-cell bars with lengths encoding cell value). We analyzed completion time, error rate, gaze-tracking data, mouse movement and participant preferences. We found that color and bar encodings help for finding maximum values. For a more complex task (comparison of proportional differences) color and bar helped less than zebra striping. We also characterize typical human behavior for the four tasks. These findings inform the design of tables and research directions for improving presentation of data in tabular form.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456403",
            "id": "r_422",
            "s_ids": [
                "s_487",
                "s_386",
                "s_782"
            ],
            "type": "rich",
            "x": -6.150609493255615,
            "y": 17.36219024658203
        },
        {
            "title": "VMC: A Grammar for Visualizing Statistical Model Checks",
            "data": "Visualizations play a critical role in validating and improving statistical models. However, the design space of model check visualizations is not well understood, making it difficult for authors to explore and specify effective graphical model checks. VMC defines a model check visualization using four components: (1) samples of distributions of checkable quantities generated from the model, including predictive distributions for new data and distributions of model parameters; (2) transformations on observed data to facilitate comparison; (3) visual representations of distributions; and (4) layouts to facilitate comparing model samples and observed data. We contribute an implementation of VMC as an R package. We validate VMC by reproducing a set of canonical model check examples, and show how using VMC to generate model checks reduces the edit distance between visualizations relative to existing visualization toolkits. The findings of an interview study with three expert modelers who used VMC highlight challenges and opportunities for encouraging exploration of correct, effective model check visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456402",
            "id": "r_423",
            "s_ids": [
                "s_270",
                "s_875",
                "s_1031",
                "s_1411"
            ],
            "type": "rich",
            "x": -3.3670296669006348,
            "y": 16.860673904418945
        },
        {
            "title": "Visual Support for the Loop Grafting Workflow on Proteins",
            "data": "In understanding and redesigning the function of proteins in modern biochemistry, protein engineers are increasingly focusing on exploring regions in proteins called loops. Analyzing various characteristics of these regions helps the experts design the transfer of the desired function from one protein to another. This process is denoted as loop grafting. We designed a set of interactive visualizations that provide experts with visual support through all the loop grafting pipeline steps. The workflow is divided into several phases, reflecting the steps of the pipeline. Each phase is supported by a specific set of abstracted 2D visual representations of proteins and their loops that are interactively linked with the 3D View of proteins. By sequentially passing through the individual phases, the user shapes the list of loops that are potential candidates for loop grafting. Finally, the actual in-silico insertion of the loop candidates from one protein to the other is performed, and the results are visually presented to the user. In this way, the fully computational rational design of proteins and their loops results in newly designed protein structures that can be further assembled and tested through in-vitro experiments. We showcase the contribution of our visual support design on a real case scenario changing the enantiomer selectivity of the engineered enzyme. Moreover, we provide the readers with the experts' feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456401",
            "id": "r_424",
            "s_ids": [
                "s_290",
                "s_484",
                "s_1378",
                "s_1001",
                "s_555",
                "s_1499",
                "s_333",
                "s_1354"
            ],
            "type": "rich",
            "x": -5.791286468505859,
            "y": 19.45684051513672
        },
        {
            "title": "Precise Embodied Data Selection with Haptic Feedback while Retaining Room-Scale Visualisation Context",
            "data": "Room-scale immersive data visualisations provide viewers a wide-scale overview of a large dataset, but to interact precisely with individual data points they typically have to navigate to change their point of view. In traditional screen-based visualisations, focus-and-context techniques allow visualisation users to keep a full dataset in view while making detailed selections. Such techniques have been studied extensively on desktop to allow precise selection within large data sets, but they have not been explored in immersive 3D modalities. In this paper we develop a novel immersive focus-and-context technique based on a \u201cmagic portal\u201d metaphor adapted specifically for data visualisation scenarios. An extendable-hand interaction technique is used to place a portal close to the region of interest. The other end of the portal then opens comfortably within the user's physical reach such that they can reach through to precisely select individual data points. Through a controlled study with 12 participants, we find strong evidence that portals reduce overshoots in selection and overall hand trajectory length, reducing arm and shoulder fatigue compared to ranged interaction without the portal. The portals also enable us to use a robot arm to provide haptic feedback for data within the limited volume of the portal region. In a second study with another 12 participants we found that haptics provided a positive experience (qualitative feedback) but did not significantly reduce fatigue. We demonstrate applications for portal-based selection through two use-case scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456399",
            "id": "r_425",
            "s_ids": [
                "s_1219",
                "s_120",
                "s_1021",
                "s_73",
                "s_210"
            ],
            "type": "rich",
            "x": -7.498112678527832,
            "y": 17.828269958496094
        },
        {
            "title": "Impact of Vertical Scaling on Normal Probability Density Function Plots",
            "data": "Probability density function (PDF) curves are among the few charts on a Cartesian coordinate system that are commonly presented without y-axes. This design decision may be due to the lack of relevance of vertical scaling in normal PDFs. In fact, as long as two normal PDFs have the same means and standard deviations (SDs), they can be scaled to occupy different amounts of vertical space while still remaining statistically identical. Because unfixed PDF height increases as SD decreases, visualization designers may find themselves tempted to vertically shrink low-SD PDFs to avoid occlusion or save white space in their figures. Although irregular vertical scaling has been explored in bar and line charts, the visualization community has yet to investigate how this visual manipulation may affect reader comparisons of PDFs. In this paper, we present two preregistered experiments (n = 600, n = 401) that systematically demonstrate that vertical scaling can lead to misinterpretations of PDFs. We also test visual interventions to mitigate misinterpretation. In some contexts, we find including a y-axis can help reduce this effect. Overall, we find that keeping vertical scaling consistent, and therefore maintaining equal pixel areas under PDF curves, results in the highest likelihood of accurate comparisons. Our findings provide insights into the impact of vertical scaling on PDFs, and reveal the complicated nature of proportional area comparisons.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456396",
            "id": "r_426",
            "s_ids": [
                "s_605",
                "s_229"
            ],
            "type": "rich",
            "x": -3.407280445098877,
            "y": 17.540597915649414
        },
        {
            "title": "Rapid and Precise Topological Comparison with Merge Tree Neural Networks",
            "data": "Merge trees are a valuable tool in the scientific visualization of scalar fields; however, current methods for merge tree comparisons are computationally expensive, primarily due to the exhaustive matching between tree nodes. To address this challenge, we introduce the Merge Tree Neural Network (MTNN), a learned neural network model designed for merge tree comparison. The MTNN enables rapid and high-quality similarity computation. We first demonstrate how to train graph neural networks, which emerged as effective encoders for graphs, in order to produce embeddings of merge trees in vector spaces for efficient similarity comparison. Next, we formulate the novel MTNN model that further improves the similarity comparisons by integrating the tree and node embeddings with a new topological attention mechanism. We demonstrate the effectiveness of our model on real-world data in different domains and examine our model's generalizability across various datasets. Our experimental analysis demonstrates our approach's superiority in accuracy and efficiency. In particular, we speed up the prior state-of-the-art by more than 100\u00d7 on the benchmark datasets while maintaining an error rate below 0.1%.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456395",
            "id": "r_427",
            "s_ids": [
                "s_370",
                "s_29",
                "s_102",
                "s_444"
            ],
            "type": "rich",
            "x": -15.092277526855469,
            "y": 7.082053184509277
        },
        {
            "title": "Uncertainty Visualization of Critical Points of 2D Scalar Fields for Parametric and Nonparametric Probabilistic Models",
            "data": "This paper presents a novel end-to-end framework for closed-form computation and visualization of critical point uncertainty in 2D uncertain scalar fields. Critical points are fundamental topological descriptors used in the visualization and analysis of scalar fields. The uncertainty inherent in data (e.g., observational and experimental data, approximations in simulations, and compression), however, creates uncertainty regarding critical point positions. Uncertainty in critical point positions, therefore, cannot be ignored, given their impact on downstream data analysis tasks. In this work, we study uncertainty in critical points as a function of uncertainty in data modeled with probability distributions. Although Monte Carlo (MC) sampling techniques have been used in prior studies to quantify critical point uncertainty, they are often expensive and are infrequently used in production-quality visualization software. We, therefore, propose a new end-to-end framework to address these challenges that comprises a threefold contribution. First, we derive the critical point uncertainty in closed form, which is more accurate and efficient than the conventional MC sampling methods. Specifically, we provide the closed-form and semianalytical (a mix of closed-form and MC methods) solutions for parametric (e.g., uniform, Epanechnikov) and nonparametric models (e.g., histograms) with finite support. Second, we accelerate critical point probability computations using a parallel implementation with the VTK-m library, which is platform portable. Finally, we demonstrate the integration of our implementation with the ParaView software system to demonstrate near-real-time results for real datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456393",
            "id": "r_428",
            "s_ids": [
                "s_1617",
                "s_1350",
                "s_1399",
                "s_1375",
                "s_861",
                "s_890",
                "s_395",
                "s_542"
            ],
            "type": "rich",
            "x": -1.8518898487091064,
            "y": 17.9200439453125
        },
        {
            "title": "CSLens: Towards Better Deploying Charging Stations via Visual Analytics \u2014 a Coupled Networks Perspective",
            "data": "In recent years, the global adoption of electric vehicles (EVs) has surged, prompting a corresponding rise in the installation of charging stations. This proliferation has underscored the importance of expediting the deployment of charging infrastructure. Both academia and industry have thus devoted to addressing the charging station location problem (CSLP) to streamline this process. However, prevailing algorithms addressing CSLP are hampered by restrictive assumptions and computational overhead, leading to a dearth of comprehensive evaluations in the spatiotemporal dimensions. Consequently, their practical viability is restricted. Moreover, the placement of charging stations exerts a significant impact on both the road network and the power grid, which necessitates the evaluation of the potential post-deployment impacts on these interconnected networks holistically. In this study, we propose CSLens, a visual analytics system designed to inform charging station deployment decisions through the lens of coupled transportation and power networks. CSLens offers multiple visualizations and interactive features, empowering users to delve into the existing charging station layout, explore alternative deployment solutions, and assess the ensuring impact. To validate the efficacy of CSLens, we conducted two case studies and engaged in interviews with domain experts. Through these efforts, we substantiated the usability and practical utility of CSLens in enhancing the decision-making process surrounding charging station deployment. Our findings underscore CSLens's potential to serve as a valuable asset in navigating the complexities of charging infrastructure planning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456392",
            "id": "r_429",
            "s_ids": [
                "s_1316",
                "s_975",
                "s_216",
                "s_573",
                "s_549",
                "s_378"
            ],
            "type": "rich",
            "x": -11.066566467285156,
            "y": 19.360008239746094
        },
        {
            "title": "DimBridge: Interactive Explanation of Visual Patterns in Dimensionality Reductions with Predicate Logic",
            "data": "Dimensionality reduction techniques are widely used for visualizing high-dimensional data. However, support for interpreting patterns of dimension reduction results in the context of the original data space is often insufficient. Consequently, users may struggle to extract insights from the projections. In this paper, we introduce DimBridge, a visual analytics tool that allows users to interact with visual patterns in a projection and retrieve corresponding data patterns. DimBridge supports several interactions, allowing users to perform various analyses, from contrasting multiple clusters to explaining complex latent structures. Leveraging first-order predicate logic, DimBridge identifies subspaces in the original dimensions relevant to a queried pattern and provides an interface for users to visualize and interact with them. We demonstrate how DimBridge can help users overcome the challenges associated with interpreting visual patterns in projections.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456391",
            "id": "r_430",
            "s_ids": [
                "s_595",
                "s_99",
                "s_98",
                "s_316",
                "s_964",
                "s_366"
            ],
            "type": "rich",
            "x": -4.797143936157227,
            "y": 15.935152053833008
        },
        {
            "title": "HiRegEx: Interactive Visual Query and Exploration of Multivariate Hierarchical Data",
            "data": "When using exploratory visual analysis to examine multivariate hierarchical data, users often need to query data to narrow down the scope of analysis. However, formulating effective query expressions remains a challenge for multivariate hierarchical data, particularly when datasets become very large. To address this issue, we develop a declarative grammar, HiRegEx (Hierarchical data Regular Expression), for querying and exploring multivariate hierarchical data. Rooted in the extended multi-level task topology framework for tree visualizations (e-MLTT), HiRegEx delineates three query targets (node, path, and subtree) and two aspects for querying these targets (features and positions), and uses operators developed based on classical regular expressions for query construction. Based on the HiRegEx grammar, we develop an exploratory framework for querying and exploring multivariate hierarchical data and integrate it into the TreeQueryER prototype system. The exploratory framework includes three major components: top-down pattern specification, bottom-up data-driven inquiry, and context-creation data overview. We validate the expressiveness of HiRegEx with the tasks from the e-MLTT framework and showcase the utility and effectiveness of TreeQueryER system through a case study involving expert users in the analysis of a citation tree dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456389",
            "id": "r_431",
            "s_ids": [
                "s_385",
                "s_557",
                "s_745",
                "s_839",
                "s_645"
            ],
            "type": "rich",
            "x": -7.8159356117248535,
            "y": 18.954437255859375
        },
        {
            "title": "HuBar: A Visual Analytics Tool to Explore Human Behavior Based on fNIRS in AR Guidance Systems",
            "data": "The concept of an intelligent augmented reality (AR) assistant has significant, wide-ranging applications, with potential uses in medicine, military, and mechanics domains. Such an assistant must be able to perceive the environment and actions, reason about the environment state in relation to a given task, and seamlessly interact with the task performer. These interactions typically involve an AR headset equipped with sensors which capture video, audio, and haptic feedback. Previous works have sought to facilitate the development of intelligent AR assistants by visualizing these sensor data streams in conjunction with the assistant's perception and reasoning model outputs. However, existing visual analytics systems do not focus on user modeling or include biometric data, and are only capable of visualizing a single task session for a single performer at a time. Moreover, they typically assume a task involves linear progression from one step to the next. We propose a visual analytics system that allows users to compare performance during multiple task sessions, focusing on non-linear tasks where different step sequences can lead to success. In particular, we design visualizations for understanding user behavior through functional near-infrared spectroscopy (fNIRS) data as a proxy for perception, attention, and memory as well as corresponding motion data (acceleration, angular velocity, and gaze). We distill these insights into embedding representations that allow users to easily select groups of sessions with similar behaviors. We provide two case studies that demonstrate how to use these visualizations to gain insights about task performance using data collected during helicopter copilot training tasks. Finally, we evaluate our approach through an in-depth examination of a think-aloud experiment with five domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456388",
            "id": "r_432",
            "s_ids": [
                "s_959",
                "s_1562",
                "s_303",
                "s_304",
                "s_1405",
                "s_406",
                "s_1347",
                "s_240",
                "s_904",
                "s_1286",
                "s_1464",
                "s_1263",
                "s_1537",
                "s_428"
            ],
            "type": "rich",
            "x": -4.055593013763428,
            "y": 20.137475967407227
        },
        {
            "title": "ModalChorus: Visual Probing and Alignment of Multi-Modal Embeddings via Modal Fusion Map",
            "data": "Multi-modal embeddings form the foundation for vision-language models, such as CLIP embeddings, the most widely used text-image embeddings. However, these embeddings are vulnerable to subtle misalignment of cross-modal features, resulting in decreased model performance and diminished generalization. To address this problem, we design ModalChorus, an interactive system for visual probing and alignment of multi-modal embeddings. ModalChorus primarily offers a two-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel parametric dimensionality reduction method that integrates both metric and nonmetric objectives to enhance modality fusion; and 2) embedding alignment that allows users to interactively articulate intentions for both point-set and set-set alignments. Quantitative and qualitative comparisons for CLIP embeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and data fusion (e.g., data context map) methods demonstrate the advantages of MFM in showcasing cross-modal features over common vision-language datasets. Case studies reveal that ModalChorus can facilitate intuitive discovery of misalignment and efficient re-alignment in scenarios ranging from zero-shot classification to cross-modal retrieval and generation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456387",
            "id": "r_433",
            "s_ids": [
                "s_1193",
                "s_798",
                "s_220",
                "s_1438"
            ],
            "type": "rich",
            "x": -9.724740028381348,
            "y": 17.93138885498047
        },
        {
            "title": "Dynamic Color Assignment for Hierarchical Data",
            "data": "Assigning discriminable and harmonic colors to samples according to their class labels and spatial distribution can generate attractive visualizations and facilitate data exploration. However, as the number of classes increases, it is challenging to generate a high-quality color assignment result that accommodates all classes simultaneously. A practical solution is to organize classes into a hierarchy and then dynamically assign colors during exploration. However, existing color assignment methods fall short in generating high-quality color assignment results and dynamically aligning them with hierarchical structures. To address this issue, we develop a dynamic color assignment method for hierarchical data, which is formulated as a multi-objective optimization problem. This method simultaneously considers color discriminability, color harmony, and spatial distribution at each hierarchical level. By using the colors of parent classes to guide the color assignment of their child classes, our method further promotes both consistency and clarity across hierarchical levels. We demonstrate the effectiveness of our method in generating dynamic color assignment results with quantitative experiments and a user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456386",
            "id": "r_434",
            "s_ids": [
                "s_250",
                "s_148",
                "s_282",
                "s_1085",
                "s_348"
            ],
            "type": "rich",
            "x": -9.304814338684082,
            "y": 17.251317977905273
        },
        {
            "title": "Shape It Up: An Empirically Grounded Approach for Designing Shape Palettes",
            "data": "Shape is commonly used to distinguish between categories in multi-class scatterplots. However, existing guidelines for choosing effective shape palettes rely largely on intuition and do not consider how these needs may change as the number of categories increases. Unlike color, shapes can not be represented by a numerical space, making it difficult to propose general guidelines or design heuristics for using shape effectively. This paper presents a series of four experiments evaluating the efficiency of 39 shapes across three tasks: relative mean judgment tasks, expert preference, and correlation estimation. Our results show that conventional means for reasoning about shapes, such as filled versus unfilled, are insufficient to inform effective palette design. Further, even expert palettes vary significantly in their use of shape and corresponding effectiveness. To support effective shape palette design, we developed a model based on pairwise relations between shapes in our experiments and the number of shapes required for a given design. We embed this model in a palette design tool to give designers agency over shape selection while incorporating empirical elements of perceptual performance captured in our study. Our model advances understanding of shape perception in visualization contexts and provides practical design guidelines that can help improve categorical data encodings.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456385",
            "id": "r_435",
            "s_ids": [
                "s_36",
                "s_468",
                "s_962",
                "s_781"
            ],
            "type": "rich",
            "x": -2.1644251346588135,
            "y": 18.043842315673828
        },
        {
            "title": "Objective Lagrangian Vortex Cores and their Visual Representations",
            "data": "The numerical extraction of vortex cores from time-dependent fluid flow attracted much attention over the past decades. A commonly agreed upon vortex definition remained elusive since a proper vortex core needs to satisfy two hard constraints: it must be objective and Lagrangian. Recent methods on objectivization met the first but not the second constraint, since there was no formal guarantee that the resulting vortex coreline is indeed a pathline of the fluid flow. In this paper, we propose the first vortex core definition that is both objective and Lagrangian. Our approach restricts observer motions to follow along pathlines, which reduces the degrees of freedoms: we only need to optimize for an observer rotation that makes the observed flow as steady as possible. This optimization succeeds along Lagrangian vortex corelines and will result in a non-zero time-partial everywhere else. By performing this optimization at each point of a spatial grid, we obtain a residual scalar field, which we call vortex deviation error. The local minima on the grid serve as seed points for a gradient descent optimization that delivers sub-voxel accurate corelines. The visualization of both 2D and 3D vortex cores is based on the separation of the movement of the vortex core and the swirling flow behavior around it. While the vortex core is represented by a pathline, the swirling motion around it is visualized by streamlines in the correct frame. We demonstrate the utility of the approach on several 2D and 3D time-dependent vector fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456384",
            "id": "r_436",
            "s_ids": [
                "s_1349",
                "s_799"
            ],
            "type": "rich",
            "x": -3.5135116577148438,
            "y": 19.799320220947266
        },
        {
            "title": "Fast Comparative Analysis of Merge Trees Using Locality Sensitive Hashing",
            "data": "Scalar field comparison is a fundamental task in scientific visualization. In topological data analysis, we compare topological descriptors of scalar fields\u2014such as persistence diagrams and merge trees\u2014because they provide succinct and robust abstract representations. Several similarity measures for topological descriptors seem to be both asymptotically and practically efficient with polynomial time algorithms, but they do not scale well when handling large-scale, time-varying scientific data and ensembles. In this paper, we propose a new framework to facilitate the comparative analysis of merge trees, inspired by tools from locality sensitive hashing (LSH). LSH hashes similar objects into the same hash buckets with high probability. We propose two new similarity measures for merge trees that can be computed via LSH, using new extensions to Recursive MinHash and subpath signature, respectively. Our similarity measures are extremely efficient to compute and closely resemble the results of existing measures such as merge tree edit distance or geometric interleaving distance. Our experiments demonstrate the utility of our LSH framework in applications such as shape matching, clustering, key event detection, and ensemble summarization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456383",
            "id": "r_437",
            "s_ids": [
                "s_1196",
                "s_1244",
                "s_698",
                "s_836"
            ],
            "type": "rich",
            "x": 7.3360514640808105,
            "y": 5.4388933181762695
        },
        {
            "title": "Causal Priors and Their Influence on Judgements of Causality in Visualized Data",
            "data": "\u201cCorrelation does not imply causation\u201d is a famous mantra in statistical and visual analysis. However, consumers of visualizations often draw causal conclusions when only correlations between variables are shown. In this paper, we investigate factors that contribute to causal relationships users perceive in visualizations. We collected a corpus of concept pairs from variables in widely used datasets and created visualizations that depict varying correlative associations using three typical statistical chart types. We conducted two MTurk studies on (1) preconceived notions on causal relations without charts, and (2) perceived causal relations with charts, for each concept pair. Our results indicate that people make assumptions about causal relationships between pairs of concepts even without seeing any visualized data. Moreover, our results suggest that these assumptions constitute causal priors that, in combination with visualized association, impact how data visualizations are interpreted. The results also suggest that causal priors may lead to over- or under-estimation in perceived causal relations in different circumstances, and that those priors can also impact users' confidence in their causal assessments. In addition, our results align with prior work, indicating that chart type may also affect causal inference. Using data from the studies, we develop a model to capture the interaction between causal priors and visualized associations as they combine to impact a user's perceived causal relations. In addition to reporting the study results and analyses, we provide an open dataset of causal priors for 56 specific concept pairs that can serve as a potential benchmark for future studies. We also suggest remaining challenges and heuristic-based guidelines to help designers improve visualization design choices to better support visual causal inference.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456381",
            "id": "r_438",
            "s_ids": [
                "s_468",
                "s_504",
                "s_1200",
                "s_718",
                "s_976"
            ],
            "type": "rich",
            "x": -9.022054672241211,
            "y": 19.827774047851562
        },
        {
            "title": "CataAnno: An Ancient Catalog Annotator for Annotation Cleaning by Recommendation",
            "data": "Classical bibliography, by researching preserved catalogs from both official archives and personal collections of accumulated books, examines the books throughout history, thereby revealing cultural development across historical periods. In this work, we collaborate with domain experts to accomplish the task of data annotation concerning Chinese ancient catalogs. We introduce the CataAnno system that facilitates users in completing annotations more efficiently through cross-linked views, recommendation methods and convenient annotation interactions. The recommendation method can learn the background knowledge and annotation patterns that experts subconsciously integrate into the data during prior annotation processes. CataAnno searches for the most relevant examples previously annotated and recommends to the user. Meanwhile, the cross-linked views assist users in comprehending the correlations between entries and offer explanations for these recommendations. Evaluation and expert feedback confirm that the CataAnno system, by offering high-quality recommendations and visualizing the relationships between entries, can mitigate the necessity for specialized knowledge during the annotation process. This results in enhanced accuracy and consistency in annotations, thereby enhancing the overall efficiency.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456379",
            "id": "r_439",
            "s_ids": [
                "s_86",
                "s_1338"
            ],
            "type": "rich",
            "x": -10.466747283935547,
            "y": 19.631103515625
        },
        {
            "title": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study on Bar Charts with Varying Layouts",
            "data": "Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways? Graphical perception literature has shown that human chart takeaways are sensitive to visualization design choices, such as spatial layouts. In this work, we examine the extent to which LLMs exhibit such sensitivity when generating takeaways, using bar charts with varying spatial layouts as a case study. We conducted three experiments and tested four common bar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. In Experiment 1, we identified the optimal configurations to generate meaningful chart takeaways by testing four LLMs, two temperature settings, nine chart specifications, and two prompting strategies. We found that even state-of-the-art LLMs struggled to generate semantically diverse and factually accurate takeaways. In Experiment 2, we used the optimal configurations to generate 30 chart takeaways each for eight visualizations across four layouts and two datasets in both zero-shot and one-shot settings. Compared to human takeaways, we found that the takeaways LLMs generated often did not match the types of comparisons made by humans. In Experiment 3, we examined the effect of chart context and data on LLM takeaways. We found that LLMs, unlike humans, exhibited variation in takeaway comparison types for different bar charts using the same bar layout. Overall, our case study evaluates the ability of LLMs to emulate human interpretations of data and points to challenges and opportunities in using LLMs to predict human chart takeaways.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456378",
            "id": "r_440",
            "s_ids": [
                "s_1453",
                "s_69",
                "s_169",
                "s_787",
                "s_1184"
            ],
            "type": "rich",
            "x": -3.9207828044891357,
            "y": 16.717437744140625
        },
        {
            "title": "SpreadLine: Visualizing Egocentric Dynamic Influence",
            "data": "Egocentric networks, often visualized as node-link diagrams, portray the complex relationship (link) dynamics between an entity (node) and others. However, common analytics tasks are multifaceted, encompassing interactions among four key aspects: strength, function, structure, and content. Current node-link visualization designs may fall short, focusing narrowly on certain aspects and neglecting the holistic, dynamic nature of egocentric networks. To bridge this gap, we introduce SpreadLine, a novel visualization framework designed to enable the visual exploration of egocentric networks from these four aspects at the microscopic level. Leveraging the intuitive appeal of storyline visualizations, SpreadLine adopts a storyline-based design to represent entities and their evolving relationships. We further encode essential topological information in the layout and condense the contextual information in a metro map metaphor, allowing for a more engaging and effective way to explore temporal and attribute-based information. To guide our work, with a thorough review of pertinent literature, we have distilled a task taxonomy that addresses the analytical needs specific to egocentric network exploration. Acknowledging the diverse analytical requirements of users, SpreadLine offers customizable encodings to enable users to tailor the framework for their tasks. We demonstrate the efficacy and general applicability of SpreadLine through three diverse real-world case studies (disease surveillance, social media trends, and academic career evolution) and a usability study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456373",
            "id": "r_441",
            "s_ids": [
                "s_233",
                "s_1160",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7596654295921326,
            "y": 17.945558547973633
        },
        {
            "title": "SurroFlow: A Flow-Based Surrogate Model for Parameter Space Exploration and Uncertainty Quantification",
            "data": "Existing deep learning-based surrogate models facilitate efficient data generation, but fall short in uncertainty quantification, efficient parameter space exploration, and reverse prediction. In our work, we introduce SurroFlow, a novel normalizing flow-based surrogate model, to learn the invertible transformation between simulation parameters and simulation outputs. The model not only allows accurate predictions of simulation outcomes for a given simulation parameter but also supports uncertainty quantification in the data generation process. Additionally, it enables efficient simulation parameter recommendation and exploration. We integrate SurroFlow and a genetic algorithm as the backend of a visual interface to support effective user-guided ensemble simulation exploration and visualization. Our framework significantly reduces the computational costs while enhancing the reliability and exploration capabilities of scientific surrogate models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456372",
            "id": "r_442",
            "s_ids": [
                "s_172",
                "s_1396",
                "s_405"
            ],
            "type": "rich",
            "x": 7.559174537658691,
            "y": 5.216010093688965
        },
        {
            "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
            "data": "To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called COMPRESS AND COMPARE. Within a single interface, COMPRESS AND COMPARE surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how COMPRESS AND COMPARE supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate COMPRESS AND COMPARE in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and COMPRESS AND COMPARE visualizations that may generalize to broader model comparison tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456371",
            "id": "r_443",
            "s_ids": [
                "s_644",
                "s_1484",
                "s_0",
                "s_1262",
                "s_646",
                "s_1104"
            ],
            "type": "rich",
            "x": -3.836148977279663,
            "y": 17.388736724853516
        },
        {
            "title": "A General Framework for Comparing Embedding Visualizations Across Class-Label Hierarchies",
            "data": "Projecting high-dimensional vectors into two dimensions for visualization, known as embedding visualization, facilitates perceptual reasoning and interpretation. Comparing multiple embedding visualizations drives decision-making in many domains, but traditional comparison methods are limited by a reliance on direct point correspondences. This requirement precludes comparisons without point correspondences, such as two different datasets of annotated images, and fails to capture meaningful higher-level relationships among point groups. To address these shortcomings, we propose a general framework for comparing embedding visualizations based on shared class labels rather than individual points. Our approach partitions points into regions corresponding to three key class concepts-confusion, neighborhood, and relative size-to characterize intra- and inter-class relationships. Informed by a preliminary user study, we implemented our framework using perceptual neighborhood graphs to define these regions and introduced metrics to quantify each concept. We demonstrate the generality of our framework with usage scenarios from machine learning and single-cell biology, highlighting our metrics' ability to draw insightful comparisons across label hierarchies. To assess the effectiveness of our approach, we conducted an evaluation study with five machine learning researchers and six single-cell biologists using an interactive and scalable prototype built with Python, JavaScript, and Rust. Our metrics enable more structured comparisons through visual guidance and increased participants' confidence in their findings.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456370",
            "id": "r_444",
            "s_ids": [
                "s_1504",
                "s_1344",
                "s_199",
                "s_743",
                "s_393"
            ],
            "type": "rich",
            "x": -8.436906814575195,
            "y": 17.000761032104492
        },
        {
            "title": "Beyond Correlation: Incorporating Counterfactual Guidance to Better Support Exploratory Visual Analysis",
            "data": "Providing effective guidance for users has long been an important and challenging task for efficient exploratory visual analytics, especially when selecting variables for visualization in high-dimensional datasets. Correlation is the most widely applied metric for guidance in statistical and analytical tools, however a reliance on correlation may lead users towards false positives when interpreting causal relations in the data. In this work, inspired by prior insights on the benefits of counterfactual visualization in supporting visual causal inference, we propose a novel, simple, and efficient counterfactual guidance method to enhance causal inference performance in guided exploratory analytics based on insights and concerns gathered from expert interviews. Our technique aims to capitalize on the benefits of counterfactual approaches while reducing their complexity for users. We integrated counterfactual guidance into an exploratory visual analytics system, and using a synthetically generated ground-truth causal dataset, conducted a comparative user study and evaluated to what extent counterfactual guidance can help lead users to more precise visual causal inferences. The results suggest that counterfactual guidance improved visual causal inference performance, and also led to different exploratory behaviors compared to correlation-based guidance. Based on these findings, we offer future directions and challenges for incorporating counterfactual guidance to better support exploratory visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456369",
            "id": "r_445",
            "s_ids": [
                "s_468",
                "s_504",
                "s_976"
            ],
            "type": "rich",
            "x": -9.00815486907959,
            "y": 19.816795349121094
        },
        {
            "title": "SpatialTouch: Exploring Spatial Data Visualizations in Cross-Reality",
            "data": "We propose and study a novel cross-reality environment that seamlessly integrates a monoscopic 2D surface (an interactive screen with touch and pen input) with a stereoscopic 3D space (an augmented reality HMD) to jointly host spatial data visualizations. This innovative approach combines the best of two conventional methods of displaying and manipulating spatial 3D data, enabling users to fluidly explore diverse visual forms using tailored interaction techniques. Providing such effective 3D data exploration techniques is pivotal for conveying its intricate spatial structures\u2014often at multiple spatial or semantic scales\u2014across various application domains and requiring diverse visual representations for effective visualization. To understand user reactions to our new environment, we began with an elicitation user study, in which we captured their responses and interactions. We observed that users adapted their interaction approaches based on perceived visual representations, with natural transitions in spatial awareness and actions while navigating across the physical surface. Our findings then informed the development of a design space for spatial data exploration in cross-reality. We thus developed cross-reality environments tailored to three distinct domains: for 3D molecular structure data, for 3D point cloud data, and for 3D anatomical data. In particular, we designed interaction techniques that account for the inherent features of interactions in both spaces, facilitating various forms of interaction, including mid-air gestures, touch interactions, pen interactions, and combinations thereof, to enhance the users' sense of presence and engagement. We assessed the usability of our environment with biologists, focusing on its use for domain research. In addition, we evaluated our interaction transition designs with virtual and mixed-reality experts to gather further insights. As a result, we provide our design suggestions for the cross-reality environment, emphasizing the interaction with diverse visual representations and seamless interaction transitions between 2D and 3D spaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456368",
            "id": "r_446",
            "s_ids": [
                "s_1289",
                "s_1323",
                "s_1543",
                "s_865",
                "s_1280"
            ],
            "type": "rich",
            "x": -9.81884479522705,
            "y": 17.18342399597168
        },
        {
            "title": "The Backstory to \u201cSwaying the Public\u201d: A Design Chronicle of Election Forecast Visualizations",
            "data": "A year ago, we submitted an IEEE VIS paper entitled \u201cSwaying the Public? Impacts of Election Forecast Visualizations on Emotion, Trust, and Intention in the 2022 U.S. Midterms\u201d [50], which was later bestowed with the honor of a best paper award. Yet, studying such a complex phenomenon required us to explore many more design paths than we could count, and certainly more than we could document in a single paper. This paper, then, is the unwritten prequel\u2014the backstory. It chronicles our journey from a simple idea\u2014to study visualizations for election forecasts\u2014through obstacles such as developing meaningfully different, easy-to-understand forecast visualizations, crafting professional-looking forecasts, and grappling with how to study perceptions of the forecasts before, during, and after the 2022 U.S. midterm elections. This journey yielded a rich set of original knowledge. We formalized a design space for two-party election forecasts, navigating through dimensions like data transformations, visual channels, and types of animated narratives. Through qualitative evaluation of ten representative prototypes with 13 participants, we then identified six core insights into the interpretation of uncertainty visualizations in a U.S. election context. These insights informed our revisions to remove ambiguity in our visual encodings and to prepare a professional-looking forecasting website. As part of this story, we also distilled challenges faced and design lessons learned to inform both designers and practitioners. Ultimately, we hope our methodical approach could inspire others in the community to tackle the hard problems inherent to designing and evaluating visualizations for the general public.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456366",
            "id": "r_447",
            "s_ids": [
                "s_607",
                "s_413",
                "s_193",
                "s_1494",
                "s_1191",
                "s_1383",
                "s_131",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.129326820373535,
            "y": 16.593528747558594
        },
        {
            "title": "TopoMap++: A Faster and More Space Efficient Technique to Compute Projections with Topological Guarantees",
            "data": "High-dimensional data, characterized by many features, can be difficult to visualize effectively. Dimensionality reduction techniques, such as PCA, UMAP, and t-SNE, address this challenge by projecting the data into a lower-dimensional space while preserving important relationships. TopoMap is another technique that excels at preserving the underlying structure of the data, leading to interpretable visualizations. In particular, TopoMap maps the high-dimensional data into a visual space, guaranteeing that the 0-dimensional persistence diagram of the Rips filtration of the visual space matches the one from the high-dimensional data. However, the original TopoMap algorithm can be slow and its layout can be too sparse for large and complex datasets. In this paper, we propose three improvements to TopoMap: 1) a more space-efficient layout, 2) a significantly faster implementation, and 3) a novel TreeMap-based representation that makes use of the topological hierarchy to aid the exploration of the projections. These advancements make TopoMap, now referred to as TopoMap++, a more powerful tool for visualizing high-dimensional data which we demonstrate through different use case scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456365",
            "id": "r_448",
            "s_ids": [
                "s_760",
                "s_1135",
                "s_28",
                "s_402",
                "s_428"
            ],
            "type": "rich",
            "x": -4.0309295654296875,
            "y": 20.13979148864746
        },
        {
            "title": "StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions",
            "data": "The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456363",
            "id": "r_449",
            "s_ids": [
                "s_369",
                "s_887",
                "s_1075",
                "s_1092",
                "s_137",
                "s_473",
                "s_156"
            ],
            "type": "rich",
            "x": -10.198663711547852,
            "y": 18.248746871948242
        },
        {
            "title": "Quantifying Emotional Responses to Immutable Data Characteristics and Designer Choices in Data Visualizations",
            "data": "Emotion is an important factor to consider when designing visualizations as it can impact the amount of trust viewers place in a visualization, how well they can retrieve information and understand the underlying data, and how much they engage with or connect to a visualization. We conducted five crowdsourced experiments to quantify the effects of color, chart type, data trend, data variability and data density on emotion (measured through self-reported arousal and valence). Results from our experiments show that there are multiple design elements which influence the emotion induced by a visualization and, more surprisingly, that certain data characteristics influence the emotion of viewers even when the data has no meaning. In light of these findings, we offer guidelines on how to use color, scale, and chart type to counterbalance and emphasize the emotional impact of immutable data characteristics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456361",
            "id": "r_450",
            "s_ids": [
                "s_228",
                "s_929",
                "s_386"
            ],
            "type": "rich",
            "x": -6.188882827758789,
            "y": 17.41791343688965
        },
        {
            "title": "A Qualitative Analysis of Common Practices in Annotations: A Taxonomy and Design Space",
            "data": "Annotations play a vital role in highlighting critical aspects of visualizations, aiding in data externalization and exploration, collaborative sensemaking, and visual storytelling. However, despite their widespread use, we identified a lack of a design space for common practices for annotations. In this paper, we evaluated over 1,800 static annotated charts to understand how people annotate visualizations in practice. Through qualitative coding of these diverse real-world annotated charts, we explored three primary aspects of annotation usage patterns: analytic purposes for chart annotations (e.g., present, identify, summarize, or compare data features), mechanisms for chart annotations (e.g., types and combinations of annotations used, frequency of different annotation types across chart types, etc.), and the data source used to generate the annotations. We then synthesized our findings into a design space of annotations, highlighting key design choices for chart annotations. We presented three case studies illustrating our design space as a practical framework for chart annotations to enhance the communication of visualization insights. All supplemental materials are available at https://shorturl.at/bAGM1.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456359",
            "id": "r_451",
            "s_ids": [
                "s_492",
                "s_962",
                "s_951",
                "s_781",
                "s_542"
            ],
            "type": "rich",
            "x": -2.0663766860961914,
            "y": 18.026708602905273
        },
        {
            "title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech",
            "data": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456358",
            "id": "r_452",
            "s_ids": [
                "s_1579",
                "s_1214",
                "s_537",
                "s_274",
                "s_5",
                "s_1305"
            ],
            "type": "rich",
            "x": -7.632698059082031,
            "y": 18.02023696899414
        },
        {
            "title": "Regularized Multi-Decoder Ensemble for an Error-Aware Scene Representation Network",
            "data": "Feature grid Scene Representation Networks (SRNs) have been applied to scientific data as compact functional surrogates for analysis and visualization. As SRNs are black-box lossy data representations, assessing the prediction quality is critical for scientific visualization applications to ensure that scientists can trust the information being visualized. Currently, existing architectures do not support inference time reconstruction quality assessment, as coordinate-level errors cannot be evaluated in the absence of ground truth data. By employing the uncertain neural network architecture in feature grid SRNs, we obtain prediction variances during inference time to facilitate confidence-aware data reconstruction. Specifically, we propose a parameter-efficient multi-decoder SRN (MDSRN) architecture consisting of a shared feature grid with multiple lightweight multilayer perceptron decoders. MDSRN can generate a set of plausible predictions for a given input coordinate to compute the mean as the prediction of the multi-decoder ensemble and the variance as a confidence score. The coordinate-level variance can be rendered along with the data to inform the reconstruction quality, or be integrated into uncertainty-aware volume visualization algorithms. To prevent the misalignment between the quantified variance and the prediction quality, we propose a novel variance regularization loss for ensemble learning that promotes the Regularized multi-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates closely to the true model error. We comprehensively evaluate the quality of variance quantification and data reconstruction of Monte Carlo Dropout (MCD), Mean Field Variational Inference (MFVI), Deep Ensemble (DE), and Predicting Variance (PV) in comparison with our proposed MDSRN and RMDSRN applied to state-of-the-art feature grid SRNs across diverse scalar field datasets. We demonstrate that RMDSRN attains the most accurate data reconstruction and competitive variance-error correlation among uncertain SRNs under the same neural network parameter budgets. Furthermore, we present an adaptation of uncertainty-aware volume rendering and shed light on the potential of incorporating uncertain predictions in improving the quality of volume rendering for uncertain SRNs. Through ablation studies on the regularization strength and decoder count, we show that MDSRN and RMDSRN are expected to perform sufficiently well with a default configuration without requiring customized hyperparameter settings for different datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456357",
            "id": "r_453",
            "s_ids": [
                "s_322",
                "s_1433",
                "s_986",
                "s_841",
                "s_405"
            ],
            "type": "rich",
            "x": 7.479953765869141,
            "y": 5.294701099395752
        },
        {
            "title": "Localized Evaluation for Constructing Discrete Vector Fields",
            "data": "Topological abstractions offer a method to summarize the behavior of vector fields, but computing them robustly can be challenging due to numerical precision issues. One alternative is to represent the vector field using a discrete approach, which constructs a collection of pairs of simplices in the input mesh that satisfies criteria introduced by Forman's discrete Morse theory. While numerous approaches exist to compute pairs in the restricted case of the gradient of a scalar field, state-of-the-art algorithms for the general case of vector fields require expensive optimization procedures. This paper introduces a fast, novel approach for pairing simplices of two-dimensional, triangulated vector fields that do not vary in time. The key insight of our approach is that we can employ a local evaluation, inspired by the approach used to construct a discrete gradient field, where every simplex in a mesh is considered by no more than one of its vertices. Specifically, we observe that for any edge in the input mesh, we can uniquely assign an outward direction of flow. We can further expand this consistent notion of outward flow at each vertex, which corresponds to the concept of a downhill flow in the case of scalar fields. Working with outward flow enables a linear-time algorithm that processes the (outward) neighborhoods of each vertex one-by-one, similar to the approach used for scalar fields. We couple our approach to constructing discrete vector fields with a method to extract, simplify, and visualize topological features. Empirical results on analytic and simulation data demonstrate drastic improvements in running time, produce features similar to the current state-of-the-art, and show the application of simplification to large, complex flows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456355",
            "id": "r_454",
            "s_ids": [
                "s_1483",
                "s_707",
                "s_1501"
            ],
            "type": "rich",
            "x": -4.0505852699279785,
            "y": 20.649600982666016
        },
        {
            "title": "LLM Comparator: Interactive Analysis of Side-by-Side Evaluation of Large Language Models",
            "data": "Evaluating large language models (LLMs) presents unique challenges. While automatic side-by-side evaluation, also known as LLM-as-a-judge, has become a promising solution, model developers and researchers face difficulties with scalability and interpretability when analyzing these evaluation outcomes. To address these challenges, we introduce LLM Comparator, a new visual analytics tool designed for side-by-side evaluations of LLMs. This tool provides analytical workflows that help users understand when and why one LLM outperforms or underperforms another, and how their responses differ. Through close collaboration with practitioners developing LLMs at Google, we have iteratively designed, developed, and refined the tool. Qualitative feedback from these users highlights that the tool facilitates in-depth analysis of individual examples while enabling users to visually overview and flexibly slice data. This empowers users to identify undesirable patterns, formulate hypotheses about model behavior, and gain insights for model improvement. LLM Comparator has been integrated into Google's LLM evaluation platforms and open-sourced.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456354",
            "id": "r_455",
            "s_ids": [
                "s_1554",
                "s_243",
                "s_560",
                "s_1277",
                "s_797",
                "s_1364",
                "s_633",
                "s_885",
                "s_31",
                "s_417"
            ],
            "type": "rich",
            "x": 9.976849555969238,
            "y": 10.564130783081055
        },
        {
            "title": "Curio: A Dataflow-Based Framework for Collaborative Urban Visual Analytics",
            "data": "Over the past decade, several urban visual analytics systems and tools have been proposed to tackle a host of challenges faced by cities, in areas as diverse as transportation, weather, and real estate. Many of these tools have been designed through collaborations with urban experts, aiming to distill intricate urban analysis workflows into interactive visualizations and interfaces. However, the design, implementation, and practical use of these tools still rely on siloed approaches, resulting in bespoke systems that are difficult to reproduce and extend. At the design level, these tools undervalue rich data workflows from urban experts, typically treating them only as data providers and evaluators. At the implementation level, they lack interoperability with other technical frameworks. At the practical use level, they tend to be narrowly focused on specific fields, inadvertently creating barriers to cross-domain collaboration. To address these gaps, we present Curio, a framework for collaborative urban visual analytics. Curio uses a dataflow model with multiple abstraction levels (code, grammar, GUI elements) to facilitate collaboration across the design and implementation of visual analytics components. The framework allows experts to intertwine data preprocessing, management, and visualization stages while tracking the provenance of code and visualizations. In collaboration with urban experts, we evaluate Curio through a diverse set of usage scenarios targeting urban accessibility, urban microclimate, and sunlight access. These scenarios use different types of data and domain methodologies to illustrate Curio's flexibility in tackling pressing societal challenges. Curio is available at urbantk.org/curio.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456353",
            "id": "r_456",
            "s_ids": [
                "s_804",
                "s_22",
                "s_479",
                "s_1568",
                "s_1582",
                "s_1086",
                "s_744",
                "s_1015",
                "s_1320"
            ],
            "type": "rich",
            "x": -7.506833553314209,
            "y": -1.170312762260437
        },
        {
            "title": "Practices and Strategies in Responsive Thematic Map Design: A Report from Design Workshops with Experts",
            "data": "This paper discusses challenges and design strategies in responsive design for thematic maps in information visualization. Thematic maps pose a number of unique challenges for responsiveness, such as inflexible aspect ratios that do not easily adapt to varying screen dimensions, or densely clustered visual elements in urban areas becoming illegible at smaller scales. However, design guidance on how to best address these issues is currently lacking. We conducted design sessions with eight professional designers and developers of web-based thematic maps for information visualization. Participants were asked to redesign a given map for various screen sizes and aspect ratios and to describe their reasoning for when and how they adapted the design. We report general observations of practitioners' motivations, decision-making processes, and personal design frameworks. We then derive seven challenges commonly encountered in responsive maps, and 17 strategies to address them, such as repositioning elements, segmenting the map, or using alternative visualizations. We compile these challenges and strategies into an illustrated cheat sheet targeted at anyone designing or learning to design responsive maps. The cheat sheet is available online: responsive-vis.github.io/map-cheat-sheet.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456352",
            "id": "r_457",
            "s_ids": [
                "s_474",
                "s_1547",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.783448696136475,
            "y": 17.17864990234375
        },
        {
            "title": "A Deixis-Centered Approach for Documenting Remote Synchronous Communication Around Data Visualizations",
            "data": "Referential gestures, or as termed in linguistics, deixis, are an essential part of communication around data visualizations. Despite their importance, such gestures are often overlooked when documenting data analysis meetings. Transcripts, for instance, fail to capture gestures, and video recordings may not adequately capture or emphasize them. We introduce a novel method for documenting collaborative data meetings that treats deixis as a first-class citizen. Our proposed framework captures cursor-based gestural data along with audio and converts them into interactive documents. The framework leverages a large language model to identify word correspondences with gestures. These identified references are used to create context-based annotations in the resulting interactive document. We assess the effectiveness of our proposed method through a user study, finding that participants preferred our automated interactive documentation over recordings, transcripts, and manual note-taking. Furthermore, we derive a preliminary taxonomy of cursor-based deictic gestures from participant actions during the study. This taxonomy offers further opportunities for better utilizing cursor-based deixis in collaborative data analysis scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456351",
            "id": "r_458",
            "s_ids": [
                "s_151",
                "s_174"
            ],
            "type": "rich",
            "x": -0.22381238639354706,
            "y": 17.102659225463867
        },
        {
            "title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models",
            "data": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\u2014DracoGPT-Rank and DracoGPT-Recommend\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456350",
            "id": "r_459",
            "s_ids": [
                "s_1453",
                "s_1508",
                "s_1025",
                "s_1386"
            ],
            "type": "rich",
            "x": -3.9952101707458496,
            "y": 16.919918060302734
        },
        {
            "title": "Evaluating and Extending Speedup Techniques for Optimal Crossing Minimization in Layered Graph Drawings",
            "data": "A layered graph is an important category of graph in which every node is assigned to a layer, and layers are drawn as parallel or radial lines. They are commonly used to display temporal data or hierarchical graphs. Previous research has demonstrated that minimizing edge crossings is the most important criterion to consider when looking to improve the readability of such graphs. While heuristic approaches exist for crossing minimization, we are interested in optimal approaches to the problem that prioritize human readability over computational scalability. We aim to improve the usefulness and applicability of such optimal methods by understanding and improving their scalability to larger graphs. This paper categorizes and evaluates the state-of-the-art linear programming formulations for exact crossing minimization and describes nine new and existing techniques that could plausibly accelerate the optimization algorithm. Through a computational evaluation, we explore each technique's effect on calculation time and how the techniques assist or inhibit one another, allowing researchers and practitioners to adapt them to the characteristics of their graphs. Our best-performing techniques yielded a median improvement of 2.5-17 \u00d7 depending on the solver used, giving us the capability to create optimal layouts faster and for larger graphs. We provide an open-source implementation of our methodology in Python, where users can pick which combination of techniques to enable according to their use case. A free copy of this paper and all supplemental materials, datasets used, and source code are available at https://osf.io/5vq79.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456349",
            "id": "r_460",
            "s_ids": [
                "s_919",
                "s_255",
                "s_1077",
                "s_286",
                "s_1006"
            ],
            "type": "rich",
            "x": -6.74872350692749,
            "y": 21.14531135559082
        },
        {
            "title": "D-Tour: Semi-Automatic Generation of Interactive Guided Tours for Visualization Dashboard Onboarding",
            "data": "Onboarding a user to a visualization dashboard entails explaining its various components, including the chart types used, the data loaded, and the interactions available. Authoring such an onboarding experience is time-consuming and requires significant knowledge and little guidance on how best to complete this task. Depending on their levels of expertise, end users being onboarded to a new dashboard can be either confused and overwhelmed or disinterested and disengaged. We propose interactive dashboard tours (D-Tours) as semi-automated onboarding experiences that preserve the agency of users with various levels of expertise to keep them interested and engaged. Our interactive tours concept draws from open-world game design to give the user freedom in choosing their path through onboarding. We have implemented the concept in a tool called D-TOUR Prototype, which allows authors to craft custom interactive dashboard tours from scratch or using automatic templates. Automatically generated tours can still be customized to use different media (e.g., video, audio, and highlighting) or new narratives to produce an onboarding experience tailored to an individual user. We demonstrate the usefulness of interactive dashboard tours through use cases and expert interviews. Our evaluation shows that authors found the automation in the D-Tour Prototype helpful and time-saving, and users found the created tours engaging and intuitive. This paper and all supplemental materials are available at https://osf.io/6fbjp/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456347",
            "id": "r_461",
            "s_ids": [
                "s_837",
                "s_680",
                "s_1333",
                "s_1237",
                "s_1033",
                "s_580"
            ],
            "type": "rich",
            "x": -2.0662312507629395,
            "y": 15.958991050720215
        },
        {
            "title": "Visual Analysis of Multi-Outcome Causal Graphs",
            "data": "We introduce a visual analysis method for multiple causal graphs with different outcome variables, namely, multi-outcome causal graphs. Multi-outcome causal graphs are important in healthcare for understanding multimorbidity and comorbidity. To support the visual analysis, we collaborated with medical experts to devise two comparative visualization techniques at different stages of the analysis process. First, a progressive visualization method is proposed for comparing multiple state-of-the-art causal discovery algorithms. The method can handle mixed-type datasets comprising both continuous and categorical variables and assist in the creation of a fine-tuned causal graph of a single o utcome. Second, a comparative graph layout technique and specialized visual encodings are devised for the quick comparison of multiple causal graphs. In our visual analysis approach, analysts start by building individual causal graphs for each outcome variable, and then, multi-outcome causal graphs are generated and visualized with our comparative technique for analyzing differences and commonalities of these causal graphs. Evaluation includes quantitative measurements on benchmark datasets, a case study with a medical expert, and expert user studies with real-world health research data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456346",
            "id": "r_462",
            "s_ids": [
                "s_149",
                "s_935",
                "s_157",
                "s_989",
                "s_106",
                "s_219"
            ],
            "type": "rich",
            "x": -8.984614372253418,
            "y": 19.56583595275879
        },
        {
            "title": "A Practical Solver for Scalar Data Topological Simplification",
            "data": "This paper presents a practical approach for the optimization of topological simplification, a central pre-processing step for the analysis and visualization of scalar data. Given an input scalar field $f$ and a set of \u201csignal\u201d persistence pairs to maintain, our approaches produces an output field $g$ that is close to $f$ and which optimizes (i) the cancellation of \u201cnon-signal\u201d pairs, while (ii) preserving the \u201csignal\u201d pairs. In contrast to pre-existing simplification algorithms, our approach is not restricted to persistence pairs involving extrema and can thus address a larger class of topological features, in particular saddle pairs in three-dimensional scalar data. Our approach leverages recent generic persistence optimization frameworks and extends them with tailored accelerations specific to the problem of topological simplification. Extensive experiments report substantial accelerations over these frameworks, thereby making topological simplification optimization practical for real-life datasets. Our approach enables a direct visualization and analysis of the topologically simplified data, e.g., via isosurfaces of simplified topology (fewer components and handles). We apply our approach to the extraction of prominent filament structures in three-dimensional data. Specifically, we show that our pre-simplification of the data leads to practical improvements over standard topological techniques for removing filament loops. We also show how our approach can be used to repair genus defects in surface processing. Finally, we provide a C++ implementation for reproducibility purposes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456345",
            "id": "r_463",
            "s_ids": [
                "s_1365",
                "s_1462",
                "s_1501",
                "s_707"
            ],
            "type": "rich",
            "x": -4.038835525512695,
            "y": 20.71723747253418
        },
        {
            "title": "Mind Drifts, Data Shifts: Utilizing Mind Wandering to Track the Evolution of User Experience with Data Visualizations",
            "data": "User experience in data visualization is typically assessed through post-viewing self-reports, but these overlook the dynamic cognitive processes during interaction. This study explores the use of mind wandering- a phenomenon where attention spontaneously shifts from a primary task to internal, task-related thoughts or unrelated distractions- as a dynamic measure during visualization exploration. Participants reported mind wandering while viewing visualizations from a pre-labeled visualization database and then provided quantitative ratings of trust, engagement, and design quality, along with qualitative descriptions and short-term/long-term recall assessments. Results show that mind wandering negatively affects short-term visualization recall and various post-viewing measures, particularly for visualizations with little text annotation. Further, the type of mind wandering impacts engagement and emotional response. Mind wandering also functions as an intermediate process linking visualization design elements to post-viewing measures, influencing how viewers engage with and interpret visual information over time. Overall, this research underscores the importance of incorporating mind wandering as a dynamic measure in visualization design and evaluation, offering novel avenues for enhancing user engagement and comprehension.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456344",
            "id": "r_464",
            "s_ids": [
                "s_236",
                "s_229",
                "s_394"
            ],
            "type": "rich",
            "x": -3.345567226409912,
            "y": 17.59274673461914
        },
        {
            "title": "What Can Interactive Visualization Do for Participatory Budgeting in Chicago?",
            "data": "Participatory budgeting (PB) is a democratic approach to allocating municipal spending that has been adopted in many places in recent years, including in Chicago. Current PB voting resembles a ballot where residents are asked which municipal projects, such as school improvements and road repairs, to fund with a limited budget. In this work, we ask how interactive visualization can benefit PB by conducting a design probe-based interview study (N=13) with policy workers and academics with expertise in PB, urban planning, and civic HCI. Our probe explores how graphical elicitation of voter preferences and a dashboard of voting statistics can be incorporated into a realistic PB tool. Through qualitative analysis, we find that visualization creates opportunities for city government to set expectations about budget constraints while also granting their constituents greater freedom to articulate a wider range of preferences. However, using visualization to provide transparency about PB requires efforts to mitigate potential access barriers and mistrust. We call for more visualization professionals to help build civic capacity by working in and studying political systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456343",
            "id": "r_465",
            "s_ids": [
                "s_875",
                "s_967",
                "s_70",
                "s_1355",
                "s_132"
            ],
            "type": "rich",
            "x": -3.49690318107605,
            "y": 16.965988159179688
        },
        {
            "title": "\u201cI Came Across a Junk\u201d: Understanding Design Flaws of Data Visualization from the Public's Perspective",
            "data": "The visualization community has a rich history of reflecting upon visualization design flaws. Although research in this area has remained lively, we believe it is essential to continuously revisit this classic and critical topic in visualization research by incorporating more empirical evidence from diverse sources, characterizing new design flaws, building more systematic theoretical frameworks, and understanding the underlying reasons for these flaws. To address the above gaps, this work investigated visualization design flaws through the lens of the public, constructed a framework to summarize and categorize the identified flaws, and explored why these flaws occur. Specifically, we analyzed 2227 flawed data visualizations collected from an online gallery and derived a design task-associated taxonomy containing 76 specific design flaws. These flaws were further classified into three high-level categories (i.e., misinformation, uninformativeness, unsociability) and ten subcategories (e.g., inaccuracy, unfairness, ambiguity). Next, we organized five focus groups to explore why these design flaws occur and identified seven causes of the flaws. Finally, we proposed a research agenda for combating visualization design flaws and summarize nine research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456341",
            "id": "r_466",
            "s_ids": [
                "s_750",
                "s_1000"
            ],
            "type": "rich",
            "x": -9.455107688903809,
            "y": 19.578113555908203
        },
        {
            "title": "DG Comics: Semi-Automatically Authoring Graph Comics for Dynamic Graphs",
            "data": "Comics are an effective method for sequential data-driven storytelling, especially for dynamic graphs\u2014graphs whose vertices and edges change over time. However, manually creating such comics is currently time-consuming, complex, and error-prone. In this paper, we propose DG COMICS, a novel comic authoring tool for dynamic graphs that allows users to semi-automatically build and annotate comics. The tool uses a newly developed hierarchical clustering algorithm to segment consecutive snapshots of dynamic graphs while preserving their chronological order. It also presents rich information on both individuals and communities extracted from dynamic graphs in multiple views, where users can explore dynamic graphs and choose what to tell in comics. For evaluation, we provide an example and report the results of a user study and an expert review.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456340",
            "id": "r_467",
            "s_ids": [
                "s_768",
                "s_1563",
                "s_259",
                "s_21",
                "s_1292",
                "s_1295",
                "s_1237"
            ],
            "type": "rich",
            "x": -1.9857072830200195,
            "y": 16.06899642944336
        },
        {
            "title": "VADIS: A Visual Analytics Pipeline for Dynamic Document Representation and Information-Seeking",
            "data": "In the biomedical domain, visualizing the document embeddings of an extensive corpus has been widely used in information-seeking tasks. However, three key challenges with existing visualizations make it difficult for clinicians to find information efficiently. First, the document embeddings used in these visualizations are generated statically by pretrained language models, which cannot adapt to the user's evolving interest. Second, existing document visualization techniques cannot effectively display how the documents are relevant to users' interest, making it difficult for users to identify the most pertinent information. Third, existing embedding generation and visualization processes suffer from a lack of interpretability, making it difficult to understand, trust and use the result for decision-making. In this paper, we present a novel visual analytics pipeline for user-driven document representation and iterative information seeking (VADIS). VADIS introduces a prompt-based attention model (PAM) that generates dynamic document embedding and document relevance adjusted to the user's query. To effectively visualize these two pieces of information, we design a new document map that leverages a circular grid layout to display documents based on both their relevance to the query and the semantic similarity. Additionally, to improve the interpretability, we introduce a corpus-level attention visualization method to improve the user's understanding of the model focus and to enable the users to identify potential oversight. This visualization, in turn, empowers users to refine, update and introduce new queries, thereby facilitating a dynamic and iterative information-seeking experience. We evaluated VADIS quantitatively and qualitatively on a real-world dataset of biomedical research papers to demonstrate its effectiveness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456339",
            "id": "r_468",
            "s_ids": [
                "s_1473",
                "s_553",
                "s_438",
                "s_405"
            ],
            "type": "rich",
            "x": 7.528132915496826,
            "y": 5.24678897857666
        },
        {
            "title": "ParamsDrag: Interactive Parameter Space Exploration via Image-Space Dragging",
            "data": "Numerical simulation serves as a cornerstone in scientific modeling, yet the process of fine-tuning simulation parameters poses significant challenges. Conventionally, parameter adjustment relies on extensive numerical simulations, data analysis, and expert insights, resulting in substantial computational costs and low efficiency. The emergence of deep learning in recent years has provided promising avenues for more efficient exploration of parameter spaces. However, existing approaches often lack intuitive methods for precise parameter adjustment and optimization. To tackle these challenges, we introduce ParamsDrag, a model that facilitates parameter space exploration through direct interaction with visualizations. Inspired by DragGAN, our ParamsDrag model operates in three steps. First, the generative component of ParamsDrag generates visualizations based on the input simulation parameters. Second, by directly dragging structure-related features in the visualizations, users can intuitively understand the controlling effect of different parameters. Third, with the understanding from the earlier step, users can steer ParamsDrag to produce dynamic visual outcomes. Through experiments conducted on real-world simulations and comparisons with state-of-the-art deep learning-based approaches, we demonstrate the efficacy of our solution.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456338",
            "id": "r_469",
            "s_ids": [
                "s_183",
                "s_795",
                "s_58",
                "s_280",
                "s_1420",
                "s_883",
                "s_1199"
            ],
            "type": "rich",
            "x": 7.6309285163879395,
            "y": 5.146270275115967
        },
        {
            "title": "MSz: An Efficient Parallel Algorithm for Correcting Morse-Smale Segmentations in Error-Bounded Lossy Compressors",
            "data": "This research explores a novel paradigm for preserving topological segmentations in existing error-bounded lossy compressors. Today's lossy compressors rarely consider preserving topologies such as Morse-Smale complexes, and the discrepancies in topology between original and decompressed datasets could potentially result in erroneous interpretations or even incorrect scientific conclusions. In this paper, we focus on preserving Morse-Smale segmentations in 2D/3D piecewise linear scalar fields, targeting the precise reconstruction of minimum/maximum labels induced by the integral line of each vertex. The key is to derive a series of edits during compression time. These edits are applied to the decompressed data, leading to an accurate reconstruction of segmentations while keeping the error within the prescribed error bound. To this end, we develop a workflow to fi x ex trema an d in tegral lines alternatively until convergence within finite iterations. We accelerate each workflow component with shared-memory/GPU parallelism to make the performance practical for coupling with compressors. We demonstrate use cases with fluid dynamics, ocean, and cosmology application datasets with a significant acceleration with an NVIDIA A100 GPU.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456337",
            "id": "r_470",
            "s_ids": [
                "s_415",
                "s_1270",
                "s_836",
                "s_435",
                "s_1581",
                "s_986"
            ],
            "type": "rich",
            "x": 7.409884452819824,
            "y": 5.364959716796875
        },
        {
            "title": "DataGarden: Formalizing Personal Sketches into Structured Visualization Templates",
            "data": "Sketching is a common practice among visualization designers and serves an approachable entry to data visualization for non-experts. However, moving from a sketch to a full fledged data visualization often requires throwing away the original sketch and recreating it from scratch. Our goal is to formalize these sketches, enabling them to support iteration and systematic data mapping through a visual-first templating workflow. In this workflow, authors sketch a representative visualization and structure it into an expressive template for an envisioned or partial dataset, capturing implicit style as well as explicit data mappings. To demonstrate our proposed workflow, we implement DataGarden and evaluate it through a reproduction and a freeform study. We investigate how DataGarden supports personal expression and delve into the variety of visualizations that authors can produce with it, identifying cases that demonstrate the limitations of our approach and discussing avenues for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456336",
            "id": "r_471",
            "s_ids": [
                "s_143",
                "s_426",
                "s_198"
            ],
            "type": "rich",
            "x": -6.619952201843262,
            "y": 16.79640007019043
        },
        {
            "title": "Talk to the Wall: The Role of Speech Interaction in Collaborative Visual Analytics",
            "data": "We present the results of an exploratory study on how pairs interact with speech commands and touch gestures on a wall-sized display during a collaborative sensemaking task. Previous work has shown that speech commands, alone or in combination with other input modalities, can support visual data exploration by individuals. However, it is still unknown whether and how speech commands can be used in collaboration, and for what tasks. To answer these questions, we developed a functioning prototype that we used as a technology probe. We conducted an in-depth exploratory study with 10 participant pairs to analyze their interaction choices, the interplay between the input modalities, and their collaboration. While touch was the most used modality, we found that participants preferred speech commands for global operations, used them for distant interaction, and that speech interaction contributed to the awareness of the partner's actions. Furthermore, the likelihood of using speech commands during collaboration was related to the personality trait of agreeableness. Regarding collaboration styles, participants interacted with speech equally often whether they were in loosely or closely coupled collaboration. While the partners stood closer to each other during close collaboration, they did not distance themselves to use speech commands. From our findings, we derive and contribute a set of design considerations for collaborative and multimodal interactive data analysis systems. All supplemental materials are available at https://osf.io/8gpv2.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456335",
            "id": "r_472",
            "s_ids": [
                "s_554",
                "s_182",
                "s_864",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.310457706451416,
            "y": 18.32771873474121
        },
        {
            "title": "Discursive Patinas: Anchoring Discussions in Data Visualizations",
            "data": "This paper presents discursive patinas, a technique to visualize discussions onto data visualizations, inspired by how people leave traces in the physical world. While data visualizations are widely discussed in online communities and social media, comments tend to be displayed separately from the visualization and we lack ways to relate these discussions back to the content of the visualization, e.g., to situate comments, explain visual patterns, or question assumptions. In our visualization annotation interface, users can designate areas within the visualization. Discursive patinas are made of overlaid visual marks (anchors), attached to textual comments with category labels, likes, and replies. By coloring and styling the anchors, a meta visualization emerges, showing what and where people comment and annotate the visualization. These patinas show regions of heavy discussions, recent commenting activity, and the distribution of questions, suggestions, or personal stories. We ran workshops with 90 students, domain experts, and visualization researchers to study how people use anchors to discuss visualizations and how patinas influence people's understanding of the discussion. Our results show that discursive patinas improve the ability to navigate discussions and guide people to comments that help understand, contextualize, or scrutinize the visualization. We discuss the potential of anchors and patinas to support discursive engagements, including critical readings of visualizations, design feedback, and feminist approaches to data visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456334",
            "id": "r_473",
            "s_ids": [
                "s_511",
                "s_719",
                "s_188",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.56011438369751,
            "y": 17.114700317382812
        },
        {
            "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
            "data": "In this study, we address the growing issue of misleading charts, a prevalent problem that undermines the integrity of information dissemination. Misleading charts can distort the viewer's perception of data, leading to misinterpretations and decisions based on false information. The development of effective automatic detection methods for misleading charts is an urgent field of research. The recent advancement of multimodal Large Language Models (LLMs) has introduced a promising direction for addressing this challenge. We explored the capabilities of these models in analyzing complex charts and assessing the impact of different prompting strategies on the models' analyses. We utilized a dataset of misleading charts collected from the internet by prior research and crafted nine distinct prompts, ranging from simple to complex, to test the ability of four different multimodal LLMs in detecting over 21 different chart issues. Through three experiments\u2013from initial exploration to detailed analysis\u2013we progressively gained insights into how to effectively prompt LLMs to identify misleading charts and developed strategies to address the scalability challenges encountered as we expanded our detection range from the initial five issues to 21 issues in the final experiment. Our findings reveal that multimodal LLMs possess a strong capability for chart comprehension and critical thinking in data interpretation. There is significant potential in employing multimodal LLMs to counter misleading information by supporting critical thinking and enhancing visualization literacy. This study demonstrates the applicability of LLMs in addressing the pressing concern of misleading charts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456333",
            "id": "r_474",
            "s_ids": [
                "s_1495",
                "s_156"
            ],
            "type": "rich",
            "x": -10.196163177490234,
            "y": 18.445701599121094
        },
        {
            "title": "Sportify: Question Answering with Embedded Visualizations and Personified Narratives for Sports Video",
            "data": "As basketball's popularity surges, fans often find themselves confused and overwhelmed by the rapid game pace and complexity. Basketball tactics, involving a complex series of actions, require substantial knowledge to be fully understood. This complexity leads to a need for additional information and explanation, which can distract fans from the game. To tackle these challenges, we present Sportify, a Visual Question Answering system that integrates narratives and embedded visualization for demystifying basketball tactical questions, aiding fans in understanding various game aspects. We propose three novel action visualizations (i.e., Pass, Cut, and Screen) to demonstrate critical action sequences. To explain the reasoning and logic behind players' actions, we leverage a large-language model (LLM) to generate narratives. We adopt a storytelling approach for complex scenarios from both first and third-person perspectives, integrating action visualizations. We evaluated Sportify with basketball fans to investigate its impact on understanding of tactics, and how different personal perspectives of narratives impact the understanding of complex tactic with action visualizations. Our evaluation with basketball fans demonstrates Sportify's capability to deepen tactical insights and amplify the viewing experience. Furthermore, third-person narration assists people in getting in-depth game explanations while first-person narration enhances fans' game engagement.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456332",
            "id": "r_475",
            "s_ids": [
                "s_544",
                "s_416",
                "s_1486",
                "s_826"
            ],
            "type": "rich",
            "x": -8.756763458251953,
            "y": 16.5018367767334
        },
        {
            "title": "Telling Data Stories with the Hero's Journey: Design Guidance for Creating Data Videos",
            "data": "Data videos increasingly becoming a popular data storytelling form represented by visual and audio integration. In recent years, more and more researchers have explored many narrative structures for effective and attractive data storytelling. Meanwhile, the Hero's Journey provides a classic narrative framework specific to the Hero's story that has been adopted by various mediums. There are continuous discussions about applying Hero's Journey to data stories. However, so far, little systematic and practical guidance on how to create a data video for a specific story type like the Hero's Journey, as well as how to manipulate its sound and visual designs simultaneously. To fulfill this gap, we first identified 48 data videos aligned with the Hero's Journey as the common storytelling from 109 high-quality data videos. Then, we examined how existing practices apply Hero's Journey for creating data videos. We coded the 48 data videos in terms of the narrative stages, sound design, and visual design according to the Hero's Journey structure. Based on our findings, we proposed a design space to provide practical guidance on the narrative, visual, and sound custom design for different narrative segments of the hero's journey (i.e., Departure, Initiation, Return) through data video creation. To validate our proposed design space, we conducted a user study where 20 participants were invited to design data videos with and without our design space guidance, which was evaluated by two experts. Results show that our design space provides useful and practical guidance for data storytellers effectively creating data videos with the Hero's Journey.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456330",
            "id": "r_476",
            "s_ids": [
                "s_1390",
                "s_156",
                "s_1115"
            ],
            "type": "rich",
            "x": -10.105833053588867,
            "y": 18.458969116210938
        },
        {
            "title": "DaedalusData: Exploration, Knowledge Externalization and Labeling of Particles in Medical Manufacturing \u2014 A Design Study",
            "data": "In medical diagnostics of both early disease detection and routine patient care, particle-based contamination of in-vitro diagnostics consumables poses a significant threat to patients. Objective data-driven decision-making on the severity of contamination is key for reducing patient risk, while saving time and cost in quality assessment. Our collaborators introduced us to their quality control process, including particle data acquisition through image recognition, feature extraction, and attributes reflecting the production context of particles. Shortcomings in the current process are limitations in exploring thousands of images, data-driven decision making, and ineffective knowledge externalization. Following the design study methodology, our contributions are a characterization of the problem space and requirements, the development and validation of DaedalusData, a comprehensive discussion of our study's learnings, and a generalizable framework for knowledge externalization. DaedalusData is a visual analytics system that enables domain experts to explore particle contamination patterns, label particles in label alphabets, and externalize knowledge through semi-supervised label-informed data projections. The results of our case study and user study show high usability of DaedalusData and its efficient support of experts in generating comprehensive overviews of thousands of particles, labeling of large quantities of particles, and externalizing knowledge to augment the dataset further. Reflecting on our approach, we discuss insights on dataset augmentation via human knowledge externalization, and on the scalability and trade-offs that come with the adoption of this approach in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456329",
            "id": "r_477",
            "s_ids": [
                "s_591",
                "s_266",
                "s_1154",
                "s_990"
            ],
            "type": "rich",
            "x": -8.625468254089355,
            "y": 18.65252113342285
        },
        {
            "title": "Ferry: Toward Better Understanding of Input/Output Space for Data Wrangling Scripts",
            "data": "Understanding the input and output of data wrangling scripts is crucial for various tasks like debugging code and onboarding new data. However, existing research on script understanding primarily focuses on revealing the process of data transformations, lacking the ability to analyze the potential scope, i.e., the space of script inputs and outputs. Meanwhile, constructing input/output space during script analysis is challenging, as the wrangling scripts could be semantically complex and diverse, and the association between different data objects is intricate. To facilitate data workers in understanding the input and output space of wrangling scripts, we summarize ten types of constraints to express table space and build a mapping between data transformations and these constraints to guide the construction of the input/output for individual transformations. Then, we propose a constraint generation model for integrating table constraints across multiple transformations. Based on the model, we develop Ferry, an interactive system that extracts and visualizes the data constraints describing the input and output space of data wrangling scripts, thereby enabling users to grasp the high-level semantics of complex scripts and locate the origins of faulty data transformations. Besides, Ferry provides example input and output data to assist users in interpreting the extracted constraints and checking and resolving the conflicts between these constraints and any uploaded dataset. Ferry's effectiveness and usability are evaluated through two usage scenarios and two case studies, including understanding, debugging, and checking both single and multiple scripts, with and without executable data. Furthermore, an illustrative application is presented to demonstrate Ferry's flexibility.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456328",
            "id": "r_478",
            "s_ids": [
                "s_668",
                "s_1070",
                "s_991",
                "s_1285",
                "s_1516",
                "s_735",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.314745903015137,
            "y": 17.222898483276367
        },
        {
            "title": "The Language of Infographics: Toward Understanding Conceptual Metaphor Use in Scientific Storytelling",
            "data": "We apply an approach from cognitive linguistics by mapping Conceptual Metaphor Theory (CMT) to the visualization domain to address patterns of visual conceptual metaphors that are often used in science infographics. Metaphors play an essential part in visual communication and are frequently employed to explain complex concepts. However, their use is often based on intuition, rather than following a formal process. At present, we lack tools and language for understanding and describing metaphor use in visualization to the extent where taxonomy and grammar could guide the creation of visual components, e.g., infographics. Our classification of the visual conceptual mappings within scientific representations is based on the breakdown of visual components in existing scientific infographics. We demonstrate the development of this mapping through a detailed analysis of data collected from four domains (biomedicine, climate, space, and anthropology) that represent a diverse range of visual conceptual metaphors used in the visual communication of science. This work allows us to identify patterns of visual conceptual metaphor use within the domains, resolve ambiguities about why specific conceptual metaphors are used, and develop a better overall understanding of visual metaphor use in scientific infographics. Our analysis shows that ontological and orientational conceptual metaphors are the most widely applied to translate complex scientific concepts. To support our findings we developed a visual exploratory tool based on the collected database that places the individual infographics on a spatio-temporal scale and illustrates the breakdown of visual conceptual metaphors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456327",
            "id": "r_479",
            "s_ids": [
                "s_503",
                "s_1323",
                "s_1013",
                "s_1354",
                "s_66"
            ],
            "type": "rich",
            "x": -5.910641670227051,
            "y": 19.3089656829834
        },
        {
            "title": "Unmasking Dunning-Kruger Effect in Visual Reasoning & Judgment",
            "data": "The Dunning-Kruger Effect (DKE) is a metacognitive phenomenon where low-skilled individuals tend to overestimate their competence while high-skilled individuals tend to underestimate their competence. This effect has been observed in a number of domains including humor, grammar, and logic. In this paper, we explore if and how DKE manifests in visual reasoning and judgment tasks. Across two online user studies involving (1) a sliding puzzle game and (2) a scatterplot-based categorization task, we demonstrate that individuals are susceptible to DKE in visual reasoning and judgment tasks: those who performed best underestimated their performance, while bottom performers overestimated their performance. In addition, we contribute novel analyses that correlate susceptibility of DKE with personality traits and user interactions. Our findings pave the way for novel modes of bias detection via interaction patterns and establish promising directions towards interventions tailored to an individual's personality traits. All materials and analyses are in supplemental materials: https://github.com/CAV-Lab/DKE_supplemental.git.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456326",
            "id": "r_480",
            "s_ids": [
                "s_832",
                "s_321",
                "s_6"
            ],
            "type": "rich",
            "x": -4.751156806945801,
            "y": 15.47903060913086
        },
        {
            "title": "SLInterpreter: An Exploratory and Iterative Human-AI Collaborative System for GNN-Based Synthetic Lethal Prediction",
            "data": "Synthetic Lethal (SL) relationships, though rare among the vast array of gene combinations, hold substantial promise for targeted cancer therapy. Despite advancements in AI model accuracy, there is still a significant need among domain experts for interpretive paths and mechanism explorations that align better with domain-specific knowledge, particularly due to the high costs of experimentation. To address this gap, we propose an iterative Human-AI collaborative framework with two key components: 1) Human-Engaged Knowledge Graph Refinement based on Metapath Strategies, which leverages insights from interpretive paths and domain expertise to refine the knowledge graph through metapath strategies with appropriate granularity. 2) Cross-Granularity SL Interpretation Enhancement and Mechanism Analysis, which aids experts in organizing and comparing predictions and interpretive paths across different granularities, uncovering new SL relationships, enhancing result interpretation, and elucidating potential mechanisms inferred by Graph Neural Network (GNN) models. These components cyclically optimize model predictions and mechanism explorations, enhancing expert involvement and intervention to build trust. Facilitated by SLInterpreter, this framework ensures that newly generated interpretive paths increasingly align with domain knowledge and adhere more closely to real-world biological principles through iterative Human-AI collaboration. We evaluate the framework's efficacy through a case study and expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456325",
            "id": "r_481",
            "s_ids": [
                "s_1019",
                "s_660",
                "s_414",
                "s_513",
                "s_549"
            ],
            "type": "rich",
            "x": -11.038872718811035,
            "y": 19.329631805419922
        },
        {
            "title": "\u201cIt's a Good Idea to Put It Into Words\u201d: Writing \u2018Rudders\u2019 in the Initial Stages of Visualization Design",
            "data": "Written language is a useful tool for non-visual creative activities like composing essays and planning searches. This paper investigates the integration of written language into the visualization design process. We create the idea of a 'writing rudder,\u2019 which acts as a guiding force or strategy for the designer. Via an interview study of 24 working visualization designers, we first established that only a minority of participants systematically use writing to aid in design. A second study with 15 visualization designers examined four different variants of written rudders: asking questions, stating conclusions, composing a narrative, and writing titles. Overall, participants had a positive reaction; designers recognized the benefits of explicitly writing down components of the design and indicated that they would use this approach in future design work. More specifically, two approaches - writing questions and writing conclusions/takeaways - were seen as beneficial across the design process, while writing narratives showed promise mainly for the creation stage. Although concerns around potential bias during data exploration were raised, participants also discussed strategies to mitigate such concerns. This paper contributes to a deeper understanding of the interplay between language and visualization, and proposes a straightforward, lightweight addition to the visualization design process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456324",
            "id": "r_482",
            "s_ids": [
                "s_1161",
                "s_1418",
                "s_1488"
            ],
            "type": "rich",
            "x": -3.9083473682403564,
            "y": 16.229412078857422
        },
        {
            "title": "Path-Based Design Model for Constructing and Exploring Alternative Visualisations",
            "data": "We present a path-based design model and system for designing and creating visualisations. Our model represents a systematic approach to constructing visual representations of data or concepts following a predefined sequence of steps. The initial step involves outlining the overall appearance of the visualisation by creating a skeleton structure, referred to as a flowpath. Subsequently, we specify objects, visual marks, properties, and appearance, storing them in a gene. Lastly, we map data onto the flowpath, ensuring suitable morphisms. Alternative designs are created by exchanging values in the gene. For example, designs that share similar traits, are created by making small incremental changes to the gene. Our design methodology fosters the generation of diverse creative concepts, space-filling visualisations, and traditional formats like bar charts, circular plots and pie charts. Through our implementation we showcase the model in action. As an example application, we integrate the output visualisations onto a smartwatch and visualisation dashboards. In this article we (1) introduce, define and explain the path model and discuss possibilities for its use, (2) present our implementation, results, and evaluation, and (3) demonstrate and evaluate an application of its use on a mobile watch.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456323",
            "id": "r_483",
            "s_ids": [
                "s_691",
                "s_988",
                "s_49",
                "s_1303"
            ],
            "type": "rich",
            "x": -5.994189262390137,
            "y": 17.30042266845703
        },
        {
            "title": "Distributed Augmentation, Hypersweeps, and Branch Decomposition of Contour Trees for Scientific Exploration",
            "data": "Contour trees describe the topology of level sets in scalar fields and are widely used in topological data analysis and visualization. A main challenge of utilizing contour trees for large-scale scientific data is their computation at scale using high-performance computing. To address this challenge, recent work has introduced distributed hierarchical contour trees for distributed computation and storage of contour trees. However, effective use of these distributed structures in analysis and visualization requires subsequent computation of geometric properties and branch decomposition to support contour extraction and exploration. In this work, we introduce distributed algorithms for augmentation, hypersweeps, and branch decomposition that enable parallel computation of geometric properties, and support the use of distributed contour trees as query structures for scientific exploration. We evaluate the parallel performance of these algorithms and apply them to identify and extract important contours for scientific visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456322",
            "id": "r_484",
            "s_ids": [
                "s_323",
                "s_1353",
                "s_873",
                "s_836",
                "s_1041"
            ],
            "type": "rich",
            "x": 7.31845235824585,
            "y": 5.4571309089660645
        },
        {
            "title": "AdaMotif: Graph Simplification via Adaptive Motif Design",
            "data": "With the increase of graph size, it becomes difficult or even impossible to visualize graph structures clearly within the limited screen space. Consequently, it is crucial to design effective visual representations for large graphs. In this paper, we propose AdaMotif, a novel approach that can capture the essential structure patterns of large graphs and effectively reveal the overall structures via adaptive motif designs. Specifically, our approach involves partitioning a given large graph into multiple subgraphs, then clustering similar subgraphs and extracting similar structural information within each cluster. Subsequently, adaptive motifs representing each cluster are generated and utilized to replace the corresponding subgraphs, leading to a simplified visualization. Our approach aims to preserve as much information as possible from the subgraphs while simplifying the graph efficiently. Notably, our approach successfully visualizes crucial community information within a large graph. We conduct case studies and a user study using real-world graphs to validate the effectiveness of our proposed approach. The results demonstrate the capability of our approach in simplifying graphs while retaining important structural and community information.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456321",
            "id": "r_485",
            "s_ids": [
                "s_374",
                "s_164",
                "s_1269",
                "s_1183",
                "s_1123",
                "s_736",
                "s_1210"
            ],
            "type": "rich",
            "x": -9.67426586151123,
            "y": 18.517900466918945
        },
        {
            "title": "User Experience of Visualizations in Motion: A Case Study and Design Considerations",
            "data": "We present a systematic review, an empirical study, and a first set of considerations for designing visualizations in motion, derived from a concrete scenario in which these visualizations were used to support a primary task. In practice, when viewers are confronted with embedded visualizations, they often have to focus on a primary task and can only quickly glance at a visualization showing rich, often dynamically updated, information. As such, the visualizations must be designed so as not to distract from the primary task, while at the same time being readable and useful for aiding the primary task. For example, in games, players who are engaged in a battle have to look at their enemies but also read the remaining health of their own game character from the health bar over their character's head. Many trade-ofts are possible in the design of embedded visualizations in such dynamic scenarios, which we explore in-depth in this paper with a focus on user experience. We use video games as an example of an application context with a rich existing set of visualizations in motion. We begin our work with a systematic review of in-game visualizations in motion. Next, we conduct an empirical user study to investigate how different embedded visualizations in motion designs impact user experience. We conclude with a set of considerations and trade-offs for designing visualizations in motion more broadly as derived from what we learned about video games. All supplemental materials of this paper are available at osf.io/3v8wm/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456319",
            "id": "r_486",
            "s_ids": [
                "s_1034",
                "s_189",
                "s_397",
                "s_182",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.269586086273193,
            "y": 18.358766555786133
        },
        {
            "title": "PREVis: Perceived Readability Evaluation for Visualizations",
            "data": "We developed and validated an instrument to measure the perceived readability in data visualization: PREVis. Researchers and practitioners can easily use this instrument as part of their evaluations to compare the perceived readability of different visual data representations. Our instrument can complement results from controlled experiments on user task performance or provide additional data during in-depth qualitative work such as design iterations when developing a new technique. Although readability is recognized as an essential quality of data visualizations, so far there has not been a unified definition of the construct in the context of visual representations. As a result, researchers often lack guidance for determining how to ask people to rate their perceived readability of a visualization. To address this issue, we engaged in a rigorous process to develop the first validated instrument targeted at the subjective readability of visual data representations. Our final instrument consists of 11 items across 4 dimensions: understandability, layout clarity, readability of data values, and readability of data patterns. We provide the questionnaire as a document with implementation guidelines on osf.io/9cg8j. Beyond this instrument, we contribute a discussion of how researchers have previously assessed visualization readability, and an analysis of the factors underlying perceived readability in visual data representations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456318",
            "id": "r_487",
            "s_ids": [
                "s_669",
                "s_1588",
                "s_1124",
                "s_1323"
            ],
            "type": "rich",
            "x": -6.318803310394287,
            "y": 18.45305633544922
        },
        {
            "title": "Mixing Linters with GUIs: A Color Palette Design Probe",
            "data": "Visualization linters are end-user facing evaluators that automatically identify potential chart issues. These spell-checker like systems offer a blend of interpretability and customization that is not found in other forms of automated assistance. However, existing linters do not model context and have primarily targeted users who do not need assistance, resulting in obvious\u2014even annoying\u2014advice. We investigate these issues within the domain of color palette design, which serves as a microcosm of visualization design concerns. We contribute a GUI-based color palette linter as a design probe that covers perception, accessibility, context, and other design criteria, and use it to explore visual explanations, integrated fixes, and user defined linting rules. Through a formative interview study and theory-driven analysis, we find that linters can be meaningfully integrated into graphical contexts thereby addressing many of their core issues. We discuss implications for integrating linters into visualization tools, developing improved assertion languages, and supporting end-user tunable advice\u2014all laying the groundwork for more effective visualization linters in any context.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456317",
            "id": "r_488",
            "s_ids": [
                "s_1293",
                "s_1368",
                "s_1386"
            ],
            "type": "rich",
            "x": -3.767261028289795,
            "y": 17.071435928344727
        },
        {
            "title": "Evaluating Force-Based Haptics for Immersive Tangible Interactions with Surface Visualizations",
            "data": "Haptic feedback provides an essential sensory stimulus crucial for interaction and analyzing three-dimensional spatio-temporal phenomena on surface visualizations. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) catalyzes haptic interactions on surface visualizations. Various interaction modes, encompassing both mid-air and on-surface interactions\u2014with or without the application of assisting force stimuli\u2014have been explored using haptic force feedback devices. In this paper, we evaluate the use of on-surface and assisted on-surface haptic modes of interaction compared to a no-haptic interaction mode. A force-based haptic stylus is used for all three modalities; the on-surface mode uses collision based forces, whereas the assisted on-surface mode is accompanied by an additional snapping force. We conducted a within-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across all three modes, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took almost the same time to brush curves using all the interaction modes. They could draw smoother curves using the on-surface interaction modes compared to the no-haptic mode. However, the assisted on-surface mode provided better accuracy than the on-surface mode. The on-surface mode was slower in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to aid the design of haptics-based tangible interactions for surface visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456316",
            "id": "r_489",
            "s_ids": [
                "s_1018",
                "s_1084"
            ],
            "type": "rich",
            "x": -7.521579265594482,
            "y": -1.1555873155593872
        },
        {
            "title": "BEMTrace: Visualization-Driven Approach for Deriving Building Energy Models from BIM",
            "data": "Building Information Modeling (BIM) describes a central data pool covering the entire life cycle of a construction project. Similarly, Building Energy Modeling (BEM) describes the process of using a 3D representation of a building as a basis for thermal simulations to assess the building's energy performance. This paper explores the intersection of BIM and BEM, focusing on the challenges and methodologies in converting BIM data into BEM representations for energy performance analysis. BEMTrace integrates 3D data wrangling techniques with visualization methodologies to enhance the accuracy and traceability of the BIM-to-BEM conversion process. Through parsing, error detection, and algorithmic correction of BIM data, our methods generate valid BEM models suitable for energy simulation. Visualization techniques provide transparent insights into the conversion process, aiding error identification, validation, and user comprehension. We introduce context-adaptive selections to facilitate user interaction and to show that the BEMTrace workflow helps users understand complex 3D data wrangling processes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456315",
            "id": "r_490",
            "s_ids": [
                "s_780",
                "s_1417",
                "s_733",
                "s_108",
                "s_1033",
                "s_692"
            ],
            "type": "rich",
            "x": -5.642838478088379,
            "y": 14.705206871032715
        },
        {
            "title": "UnDRground Tubes: Exploring Spatial Data with Multidimensional Projections and Set Visualization",
            "data": "In various scientific and industrial domains, analyzing multivariate spatial data, i.e., vectors associated with spatial locations, is common practice. To analyze those datasets, analysts may turn to methods such as Spatial Blind Source Separation (SBSS). Designed explicitly for spatial data analysis, SBSS finds latent components in the dataset and is superior to popular non-spatial methods, like PCA. However, when analysts try different tuning parameter settings, the amount of latent components complicates analytical tasks. Based on our years-long collaboration with SBSS researchers, we propose a visualization approach to tackle this challenge. The main component is UnDRground Tubes (UT), a general-purpose idiom combining ideas from set visualization and multidimensional projections. We describe the UT visualization pipeline and integrate UT into an interactive multiple-view system. We demonstrate its effectiveness through interviews with SBSS experts, a qualitative evaluation with visualization experts, and computational experiments. SBSS experts were excited about our approach. They saw many benefits for their work and potential applications for geostatistical data analysis more generally. UT was also well received by visualization experts. Our benchmarks show that UT projections and its heuristics are appropriate.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456314",
            "id": "r_491",
            "s_ids": [
                "s_59",
                "s_262",
                "s_1195",
                "s_90"
            ],
            "type": "rich",
            "x": -5.484437465667725,
            "y": 14.977408409118652
        },
        {
            "title": "Quality Metrics and Reordering Strategies for Revealing Patterns in BioFabric Visualizations",
            "data": "Visualizing relational data is crucial for understanding complex connections between entities in social networks, political affiliations, or biological interactions. Well-known representations like node-link diagrams and adjacency matrices offer valuable insights, but their effectiveness relies on the ability to identify patterns in the underlying topological structure. Reordering strategies and layout algorithms play a vital role in the visualization process since the arrangement of nodes, edges, or cells influences the visibility of these patterns. The BioFabric visualization combines elements of node-link diagrams and adjacency matrices, leveraging the strengths of both, the visual clarity of node-link diagrams and the tabular organization of adjacency matrices. A unique characteristic of BioFabric is the possibility to reorder nodes and edges separately. This raises the question of which combination of layout algorithms best reveals certain patterns. In this paper, we discuss patterns and anti-patterns in BioFabric, such as staircases or escalators, relate them to already established patterns, and propose metrics to evaluate their quality. Based on these quality metrics, we compared combinations of well-established reordering techniques applied to BioFabric with a well-known benchmark data set. Our experiments indicate that the edge order has a stronger influence on revealing patterns than the node layout. The results show that the best combination for revealing staircases is a barycentric node layout, together with an edge order based on node indices and length. Our research contributes a first building block for many promising future research directions, which we also share and discuss. A free copy of this paper and all supplemental materials are available at https://osf.io/9mt8r/?view_only=b7t0dfbe550e3404f83059afdc60184c6.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456312",
            "id": "r_492",
            "s_ids": [
                "s_436",
                "s_776",
                "s_330",
                "s_1100",
                "s_286"
            ],
            "type": "rich",
            "x": -6.8480119705200195,
            "y": 21.0538387298584
        },
        {
            "title": "Visualization Atlases: Explaining and Exploring Complex Topics Through Data, Visualization, and Narration",
            "data": "This paper defines, analyzes, and discusses the emerging genre of visualization atlases. We currently witness an increase in web-based, data-driven initiatives that call themselves \u201catlases\u201d while explaining complex, contemporary issues through data and visualizations: climate change, sustainability, AI, or cultural discoveries. To understand this emerging genre and inform their design, study, and authoring support, we conducted a systematic analysis of 33 visualization atlases and semi-structured interviews with eight visualization atlas creators. Based on our results, we contribute (1) a definition of a visualization atlas as a compendium of (web) pages aimed at explaining and supporting exploration of data about a dedicated topic through data, visualizations and narration. (2) a set of design patterns of 8 design dimensions, (3) insights into the atlas creation from interviews and (4) the definition of 5 visualization atlas genres. We found that visualization atlases are unique in the way they combine i) exploratory visualization, ii) narrative elements from data-driven storytelling and iii) structured navigation mechanisms. They target a wide range of audiences with different levels of domain knowledge, acting as tools for study, communication, and discovery. We conclude with a discussion of current design practices and emerging questions around the ethics and potential real-world impact of visualization atlases, aimed to inform the design and study of visualization atlases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456311",
            "id": "r_493",
            "s_ids": [
                "s_831",
                "s_1516",
                "s_1221",
                "s_1547"
            ],
            "type": "rich",
            "x": -6.680880069732666,
            "y": 17.1116886138916
        },
        {
            "title": "Promises and Pitfalls: Using Large Language Models to Generate Visualization Items",
            "data": "Visualization items\u2014factual questions about visualizations that ask viewers to accomplish visualization tasks-are regularly used in the field of information visualization as educational and evaluative materials. For example, researchers of visualization literacy require large, diverse banks of items to conduct studies where the same skill is measured repeatedly on the same participants. Yet, generating a large number of high-quality, diverse items requires significant time and expertise. To address the critical need for a large number of diverse visualization items in education and research, this paper investigates the potential for large language models (LLMS) to automate the generation of multiple-choice visualization items. Through an iterative design process, we develop the VILA (Visualization Items Generated by Large LAnguage Models) pipeline, for efficiently generating visualization items that measure people's ability to accomplish visualization tasks. We use the VILA pipeline to generate 1,404 candidate items across 12 chart types and 13 visualization tasks. In collaboration with 11 visualization experts, we develop an evaluation rulebook which we then use to rate the quality of all candidate items. The result is the VILA bank of ~1, 100 items. From this evaluation, we also identify and classify current limitations of the VILA pipeline, and discuss the role of human oversight in ensuring quality. In addition, we demonstrate an application of our work by creating a visualization literacy test, VILA-VLAT, which measures people's ability to complete a diverse set of tasks on various types of visualizations; comparing it to the existing VLAT, VILA-VLAT shows moderate to high convergent validity (R = 0.70). Lastly, we discuss the application areas of the VILA pipeline and the VILA bank and provide practical recommendations for their use. All supplemental materials are available at https://osf.io/ysrhq/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456309",
            "id": "r_494",
            "s_ids": [
                "s_1058",
                "s_1482",
                "s_594",
                "s_145",
                "s_607",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.1520731449127197,
            "y": 16.572782516479492
        },
        {
            "title": "A Large-Scale Sensitivity Analysis on Latent Embeddings and Dimensionality Reductions for Text Spatializations",
            "data": "The semantic similarity between documents of a text corpus can be visualized using map-like metaphors based on two-dimensional scatterplot layouts. These layouts result from a dimensionality reduction on the document-term matrix or a representation within a latent embedding, including topic models. Thereby, the resulting layout depends on the input data and hyperparameters of the dimensionality reduction and is therefore affected by changes in them. Furthermore, the resulting layout is affected by changes in the input data and hyperparameters of the dimensionality reduction. However, such changes to the layout require additional cognitive efforts from the user. In this work, we present a sensitivity study that analyzes the stability of these layouts concerning (1) changes in the text corpora, (2) changes in the hyperparameter, and (3) randomness in the initialization. Our approach has two stages: data measurement and data analysis. First, we derived layouts for the combination of three text corpora and six text embeddings and a grid-search-inspired hyperparameter selection of the dimensionality reductions. Afterward, we quantified the similarity of the layouts through ten metrics, concerning local and global structures and class separation. Second, we analyzed the resulting 42 817 tabular data points in a descriptive statistical analysis. From this, we derived guidelines for informed decisions on the layout algorithm and highlight specific hyperparameter settings. We provide our implementation as a Git repository at hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study and results as Zenodo archive at DOI:10.5281/zenodo.12772898.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456308",
            "id": "r_495",
            "s_ids": [
                "s_921",
                "s_1564",
                "s_530",
                "s_1376",
                "s_632",
                "s_578"
            ],
            "type": "rich",
            "x": -8.834040641784668,
            "y": 18.476030349731445
        },
        {
            "title": "Who Let the Guards Out: Visual Support for Patrolling Games",
            "data": "Effective security patrol management is critical for ensuring safety in diverse environments such as art galleries, airports, and factories. The behavior of patrols in these situations can be modeled by patrolling games. They simulate the behavior of the patrol and adversary in the building, which is modeled as a graph of interconnected nodes representing rooms. The designers of algorithms solving the game face the problem of analyzing complex graph layouts with temporal dependencies. Therefore, appropriate visual support is crucial for them to work effectively. In this paper, we present a novel tool that helps the designers of patrolling games explore the outcomes of the proposed algorithms and approaches, evaluate their success rate, and propose modifications that can improve their solutions. Our tool offers an intuitive and interactive interface, featuring a detailed exploration of patrol routes and probabilities of taking them, simulation of patrols, and other requested features. In close collaboration with experts in designing patrolling games, we conducted three case studies demonstrating the usage and usefulness of our tool. The prototype of the tool, along with exemplary datasets, is available at https://gitlab.fi.muni.cz/formela/strategy-vizualizer.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456306",
            "id": "r_496",
            "s_ids": [
                "s_483",
                "s_88",
                "s_650",
                "s_469",
                "s_1354"
            ],
            "type": "rich",
            "x": -5.7852983474731445,
            "y": 19.463993072509766
        },
        {
            "title": "Beware of Validation by Eye: Visual Validation of Linear Trends in Scatterplots",
            "data": "Visual validation of regression models in scatterplots is a common practice for assessing model quality, yet its efficacy remains unquantified. We conducted two empirical experiments to investigate individuals' ability to visually validate linear regression models (linear trends) and to examine the impact of common visualization designs on validation quality. The first experiment showed that the level of accuracy for visual estimation of slope (i.e., fitting a line to data) is higher than for visual validation of s lope (i.e., accepting a shown line). Notably, we found bias toward slopes that are \u201ctoo steep\u201d in both cases. This lead to novel insights that participants naturally assessed regression with orthogonal distances between the points and the line (i.e., ODR regression) rather than the common vertical distances (OLS regression). In the second experiment, we investigated whether incorporating common designs for regression visualization (error lines, bounding boxes, and confidence intervals) would improve visual validation. Even though error lines reduced validation bias, results failed to show the desired improvements in accuracy for any design. Overall, our findings suggest caution in using visual model validation for linear trends in scatterplots.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456305",
            "id": "r_497",
            "s_ids": [
                "s_1314",
                "s_366",
                "s_179",
                "s_267"
            ],
            "type": "rich",
            "x": -4.879156589508057,
            "y": 15.959578514099121
        },
        {
            "title": "Defogger: A Visual Analysis Approach for Data Exploration of Sensitive Data Protected by Differential Privacy",
            "data": "Differential privacy ensures the security of individual privacy but poses challenges to data exploration processes because the limited privacy budget incapacitates the flexibility of exploration and the noisy feedback of data requests leads to confusing uncertainty. In this study, we take the lead in describing corresponding exploration scenarios, including underlying requirements and available exploration strategies. To facilitate practical applications, we propose a visual analysis approach to the formulation of exploration strategies. Our approach applies a reinforcement learning model to provide diverse suggestions for exploration strategies according to the exploration intent of users. A novel visual design for representing uncertainty in correlation patterns is integrated into our prototype system to support the proposed approach. Finally, we implemented a user study and two case studies. The results of these studies verified that our approach can help develop strategies that satisfy the exploration intent of users.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456304",
            "id": "r_498",
            "s_ids": [
                "s_1531",
                "s_154",
                "s_394"
            ],
            "type": "rich",
            "x": -3.320925235748291,
            "y": 17.65802574157715
        },
        {
            "title": "Attention-Aware Visualization: Tracking and Responding to User Perception Over Time",
            "data": "We propose the notion of attention-aware visualizations (AAVs) that track the user's perception of a visual representation over time and feed this information back to the visualization. Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user's attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user's gaze on a visualization and its parts; (2) tracking the user's attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user's gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456300",
            "id": "r_499",
            "s_ids": [
                "s_1281",
                "s_1023",
                "s_49",
                "s_988",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.002643585205078,
            "y": 16.02139663696289
        },
        {
            "title": "Understanding Visualization Authoring Techniques for Genomics Data in the Context of Personas and Tasks",
            "data": "Genomics experts rely on visualization to extract and share insights from complex and large-scale datasets. Beyond off-the-shelf tools for data exploration, there is an increasing need for platforms that aid experts in authoring customized visualizations for both exploration and communication of insights. A variety of interactive techniques have been proposed for authoring data visualizations, such as template editing, shelf configuration, natural language input, and code editors. However, it remains unclear how genomics experts create visualizations and which techniques best support their visualization tasks and needs. To address this gap, we conducted two user studies with genomics researchers: (1) semi-structured interviews (n=20) to identify the tasks, user contexts, and current visualization authoring techniques and (2) an exploratory study (n=13) using visual probes to elicit users' intents and desired techniques when creating visualizations. Our contributions include (1) a characterization of how visualization authoring is currently utilized in genomics visualization, identifying limitations and benefits in light of common criteria for authoring tools, and (2) generalizable design implications for genomics visualization authoring tools based on our findings on task- and user-specific usefulness of authoring techniques. All supplemental materials are available at https://osf.io/bdj4v/",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456298",
            "id": "r_500",
            "s_ids": [
                "s_1402",
                "s_1266",
                "s_260",
                "s_478",
                "s_393"
            ],
            "type": "rich",
            "x": -8.36540412902832,
            "y": 16.870927810668945
        },
        {
            "title": "Manipulable Semantic Components: A Computational Representation of Data Visualization Scenes",
            "data": "Various data visualization applications such as reverse engineering and interactive authoring require a vocabulary that describes the structure of visualization scenes and the procedure to manipulate them. A few scene abstractions have been proposed, but they are restricted to specific applications for a limited set of visualization types. A unified and expressive model of data visualization scenes for different applications has been missing. To fill this gap, we present Manipulable Semantic Components (MSC), a computational representation of data visualization scenes, to support applications in scene understanding and augmentation. MSC consists of two parts: a unified object model describing the structure of a visualization scene in terms of semantic components, and a set of operations to generate and modify the scene components. We demonstrate the benefits of MSC in three applications: visualization authoring, visualization deconstruction and reuse, and animation specification.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456296",
            "id": "r_501",
            "s_ids": [
                "s_1185",
                "s_908",
                "s_307"
            ],
            "type": "rich",
            "x": -4.359776973724365,
            "y": 17.025625228881836
        },
        {
            "title": "What University Students Learn In Visualization Classes",
            "data": "As a step towards improving visualization literacy, this work investigates how students approach reading visualizations differently after taking a university-level visualization course. We asked students to verbally walk through their process of making sense of unfamiliar visualizations, and conducted a qualitative analysis of these walkthroughs. Our qualitative analysis found that after taking a visualization course, students engaged with visualizations in more sophisticated ways: they were more likely to exhibit design empathy by thinking critically about the tradeoffs behind why a chart was designed in a particular way, and were better able to deconstruct a chart to make sense of it. We also gave students a quantitative assessment of visualization literacy and found no evidence of scores improving after the class, likely because the test we used focused on a different set of skills than those emphasized in visualization classes. While current measurement instruments for visualization literacy are useful, we propose developing standardized assessments for additional aspects of visualization literacy, such as deconstruction and design empathy. We also suggest that these additional aspects could be incorporated more explicitly in visualization courses. All supplemental materials are available at https://osf.io/w5pum/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456291",
            "id": "r_502",
            "s_ids": [
                "s_1318",
                "s_1031"
            ],
            "type": "rich",
            "x": -3.185474157333374,
            "y": 16.631906509399414
        },
        {
            "title": "Charting EDA: Characterizing Interactive Visualization Use in Computational Notebooks with a Mixed-Methods Formalism",
            "data": "Interactive visualizations are powerful tools for Exploratory Data Analysis (EDA), but how do they affect the observations analysts make about their data? We conducted a qualitative experiment with 13 professional data scientists analyzing two datasets with Jupyter notebooks, collecting a rich dataset of interaction traces and think-aloud utterances. By qualitatively coding participant utterances, we introduce a formalism that describes EDA as a sequence of analysis states, where each state is comprised of either a representation an analyst constructs (e.g., the output of a data frame, an interactive visualization, etc.) or an observation the analyst makes (e.g., about missing data, the relationship between variables, etc.). By applying our formalism to our dataset, we identify that interactive visualizations, on average, lead to earlier and more complex insights about relationships between dataset attributes compared to static visualizations. Moreover, by calculating metrics such as revisit count and representational diversity, we uncover that some representations serve more as \u201cplanning aids\u201d during EDA rather than tools strictly for hypothesis-answering. We show how these measures help identify other patterns of analysis behavior, such as the \u201c80-20 rule\u201d, where a small subset of representations drove the majority of observations. Based on these findings, we offer design guidelines for interactive exploratory analysis tooling and reflect on future directions for studying the role that visualizations play in EDA.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456217",
            "id": "r_503",
            "s_ids": [
                "s_130",
                "s_403",
                "s_494",
                "s_920"
            ],
            "type": "rich",
            "x": -4.2957868576049805,
            "y": 16.087648391723633
        },
        {
            "title": "Team-Scouter: Simulative Visual Analytics of Soccer Player Scouting",
            "data": "In soccer, player scouting aims to find players suitable for a team to increase the winning chance in future matches. To scout suitable players, coaches and analysts need to consider whether the players will perform well in a new team, which is hard to learn directly from their historical performances. Match simulation methods have been introduced to scout players by estimating their expected contributions to a new team. However, they usually focus on the simulation of match results and hardly support interactive analysis to navigate potential target players and compare them in fine-grained simulated behaviors. In this work, we propose a visual analytics method to assist soccer player scouting based on match simulation. We construct a two-level match simulation framework for estimating both match results and player behaviors when a player comes to a new team. Based on the framework, we develop a visual analytics system, Team-Scouter, to facilitate the simulative-based soccer player scouting process through player navigation, comparison, and investigation. With our system, coaches and analysts can find potential players suitable for the team and compare them on historical and expected performances. For an in-depth investigation of the players' expected performances, the system provides a visual comparison between the simulated behaviors of the player and the actual ones. The usefulness and effectiveness of the system are demonstrated by two case studies on a real-world dataset and an expert interview.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456216",
            "id": "r_504",
            "s_ids": [
                "s_400",
                "s_1574",
                "s_643",
                "s_1147",
                "s_1065",
                "s_647",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.370741844177246,
            "y": 17.23230743408203
        },
        {
            "title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets",
            "data": "Acute stroke demands prompt diagnosis and treatment to achieve optimal patient outcomes. However, the intricate and irregular nature of clinical data associated with acute stroke, particularly blood pressure (BP) measurements, presents substantial obstacles to effective visual analytics and decision-making. Through a year-long collaboration with experienced neurologists, we developed PhenoFlow, a visual analytics system that leverages the collaboration between human and Large Language Models (LLMs) to analyze the extensive and complex data of acute ischemic stroke patients. PhenoFlow pioneers an innovative workflow, where the LLM serves as a data wrangler while neurologists explore and supervise the output using visualizations and natural language interactions. This approach enables neurologists to focus more on decision-making with reduced cognitive load. To protect sensitive patient information, PhenoFlow only utilizes metadata to make inferences and synthesize executable codes, without accessing raw patient data. This ensures that the results are both reproducible and interpretable while maintaining patient privacy. The system incorporates a slice-and-wrap design that employs temporal folding to create an overlaid circular visualization. Combined with a linear bar graph, this design aids in exploring meaningful patterns within irregularly measured BP data. Through case studies, PhenoFlow has demonstrated its capability to support iterative analysis of extensive clinical datasets, reducing cognitive load and enabling neurologists to make well-informed decisions. Grounded in long-term collaboration with domain experts, our research demonstrates the potential of utilizing LLMs to tackle current challenges in data-driven clinical decision-making for acute ischemic stroke patients.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456215",
            "id": "r_505",
            "s_ids": [
                "s_952",
                "s_1319",
                "s_649",
                "s_235",
                "s_1430",
                "s_874",
                "s_1227"
            ],
            "type": "rich",
            "x": -1.3571066856384277,
            "y": 18.06316375732422
        },
        {
            "title": "CompositingVis: Exploring Interactions for Creating Composite Visualizations in Immersive Environments",
            "data": "Composite visualization represents a widely embraced design that combines multiple visual representations to create an integrated view. However, the traditional approach of creating composite visualizations in immersive environments typically occurs asynchronously outside of the immersive space and is carried out by experienced experts. In this work, we aim to empower users to participate in the creation of composite visualization within immersive environments through embodied interactions. This could provide a flexible and fluid experience with immersive visualization and has the potential to facilitate understanding of the relationship between visualization views. We begin with developing a design space of embodied interactions to create various types of composite visualizations with the consideration of data relationships. Drawing inspiration from people's natural experience of manipulating physical objects, we design interactions based on the combination of 3D manipulations in immersive environments. Building upon the design space, we present a series of case studies showcasing the interaction to create different kinds of composite visualizations in virtual reality. Subsequently, we conduct a user study to evaluate the usability of the derived interaction techniques and user experience of creating composite visualizations through embodied interactions. We find that empowering users to participate in composite visualizations through embodied interactions enables them to flexibly leverage different visualization views for understanding and communicating the relationships between different views, which underscores the potential of several future application scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456210",
            "id": "r_506",
            "s_ids": [
                "s_191",
                "s_866",
                "s_459",
                "s_1556",
                "s_41"
            ],
            "type": "rich",
            "x": -10.837485313415527,
            "y": 19.102163314819336
        },
        {
            "title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent",
            "data": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456200",
            "id": "r_507",
            "s_ids": [
                "s_104",
                "s_1574",
                "s_144",
                "s_498",
                "s_682",
                "s_248",
                "s_647",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.334527015686035,
            "y": 17.172325134277344
        },
        {
            "title": "Aardvark: Composite Visualizations of Trees, Time-Series, and Images",
            "data": "How do cancer cells grow, divide, proliferate, and die? How do drugs influence these processes? These are difficult questions that we can attempt to answer with a combination of time-series microscopy experiments, classification algorithms, and data visualization. However, collecting this type of data and applying algorithms to segment and track cells and construct lineages of proliferation is error-prone; and identifying the errors can be challenging since it often requires cross-checking multiple data types. Similarly, analyzing and communicating the results necessitates synthesizing different data types into a single narrative. State-of-the-art visualization methods for such data use independent line charts, tree diagrams, and images in separate views. However, this spatial separation requires the viewer of these charts to combine the relevant pieces of data in memory. To simplify this challenging task, we describe design principles for weaving cell images, time-series data, and tree data into a cohesive visualization. Our design principles are based on choosing a primary data type that drives the layout and integrates the other data types into that layout. We then introduce Aardvark, a system that uses these principles to implement novel visualization techniques. Based on Aardvark, we demonstrate the utility of each of these approaches for discovery, communication, and data debugging in a series of case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456193",
            "id": "r_508",
            "s_ids": [
                "s_160",
                "s_945",
                "s_612",
                "s_215"
            ],
            "type": "rich",
            "x": -2.233802318572998,
            "y": 15.414934158325195
        },
        {
            "title": "Does This Have a Particular Meaning? Interactive Pattern Explanation for Network Visualizations",
            "data": "This paper presents an interactive technique to explain visual patterns in network visualizations to analysts who do not understand these visualizations and who are learning to read them. Learning a visualization requires mastering its visual grammar and decoding information presented through visual marks, graphical encodings, and spatial configurations. To help people learn network visualization designs and extract meaningful information, we introduce the concept of interactive pattern explanation that allows viewers to select an arbitrary area in a visualization, then automatically mines the underlying data patterns, and explains both visual and data patterns present in the viewer's selection. In a qualitative and a quantitative user study with a total of 32 participants, we compare interactive pattern explanations to textual-only and visual-only (cheatsheets) explanations. Our results show that interactive explanations increase learning of i) unfamiliar visualizations, ii) patterns in network science, and iii) the respective network terminology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456192",
            "id": "r_509",
            "s_ids": [
                "s_1516",
                "s_318",
                "s_765",
                "s_198",
                "s_1221"
            ],
            "type": "rich",
            "x": -6.52415132522583,
            "y": 16.902193069458008
        },
        {
            "title": "Revealing Interaction Dynamics: Multi-Level Visual Exploration of User Strategies with an Interactive Digital Environment",
            "data": "We present a visual analytics approach for multi-level visual exploration of users' interaction strategies in an interactive digital environment. The use of interactive touchscreen exhibits in informal learning environments, such as museums and science centers, often incorporate frameworks that classify learning processes, such as Bloom's taxonomy, to achieve better user engagement and knowledge transfer. To analyze user behavior within these digital environments, interaction logs are recorded to capture diverse exploration strategies. However, analysis of such logs is challenging, especially in terms of coupling interactions and cognitive learning processes, and existing work within learning and educational contexts remains limited. To address these gaps, we develop a visual analytics approach for analyzing interaction logs that supports exploration at the individual user level and multi-user comparison. The approach utilizes algorithmic methods to identify similarities in users' interactions and reveal their exploration strategies. We motivate and illustrate our approach through an application scenario, using event sequences derived from interaction log data in an experimental study conducted with science center visitors from diverse backgrounds and demographics. The study involves 14 users completing tasks of increasing complexity, designed to stimulate different levels of cognitive learning processes. We implement our approach in an interactive visual analytics prototype system, named VISID, and together with domain experts, discover a set of task-solving exploration strategies, such as \u201ccascading\u201d and \u201cnested-loop\u201d, which reflect different levels of learning processes from Bloom's taxonomy. Finally, we discuss the generalizability and scalability of the presented system and the need for further research with data acquired in the wild.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456187",
            "id": "r_510",
            "s_ids": [
                "s_317",
                "s_1403",
                "s_1278",
                "s_1391",
                "s_73",
                "s_238"
            ],
            "type": "rich",
            "x": -7.479556560516357,
            "y": 17.764080047607422
        },
        {
            "title": "Loops: Leveraging Provenance and Visualization to Support Exploratory Data Analysis in Notebooks",
            "data": "Exploratory data science is an iterative process of obtaining, cleaning, profiling, analyzing, and interpreting data. This cyclical way of working creates challenges within the linear structure of computational notebooks, leading to issues with code quality, recall, and reproducibility. To remedy this, we present Loops, a set of visual support techniques for iterative and exploratory data analysis in computational notebooks. Loops leverages provenance information to visualize the impact of changes made within a notebook. In visualizations of the notebook provenance, we trace the evolution of the notebook over time and highlight differences between versions. Loops visualizes the provenance of code, markdown, tables, visualizations, and images and their respective differences. Analysts can explore these differences in detail in a separate view. Loops not only makes the analysis process transparent but also supports analysts in their data science work by showing the effects of changes and facilitating comparison of multiple versions. We demonstrate our approach's utility and potential impact in two use cases and feedback from notebook users from various backgrounds. This paper and all supplemental materials are available at https://osf.io/79eyn.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456186",
            "id": "r_511",
            "s_ids": [
                "s_338",
                "s_619",
                "s_215",
                "s_580"
            ],
            "type": "rich",
            "x": -2.2293989658355713,
            "y": 15.437396049499512
        },
        {
            "title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations",
            "data": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456182",
            "id": "r_512",
            "s_ids": [
                "s_1066",
                "s_759",
                "s_1080"
            ],
            "type": "rich",
            "x": -2.1861181259155273,
            "y": 18.04265785217285
        },
        {
            "title": "Blowing Seeds Across Gardens: Visualizing Implicit Propagation of Cross-Platform Social Media Posts",
            "data": "Propagation analysis refers to studying how information spreads on social media, a pivotal endeavor for understanding social sentiment and public opinions. Numerous studies contribute to visualizing information spread, but few have considered the implicit and complex diffusion patterns among multiple platforms. To bridge the gap, we summarize cross-platform diffusion patterns with experts and identify significant factors that dissect the mechanisms of cross-platform information spread. Based on that, we propose an information diffusion model that estimates the likelihood of a topic/post spreading among different social media platforms. Moreover, we propose a novel visual metaphor that encapsulates cross-platform propagation in a manner analogous to the spread of seeds across gardens. Specifically, we visualize platforms, posts, implicit cross-platform routes, and salient instances as elements of a virtual ecosystem \u2014 gardens, flowers, winds, and seeds, respectively. We further develop a visual analytic system, namely BloomWind, that enables users to quickly identify the cross-platform diffusion patterns and investigate the relevant social media posts. Ultimately, we demonstrate the usage of BloomWind through two case studies and validate its effectiveness using expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456181",
            "id": "r_513",
            "s_ids": [
                "s_1424",
                "s_1470",
                "s_1179",
                "s_456",
                "s_763",
                "s_626",
                "s_46",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.347116470336914,
            "y": 17.360111236572266
        },
        {
            "title": "Entanglements for Visualization: Changing Research Outcomes through Feminist Theory",
            "data": "A growing body of work draws on feminist thinking to challenge assumptions about how people engage with and use visualizations. This work draws on feminist values, driving design and research guidelines that account for the influences of power and neglect. This prior work is largely prescriptive, however, forgoing articulation of how feminist theories of knowledge \u2014 or feminist epistemology \u2014 can alter research design and outcomes. At the core of our work is an engagement with feminist epistemology, drawing attention to how a new framework for how we know what we know enabled us to overcome intellectual tensions in our research. Specifically, we focus on the theoretical concept of entanglement, central to recent feminist scholarship, and contribute: a history of entanglement in the broader scope of feminist theory; an articulation of the main points of entanglement theory for a visualization context; and a case study of research outcomes as evidence of the potential of feminist epistemology to impact visualization research. This work answers a call in the community to embrace a broader set of theoretical and epistemic foundations and provides a starting point for bringing feminist theories into visualization research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456171",
            "id": "r_514",
            "s_ids": [
                "s_719",
                "s_150",
                "s_1517"
            ],
            "type": "rich",
            "x": -2.2836601734161377,
            "y": 15.371753692626953
        },
        {
            "title": "SimpleSets: Capturing Categorical Point Patterns with Simple Shapes",
            "data": "Points of interest on a map such as restaurants, hotels, or subway stations, give rise to categorical point data: data that have a fixed location and one or more categorical attributes. Consequently, recent years have seen various set visualization approaches that visually connect points of the same category to support users in understanding the spatial distribution of categories. Existing methods use complex and often highly irregular shapes to connect points of the same category, leading to high cognitive load for the user. In this paper we introduce SimpleSets, which uses simple shapes to enclose categorical point patterns, thereby providing a clean overview of the data distribution. SimpleSets is designed to visualize sets of points with a single categorical attribute; as a result, the point patterns enclosed by SimpleSets form a partition of the data. We give formal definitions of point patterns that correspond to simple shapes and describe an algorithm that partitions categorical points into few such patterns. Our second contribution is a rendering algorithm that transforms a given partition into a clean set of shapes resulting in an aesthetically pleasing set visualization. Our algorithm pays particular attention to resolving intersections between nearby shapes in a consistent manner. We compare SimpleSets to the state-of-the-art set visualizations using standard datasets from the literature.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456168",
            "id": "r_515",
            "s_ids": [
                "s_460",
                "s_545",
                "s_347"
            ],
            "type": "rich",
            "x": -5.960585594177246,
            "y": 15.672375679016113
        },
        {
            "title": "Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning",
            "data": "Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (i.e., charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research. Source codes and datasets of this paper are available at https://github.com/zengxingchen/ChartQA-MLLM.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456159",
            "id": "r_516",
            "s_ids": [
                "s_220",
                "s_818",
                "s_1193",
                "s_1438"
            ],
            "type": "rich",
            "x": -9.719013214111328,
            "y": 17.91611671447754
        },
        {
            "title": "An Empirical Evaluation of the GPT-4 Multimodal Language Model on Visualization Literacy Tasks",
            "data": "Large Language Models (LLMs) like GPT-4 which support multimodal input (i.e., prompts containing images in addition to text) have immense potential to advance visualization research. However, many questions exist about the visual capabilities of such models, including how well they can read and interpret visually represented data. In our work, we address this question by evaluating the GPT-4 multimodal LLM using a suite of task sets meant to assess the model's visualization literacy. The task sets are based on existing work in the visualization community addressing both automated chart question answering and human visualization literacy across multiple settings. Our assessment finds that GPT-4 can perform tasks such as recognizing trends and extreme values, and also demonstrates some understanding of visualization design best-practices. By contrast, GPT-4 struggles with simple value retrieval when not provided with the original dataset, lacks the ability to reliably distinguish between colors in charts, and occasionally suffers from hallucination and inconsistency. We conclude by reflecting on the model's strengths and weaknesses as well as the potential utility of models like GPT-4 for future visualization research. We also release all code, stimuli, and results for the task sets at the following link: https://doi.org/10.17605/OSF.IO/F39J6",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456155",
            "id": "r_517",
            "s_ids": [
                "s_1024",
                "s_850"
            ],
            "type": "rich",
            "x": -4.504673957824707,
            "y": 15.33167552947998
        },
        {
            "title": "AdversaFlow: Visual Red Teaming for Large Language Models with Multi-Level Adversarial Flow",
            "data": "Large Language Models (LLMs) are powerful but also raise significant security concerns, particularly regarding the harm they can cause, such as generating fake news that manipulates public opinion on social media and providing responses to unethical activities. Traditional red teaming approaches for identifying AI vulnerabilities rely on manual prompt construction and expertise. This paper introduces AdversaFlow, a novel visual analytics system designed to enhance LLM security against adversarial attacks through human-AI collaboration. AdversaFlow involves adversarial training between a target model and a red model, featuring unique multi-level adversarial flow and fluctuation path visualizations. These features provide insights into adversarial dynamics and LLM robustness, enabling experts to identify and mitigate vulnerabilities effectively. We present quantitative evaluations and case studies validating our system's utility and offering insights for future AI security solutions. Our method can enhance LLM security, supporting downstream scenarios like social media regulation by enabling more effective detection, monitoring, and mitigation of harmful content and behaviors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456150",
            "id": "r_518",
            "s_ids": [
                "s_1341",
                "s_1089",
                "s_437",
                "s_1569",
                "s_1128",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.312393188476562,
            "y": 17.11205291748047
        },
        {
            "title": "Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education",
            "data": "Large Language Models (LLMs) have shown great potential in intelligent visualization systems, especially for domain-specific applications. Integrating LLMs into visualization systems presents challenges, and we categorize these challenges into three alignments: domain problems with LLMs, visualization with LLMs, and interaction with LLMs. To achieve these alignments, we propose a framework and outline a workflow to guide the application of fine-tuned LLMs to enhance visual interactions for domain-specific tasks. These alignment challenges are critical in education because of the need for an intelligent visualization system to support beginners' self-regulated learning. Therefore, we apply the framework to education and introduce Tailor-Mind, an interactive visualization system designed to facilitate self-regulated learning for artificial intelligence beginners. Drawing on insights from a preliminary study, we identify self-regulated learning tasks and fine-tuning objectives to guide visualization design and tuning data construction. Our focus on aligning visualization with fine-tuned LLM makes Tailor-Mind more like a personalized tutor. Tailor-Mind also supports interactive recommendations to help beginners better achieve their learning goals. Model performance evaluations and user studies confirm that Tailor-Mind improves the self-regulated learning experience, effectively validating the proposed framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456145",
            "id": "r_519",
            "s_ids": [
                "s_568",
                "s_1306",
                "s_1471",
                "s_527",
                "s_683",
                "s_1614",
                "s_1046",
                "s_757",
                "s_1260",
                "s_360"
            ],
            "type": "rich",
            "x": -10.125661849975586,
            "y": 19.605175018310547
        },
        {
            "title": "ProvenanceWidgets: A Library of UI Control Elements to Track and Dynamically Overlay Analytic Provenance",
            "data": "We present ProvenanceWidgets, a Javascript library of UI control elements such as radio buttons, checkboxes, and dropdowns to track and dynamically overlay a user's analytic provenance. These in situ overlays not only save screen space but also minimize the amount of time and effort needed to access the same information from elsewhere in the UI. In this paper, we discuss how we design modular UI control elements to track how often and how recently a user interacts with them and design visual overlays showing an aggregated summary as well as a detailed temporal history. We demonstrate the capability of ProvenanceWidgets by recreating three prior widget libraries: (1) Scented Widgets, (2) Phosphor objects, and (3) Dynamic Query Widgets. We also evaluated its expressiveness and conducted case studies with visualization developers to evaluate its effectiveness. We find that ProvenanceWidgets enables developers to implement custom provenance-tracking applications effectively. ProvenanceWidgets is available as open-source software at https://github.com/ProvenanceWidgets to help application developers build custom provenance-based systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456144",
            "id": "r_520",
            "s_ids": [
                "s_1027",
                "s_430",
                "s_891",
                "s_1599"
            ],
            "type": "rich",
            "x": -4.849394798278809,
            "y": 15.44919490814209
        },
        {
            "title": "ParetoTracker: Understanding Population Dynamics in Multi-Objective Evolutionary Algorithms Through Visual Analytics",
            "data": "Multi-objective evolutionary algorithms (MOEAs) have emerged as powerful tools for solving complex optimization problems characterized by multiple, often conflicting, objectives. While advancements have been made in computational efficiency as well as diversity and convergence of solutions, a critical challenge persists: the internal evolutionary mechanisms are opaque to human users. Drawing upon the successes of explainable AI in explaining complex algorithms and models, we argue that the need to understand the underlying evolutionary operators and population dynamics within MOEAs aligns well with a visual analytics paradigm. This paper introduces ParetoTracker, a visual analytics framework designed to support the comprehension and inspection of population dynamics in the evolutionary processes of MOEAs. Informed by preliminary literature review and expert interviews, the framework establishes a multi-level analysis scheme, which caters to user engagement and exploration ranging from examining overall trends in performance metrics to conducting fine-grained inspections of evolutionary operations. In contrast to conventional practices that require manual plotting of solutions for each generation, ParetoTracker facilitates the examination of temporal trends and dynamics across consecutive generations in an integrated visual interface. The effectiveness of the framework is demonstrated through case studies and expert interviews focused on widely adopted benchmark optimization problems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2024.3456142",
            "id": "r_521",
            "s_ids": [
                "s_995",
                "s_971",
                "s_1110",
                "s_48"
            ],
            "type": "rich",
            "x": -4.299890995025635,
            "y": 21.124576568603516
        },
        {
            "title": "Causality-Based Visual Analysis of Questionnaire Responses",
            "data": "As the final stage of questionnaire analysis, causal reasoning is the key to turning responses into valuable insights and actionable items for decision-makers. During the questionnaire analysis, classical statistical methods (e.g., Differences-in-Differences) have been widely exploited to evaluate causality between questions. However, due to the huge search space and complex causal structure in data, causal reasoning is still extremely challenging and time-consuming, and often conducted in a trial-and-error manner. On the other hand, existing visual methods of causal reasoning face the challenge of bringing scalability and expert knowledge together and can hardly be used in the questionnaire scenario. In this work, we present a systematic solution to help analysts effectively and efficiently explore questionnaire data and derive causality. Based on the association mining algorithm, we dig question combinations with potential inner causality and help analysts interactively explore the causal sub-graph of each question combination. Furthermore, leveraging the requirements collected from the experts, we built a visualization tool and conducted a comparative study with the state-of-the-art system to show the usability and efficiency of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327376",
            "id": "r_522",
            "s_ids": [
                "s_62",
                "s_1010",
                "s_928",
                "s_1574",
                "s_258",
                "s_737",
                "s_404",
                "s_1155",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.569979667663574,
            "y": 17.626039505004883
        },
        {
            "title": "Eleven Years of Gender Data Visualization: A Step Towards More Inclusive Gender Representation",
            "data": "We present an analysis of the representation of gender as a data dimension in data visualizations and propose a set of considerations around visual variables and annotations for gender-related data. Gender is a common demographic dimension of data collected from study or survey participants, passengers, or customers, as well as across academic studies, especially in certain disciplines like sociology. Our work contributes to multiple ongoing discussions on the ethical implications of data visualizations. By choosing specific data, visual variables, and text labels, visualization designers may, inadvertently or not, perpetuate stereotypes and biases. Here, our goal is to start an evolving discussion on how to represent data on gender in data visualizations and raise awareness of the subtleties of choosing visual variables and words in gender visualizations. In order to ground this discussion, we collected and coded gender visualizations and their captions from five different scientific communities (Biology, Politics, Social Studies, Visualisation, and Human-Computer Interaction), in addition to images from Tableau Public and the Information Is Beautiful awards showcase. Overall we found that representation types are community-specific, color hue is the dominant visual channel for gender data, and nonconforming gender is under-represented. We end our paper with a discussion of considerations for gender visualization derived from our coding and the literature and recommendations for large data collection bodies. A free copy of this paper and all supplemental materials are available at https://osf.io/v9ams/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327369",
            "id": "r_523",
            "s_ids": [
                "s_711",
                "s_1057",
                "s_993",
                "s_688",
                "s_1124"
            ],
            "type": "rich",
            "x": -6.3095808029174805,
            "y": 18.366403579711914
        },
        {
            "title": "TransforLearn: Interactive Visual Tutorial for the Transformer Model",
            "data": "The widespread adoption of Transformers in deep learning, serving as the core framework for numerous large-scale language models, has sparked significant interest in understanding their underlying mechanisms. However, beginners face difficulties in comprehending and learning Transformers due to its complex structure and abstract data representation. We present TransforLearn, the first interactive visual tutorial designed for deep learning beginners and non-experts to comprehensively learn about Transformers. TransforLearn supports interactions for architecture-driven exploration and task-driven exploration, providing insight into different levels of model details and their working processes. It accommodates interactive views of each layer's operation and mathematical formula, helping users to understand the data flow of long text sequences. By altering the current decoder-based recursive prediction results and combining the downstream task abstractions, users can deeply explore model processes. Our user study revealed that the interactions of TransforLearn are positively received. We observe that TransforLearn facilitates users' accomplishment of study tasks and a grasp of key concepts in Transformer effectively.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327353",
            "id": "r_524",
            "s_ids": [
                "s_568",
                "s_1471",
                "s_922",
                "s_1392",
                "s_96",
                "s_360"
            ],
            "type": "rich",
            "x": -10.045656204223633,
            "y": 19.604244232177734
        },
        {
            "title": "Data Type Agnostic Visual Sensitivity Analysis",
            "data": "Modern science and industry rely on computational models for simulation, prediction, and data analysis. Spatial blind source separation (SBSS) is a model used to analyze spatial data. Designed explicitly for spatial data analysis, it is superior to popular non-spatial methods, like PCA. However, a challenge to its practical use is setting two complex tuning parameters, which requires parameter space analysis. In this paper, we focus on sensitivity analysis (SA). SBSS parameters and outputs are spatial data, which makes SA difficult as few SA approaches in the literature assume such complex data on both sides of the model. Based on the requirements in our design study with statistics experts, we developed a visual analytics prototype for data type agnostic visual sensitivity analysis that fits SBSS and other contexts. The main advantage of our approach is that it requires only dissimilarity measures for parameter settings and outputs (Fig. 1). We evaluated the prototype heuristically with visualization experts and through interviews with two SBSS experts. In addition, we show the transferability of our approach by applying it to microclimate simulations. Study participants could confirm suspected and known parameter-output relations, find surprising associations, and identify parameter subspaces to examine in the future. During our design study and evaluation, we identified challenging future research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327203",
            "id": "r_525",
            "s_ids": [
                "s_59",
                "s_90",
                "s_973",
                "s_860",
                "s_1530",
                "s_692",
                "s_1195"
            ],
            "type": "rich",
            "x": -5.404380798339844,
            "y": 14.891864776611328
        },
        {
            "title": "Radial Icicle Tree (RIT): Node Separation and Area Constancy",
            "data": "Icicles and sunbursts are two commonly-used visual representations of trees. While icicle trees can map data values faithfully to rectangles of different sizes, often some rectangles are too narrow to be noticed easily. When an icicle tree is transformed into a sunburst tree, the width of each rectangle becomes the length of an annular sector that is usually longer than the original width. While sunburst trees alleviate the problem of narrow rectangles in icicle trees, it no longer maintains the consistency of size encoding. At different tree depths, nodes of the same data values are displayed in annular sections of different sizes in a sunburst tree, though they are represented by rectangles of the same size in an icicle tree. Furthermore, two nodes from different subtrees could sometimes appear as a single node in both icicle trees and sunburst trees. In this paper, we propose a new visual representation, referred to as radial icicle tree (RIT), which transforms the rectangular bounding box of an icicle tree into a circle, circular sector, or annular sector while introducing gaps between nodes and maintaining area constancy for nodes of the same size. We applied the new visual design to several datasets. Both the analytical design process and user-centered evaluation have confirmed that this new design has improved the design of icicles and sunburst trees without introducing any relative demerit.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327178",
            "id": "r_526",
            "s_ids": [
                "s_1357",
                "s_454",
                "s_92",
                "s_298"
            ],
            "type": "rich",
            "x": -7.271603584289551,
            "y": 16.881772994995117
        },
        {
            "title": "DIVI: Dynamically Interactive Visualization",
            "data": "Dynamically Interactive Visualization (DIVI) is a novel approach for orchestrating interactions within and across static visualizations. DIVI deconstructs Scalable Vector Graphics charts at runtime to infer content and coordinate user input, decoupling interaction from specification logic. This decoupling allows interactions to extend and compose freely across different tools, chart types, and analysis goals. DIVI exploits positional relations of marks to detect chart components such as axes and legends, reconstruct scales and view encodings, and infer data fields. DIVI then enumerates candidate transformations across inferred data to perform linking between views. To support dynamic interaction without prior specification, we introduce a taxonomy that formalizes the space of standard interactions by chart element, interaction type, and input event. We demonstrate DIVI's usefulness for rapid data exploration and analysis through a usability study with 13 participants and a diverse gallery of dynamically interactive visualizations, including single chart, multi-view, and cross-tool configurations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327172",
            "id": "r_527",
            "s_ids": [
                "s_488",
                "s_1386"
            ],
            "type": "rich",
            "x": -3.7628376483917236,
            "y": 17.05636215209961
        },
        {
            "title": "Why Change My Design: Explaining Poorly Constructed Visualization Designs with Explorable Explanations",
            "data": "Although visualization tools are widely available and accessible, not everyone knows the best practices and guidelines for creating accurate and honest visual representations of data. Numerous books and articles have been written to expose the misleading potential of poorly constructed charts and teach people how to avoid being deceived by them or making their own mistakes. These readings use various rhetorical devices to explain the concepts to their readers. In our analysis of a collection of books, online materials, and a design workshop, we identified six common explanation methods. To assess the effectiveness of these methods, we conducted two crowdsourced studies (each with $N=125$) to evaluate their ability to teach and persuade people to make design changes. In addition to these existing methods, we brought in the idea of Explorable Explanations, which allows readers to experiment with different chart settings and observe how the changes are reflected in the visualization. While we did not find significant differences across explanation methods, the results of our experiments indicate that, following the exposure to the explanations, the participants showed improved proficiency in identifying deceptive charts and were more receptive to proposed alterations of the visualization design. We discovered that participants were willing to accept more than 60% of the proposed adjustments in the persuasiveness assessment. Nevertheless, we found no significant differences among different explanation methods in convincing participants to accept the modifications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327155",
            "id": "r_528",
            "s_ids": [
                "s_1495",
                "s_1241",
                "s_1311",
                "s_156"
            ],
            "type": "rich",
            "x": -10.207878112792969,
            "y": 18.38555908203125
        },
        {
            "title": "Photon Field Networks for Dynamic Real-Time Volumetric Global Illumination",
            "data": "Volume data is commonly found in many scientific disciplines, like medicine, physics, and biology. Experts rely on robust scientific visualization techniques to extract valuable insights from the data. Recent years have shown path tracing to be the preferred approach for volumetric rendering, given its high levels of realism. However, real-time volumetric path tracing often suffers from stochastic noise and long convergence times, limiting interactive exploration. In this paper, we present a novel method to enable real-time global illumination for volume data visualization. We develop Photon Field Networks\u2014a phase-function-aware, multi-light neural representation of indirect volumetric global illumination. The fields are trained on multi-phase photon caches that we compute a priori. Training can be done within seconds, after which the fields can be used in various rendering tasks. To showcase their potential, we develop a custom neural path tracer, with which our photon fields achieve interactive framerates even on large datasets. We conduct in-depth evaluations of the method's performance, including visual quality, stochastic noise, inference and rendering speeds, and accuracy regarding illumination and phase function awareness. Results are compared to ray marching, path tracing and photon mapping. Our findings show that Photon Field Networks can faithfully represent indirect global illumination within the boundaries of the trained phase spectrum while exhibiting less stochastic noise and rendering at a significantly faster rate than traditional methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3327107",
            "id": "r_529",
            "s_ids": [
                "s_895",
                "s_869",
                "s_789"
            ],
            "type": "rich",
            "x": -0.7566099166870117,
            "y": 17.97159194946289
        },
        {
            "title": "Design Characterization for Black-and-White Textures in Visualization",
            "data": "We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326941",
            "id": "r_530",
            "s_ids": [
                "s_1588",
                "s_1448",
                "s_1124",
                "s_1323"
            ],
            "type": "rich",
            "x": -6.318894863128662,
            "y": 18.483247756958008
        },
        {
            "title": "SkiVis: Visual Exploration and Route Planning in Ski Resorts",
            "data": "Optimal ski route selection is a challenge based on a multitude of factors, such as the steepness, compass direction, or crowdedness. The personal preferences of every skier towards these factors require individual adaptations, which aggravate this task. Current approaches within this domain do not combine automated routing capabilities with user preferences, missing out on the possibility of integrating domain knowledge in the analysis process. We introduce SkiVis, a visual analytics application to interactively explore ski slopes and provide routing recommendations based on user preferences. In collaboration with ski guides and enthusiasts, we elicited requirements and guidelines for such an application and propose different workflows depending on the skiers' familiarity with the resort. In a case study on the resort of Ski Arlberg, we illustrate how to leverage volunteered geographic information to enable a numerical comparison between slopes. We evaluated our approach through a pair-analytics study and demonstrate how it supports skiers in discovering relevant and preference-based ski routes. Besides the tasks investigated in the study, we derive additional use cases from the interviews that showcase the further potential of SkiVis, and contribute directions for further research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326940",
            "id": "r_531",
            "s_ids": [
                "s_201",
                "s_1118",
                "s_1100",
                "s_1580"
            ],
            "type": "rich",
            "x": -7.024308681488037,
            "y": 20.88047981262207
        },
        {
            "title": "Marjorie: Visualizing Type 1 Diabetes Data to Support Pattern Exploration",
            "data": "In this work we propose Marjorie, a visual analytics approach to address the challenge of analyzing patients' diabetes data during brief regular appointments with their diabetologists. Designed in consultation with diabetologists, Marjorie uses a combination of visual and algorithmic methods to support the exploration of patterns in the data. Patterns of interest include seasonal variations of the glucose profiles, and non-periodic patterns such as fluctuations around mealtimes or periods of hypoglycemia (i.e., glucose levels below the normal range). We introduce a unique representation of glucose data based on modified horizon graphs and hierarchical clustering of adjacent carbohydrate or insulin entries. Semantic zooming allows the exploration of patterns on different levels of temporal detail. We evaluated our solution in a case study, which demonstrated Marjorie's potential to provide valuable insights into therapy parameters and unfavorable eating habits, among others. The study results and informal feedback collected from target users suggest that Marjorie effectively supports patients and diabetologists in the joint exploration of patterns in diabetes data, potentially enabling more informed treatment decisions. A free copy of this paper and all supplemental materials are available at https://osf.io/34t8c/.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326936",
            "id": "r_532",
            "s_ids": [
                "s_383",
                "s_338",
                "s_580",
                "s_680"
            ],
            "type": "rich",
            "x": -2.192924976348877,
            "y": 15.500778198242188
        },
        {
            "title": "Visual Analysis of Displacement Processes in Porous Media using Spatio-Temporal Flow Graphs",
            "data": "We developed a new approach comprised of different visualizations for the comparative spatio-temporal analysis of displacement processes in porous media. We aim to analyze and compare ensemble datasets from experiments to gain insight into the influence of different parameters on fluid flow. To capture the displacement of a defending fluid by an invading fluid, we first condense an input image series to a single time map. From this map, we generate a spatio-temporal flow graph covering the whole process. This graph is further simplified to only reflect topological changes in the movement of the invading fluid. Our interactive tools allow the visual analysis of these processes by visualizing the graph structure and the context of the experimental setup, as well as by providing charts for multiple metrics. We apply our approach to analyze and compare ensemble datasets jointly with domain experts, where we vary either fluid properties or the solid structure of the porous medium. We finally report the generated insights from the domain experts and discuss our contribution's advantages, generality, and limitations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326931",
            "id": "r_533",
            "s_ids": [
                "s_422",
                "s_1224",
                "s_846",
                "s_281",
                "s_561",
                "s_346"
            ],
            "type": "rich",
            "x": -9.77489948272705,
            "y": 16.85208511352539
        },
        {
            "title": "QEVIS: Multi-Grained Visualization of Distributed Query Execution",
            "data": "Distributed query processing systems such as Apache Hive and Spark are widely-used in many organizations for large-scale data analytics. Analyzing and understanding the query execution process of these systems are daily routines for engineers and crucial for identifying performance problems, optimizing system configurations, and rectifying errors. However, existing visualization tools for distributed query execution are insufficient because (i) most of them (if not all) do not provide fine-grained visualization (i.e., the atomic task level), which can be crucial for understanding query performance and reasoning about the underlying execution anomalies, and (ii) they do not support proper linkages between system status and query execution, which makes it difficult to identify the causes of execution problems. To tackle these limitations, we propose QEVIS, which visualizes distributed query execution process with multiple views that focus on different granularities and complement each other. Specifically, we first devise a query logical plan layout algorithm to visualize the overall query execution progress compactly and clearly. We then propose two novel scoring methods to summarize the anomaly degrees of the jobs and machines during query execution, and visualize the anomaly scores intuitively, which allow users to easily identify the components that are worth paying attention to. Moreover, we devise a scatter plot-based task view to show a massive number of atomic tasks, where task distribution patterns are informative for execution problems. We also equip QEVIS with a suite of auxiliary views and interaction methods to support easy and effective cross-view exploration, which makes it convenient to track the causes of execution problems. QEVIS has been used in the production environment of our industry partner, and we present three use cases from real-world applications and user interview to demonstrate its effectiveness. QEVIS is open-source at https://github.com/DBGroup-SUSTech/QEVIS.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326930",
            "id": "r_534",
            "s_ids": [
                "s_1561",
                "s_491",
                "s_1342",
                "s_1428",
                "s_25",
                "s_170",
                "s_206",
                "s_476"
            ],
            "type": "rich",
            "x": -9.279685974121094,
            "y": 20.12973976135254
        },
        {
            "title": "Visualizing Historical Book Trade Data: An Iterative Design Study with Close Collaboration with Domain Experts",
            "data": "The circulation of historical books has always been an area of interest for historians. However, the data used to represent the journey of a book across different places and times can be difficult for domain experts to digest due to buried geographical and chronological features within text-based presentations. This situation provides an opportunity for collaboration between visualization researchers and historians. This paper describes a design study where a variant of the Nine-Stage Framework [46] was employed to develop a Visual Analytics (VA) tool called DanteExploreVis. This tool was designed to aid domain experts in exploring, explaining, and presenting book trade data from multiple perspectives. We discuss the design choices made and how each panel in the interface meets the domain requirements. We also present the results of a qualitative evaluation conducted with domain experts. The main contributions of this paper include: 1) the development of a VA tool to support domain experts in exploring, explaining, and presenting book trade data; 2) a comprehensive documentation of the iterative design, development, and evaluation process following the variant Nine-Stage Framework; 3) a summary of the insights gained and lessons learned from this design study in the context of the humanities field; and 4) reflections on how our approach could be applied in a more generalizable way.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326923",
            "id": "r_535",
            "s_ids": [
                "s_972",
                "s_969",
                "s_1282",
                "s_1490"
            ],
            "type": "rich",
            "x": -7.134401798248291,
            "y": 16.828447341918945
        },
        {
            "title": "Visual Analytics for Understanding Draco's Knowledge Base",
            "data": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326912",
            "id": "r_536",
            "s_ids": [
                "s_692",
                "s_140",
                "s_1195"
            ],
            "type": "rich",
            "x": -5.432549476623535,
            "y": 14.8808012008667
        },
        {
            "title": "Metrics-Based Evaluation and Comparison of Visualization Notations",
            "data": "A visualization notation is a recurring pattern of symbols used to author specifications of visualizations, from data transformation to visual mapping. Programmatic notations use symbols defined by grammars or domain-specific languages (e.g. ggplot2, dplyr, Vega-Lite) or libraries (e.g. Matplotlib, Pandas). Designers and prospective users of grammars and libraries often evaluate visualization notations by inspecting galleries of examples. While such collections demonstrate usage and expressiveness, their construction and evaluation are usually ad hoc, making comparisons of different notations difficult. More rarely, experts analyze notations via usability heuristics, such as the Cognitive Dimensions of Notations framework. These analyses, akin to structured close readings of text, can reveal design deficiencies, but place a burden on the expert to simultaneously consider many facets of often complex systems. To alleviate these issues, we introduce a metrics-based approach to usability evaluation and comparison of notations in which metrics are computed for a gallery of examples across a suite of notations. While applicable to any visualization domain, we explore the utility of our approach via a case study considering statistical graphics that explores 40 visualizations across 9 widely used notations. We facilitate the computation of appropriate metrics and analysis via a new tool called NotaScope. We gathered feedback via interviews with authors or maintainers of prominent charting libraries ($n=6$). We find that this approach is a promising way to formalize, externalize, and extend evaluations and comparisons of visualization notations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326907",
            "id": "r_537",
            "s_ids": [
                "s_1500",
                "s_1293",
                "s_324"
            ],
            "type": "rich",
            "x": -3.7565083503723145,
            "y": 17.155479431152344
        },
        {
            "title": "Dr. KID: Direct Remeshing and K-Set Isometric Decomposition for Scalable Physicalization of Organic Shapes",
            "data": "Dr. KID is an algorithm that uses isometric decomposition for the physicalization of potato-shaped organic models in a puzzle fashion. The algorithm begins with creating a simple, regular triangular surface mesh of organic shapes, followed by iterative K-means clustering and remeshing. For clustering, we need similarity between triangles (segments) which is defined as a distance function. The distance function maps each triangle's shape to a single point in the virtual 3D space. Thus, the distance between the triangles indicates their degree of dissimilarity. K-means clustering uses this distance and sorts segments into $k$ classes. After this, remeshing is applied to minimize the distance between triangles within the same cluster by making their shapes identical. Clustering and remeshing are repeated until the distance between triangles in the same cluster reaches an acceptable threshold. We adopt a curvature-aware strategy to determine the surface thickness and finalize puzzle pieces for 3D printing. Identical hinges and holes are created for assembling the puzzle components. For smoother outcomes, we use triangle subdivision along with curvature-aware clustering, generating curved triangular patches for 3D printing. Our algorithm was evaluated using various models, and the 3D-printed results were analyzed. Findings indicate that our algorithm performs reliably on target organic shapes with minimal loss of input geometry.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326595",
            "id": "r_538",
            "s_ids": [
                "s_1606",
                "s_146",
                "s_510"
            ],
            "type": "rich",
            "x": -6.237335681915283,
            "y": 18.990806579589844
        },
        {
            "title": "A Comparative Study of the Perceptual Sensitivity of Topological Visualizations to Feature Variations",
            "data": "Color maps are a commonly used visualization technique in which data are mapped to optical properties, e.g., color or opacity. Color maps, however, do not explicitly convey structures (e.g., positions and scale of features) within data. Topology-based visualizations reveal and explicitly communicate structures underlying data. Although our understanding of what types of features are captured by topological visualizations is good, our understanding of people's perception of those features is not. This paper evaluates the sensitivity of topology-based isocontour, Reeb graph, and persistence diagram visualizations compared to a reference color map visualization for synthetically generated scalar fields on 2-manifold triangular meshes embedded in 3D. In particular, we built and ran a human-subject study that evaluated the perception of data features characterized by Gaussian signals and measured how effectively each visualization technique portrays variations of data features arising from the position and amplitude variation of a mixture of Gaussians. For positional feature variations, the results showed that only the Reeb graph visualization had high sensitivity. For amplitude feature variations, persistence diagrams and color maps demonstrated the highest sensitivity, whereas isocontours showed only weak sensitivity. These results take an important step toward understanding which topology-based tools are best for various data and task scenarios and their effectiveness in conveying topological variations as compared to conventional color mapping.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326592",
            "id": "r_539",
            "s_ids": [
                "s_1617",
                "s_1188",
                "s_1497",
                "s_2",
                "s_542"
            ],
            "type": "rich",
            "x": -1.8753790855407715,
            "y": 17.931957244873047
        },
        {
            "title": "The Arrangement of Marks Impacts Afforded Messages: Ordering, Partitioning, Spacing, and Coloring in Bar Charts",
            "data": "Data visualizations present a massive number of potential messages to an observer. One might notice that one group's average is larger than another's, or that a difference in values is smaller than a difference between two others, or any of a combinatorial explosion of other possibilities. The message that a viewer tends to notice \u2013 the message that a visualization \u2018affords\u2019 \u2013 is strongly affected by how values are arranged in a chart, e.g., how the values are colored or positioned. Although understanding the mapping between a chart's arrangement and what viewers tend to notice is critical for creating guidelines and recommendation systems, current empirical work is insufficient to lay out clear rules. We present a set of empirical evaluations of how different messages-including ranking, grouping, and part-to-whole relationships\u2013are afforded by variations in ordering, partitioning, spacing, and coloring of values, within the ubiquitous case study of bar graphs. In doing so, we introduce a quantitative method that is easily scalable, reviewable, and replicable, laying groundwork for further investigation of the effects of arrangement on message affordances across other visualizations and tasks. Pre-registration and all supplemental materials are available at https://osf.io/np3q7 and https://osf.io/bvy95, respectively.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326590",
            "id": "r_540",
            "s_ids": [
                "s_605",
                "s_942",
                "s_533"
            ],
            "type": "rich",
            "x": -3.174990177154541,
            "y": 16.640670776367188
        },
        {
            "title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring",
            "data": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326585",
            "id": "r_541",
            "s_ids": [
                "s_1334",
                "s_114",
                "s_274"
            ],
            "type": "rich",
            "x": -7.685797214508057,
            "y": 18.119571685791016
        },
        {
            "title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples",
            "data": "Open-world object detection (OWOD) is an emerging computer vision problem that involves not only the identification of predefined object classes, like what general object detectors do, but also detects new unknown objects simultaneously. Recently, several end-to-end deep learning models have been proposed to address the OWOD problem. However, these approaches face several challenges: a) significant changes in both network architecture and training procedure are required; b) they are trained from scratch, which can not leverage existing pre-trained general detectors; c) costly annotations for all unknown classes are needed. To overcome these challenges, we present a visual analytic framework called OW-Adapter. It acts as an adaptor to enable pre-trained general object detectors to handle the OWOD problem. Specifically, OW-Adapter is designed to identify, summarize, and annotate unknown examples with minimal human effort. Moreover, we introduce a lightweight classifier to learn newly annotated unknown classes and plug the classifier into pre-trained general detectors to detect unknown objects. We demonstrate the effectiveness of our framework through two case studies of different domains, including common object recognition and autonomous driving. The studies show that a simple yet powerful adaptor can extend the capability of pre-trained general detectors to detect unknown objects and improve the performance on known classes simultaneously.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326577",
            "id": "r_542",
            "s_ids": [
                "s_1615",
                "s_882",
                "s_1095",
                "s_141",
                "s_1474"
            ],
            "type": "rich",
            "x": -0.3610560894012451,
            "y": 17.575063705444336
        },
        {
            "title": "Perceptually Uniform Construction of Illustrative Textures",
            "data": "Illustrative textures, such as stippling or hatching, were predominantly used as an alternative to conventional Phong rendering. Recently, the potential of encoding information on surfaces or maps using different densities has also been recognized. This has the significant advantage that additional color can be used as another visual channel and the illustrative textures can then be overlaid. Effectively, it is thus possible to display multiple information, such as two different scalar fields on surfaces simultaneously. In previous work, these textures were manually generated and the choice of density was unempirically determined. Here, we first want to determine and understand the perceptual space of illustrative textures. We chose a succession of simplices with increasing dimensions as primitives for our textures: Dots, lines, and triangles. Thus, we explore the texture types of stippling, hatching, and triangles. We create a range of textures by sampling the density space uniformly. Then, we conduct three perceptual studies in which the participants performed pairwise comparisons for each texture type. We use multidimensional scaling (MDS) to analyze the perceptual spaces per category. The perception of stippling and triangles seems relatively similar. Both are adequately described by a 1D manifold in 2D space. The perceptual space of hatching consists of two main clusters: Crosshatched textures, and textures with only one hatching direction. However, the perception of hatching textures with only one hatching direction is similar to the perception of stippling and triangles. Based on our findings, we construct perceptually uniform illustrative textures. Afterwards, we provide concrete application examples for the constructed textures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326574",
            "id": "r_543",
            "s_ids": [
                "s_1550",
                "s_936",
                "s_722",
                "s_1598"
            ],
            "type": "rich",
            "x": -3.5640182495117188,
            "y": 19.83205795288086
        },
        {
            "title": "Guided Visual Analytics for Image Selection in Time and Space",
            "data": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326572",
            "id": "r_544",
            "s_ids": [
                "s_1165",
                "s_898",
                "s_1195"
            ],
            "type": "rich",
            "x": -5.309175968170166,
            "y": 14.926690101623535
        },
        {
            "title": "Designing for Ambiguity in Visual Analytics: Lessons from Risk Assessment and Prediction",
            "data": "Ambiguity is pervasive in the complex sensemaking domains of risk assessment and prediction but there remains little research on how to design visual analytics tools to accommodate it. We report on findings from a qualitative study based on a conceptual framework of sensemaking processes to investigate how both new visual analytics designs and existing tools, primarily data tables, support the cognitive work demanded in avalanche forecasting. While both systems yielded similar analytic outcomes we observed differences in ambiguous sensemaking and the analytic actions either afforded. Our findings challenge conventional visualization design guidance in both perceptual and interaction design, highlighting the need for data interfaces that encourage reflection, provoke alternative interpretations, and support the inherently ambiguous nature of sensemaking in this critical application. We review how different visual and interactive forms support or impede analytic processes and introduce \u201cgisting\u201d as a significant yet unexplored analytic action for visual analytics research. We conclude with design implications for enabling ambiguity in visual analytics tools to scaffold sensemaking in risk assessment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326571",
            "id": "r_545",
            "s_ids": [
                "s_1116",
                "s_842"
            ],
            "type": "rich",
            "x": -4.209714889526367,
            "y": 16.095321655273438
        },
        {
            "title": "ExTreeM: Scalable Augmented Merge Tree Computation via Extremum Graphs",
            "data": "Over the last decade merge trees have been proven to support a plethora of visualization and analysis tasks since they effectively abstract complex datasets. This paper describes the ExTreeM-Algorithm: A scalable algorithm for the computation of merge trees via extremum graphs. The core idea of ExTreeM is to first derive the extremum graph $\\mathcal{G}$ of an input scalar field $f$ defined on a cell complex $\\mathcal{K}$, and subsequently compute the unaugmented merge tree of $f$ on $\\mathcal{G}$ instead of $\\mathcal{K}$; which are equivalent. Any merge tree algorithm can be carried out significantly faster on $\\mathcal{G}$, since $\\mathcal{K}$ in general contains substantially more cells than $\\mathcal{G}$. To further speed up computation, ExTreeM includes a tailored procedure to derive merge trees of extremum graphs. The computation of the fully augmented merge tree, i.e., a merge tree domain segmentation of $\\mathcal{K}$, can then be performed in an optional post-processing step. All steps of ExTreeM consist of procedures with high parallel efficiency, and we provide a formal proof of its correctness. Our experiments, performed on publicly available datasets, report a speedup of up to one order of magnitude over the state-of-the-art algorithms included in the TTK and VTK-m software libraries, while also requiring significantly less memory and exhibiting excellent scaling behavior.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326526",
            "id": "r_546",
            "s_ids": [
                "s_35",
                "s_896",
                "s_279",
                "s_1041",
                "s_1068"
            ],
            "type": "rich",
            "x": -3.9868276119232178,
            "y": 20.80342674255371
        },
        {
            "title": "Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer",
            "data": "In soccer, player action evaluation provides a fine-grained method to analyze player performance and plays an important role in improving winning chances in future matches. However, previous studies on action evaluation only provide a score for each action, and hardly support inspecting and comparing player actions integrated with complex match context information such as team tactics and player locations. In this work, we collaborate with soccer analysts and coaches to characterize the domain problems of evaluating player performance based on action scores. We design a tailored visualization of soccer player actions that places the action choice together with the tactic it belongs to as well as the player locations in the same view. Based on the design, we introduce a visual analytics system, Action-Evaluator, to facilitate a comprehensive player action evaluation through player navigation, action investigation, and action explanation. With the system, analysts can find players to be analyzed efficiently, learn how they performed under various match situations, and obtain valuable insights to improve their action choices. The usefulness and effectiveness of this work are demonstrated by two case studies on a real-world dataset and an expert interview.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326524",
            "id": "r_547",
            "s_ids": [
                "s_400",
                "s_1574",
                "s_678",
                "s_647",
                "s_79",
                "s_1415"
            ],
            "type": "rich",
            "x": -10.274897575378418,
            "y": 17.32027244567871
        },
        {
            "title": "Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics",
            "data": "Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326521",
            "id": "r_548",
            "s_ids": [
                "s_788",
                "s_1237"
            ],
            "type": "rich",
            "x": -2.006758213043213,
            "y": 16.018020629882812
        },
        {
            "title": "A Parallel Framework for Streaming Dimensionality Reduction",
            "data": "The visualization of streaming high-dimensional data often needs to consider the speed in dimensionality reduction algorithms, the quality of visualized data patterns, and the stability of view graphs that usually change over time with new data. Existing methods of streaming high-dimensional data visualization primarily line up essential modules in a serial manner and often face challenges in satisfying all these design considerations. In this research, we propose a novel parallel framework for streaming high-dimensional data visualization to achieve high data processing speed, high quality in data patterns, and good stability in visual presentations. This framework arranges all essential modules in parallel to mitigate the delays caused by module waiting in serial setups. In addition, to facilitate the parallel pipeline, we redesign these modules with a parametric non-linear embedding method for new data embedding, an incremental learning method for online embedding function updating, and a hybrid strategy for optimized embedding updating. We also improve the coordination mechanism among these modules. Our experiments show that our method has advantages in embedding speed, quality, and stability over other existing methods to visualize streaming high-dimensional data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326515",
            "id": "r_549",
            "s_ids": [
                "s_308",
                "s_1197",
                "s_848",
                "s_34",
                "s_518",
                "s_1072"
            ],
            "type": "rich",
            "x": -9.376744270324707,
            "y": 18.113252639770508
        },
        {
            "title": "PROWIS: A Visual Approach for Building, Managing, and Analyzing Weather Simulation Ensembles at Runtime",
            "data": "Weather forecasting is essential for decision-making and is usually performed using numerical modeling. Numerical weather models, in turn, are complex tools that require specialized training and laborious setup and are challenging even for weather experts. Moreover, weather simulations are data-intensive computations and may take hours to days to complete. When the simulation is finished, the experts face challenges analyzing its outputs, a large mass of spatiotemporal and multivariate data. From the simulation setup to the analysis of results, working with weather simulations involves several manual and error-prone steps. The complexity of the problem increases exponentially when the experts must deal with ensembles of simulations, a frequent task in their daily duties. To tackle these challenges, we propose ProWis: an interactive and provenance-oriented system to help weather experts build, manage, and analyze simulation ensembles at runtime. Our system follows a human-in-the-loop approach to enable the exploration of multiple atmospheric variables and weather scenarios. ProWis was built in close collaboration with weather experts, and we demonstrate its effectiveness by presenting two case studies of rainfall events in Brazil.",
            "url": "http://dx.doi.org/10.1109/TVCG.2023.3326514",
            "id": "r_550",
            "s_ids": [
                "s_479",
                "s_562",
                "s_1086",
                "s_17",
                "s_1320",
                "s_1015"
            ],
            "type": "rich",
            "x": -7.514422416687012,
            "y": -1.1627044677734375
        },
        {
            "title": "SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Pairing",
            "data": "We present SizePairs, a new technique to create stable and balanced treemap layouts that visualize values changing over time in hierarchical data. To achieve an overall high-quality result across all time steps in terms of stability and aspect ratio, SizePairs employs a new hierarchical size-based pairing algorithm that recursively pairs two nodes that complement their size changes over time and have similar sizes. SizePairs maximizes the visual quality and stability by optimizing the splitting orientation of each internal node and flipping leaf nodes, if necessary. We also present a comprehensive comparison of SizePairs against the state-of-the-art treemaps developed for visualizing time-dependent data. SizePairs outperforms existing techniques in both visual quality and stability, while being faster than the local moves technique.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209450",
            "id": "r_551",
            "s_ids": [
                "s_151",
                "s_1442",
                "s_299",
                "s_274",
                "s_1345",
                "s_1146"
            ],
            "type": "rich",
            "x": -8.001852035522461,
            "y": 18.55634307861328
        },
        {
            "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry",
            "data": "Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding \u201cin what direction to proceed\u201d is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209434",
            "id": "r_552",
            "s_ids": [
                "s_730",
                "s_1102",
                "s_651",
                "s_217",
                "s_1394",
                "s_1556",
                "s_672"
            ],
            "type": "rich",
            "x": -10.915762901306152,
            "y": 19.00130844116211
        },
        {
            "title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections",
            "data": "Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction. ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209432",
            "id": "r_553",
            "s_ids": [
                "s_853",
                "s_980",
                "s_946",
                "s_614",
                "s_540"
            ],
            "type": "rich",
            "x": -6.531254768371582,
            "y": 13.87296199798584
        },
        {
            "title": "Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution",
            "data": "We introduce relaxed dot plots as an improvement of nonlinear dot plots for unit visualization. Our plots produce more faithful data representations and reduce moir\u00e9 effects. Their contour is based on a customized kernel frequency estimation to match the shape of the distribution of underlying data values. Previous nonlinear layouts introduce column-centric nonlinear scaling of dot diameters for visualization of high-dynamic-range data with high peaks. We provide a mathematical approach to convert that column-centric scaling to our smooth envelope shape. This formalism allows us to use linear, root, and logarithmic scaling to find ideal dot sizes. Our method iteratively relaxes the dot layout for more correct and aesthetically pleasing results. To achieve this, we modified Lloyd's algorithm with additional constraints and heuristics. We evaluate the layouts of relaxed dot plots against a previously existing nonlinear variant and show that our algorithm produces less error regarding the underlying data while establishing the blue noise property that works against moir\u00e9 effects. Further, we analyze the readability of our relaxed plots in three crowd-sourced experiments. The results indicate that our proposed technique surpasses traditional dot plots.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209429",
            "id": "r_554",
            "s_ids": [
                "s_701",
                "s_622",
                "s_117",
                "s_810",
                "s_1302",
                "s_157"
            ],
            "type": "rich",
            "x": -8.492493629455566,
            "y": 19.267974853515625
        },
        {
            "title": "TrafficVis: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting and Labeling Human Trafficking",
            "data": "Law enforcement and domain experts can detect human trafficking (HT) in online escort websites by analyzing suspicious clusters of connected ads. How can we explain clustering results intuitively and interactively, visualizing potential evidence for experts to analyze? We present TrafficVis, the first interface for cluster-level HT detection and labeling. Developed through months of participatory design with domain experts, TrafficVis provides coordinated views in conjunction with carefully chosen backend algorithms to effectively show spatio-temporal and text patterns to a wide variety of anti-HT stakeholders. We build upon state-of-the-art text clustering algorithms by incorporating shared metadata as a signal of connected and possibly suspicious activity, then visualize the results. Domain experts can use TrafficVis to label clusters as HT, or other, suspicious, but non-HT activity such as spam and scam, quickly creating labeled datasets to enable further HT research. Through domain expert feedback and a usage scenario, we demonstrate TRAFFICVIS's efficacy. The feedback was overwhelmingly positive, with repeated high praises for the usability and explainability of our tool, the latter being vital for indicting possible criminals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209403",
            "id": "r_555",
            "s_ids": [
                "s_381",
                "s_341",
                "s_1526",
                "s_1255",
                "s_1455",
                "s_1231",
                "s_559",
                "s_129",
                "s_878",
                "s_1252"
            ],
            "type": "rich",
            "x": 10.325541496276855,
            "y": 10.22323226928711
        },
        {
            "title": "PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays",
            "data": "Parallel coordinate plots (PCPs) have been widely used for high-dimensional (HD) data storytelling because they allow for presenting a large number of dimensions without distortions. The axes ordering in PCP presents a particular story from the data based on the user perception of PCP polylines. Existing works focus on directly optimizing for PCP axes ordering based on some common analysis tasks like clustering, neighborhood, and correlation. However, direct optimization for PCP axes based on these common properties is restrictive because it does not account for multiple properties occurring between the axes, and for local properties that occur in small regions in the data. Also, many of these techniques do not support the human-in-the-loop (HIL) paradigm, which is crucial (i) for explainability and (ii) in cases where no single reordering scheme fits the users' goals. To alleviate these problems, we present PC-Expo, a real-time visual analytics framework for all-in-one PCP line pattern detection and axes reordering. We studied the connection of line patterns in PCPs with different data analysis tasks and datasets. PC-Expo expands prior work on PCP axes reordering by developing real-time, local detection schemes for the 12 most common analysis tasks (properties). Users can choose the story they want to present with PCPs by optimizing directly over their choice of properties. These properties can be ranked, or combined using individual weights, creating a custom optimization scheme for axes reordering. Users can control the granularity at which they want to work with their detection scheme in the data, allowing exploration of local regions. PC-Expo also supports HIL axes reordering via local-property visualization, which shows the regions of granular activity for every axis pair. Local-property visualization is helpful for PCP axes reordering based on multiple properties, when no single reordering scheme fits the user goals. A comprehensive evaluation was done with real users and diverse datasets confirm the efficacy of PC-Expo in data storytelling with PCPs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2022.3209392",
            "id": "r_556",
            "s_ids": [
                "s_265",
                "s_1541",
                "s_396",
                "s_1208",
                "s_277"
            ],
            "type": "rich",
            "x": -9.264181137084961,
            "y": 18.79216957092285
        },
        {
            "id": "s_0",
            "name": "Yannick Assogba",
            "type": "sparse",
            "x": -3.8696022033691406,
            "y": 17.399070739746094
        },
        {
            "id": "s_1",
            "name": "Jacob Ritchie",
            "type": "sparse",
            "x": -6.766488552093506,
            "y": 16.953500747680664
        },
        {
            "id": "s_2",
            "name": "Dave Pugmire",
            "type": "sparse",
            "x": -1.8696587085723877,
            "y": 17.952234268188477
        },
        {
            "id": "s_3",
            "name": "Chen Zhao",
            "type": "sparse",
            "x": -3.9599037170410156,
            "y": 20.100914001464844
        },
        {
            "id": "s_4",
            "name": "Ethan Cerami",
            "type": "sparse",
            "x": -8.419800758361816,
            "y": 17.06220054626465
        },
        {
            "id": "s_5",
            "name": "Lizhen Qu",
            "type": "sparse",
            "x": -7.662174701690674,
            "y": 18.05329132080078
        },
        {
            "id": "s_6",
            "name": "Emily Wall",
            "type": "sparse",
            "x": -4.747397422790527,
            "y": 15.480459213256836
        },
        {
            "id": "s_7",
            "name": "Daehyun Kim 0005",
            "type": "sparse",
            "x": -4.119040489196777,
            "y": 16.242109298706055
        },
        {
            "id": "s_8",
            "name": "Fabian Jirasek",
            "type": "sparse",
            "x": -3.749201536178589,
            "y": 20.918596267700195
        },
        {
            "id": "s_9",
            "name": "Chuan Bu",
            "type": "sparse",
            "x": -8.170709609985352,
            "y": 18.548873901367188
        },
        {
            "id": "s_10",
            "name": "Mirek Riedewald",
            "type": "sparse",
            "x": -6.750594615936279,
            "y": 21.147092819213867
        },
        {
            "id": "s_11",
            "name": "Natalia V. Andrienko",
            "type": "sparse",
            "x": -10.26736068725586,
            "y": 19.6679630279541
        },
        {
            "id": "s_12",
            "name": "Matthew Brehmer",
            "type": "sparse",
            "x": -6.536952972412109,
            "y": 16.680370330810547
        },
        {
            "id": "s_13",
            "name": "Shenghan Gao",
            "type": "sparse",
            "x": -11.04520034790039,
            "y": 19.293766021728516
        },
        {
            "id": "s_14",
            "name": "Shahid Latif",
            "type": "sparse",
            "x": -10.854389190673828,
            "y": 17.260061264038086
        },
        {
            "id": "s_15",
            "name": "Mohamed Ibrahim",
            "type": "sparse",
            "x": -8.862010955810547,
            "y": 16.241273880004883
        },
        {
            "id": "s_16",
            "name": "Robert Barnes",
            "type": "sparse",
            "x": -6.680916786193848,
            "y": 14.2233304977417
        },
        {
            "id": "s_17",
            "name": "M\u00e1rcio Cataldi",
            "type": "sparse",
            "x": -7.501638412475586,
            "y": -1.1755280494689941
        },
        {
            "id": "s_18",
            "name": "Qisen Yang",
            "type": "sparse",
            "x": -9.771199226379395,
            "y": 16.9982852935791
        },
        {
            "id": "s_19",
            "name": "Xin Zhao",
            "type": "sparse",
            "x": -9.359938621520996,
            "y": 17.905302047729492
        },
        {
            "id": "s_20",
            "name": "Alper Sahistan",
            "type": "sparse",
            "x": -15.055231094360352,
            "y": 7.119110584259033
        },
        {
            "id": "s_21",
            "name": "Minjeong Shin",
            "type": "sparse",
            "x": -1.9981287717819214,
            "y": 16.003211975097656
        },
        {
            "id": "s_22",
            "name": "Maryam Hosseini",
            "type": "sparse",
            "x": -7.510098934173584,
            "y": -1.1670669317245483
        },
        {
            "id": "s_23",
            "name": "Jakob Bonart",
            "type": "sparse",
            "x": -8.574953079223633,
            "y": 18.677623748779297
        },
        {
            "id": "s_24",
            "name": "Subhajit Das 0002",
            "type": "sparse",
            "x": -4.7892069816589355,
            "y": 15.772269248962402
        },
        {
            "id": "s_25",
            "name": "Ke Xu",
            "type": "sparse",
            "x": -9.322138786315918,
            "y": 19.765628814697266
        },
        {
            "id": "s_26",
            "name": "Haekyu Park",
            "type": "sparse",
            "x": 10.040589332580566,
            "y": 10.498444557189941
        },
        {
            "id": "s_27",
            "name": "Arjun Srinivasan",
            "type": "sparse",
            "x": -3.9878365993499756,
            "y": 16.108932495117188
        },
        {
            "id": "s_28",
            "name": "Harish Doraiswamy",
            "type": "sparse",
            "x": -4.052337646484375,
            "y": 20.29960060119629
        },
        {
            "id": "s_29",
            "name": "Brittany Terese Fasy",
            "type": "sparse",
            "x": -15.11682415008545,
            "y": 7.057567596435547
        },
        {
            "id": "s_30",
            "name": "Martin Hachet",
            "type": "sparse",
            "x": -7.124034881591797,
            "y": 17.91957664489746
        },
        {
            "id": "s_31",
            "name": "Michael Terry",
            "type": "sparse",
            "x": 9.90602970123291,
            "y": 10.635871887207031
        },
        {
            "id": "s_32",
            "name": "Shaolun Ruan",
            "type": "sparse",
            "x": -9.806717872619629,
            "y": 18.652456283569336
        },
        {
            "id": "s_33",
            "name": "Amani Ageeli",
            "type": "sparse",
            "x": -8.84898853302002,
            "y": 16.184431076049805
        },
        {
            "id": "s_34",
            "name": "Zhiwei Deng",
            "type": "sparse",
            "x": -9.30379581451416,
            "y": 18.14384651184082
        },
        {
            "id": "s_35",
            "name": "Jonas Lukasczyk",
            "type": "sparse",
            "x": -4.064417839050293,
            "y": 20.854393005371094
        },
        {
            "id": "s_36",
            "name": "Chin Tseng",
            "type": "sparse",
            "x": -2.140932321548462,
            "y": 18.053924560546875
        },
        {
            "id": "s_37",
            "name": "Zhendong Yang",
            "type": "sparse",
            "x": -9.234454154968262,
            "y": 17.931896209716797
        },
        {
            "id": "s_38",
            "name": "Ga\u00eblle Richer",
            "type": "sparse",
            "x": -3.92429518699646,
            "y": 17.428447723388672
        },
        {
            "id": "s_39",
            "name": "Mingdong Zhang",
            "type": "sparse",
            "x": -10.943045616149902,
            "y": 19.4132137298584
        },
        {
            "id": "s_40",
            "name": "Heike Leitte",
            "type": "sparse",
            "x": -3.8950588703155518,
            "y": 20.8790225982666
        },
        {
            "id": "s_41",
            "name": "Yalong Yang 0001",
            "type": "sparse",
            "x": -7.896790027618408,
            "y": 17.564573287963867
        },
        {
            "id": "s_42",
            "name": "Dominik J\u00e4ckle",
            "type": "sparse",
            "x": -8.665327072143555,
            "y": 18.609182357788086
        },
        {
            "id": "s_43",
            "name": "Harry X. Li",
            "type": "sparse",
            "x": -4.811498165130615,
            "y": 15.9360990524292
        },
        {
            "id": "s_44",
            "name": "Adam Kunen",
            "type": "sparse",
            "x": -0.20820458233356476,
            "y": 17.10364532470703
        },
        {
            "id": "s_45",
            "name": "Nathan Daniel Schiele",
            "type": "sparse",
            "x": -8.431318283081055,
            "y": 19.1435489654541
        },
        {
            "id": "s_46",
            "name": "Tai-Quan Peng",
            "type": "sparse",
            "x": -10.271053314208984,
            "y": 17.61513900756836
        },
        {
            "id": "s_47",
            "name": "Yixuan Li",
            "type": "sparse",
            "x": -9.716283798217773,
            "y": 19.69922637939453
        },
        {
            "id": "s_48",
            "name": "Yuxin Ma",
            "type": "sparse",
            "x": -4.26697301864624,
            "y": 21.084171295166016
        },
        {
            "id": "s_49",
            "name": "Peter W. S. Butcher",
            "type": "sparse",
            "x": -1.9881179332733154,
            "y": 16.049924850463867
        },
        {
            "id": "s_50",
            "name": "Fengjie Wang",
            "type": "sparse",
            "x": -10.220026016235352,
            "y": 18.548667907714844
        },
        {
            "id": "s_51",
            "name": "Xiaoyu Yang",
            "type": "sparse",
            "x": -9.903128623962402,
            "y": 18.32742691040039
        },
        {
            "id": "s_52",
            "name": "Linfang Li",
            "type": "sparse",
            "x": -10.46876049041748,
            "y": 19.671707153320312
        },
        {
            "id": "s_53",
            "name": "Zhanglin Cheng",
            "type": "sparse",
            "x": -8.178196907043457,
            "y": 18.605941772460938
        },
        {
            "id": "s_54",
            "name": "Luis Ceferino",
            "type": "sparse",
            "x": -7.523111343383789,
            "y": -1.1541104316711426
        },
        {
            "id": "s_55",
            "name": "Bernhard Jenny",
            "type": "sparse",
            "x": -7.463726043701172,
            "y": 17.905654907226562
        },
        {
            "id": "s_56",
            "name": "Zuobin Wang",
            "type": "sparse",
            "x": -10.346285820007324,
            "y": 17.36063575744629
        },
        {
            "id": "s_57",
            "name": "Luhao Ge",
            "type": "sparse",
            "x": -9.252093315124512,
            "y": 17.716890335083008
        },
        {
            "id": "s_58",
            "name": "Guihua Shan",
            "type": "sparse",
            "x": 7.575558185577393,
            "y": 5.200226306915283
        },
        {
            "id": "s_59",
            "name": "Nikolaus Piccolotto",
            "type": "sparse",
            "x": -5.476596832275391,
            "y": 14.958902359008789
        },
        {
            "id": "s_60",
            "name": "Haoren Wang",
            "type": "sparse",
            "x": -10.288528442382812,
            "y": 17.438018798828125
        },
        {
            "id": "s_61",
            "name": "Haomin Li",
            "type": "sparse",
            "x": -10.119996070861816,
            "y": 18.336301803588867
        },
        {
            "id": "s_62",
            "name": "Renzhong Li",
            "type": "sparse",
            "x": -10.105141639709473,
            "y": 17.36746597290039
        },
        {
            "id": "s_63",
            "name": "Bruno Pinaud",
            "type": "sparse",
            "x": -3.924896717071533,
            "y": 19.768491744995117
        },
        {
            "id": "s_64",
            "name": "Hyeok Kim",
            "type": "sparse",
            "x": -3.533512830734253,
            "y": 17.013818740844727
        },
        {
            "id": "s_65",
            "name": "Phong Hai Nguyen",
            "type": "sparse",
            "x": -7.166531085968018,
            "y": 17.053861618041992
        },
        {
            "id": "s_66",
            "name": "Laura A. Garrison",
            "type": "sparse",
            "x": -5.896170139312744,
            "y": 19.295854568481445
        },
        {
            "id": "s_67",
            "name": "Melissa Perez",
            "type": "sparse",
            "x": 9.953734397888184,
            "y": 10.587703704833984
        },
        {
            "id": "s_68",
            "name": "Jia-Ren Lin",
            "type": "sparse",
            "x": -8.6782865524292,
            "y": 16.533878326416016
        },
        {
            "id": "s_69",
            "name": "Jane Hoffswell",
            "type": "sparse",
            "x": -3.809765338897705,
            "y": 16.632869720458984
        },
        {
            "id": "s_70",
            "name": "Maria Gabriela Ayala",
            "type": "sparse",
            "x": -3.4562559127807617,
            "y": 16.957717895507812
        },
        {
            "id": "s_71",
            "name": "Francis Nguyen",
            "type": "sparse",
            "x": -3.71380352973938,
            "y": 17.453720092773438
        },
        {
            "id": "s_72",
            "name": "Tim Althoff",
            "type": "sparse",
            "x": -3.667416572570801,
            "y": 17.044673919677734
        },
        {
            "id": "s_73",
            "name": "Lonni Besan\u00e7on",
            "type": "sparse",
            "x": -7.486634254455566,
            "y": 17.850975036621094
        },
        {
            "id": "s_74",
            "name": "Jarke J. van Wijk",
            "type": "sparse",
            "x": -7.000528812408447,
            "y": 20.900564193725586
        },
        {
            "id": "s_75",
            "name": "Elsie Lee",
            "type": "sparse",
            "x": -3.1546173095703125,
            "y": 16.87000846862793
        },
        {
            "id": "s_76",
            "name": "Florian Heimerl",
            "type": "sparse",
            "x": -4.7921929359436035,
            "y": 15.763275146484375
        },
        {
            "id": "s_77",
            "name": "Runfei Li",
            "type": "sparse",
            "x": -7.843496799468994,
            "y": 18.983041763305664
        },
        {
            "id": "s_78",
            "name": "Binjie Chen",
            "type": "sparse",
            "x": -9.256668090820312,
            "y": 17.872032165527344
        },
        {
            "id": "s_79",
            "name": "Mingliang Xu 0001",
            "type": "sparse",
            "x": -9.828402519226074,
            "y": 17.62334442138672
        },
        {
            "id": "s_80",
            "name": "Robert Kosara",
            "type": "sparse",
            "x": -6.46970796585083,
            "y": 16.546293258666992
        },
        {
            "id": "s_81",
            "name": "Yunwon Tae",
            "type": "sparse",
            "x": -1.6952487230300903,
            "y": 16.43196678161621
        },
        {
            "id": "s_82",
            "name": "Ellen Yi-Luen Do",
            "type": "sparse",
            "x": -2.1075596809387207,
            "y": 18.101329803466797
        },
        {
            "id": "s_83",
            "name": "Samuel Gratzl",
            "type": "sparse",
            "x": -7.481410980224609,
            "y": 17.80852508544922
        },
        {
            "id": "s_84",
            "name": "Dave Brown",
            "type": "sparse",
            "x": -7.5129008293151855,
            "y": 17.888559341430664
        },
        {
            "id": "s_85",
            "name": "Andrea Pinceti",
            "type": "sparse",
            "x": -3.2844626903533936,
            "y": 17.63568115234375
        },
        {
            "id": "s_86",
            "name": "Hanning Shao",
            "type": "sparse",
            "x": -10.442296981811523,
            "y": 19.642274856567383
        },
        {
            "id": "s_87",
            "name": "Connor Scully-Allison",
            "type": "sparse",
            "x": -0.21544180810451508,
            "y": 17.09624481201172
        },
        {
            "id": "s_88",
            "name": "Adam J. Step\u00e1nek",
            "type": "sparse",
            "x": -5.738799095153809,
            "y": 19.509639739990234
        },
        {
            "id": "s_89",
            "name": "Sven Gedicke",
            "type": "sparse",
            "x": -5.625068187713623,
            "y": 15.063183784484863
        },
        {
            "id": "s_90",
            "name": "Markus B\u00f6gl",
            "type": "sparse",
            "x": -5.476320743560791,
            "y": 14.944557189941406
        },
        {
            "id": "s_91",
            "name": "Yingchaojie Feng",
            "type": "sparse",
            "x": -9.559098243713379,
            "y": 18.032814025878906
        },
        {
            "id": "s_92",
            "name": "Martijn Tennekes",
            "type": "sparse",
            "x": -7.259284496307373,
            "y": 16.902372360229492
        },
        {
            "id": "s_93",
            "name": "Mingliang Xu",
            "type": "sparse",
            "x": -8.286060333251953,
            "y": 18.454744338989258
        },
        {
            "id": "s_94",
            "name": "Pavol Klacansky",
            "type": "sparse",
            "x": -15.09214973449707,
            "y": 7.082183837890625
        },
        {
            "id": "s_95",
            "name": "Markus H\u00f6hn",
            "type": "sparse",
            "x": -5.852603912353516,
            "y": 15.344143867492676
        },
        {
            "id": "s_96",
            "name": "Cagatay Turkay",
            "type": "sparse",
            "x": -8.872102737426758,
            "y": 17.74068260192871
        },
        {
            "id": "s_97",
            "name": "He He 0001",
            "type": "sparse",
            "x": -3.9494218826293945,
            "y": 20.074542999267578
        },
        {
            "id": "s_98",
            "name": "Jen Rogers",
            "type": "sparse",
            "x": -2.4677751064300537,
            "y": 15.447752952575684
        },
        {
            "id": "s_99",
            "name": "Gabriel Appleby",
            "type": "sparse",
            "x": -4.78496789932251,
            "y": 15.939104080200195
        },
        {
            "id": "s_100",
            "name": "Sarah Goodwin",
            "type": "sparse",
            "x": -7.43504524230957,
            "y": 17.831125259399414
        },
        {
            "id": "s_101",
            "name": "Jorge Piazentin Ono",
            "type": "sparse",
            "x": -0.7167224884033203,
            "y": 17.91783332824707
        },
        {
            "id": "s_102",
            "name": "Carola Wenk",
            "type": "sparse",
            "x": -15.099610328674316,
            "y": 7.0747833251953125
        },
        {
            "id": "s_103",
            "name": "Ute Schmid",
            "type": "sparse",
            "x": -8.579665184020996,
            "y": 18.688690185546875
        },
        {
            "id": "s_104",
            "name": "Ziao Liu",
            "type": "sparse",
            "x": -10.32309341430664,
            "y": 17.368927001953125
        },
        {
            "id": "s_105",
            "name": "Yu Zheng 0004",
            "type": "sparse",
            "x": -10.041695594787598,
            "y": 17.40396499633789
        },
        {
            "id": "s_106",
            "name": "Huai-Yu Wang",
            "type": "sparse",
            "x": -9.004141807556152,
            "y": 19.59464454650879
        },
        {
            "id": "s_107",
            "name": "Jean-Daniel Fekete",
            "type": "sparse",
            "x": -6.328414440155029,
            "y": 17.154582977294922
        },
        {
            "id": "s_108",
            "name": "Thomas Ortner",
            "type": "sparse",
            "x": -6.520243167877197,
            "y": 14.340694427490234
        },
        {
            "id": "s_109",
            "name": "Jun Zhu",
            "type": "sparse",
            "x": -10.090773582458496,
            "y": 19.584035873413086
        },
        {
            "id": "s_110",
            "name": "Eunyee Koh",
            "type": "sparse",
            "x": -3.7013328075408936,
            "y": 16.513288497924805
        },
        {
            "id": "s_111",
            "name": "Aditi Mishra",
            "type": "sparse",
            "x": -3.2704827785491943,
            "y": 17.64308738708496
        },
        {
            "id": "s_112",
            "name": "Sabine Bauer 0001",
            "type": "sparse",
            "x": -3.5577714443206787,
            "y": 19.837871551513672
        },
        {
            "id": "s_113",
            "name": "Jing Qian",
            "type": "sparse",
            "x": -3.9872097969055176,
            "y": 20.08298683166504
        },
        {
            "id": "s_114",
            "name": "John Thompson 0002",
            "type": "sparse",
            "x": -7.679284572601318,
            "y": 18.228473663330078
        },
        {
            "id": "s_115",
            "name": "Dai Li",
            "type": "sparse",
            "x": -11.04183292388916,
            "y": 19.276702880859375
        },
        {
            "id": "s_116",
            "name": "Zhao Zhang",
            "type": "sparse",
            "x": -9.247947692871094,
            "y": 17.735679626464844
        },
        {
            "id": "s_117",
            "name": "S\u00f6ren D\u00f6ring",
            "type": "sparse",
            "x": -8.575455665588379,
            "y": 19.37781524658203
        },
        {
            "id": "s_118",
            "name": "Ruike Jiang",
            "type": "sparse",
            "x": -10.308695793151855,
            "y": 19.6336727142334
        },
        {
            "id": "s_119",
            "name": "Grace Ko",
            "type": "sparse",
            "x": -4.285093307495117,
            "y": 16.9532470703125
        },
        {
            "id": "s_120",
            "name": "Yi Li",
            "type": "sparse",
            "x": -9.957212448120117,
            "y": 17.478349685668945
        },
        {
            "id": "s_121",
            "name": "Nate Morrical",
            "type": "sparse",
            "x": -15.02820110321045,
            "y": 7.1461262702941895
        },
        {
            "id": "s_122",
            "name": "Andrea G. Parker",
            "type": "sparse",
            "x": -6.660942554473877,
            "y": 21.247249603271484
        },
        {
            "id": "s_123",
            "name": "Jingtao Zhou",
            "type": "sparse",
            "x": -6.743841648101807,
            "y": 16.868486404418945
        },
        {
            "id": "s_124",
            "name": "Martin N\u00f6llenburg",
            "type": "sparse",
            "x": -5.694780349731445,
            "y": 15.177310943603516
        },
        {
            "id": "s_125",
            "name": "Pramod Chundury",
            "type": "sparse",
            "x": -2.024251937866211,
            "y": 16.010530471801758
        },
        {
            "id": "s_126",
            "name": "Timothy F. Brady",
            "type": "sparse",
            "x": -3.1434996128082275,
            "y": 16.563941955566406
        },
        {
            "id": "s_127",
            "name": "Maoyuan Sun",
            "type": "sparse",
            "x": -0.5789170265197754,
            "y": 18.18124771118164
        },
        {
            "id": "s_128",
            "name": "Marie Shvakel",
            "type": "sparse",
            "x": -3.9830594062805176,
            "y": 16.667909622192383
        },
        {
            "id": "s_129",
            "name": "Namyong Park",
            "type": "sparse",
            "x": 10.397810935974121,
            "y": 10.1524019241333
        },
        {
            "id": "s_130",
            "name": "Dylan Wootton",
            "type": "sparse",
            "x": -4.312469959259033,
            "y": 16.05820655822754
        },
        {
            "id": "s_131",
            "name": "Erik C. Nisbet",
            "type": "sparse",
            "x": -3.2025580406188965,
            "y": 16.651578903198242
        },
        {
            "id": "s_132",
            "name": "Andrew McNutt",
            "type": "sparse",
            "x": -3.4173948764801025,
            "y": 16.918567657470703
        },
        {
            "id": "s_133",
            "name": "Ying Mao 0001",
            "type": "sparse",
            "x": -9.845970153808594,
            "y": 18.582170486450195
        },
        {
            "id": "s_134",
            "name": "Fangfang Zhou",
            "type": "sparse",
            "x": -9.217703819274902,
            "y": 17.68551254272461
        },
        {
            "id": "s_135",
            "name": "Jason D. Hartline",
            "type": "sparse",
            "x": -3.3978283405303955,
            "y": 16.810693740844727
        },
        {
            "id": "s_136",
            "name": "Junhong Wang",
            "type": "sparse",
            "x": -10.089485168457031,
            "y": 19.570478439331055
        },
        {
            "id": "s_137",
            "name": "Dingdong Liu",
            "type": "sparse",
            "x": -10.129855155944824,
            "y": 18.333223342895508
        },
        {
            "id": "s_138",
            "name": "Quanjie Zhang",
            "type": "sparse",
            "x": -8.181195259094238,
            "y": 18.47355079650879
        },
        {
            "id": "s_139",
            "name": "Jonathan Zhang",
            "type": "sparse",
            "x": -9.012657165527344,
            "y": 19.827030181884766
        },
        {
            "id": "s_140",
            "name": "Bernhard Pointner",
            "type": "sparse",
            "x": -5.420678615570068,
            "y": 14.877758026123047
        },
        {
            "id": "s_141",
            "name": "Liang Gou",
            "type": "sparse",
            "x": -0.5168524384498596,
            "y": 17.72507667541504
        },
        {
            "id": "s_142",
            "name": "Lihong Cai",
            "type": "sparse",
            "x": -9.367019653320312,
            "y": 17.763614654541016
        },
        {
            "id": "s_143",
            "name": "Anna Offenwanger",
            "type": "sparse",
            "x": -6.535815715789795,
            "y": 16.680086135864258
        },
        {
            "id": "s_144",
            "name": "Moqi He",
            "type": "sparse",
            "x": -10.19063949584961,
            "y": 17.29705047607422
        },
        {
            "id": "s_145",
            "name": "Lane Harrison",
            "type": "sparse",
            "x": -3.1932032108306885,
            "y": 16.58115577697754
        },
        {
            "id": "s_146",
            "name": "Ciril Bohak",
            "type": "sparse",
            "x": -6.246274948120117,
            "y": 18.985488891601562
        },
        {
            "id": "s_147",
            "name": "Sabin Devkota",
            "type": "sparse",
            "x": -0.2085176706314087,
            "y": 17.104768753051758
        },
        {
            "id": "s_148",
            "name": "Weikai Yang",
            "type": "sparse",
            "x": -9.141069412231445,
            "y": 16.893831253051758
        },
        {
            "id": "s_149",
            "name": "Mengjie Fan",
            "type": "sparse",
            "x": -8.988927841186523,
            "y": 19.570199966430664
        },
        {
            "id": "s_150",
            "name": "Lauren F. Klein",
            "type": "sparse",
            "x": -2.337010145187378,
            "y": 15.317687034606934
        },
        {
            "id": "s_151",
            "name": "Chang Han",
            "type": "sparse",
            "x": -7.836178779602051,
            "y": 18.387907028198242
        },
        {
            "id": "s_152",
            "name": "Xiangtong Chu",
            "type": "sparse",
            "x": -10.388622283935547,
            "y": 17.604663848876953
        },
        {
            "id": "s_153",
            "name": "Yuchu Luo",
            "type": "sparse",
            "x": -10.437369346618652,
            "y": 19.701725006103516
        },
        {
            "id": "s_154",
            "name": "Shuangcheng Jiao",
            "type": "sparse",
            "x": -3.3091962337493896,
            "y": 17.64661407470703
        },
        {
            "id": "s_155",
            "name": "Dongmei Zhang 0001",
            "type": "sparse",
            "x": -10.479697227478027,
            "y": 18.21605110168457
        },
        {
            "id": "s_156",
            "name": "Huamin Qu",
            "type": "sparse",
            "x": -10.103364944458008,
            "y": 18.451417922973633
        },
        {
            "id": "s_157",
            "name": "Daniel Weiskopf",
            "type": "sparse",
            "x": -8.837234497070312,
            "y": 19.52309799194336
        },
        {
            "id": "s_158",
            "name": "Ethan Brewer",
            "type": "sparse",
            "x": -4.000626564025879,
            "y": 20.083498001098633
        },
        {
            "id": "s_159",
            "name": "Dongping Zhang",
            "type": "sparse",
            "x": -3.3425610065460205,
            "y": 16.8185977935791
        },
        {
            "id": "s_160",
            "name": "Devin Lange",
            "type": "sparse",
            "x": -2.2548649311065674,
            "y": 15.369200706481934
        },
        {
            "id": "s_161",
            "name": "Kin Chung Kwan",
            "type": "sparse",
            "x": -8.095637321472168,
            "y": 18.630802154541016
        },
        {
            "id": "s_162",
            "name": "Haozhe Feng",
            "type": "sparse",
            "x": -9.923013687133789,
            "y": 18.354732513427734
        },
        {
            "id": "s_163",
            "name": "Shubham Mehta",
            "type": "sparse",
            "x": -3.386202096939087,
            "y": 16.425907135009766
        },
        {
            "id": "s_164",
            "name": "Peifeng Lai",
            "type": "sparse",
            "x": -9.72276496887207,
            "y": 18.574960708618164
        },
        {
            "id": "s_165",
            "name": "Victor Antonio Paludetto Magri",
            "type": "sparse",
            "x": -15.09267520904541,
            "y": 7.081839084625244
        },
        {
            "id": "s_166",
            "name": "Glenn Sun",
            "type": "sparse",
            "x": -3.3583521842956543,
            "y": 16.875890731811523
        },
        {
            "id": "s_167",
            "name": "Elmar Eisemann",
            "type": "sparse",
            "x": -6.834035873413086,
            "y": 14.339221954345703
        },
        {
            "id": "s_168",
            "name": "Running Zhao",
            "type": "sparse",
            "x": -9.30547046661377,
            "y": 18.909528732299805
        },
        {
            "id": "s_169",
            "name": "Sao Myat Thazin Thane",
            "type": "sparse",
            "x": -3.9255125522613525,
            "y": 16.67899513244629
        },
        {
            "id": "s_170",
            "name": "Dan Zeng 0002",
            "type": "sparse",
            "x": -9.291030883789062,
            "y": 20.24965476989746
        },
        {
            "id": "s_171",
            "name": "Sara Tandon",
            "type": "sparse",
            "x": -7.1587300300598145,
            "y": 16.818561553955078
        },
        {
            "id": "s_172",
            "name": "Jingyi Shen",
            "type": "sparse",
            "x": 7.540521144866943,
            "y": 5.234576225280762
        },
        {
            "id": "s_173",
            "name": "Yea-Seul Kim",
            "type": "sparse",
            "x": -3.4356954097747803,
            "y": 16.61370086669922
        },
        {
            "id": "s_174",
            "name": "Katherine E. Isaacs",
            "type": "sparse",
            "x": -0.25756198167800903,
            "y": 17.14628791809082
        },
        {
            "id": "s_175",
            "name": "Ruizhen Hu",
            "type": "sparse",
            "x": -7.9874043464660645,
            "y": 18.68158721923828
        },
        {
            "id": "s_176",
            "name": "Natasha Alvarado",
            "type": "sparse",
            "x": -6.443319797515869,
            "y": 16.492334365844727
        },
        {
            "id": "s_177",
            "name": "Song Ge",
            "type": "sparse",
            "x": -10.860945701599121,
            "y": 18.283559799194336
        },
        {
            "id": "s_178",
            "name": "Etowah Adams",
            "type": "sparse",
            "x": -8.44494915008545,
            "y": 17.02249526977539
        },
        {
            "id": "s_179",
            "name": "Michael Gleicher",
            "type": "sparse",
            "x": -4.857936859130859,
            "y": 15.753843307495117
        },
        {
            "id": "s_180",
            "name": "Martin Wattenberg",
            "type": "sparse",
            "x": -10.49087142944336,
            "y": 17.93783187866211
        },
        {
            "id": "s_181",
            "name": "Alireza Entezari",
            "type": "sparse",
            "x": -1.8265119791030884,
            "y": 17.901538848876953
        },
        {
            "id": "s_182",
            "name": "Anastasia Bezerianos",
            "type": "sparse",
            "x": -6.167418956756592,
            "y": 18.262300491333008
        },
        {
            "id": "s_183",
            "name": "Guan Li",
            "type": "sparse",
            "x": 7.582882404327393,
            "y": 5.193093776702881
        },
        {
            "id": "s_184",
            "name": "Yang Shi 0007",
            "type": "sparse",
            "x": -9.465765953063965,
            "y": 19.643781661987305
        },
        {
            "id": "s_185",
            "name": "Martina Maritan",
            "type": "sparse",
            "x": -6.236461162567139,
            "y": 19.014816284179688
        },
        {
            "id": "s_186",
            "name": "Md. Iqbal Hossain 0001",
            "type": "sparse",
            "x": -3.974012851715088,
            "y": 19.666589736938477
        },
        {
            "id": "s_187",
            "name": "Biswaksen Patnaik",
            "type": "sparse",
            "x": -1.9875695705413818,
            "y": 16.037857055664062
        },
        {
            "id": "s_188",
            "name": "Marian D\u00f6rk",
            "type": "sparse",
            "x": -6.905625343322754,
            "y": 17.042768478393555
        },
        {
            "id": "s_189",
            "name": "Federica Bucchieri",
            "type": "sparse",
            "x": -6.274756908416748,
            "y": 18.361392974853516
        },
        {
            "id": "s_190",
            "name": "Rongchen Zhu",
            "type": "sparse",
            "x": -10.061190605163574,
            "y": 18.581626892089844
        },
        {
            "id": "s_191",
            "name": "Qian Zhu 0010",
            "type": "sparse",
            "x": -10.737481117248535,
            "y": 19.0037784576416
        },
        {
            "id": "s_192",
            "name": "Huanliang Wang",
            "type": "sparse",
            "x": -9.902630805969238,
            "y": 18.3363094329834
        },
        {
            "id": "s_193",
            "name": "Chloe Mortenson",
            "type": "sparse",
            "x": -3.1358673572540283,
            "y": 16.609472274780273
        },
        {
            "id": "s_194",
            "name": "Da Cheng",
            "type": "sparse",
            "x": -8.122654914855957,
            "y": 18.64936065673828
        },
        {
            "id": "s_195",
            "name": "Mingliang Xue",
            "type": "sparse",
            "x": -8.138163566589355,
            "y": 18.582908630371094
        },
        {
            "id": "s_196",
            "name": "Jan-Tobias Sohns",
            "type": "sparse",
            "x": -3.717907190322876,
            "y": 20.94751739501953
        },
        {
            "id": "s_197",
            "name": "Camelia Daniela Brumar",
            "type": "sparse",
            "x": -4.813684940338135,
            "y": 15.950591087341309
        },
        {
            "id": "s_198",
            "name": "Fanny Chevalier",
            "type": "sparse",
            "x": -6.601778984069824,
            "y": 16.777299880981445
        },
        {
            "id": "s_199",
            "name": "Evan Greene",
            "type": "sparse",
            "x": -8.451606750488281,
            "y": 16.97917938232422
        },
        {
            "id": "s_200",
            "name": "Haipeng Zhang",
            "type": "sparse",
            "x": -11.02796745300293,
            "y": 19.327213287353516
        },
        {
            "id": "s_201",
            "name": "Julius Rauscher",
            "type": "sparse",
            "x": -7.048274993896484,
            "y": 20.8579044342041
        },
        {
            "id": "s_202",
            "name": "Tiankai Xie",
            "type": "sparse",
            "x": -4.286441802978516,
            "y": 21.113168716430664
        },
        {
            "id": "s_203",
            "name": "Evanthia Dimara",
            "type": "sparse",
            "x": -4.493765830993652,
            "y": 15.306496620178223
        },
        {
            "id": "s_204",
            "name": "Huaqiang Zhang",
            "type": "sparse",
            "x": -10.200718879699707,
            "y": 17.21214485168457
        },
        {
            "id": "s_205",
            "name": "Bernhard Woschizka",
            "type": "sparse",
            "x": -8.828532218933105,
            "y": 16.24837875366211
        },
        {
            "id": "s_206",
            "name": "Jianbin Qin",
            "type": "sparse",
            "x": -9.291751861572266,
            "y": 20.25324821472168
        },
        {
            "id": "s_207",
            "name": "Ria Chawla",
            "type": "sparse",
            "x": -4.39115047454834,
            "y": 15.190711975097656
        },
        {
            "id": "s_208",
            "name": "Senthil K. Chandrasegaran",
            "type": "sparse",
            "x": -0.7743688225746155,
            "y": 17.985130310058594
        },
        {
            "id": "s_209",
            "name": "Sungbok Shin",
            "type": "sparse",
            "x": -1.9854587316513062,
            "y": 16.04341697692871
        },
        {
            "id": "s_210",
            "name": "Tim Dwyer",
            "type": "sparse",
            "x": -7.577081680297852,
            "y": 17.783445358276367
        },
        {
            "id": "s_211",
            "name": "Julian Thijssen",
            "type": "sparse",
            "x": -6.862927436828613,
            "y": 14.342658996582031
        },
        {
            "id": "s_212",
            "name": "Baldur van Lew",
            "type": "sparse",
            "x": -6.861311912536621,
            "y": 14.338139533996582
        },
        {
            "id": "s_213",
            "name": "Ran Zhou 0003",
            "type": "sparse",
            "x": -2.1672394275665283,
            "y": 18.04525375366211
        },
        {
            "id": "s_214",
            "name": "Zezhong Wang 0001",
            "type": "sparse",
            "x": -6.711562156677246,
            "y": 16.920602798461914
        },
        {
            "id": "s_215",
            "name": "Alexander Lex",
            "type": "sparse",
            "x": -2.2368006706237793,
            "y": 15.413448333740234
        },
        {
            "id": "s_216",
            "name": "Shaocong Tao",
            "type": "sparse",
            "x": -11.028627395629883,
            "y": 19.37005043029785
        },
        {
            "id": "s_217",
            "name": "Yige Xu 0001",
            "type": "sparse",
            "x": -10.923994064331055,
            "y": 19.000003814697266
        },
        {
            "id": "s_218",
            "name": "Harsh Bhatia",
            "type": "sparse",
            "x": -15.078545570373535,
            "y": 7.095775127410889
        },
        {
            "id": "s_219",
            "name": "Liang Zhou 0001",
            "type": "sparse",
            "x": -8.273073196411133,
            "y": 19.017717361450195
        },
        {
            "id": "s_220",
            "name": "Xingchen Zeng",
            "type": "sparse",
            "x": -9.714146614074707,
            "y": 17.867504119873047
        },
        {
            "id": "s_221",
            "name": "A. Dalpke",
            "type": "sparse",
            "x": -5.868326663970947,
            "y": 15.340116500854492
        },
        {
            "id": "s_222",
            "name": "Ruwayda Alharbi",
            "type": "sparse",
            "x": -6.238568305969238,
            "y": 19.009687423706055
        },
        {
            "id": "s_223",
            "name": "Christopher Taylor",
            "type": "sparse",
            "x": -0.24961327016353607,
            "y": 17.134973526000977
        },
        {
            "id": "s_224",
            "name": "Yuhong Lu",
            "type": "sparse",
            "x": -9.626243591308594,
            "y": 18.015466690063477
        },
        {
            "id": "s_225",
            "name": "Hongzeng Zhang",
            "type": "sparse",
            "x": -10.425389289855957,
            "y": 17.503761291503906
        },
        {
            "id": "s_226",
            "name": "Fuling Sun",
            "type": "sparse",
            "x": -9.329188346862793,
            "y": 19.68181037902832
        },
        {
            "id": "s_227",
            "name": "Markus Billeter",
            "type": "sparse",
            "x": -6.872035503387451,
            "y": 14.350930213928223
        },
        {
            "id": "s_228",
            "name": "Carter Blair",
            "type": "sparse",
            "x": -6.163191318511963,
            "y": 17.406400680541992
        },
        {
            "id": "s_229",
            "name": "Lace M. K. Padilla",
            "type": "sparse",
            "x": -3.4863884449005127,
            "y": 17.5113525390625
        },
        {
            "id": "s_230",
            "name": "Joel Lanir",
            "type": "sparse",
            "x": -8.042034149169922,
            "y": 18.697757720947266
        },
        {
            "id": "s_231",
            "name": "Paulo J. S. Silva",
            "type": "sparse",
            "x": -4.038634777069092,
            "y": 20.41367530822754
        },
        {
            "id": "s_232",
            "name": "Alberto Jaspe-Villanueva",
            "type": "sparse",
            "x": -8.854843139648438,
            "y": 16.18100929260254
        },
        {
            "id": "s_233",
            "name": "Yun-Hsin Kuo",
            "type": "sparse",
            "x": -0.7634889483451843,
            "y": 18.027233123779297
        },
        {
            "id": "s_234",
            "name": "Minh Hoai",
            "type": "sparse",
            "x": -0.21044743061065674,
            "y": 17.61857795715332
        },
        {
            "id": "s_235",
            "name": "Keon-Joo Lee",
            "type": "sparse",
            "x": -1.3130921125411987,
            "y": 18.12100601196289
        },
        {
            "id": "s_236",
            "name": "Anjana Arunkumar",
            "type": "sparse",
            "x": -3.3339037895202637,
            "y": 17.622817993164062
        },
        {
            "id": "s_237",
            "name": "Siwei Fu",
            "type": "sparse",
            "x": -10.251482009887695,
            "y": 17.677297592163086
        },
        {
            "id": "s_238",
            "name": "Katerina Vrotsou",
            "type": "sparse",
            "x": -7.427745342254639,
            "y": 17.708621978759766
        },
        {
            "id": "s_239",
            "name": "Seungmin Jin",
            "type": "sparse",
            "x": -1.7022782564163208,
            "y": 16.422222137451172
        },
        {
            "id": "s_240",
            "name": "Bea Steers",
            "type": "sparse",
            "x": -4.052157878875732,
            "y": 20.040678024291992
        },
        {
            "id": "s_241",
            "name": "Hongtao Hao 0002",
            "type": "sparse",
            "x": -3.3995790481567383,
            "y": 16.404438018798828
        },
        {
            "id": "s_242",
            "name": "Md. Amiruzzaman",
            "type": "sparse",
            "x": -0.2511288821697235,
            "y": 17.62074089050293
        },
        {
            "id": "s_243",
            "name": "Ian Tenney",
            "type": "sparse",
            "x": 9.91894817352295,
            "y": 10.622998237609863
        },
        {
            "id": "s_244",
            "name": "Nilaksh Das",
            "type": "sparse",
            "x": 10.030645370483398,
            "y": 10.508485794067383
        },
        {
            "id": "s_245",
            "name": "Pierre Dragicevic",
            "type": "sparse",
            "x": -6.382512092590332,
            "y": 18.280961990356445
        },
        {
            "id": "s_246",
            "name": "Qingyang Xu",
            "type": "sparse",
            "x": -10.024184226989746,
            "y": 17.30292320251465
        },
        {
            "id": "s_247",
            "name": "Jochen G\u00f6rtler",
            "type": "sparse",
            "x": -8.128594398498535,
            "y": 18.569421768188477
        },
        {
            "id": "s_248",
            "name": "Liqi Cheng",
            "type": "sparse",
            "x": -10.283124923706055,
            "y": 17.35602569580078
        },
        {
            "id": "s_249",
            "name": "Zihan Yan",
            "type": "sparse",
            "x": -9.784022331237793,
            "y": 18.242460250854492
        },
        {
            "id": "s_250",
            "name": "Jiashu Chen",
            "type": "sparse",
            "x": -9.599089622497559,
            "y": 17.249099731445312
        },
        {
            "id": "s_251",
            "name": "Benjamin Hoover",
            "type": "sparse",
            "x": -8.791499137878418,
            "y": 16.480222702026367
        },
        {
            "id": "s_252",
            "name": "Nicolas Souli\u00e9",
            "type": "sparse",
            "x": -6.31829833984375,
            "y": 18.39763069152832
        },
        {
            "id": "s_253",
            "name": "Delyar Tabatabai",
            "type": "sparse",
            "x": 9.96119213104248,
            "y": 10.580102920532227
        },
        {
            "id": "s_254",
            "name": "Michael Hofmann 0010",
            "type": "sparse",
            "x": -0.3914021849632263,
            "y": 17.573331832885742
        },
        {
            "id": "s_255",
            "name": "Eduardo Puerta",
            "type": "sparse",
            "x": -6.765383720397949,
            "y": 21.127744674682617
        },
        {
            "id": "s_256",
            "name": "Tom Baumgartl",
            "type": "sparse",
            "x": -5.842791557312012,
            "y": 15.335026741027832
        },
        {
            "id": "s_257",
            "name": "Patrick Reipschl\u00e4ger",
            "type": "sparse",
            "x": -5.7762274742126465,
            "y": 18.488183975219727
        },
        {
            "id": "s_258",
            "name": "Rui Ding 0001",
            "type": "sparse",
            "x": -10.520206451416016,
            "y": 17.649459838867188
        },
        {
            "id": "s_259",
            "name": "Duc M. Nguyen",
            "type": "sparse",
            "x": -1.9318650960922241,
            "y": 16.083337783813477
        },
        {
            "id": "s_260",
            "name": "Huyen N. Nguyen",
            "type": "sparse",
            "x": -8.414558410644531,
            "y": 16.97336769104004
        },
        {
            "id": "s_261",
            "name": "Hyung-Kwon Ko",
            "type": "sparse",
            "x": -1.4112964868545532,
            "y": 18.077756881713867
        },
        {
            "id": "s_262",
            "name": "Markus Wallinger",
            "type": "sparse",
            "x": -5.556766033172607,
            "y": 15.069051742553711
        },
        {
            "id": "s_263",
            "name": "Majid Behravan",
            "type": "sparse",
            "x": -6.5099287033081055,
            "y": 13.802836418151855
        },
        {
            "id": "s_264",
            "name": "Shifu Chen",
            "type": "sparse",
            "x": -10.249641418457031,
            "y": 17.538061141967773
        },
        {
            "id": "s_265",
            "name": "Anjul Kumar Tyagi",
            "type": "sparse",
            "x": -9.264887809753418,
            "y": 18.79281997680664
        },
        {
            "id": "s_266",
            "name": "Gabriela Morgenshtern",
            "type": "sparse",
            "x": -8.619115829467773,
            "y": 18.653501510620117
        },
        {
            "id": "s_267",
            "name": "Tatiana von Landesberger",
            "type": "sparse",
            "x": -5.030974864959717,
            "y": 15.895689010620117
        },
        {
            "id": "s_268",
            "name": "Xianyuan Zhan",
            "type": "sparse",
            "x": -10.363595962524414,
            "y": 17.13089942932129
        },
        {
            "id": "s_269",
            "name": "Arnaud Prouzeau",
            "type": "sparse",
            "x": -7.359927177429199,
            "y": 17.938852310180664
        },
        {
            "id": "s_270",
            "name": "Ziyang Guo",
            "type": "sparse",
            "x": -3.553253412246704,
            "y": 16.962146759033203
        },
        {
            "id": "s_271",
            "name": "Sietse J. Luk",
            "type": "sparse",
            "x": -6.797964572906494,
            "y": 14.297378540039062
        },
        {
            "id": "s_272",
            "name": "Daniel Seebacher",
            "type": "sparse",
            "x": -7.01154088973999,
            "y": 20.886455535888672
        },
        {
            "id": "s_273",
            "name": "Yashwanthi Anand",
            "type": "sparse",
            "x": 9.94604778289795,
            "y": 10.595254898071289
        },
        {
            "id": "s_274",
            "name": "Bongshin Lee",
            "type": "sparse",
            "x": -7.58982515335083,
            "y": 18.21053123474121
        },
        {
            "id": "s_275",
            "name": "J. Bern Jordan",
            "type": "sparse",
            "x": -2.0159823894500732,
            "y": 16.03453254699707
        },
        {
            "id": "s_276",
            "name": "Christopher Hahn",
            "type": "sparse",
            "x": -5.737843990325928,
            "y": 18.455602645874023
        },
        {
            "id": "s_277",
            "name": "Klaus Mueller 0001",
            "type": "sparse",
            "x": -9.28906536102295,
            "y": 18.771554946899414
        },
        {
            "id": "s_278",
            "name": "Mengtian Guo",
            "type": "sparse",
            "x": -9.028511047363281,
            "y": 19.860349655151367
        },
        {
            "id": "s_279",
            "name": "Florian Wetzels",
            "type": "sparse",
            "x": -4.028523921966553,
            "y": 20.68362045288086
        },
        {
            "id": "s_280",
            "name": "Shiyu Cheng",
            "type": "sparse",
            "x": 7.7060956954956055,
            "y": 5.0730881690979
        },
        {
            "id": "s_281",
            "name": "Steffen Frey",
            "type": "sparse",
            "x": -9.810927391052246,
            "y": 16.83479118347168
        },
        {
            "id": "s_282",
            "name": "Zelin Jia",
            "type": "sparse",
            "x": -9.315935134887695,
            "y": 17.25941276550293
        },
        {
            "id": "s_283",
            "name": "Xiaolong Zhang 0001",
            "type": "sparse",
            "x": -9.300819396972656,
            "y": 18.2258243560791
        },
        {
            "id": "s_284",
            "name": "Lei Zhang",
            "type": "sparse",
            "x": -9.247359275817871,
            "y": 18.219144821166992
        },
        {
            "id": "s_285",
            "name": "Mingming Fan 0001",
            "type": "sparse",
            "x": -0.6135811805725098,
            "y": 18.153751373291016
        },
        {
            "id": "s_286",
            "name": "Sara Di Bartolomeo",
            "type": "sparse",
            "x": -6.795009613037109,
            "y": 21.104623794555664
        },
        {
            "id": "s_287",
            "name": "Dongshi Xu",
            "type": "sparse",
            "x": -9.418328285217285,
            "y": 17.84171485900879
        },
        {
            "id": "s_288",
            "name": "Ryan Cotterell",
            "type": "sparse",
            "x": -5.104703903198242,
            "y": 15.123642921447754
        },
        {
            "id": "s_289",
            "name": "Florian Mannuss",
            "type": "sparse",
            "x": -8.853201866149902,
            "y": 16.178115844726562
        },
        {
            "id": "s_290",
            "name": "Filip Op\u00e1len\u00fd",
            "type": "sparse",
            "x": -5.801294326782227,
            "y": 19.451934814453125
        },
        {
            "id": "s_291",
            "name": "Anna Eschenbacher",
            "type": "sparse",
            "x": -6.601714134216309,
            "y": 17.076725006103516
        },
        {
            "id": "s_292",
            "name": "Anders Ynnerman",
            "type": "sparse",
            "x": -2.1588430404663086,
            "y": 18.175296783447266
        },
        {
            "id": "s_293",
            "name": "Qing Chen 0001",
            "type": "sparse",
            "x": -9.46828842163086,
            "y": 19.72757339477539
        },
        {
            "id": "s_294",
            "name": "Joscha Eirich",
            "type": "sparse",
            "x": -8.575499534606934,
            "y": 18.663877487182617
        },
        {
            "id": "s_295",
            "name": "Ugur G\u00fcd\u00fckbay",
            "type": "sparse",
            "x": -15.087055206298828,
            "y": 7.087277412414551
        },
        {
            "id": "s_296",
            "name": "Brock A. Wester",
            "type": "sparse",
            "x": -8.865531921386719,
            "y": 16.56217384338379
        },
        {
            "id": "s_297",
            "name": "Alan Lundgard",
            "type": "sparse",
            "x": -4.335507869720459,
            "y": 16.033252716064453
        },
        {
            "id": "s_298",
            "name": "Min Chen 0001",
            "type": "sparse",
            "x": -7.41715669631958,
            "y": 17.013395309448242
        },
        {
            "id": "s_299",
            "name": "Anyi Li",
            "type": "sparse",
            "x": -7.972023963928223,
            "y": 18.447446823120117
        },
        {
            "id": "s_300",
            "name": "Guodao Sun",
            "type": "sparse",
            "x": -10.432847023010254,
            "y": 17.30392074584961
        },
        {
            "id": "s_301",
            "name": "Stefan P. Feyer",
            "type": "sparse",
            "x": -3.921639919281006,
            "y": 19.76740264892578
        },
        {
            "id": "s_302",
            "name": "Jinbin Huang",
            "type": "sparse",
            "x": -3.2804245948791504,
            "y": 17.600648880004883
        },
        {
            "id": "s_303",
            "name": "Parikshit Solunke",
            "type": "sparse",
            "x": -4.079611301422119,
            "y": 20.145158767700195
        },
        {
            "id": "s_304",
            "name": "Erin McGowan",
            "type": "sparse",
            "x": -4.065404891967773,
            "y": 20.066570281982422
        },
        {
            "id": "s_305",
            "name": "Vanessa Pe\u00f1a Araya",
            "type": "sparse",
            "x": -6.21030330657959,
            "y": 18.302980422973633
        },
        {
            "id": "s_306",
            "name": "Ligan Cai",
            "type": "sparse",
            "x": -9.367627143859863,
            "y": 19.68194580078125
        },
        {
            "id": "s_307",
            "name": "John Hooker",
            "type": "sparse",
            "x": -4.370934963226318,
            "y": 17.024423599243164
        },
        {
            "id": "s_308",
            "name": "Jiazhi Xia",
            "type": "sparse",
            "x": -9.413414001464844,
            "y": 17.976898193359375
        },
        {
            "id": "s_309",
            "name": "Bo Dong 0001",
            "type": "sparse",
            "x": -10.01152515411377,
            "y": 18.59608268737793
        },
        {
            "id": "s_310",
            "name": "Jian Kang 0008",
            "type": "sparse",
            "x": -4.25459098815918,
            "y": 21.06980323791504
        },
        {
            "id": "s_311",
            "name": "Shuhan Liu",
            "type": "sparse",
            "x": -10.159806251525879,
            "y": 17.20073127746582
        },
        {
            "id": "s_312",
            "name": "Elaine Huynh",
            "type": "sparse",
            "x": -6.616813659667969,
            "y": 16.80641746520996
        },
        {
            "id": "s_313",
            "name": "Rupayan Neogy",
            "type": "sparse",
            "x": -4.273473262786865,
            "y": 16.08411407470703
        },
        {
            "id": "s_314",
            "name": "Bin Chen",
            "type": "sparse",
            "x": -8.077981948852539,
            "y": 18.64623260498047
        },
        {
            "id": "s_315",
            "name": "Ronell Sicat",
            "type": "sparse",
            "x": -8.852324485778809,
            "y": 16.174314498901367
        },
        {
            "id": "s_316",
            "name": "Camelia D. Brumar",
            "type": "sparse",
            "x": -4.830471992492676,
            "y": 15.941876411437988
        },
        {
            "id": "s_317",
            "name": "Peilin Yu",
            "type": "sparse",
            "x": -7.432558536529541,
            "y": 17.72542953491211
        },
        {
            "id": "s_318",
            "name": "Alexis Pister",
            "type": "sparse",
            "x": -6.644541263580322,
            "y": 16.9797420501709
        },
        {
            "id": "s_319",
            "name": "Ruhan Yang",
            "type": "sparse",
            "x": -2.1631357669830322,
            "y": 18.045433044433594
        },
        {
            "id": "s_320",
            "name": "Ajit Johnson Nirmal",
            "type": "sparse",
            "x": -8.71357250213623,
            "y": 16.43828010559082
        },
        {
            "id": "s_321",
            "name": "Yijun Liu",
            "type": "sparse",
            "x": -4.757218360900879,
            "y": 15.492422103881836
        },
        {
            "id": "s_322",
            "name": "Tianyu Xiong",
            "type": "sparse",
            "x": 7.504916667938232,
            "y": 5.269717216491699
        },
        {
            "id": "s_323",
            "name": "Mingzhe Li",
            "type": "sparse",
            "x": 7.338562488555908,
            "y": 5.4366912841796875
        },
        {
            "id": "s_324",
            "name": "Michael J. McGuffin",
            "type": "sparse",
            "x": -3.739264488220215,
            "y": 17.138181686401367
        },
        {
            "id": "s_325",
            "name": "Guohua Geng",
            "type": "sparse",
            "x": -10.230243682861328,
            "y": 17.34980583190918
        },
        {
            "id": "s_326",
            "name": "Lora Oehlberg",
            "type": "sparse",
            "x": -6.393904685974121,
            "y": 18.181835174560547
        },
        {
            "id": "s_327",
            "name": "Joseph M. Hellerstein",
            "type": "sparse",
            "x": -4.619142055511475,
            "y": 16.005130767822266
        },
        {
            "id": "s_328",
            "name": "Joachim Giesen",
            "type": "sparse",
            "x": -3.5133204460144043,
            "y": 19.787229537963867
        },
        {
            "id": "s_329",
            "name": "Md Nafiul Alam Nipu",
            "type": "sparse",
            "x": -7.488616466522217,
            "y": -1.188517451286316
        },
        {
            "id": "s_330",
            "name": "Maria-Viktoria Heinle",
            "type": "sparse",
            "x": -6.904484272003174,
            "y": 20.999706268310547
        },
        {
            "id": "s_331",
            "name": "Chufeng Wang",
            "type": "sparse",
            "x": -8.041877746582031,
            "y": 18.679161071777344
        },
        {
            "id": "s_332",
            "name": "Yuheng Zhao",
            "type": "sparse",
            "x": -10.333707809448242,
            "y": 19.625324249267578
        },
        {
            "id": "s_333",
            "name": "Katar\u00edna Furmanov\u00e1",
            "type": "sparse",
            "x": -5.7969889640808105,
            "y": 19.458219528198242
        },
        {
            "id": "s_334",
            "name": "Vahan Huroyan",
            "type": "sparse",
            "x": -3.9713375568389893,
            "y": 19.69147300720215
        },
        {
            "id": "s_335",
            "name": "Wenwen Dou",
            "type": "sparse",
            "x": -4.730331897735596,
            "y": 15.397969245910645
        },
        {
            "id": "s_336",
            "name": "Haolin Lu",
            "type": "sparse",
            "x": -10.341597557067871,
            "y": 17.35842514038086
        },
        {
            "id": "s_337",
            "name": "Natkamon Tovanich",
            "type": "sparse",
            "x": -6.392942428588867,
            "y": 18.358549118041992
        },
        {
            "id": "s_338",
            "name": "Klaus Eckelt",
            "type": "sparse",
            "x": -2.2243456840515137,
            "y": 15.428786277770996
        },
        {
            "id": "s_339",
            "name": "Patrick C. Shriwise",
            "type": "sparse",
            "x": -15.02548599243164,
            "y": 7.148834705352783
        },
        {
            "id": "s_340",
            "name": "Rebecca Nowak",
            "type": "sparse",
            "x": -6.689805507659912,
            "y": 14.229635238647461
        },
        {
            "id": "s_341",
            "name": "Duen Horng Chau",
            "type": "sparse",
            "x": 10.201192855834961,
            "y": 10.340925216674805
        },
        {
            "id": "s_342",
            "name": "Frederick Federer",
            "type": "sparse",
            "x": -14.992337226867676,
            "y": 7.181972980499268
        },
        {
            "id": "s_343",
            "name": "Valerio Pascucci",
            "type": "sparse",
            "x": -15.04033088684082,
            "y": 7.134078025817871
        },
        {
            "id": "s_344",
            "name": "Daniel Kl\u00f6tzl",
            "type": "sparse",
            "x": -8.535869598388672,
            "y": 19.31517219543457
        },
        {
            "id": "s_345",
            "name": "Xiangyang Xue 0001",
            "type": "sparse",
            "x": -10.084362983703613,
            "y": 19.556108474731445
        },
        {
            "id": "s_346",
            "name": "Thomas Ertl",
            "type": "sparse",
            "x": -9.873344421386719,
            "y": 17.080623626708984
        },
        {
            "id": "s_347",
            "name": "Bettina Speckmann",
            "type": "sparse",
            "x": -5.9598388671875,
            "y": 15.667512893676758
        },
        {
            "id": "s_348",
            "name": "Shixia Liu",
            "type": "sparse",
            "x": -9.341776847839355,
            "y": 17.347368240356445
        },
        {
            "id": "s_349",
            "name": "Sihang Li",
            "type": "sparse",
            "x": -10.406641006469727,
            "y": 19.655384063720703
        },
        {
            "id": "s_350",
            "name": "Austin P. Wright",
            "type": "sparse",
            "x": 10.036605834960938,
            "y": 10.500753402709961
        },
        {
            "id": "s_351",
            "name": "Jiazhe Wang",
            "type": "sparse",
            "x": -9.364328384399414,
            "y": 19.66368293762207
        },
        {
            "id": "s_352",
            "name": "Anita Ruangrotsakun",
            "type": "sparse",
            "x": 9.959892272949219,
            "y": 10.581489562988281
        },
        {
            "id": "s_353",
            "name": "Songheng Zhang",
            "type": "sparse",
            "x": -10.127256393432617,
            "y": 18.552356719970703
        },
        {
            "id": "s_354",
            "name": "Zhengxiang Wang",
            "type": "sparse",
            "x": -3.4171860218048096,
            "y": 16.415807723999023
        },
        {
            "id": "s_355",
            "name": "Ludovic Autin",
            "type": "sparse",
            "x": -6.24633264541626,
            "y": 19.018524169921875
        },
        {
            "id": "s_356",
            "name": "Krist\u00edna Z\u00e1kopcanov\u00e1",
            "type": "sparse",
            "x": -5.769411087036133,
            "y": 19.485321044921875
        },
        {
            "id": "s_357",
            "name": "Zhi Wang",
            "type": "sparse",
            "x": -8.213494300842285,
            "y": 18.584718704223633
        },
        {
            "id": "s_358",
            "name": "Sayef Azad Sakin",
            "type": "sparse",
            "x": -0.21606722474098206,
            "y": 17.089431762695312
        },
        {
            "id": "s_359",
            "name": "Clio Andris",
            "type": "sparse",
            "x": -6.672466278076172,
            "y": 21.237279891967773
        },
        {
            "id": "s_360",
            "name": "Siming Chen 0001",
            "type": "sparse",
            "x": -10.010536193847656,
            "y": 19.372161865234375
        },
        {
            "id": "s_361",
            "name": "Haojin Jiang",
            "type": "sparse",
            "x": -9.252076148986816,
            "y": 17.51276206970215
        },
        {
            "id": "s_362",
            "name": "Brian D. Ondov",
            "type": "sparse",
            "x": -2.753061294555664,
            "y": 16.384920120239258
        },
        {
            "id": "s_363",
            "name": "Xin Chen",
            "type": "sparse",
            "x": -8.14640998840332,
            "y": 18.66517448425293
        },
        {
            "id": "s_364",
            "name": "Jaegul Choo",
            "type": "sparse",
            "x": -1.7059558629989624,
            "y": 16.436241149902344
        },
        {
            "id": "s_365",
            "name": "Ying Chen",
            "type": "sparse",
            "x": -9.363203048706055,
            "y": 19.74066925048828
        },
        {
            "id": "s_366",
            "name": "Remco Chang",
            "type": "sparse",
            "x": -4.676591873168945,
            "y": 15.971879959106445
        },
        {
            "id": "s_367",
            "name": "Mengyu Zhou",
            "type": "sparse",
            "x": -10.470662117004395,
            "y": 18.270545959472656
        },
        {
            "id": "s_368",
            "name": "Mai Elshehaly",
            "type": "sparse",
            "x": -6.434098243713379,
            "y": 16.483139038085938
        },
        {
            "id": "s_369",
            "name": "Zixin Chen",
            "type": "sparse",
            "x": -10.02503490447998,
            "y": 18.133514404296875
        },
        {
            "id": "s_370",
            "name": "Yu Qin",
            "type": "sparse",
            "x": -15.092472076416016,
            "y": 7.081907749176025
        },
        {
            "id": "s_371",
            "name": "Ngan V. T. Nguyen",
            "type": "sparse",
            "x": -6.254764556884766,
            "y": 19.053089141845703
        },
        {
            "id": "s_372",
            "name": "Martin Wikelski",
            "type": "sparse",
            "x": -7.056940078735352,
            "y": 20.848669052124023
        },
        {
            "id": "s_373",
            "name": "Haoxuan Li",
            "type": "sparse",
            "x": -9.370759963989258,
            "y": 18.079845428466797
        },
        {
            "id": "s_374",
            "name": "Hong Zhou",
            "type": "sparse",
            "x": -9.721454620361328,
            "y": 18.555545806884766
        },
        {
            "id": "s_375",
            "name": "Sonja Thoma",
            "type": "sparse",
            "x": -6.198131561279297,
            "y": 17.64057731628418
        },
        {
            "id": "s_376",
            "name": "Liwenhan Xie",
            "type": "sparse",
            "x": -10.313232421875,
            "y": 19.624975204467773
        },
        {
            "id": "s_377",
            "name": "Caglar Yildirim",
            "type": "sparse",
            "x": -6.712283611297607,
            "y": 21.18216323852539
        },
        {
            "id": "s_378",
            "name": "Haipeng Zeng",
            "type": "sparse",
            "x": -11.031438827514648,
            "y": 19.317689895629883
        },
        {
            "id": "s_379",
            "name": "Markus Petzold",
            "type": "sparse",
            "x": -5.851130962371826,
            "y": 15.337526321411133
        },
        {
            "id": "s_380",
            "name": "Falk Schreiber",
            "type": "sparse",
            "x": -3.944478750228882,
            "y": 19.770381927490234
        },
        {
            "id": "s_381",
            "name": "Catalina Vajiac",
            "type": "sparse",
            "x": 10.380019187927246,
            "y": 10.170090675354004
        },
        {
            "id": "s_382",
            "name": "Tyler Jacks",
            "type": "sparse",
            "x": -8.744694709777832,
            "y": 16.46259307861328
        },
        {
            "id": "s_383",
            "name": "Anna Scimone",
            "type": "sparse",
            "x": -2.191467523574829,
            "y": 15.494765281677246
        },
        {
            "id": "s_384",
            "name": "Alireza Karduni",
            "type": "sparse",
            "x": -4.749217510223389,
            "y": 15.439969062805176
        },
        {
            "id": "s_385",
            "name": "Guozheng Li 0002",
            "type": "sparse",
            "x": -7.819642066955566,
            "y": 18.950767517089844
        },
        {
            "id": "s_386",
            "name": "Charles Perin",
            "type": "sparse",
            "x": -6.325226783752441,
            "y": 17.374074935913086
        },
        {
            "id": "s_387",
            "name": "Torin McDonald",
            "type": "sparse",
            "x": -15.02756118774414,
            "y": 7.146757125854492
        },
        {
            "id": "s_388",
            "name": "Markus Hadwiger",
            "type": "sparse",
            "x": -8.79895305633545,
            "y": 16.293989181518555
        },
        {
            "id": "s_389",
            "name": "Jakob Jakob",
            "type": "sparse",
            "x": -3.497204065322876,
            "y": 19.78793716430664
        },
        {
            "id": "s_390",
            "name": "Justin Raynor",
            "type": "sparse",
            "x": -6.729195594787598,
            "y": 21.167015075683594
        },
        {
            "id": "s_391",
            "name": "Beno\u00eet Fr\u00e9nay",
            "type": "sparse",
            "x": -8.394855499267578,
            "y": 18.79997444152832
        },
        {
            "id": "s_392",
            "name": "Wesley Willett",
            "type": "sparse",
            "x": -6.30684757232666,
            "y": 18.314929962158203
        },
        {
            "id": "s_393",
            "name": "Nils Gehlenborg",
            "type": "sparse",
            "x": -8.429952621459961,
            "y": 17.080787658691406
        },
        {
            "id": "s_394",
            "name": "Chris Bryan",
            "type": "sparse",
            "x": -3.3610875606536865,
            "y": 17.59807014465332
        },
        {
            "id": "s_395",
            "name": "Chris R. Johnson 0001",
            "type": "sparse",
            "x": -1.9059981107711792,
            "y": 17.973011016845703
        },
        {
            "id": "s_396",
            "name": "Geoff Kuenning",
            "type": "sparse",
            "x": -9.237399101257324,
            "y": 18.76217269897461
        },
        {
            "id": "s_397",
            "name": "Victoria McArthur",
            "type": "sparse",
            "x": -6.270222187042236,
            "y": 18.350364685058594
        },
        {
            "id": "s_398",
            "name": "Seulgi Choi",
            "type": "sparse",
            "x": -4.099717140197754,
            "y": 16.24912452697754
        },
        {
            "id": "s_399",
            "name": "Yu-An Chen",
            "type": "sparse",
            "x": -8.70615291595459,
            "y": 16.505306243896484
        },
        {
            "id": "s_400",
            "name": "Anqi Cao",
            "type": "sparse",
            "x": -10.274827003479004,
            "y": 17.24553680419922
        },
        {
            "id": "s_401",
            "name": "Dennis Dimov",
            "type": "sparse",
            "x": -5.737852573394775,
            "y": 18.459369659423828
        },
        {
            "id": "s_402",
            "name": "Luis Gustavo Nonato",
            "type": "sparse",
            "x": -4.019824028015137,
            "y": 20.377649307250977
        },
        {
            "id": "s_403",
            "name": "Amy Rae Fox",
            "type": "sparse",
            "x": -4.312465190887451,
            "y": 16.065052032470703
        },
        {
            "id": "s_404",
            "name": "Haidong Zhang",
            "type": "sparse",
            "x": -10.570615768432617,
            "y": 17.891490936279297
        },
        {
            "id": "s_405",
            "name": "Han-Wei Shen",
            "type": "sparse",
            "x": 7.531291961669922,
            "y": 5.243732929229736
        },
        {
            "id": "s_406",
            "name": "Ir\u00e1n R. Rom\u00e1n",
            "type": "sparse",
            "x": -4.05997371673584,
            "y": 20.068073272705078
        },
        {
            "id": "s_407",
            "name": "Yucong Yao",
            "type": "sparse",
            "x": -8.054242134094238,
            "y": 18.684934616088867
        },
        {
            "id": "s_408",
            "name": "Yifan Wu 0005",
            "type": "sparse",
            "x": -3.363607883453369,
            "y": 16.825958251953125
        },
        {
            "id": "s_409",
            "name": "Shoubin Cheng",
            "type": "sparse",
            "x": -10.019868850708008,
            "y": 17.564565658569336
        },
        {
            "id": "s_410",
            "name": "Liangjun Liu",
            "type": "sparse",
            "x": -9.440213203430176,
            "y": 18.012943267822266
        },
        {
            "id": "s_411",
            "name": "Aaron Sun Chen",
            "type": "sparse",
            "x": -4.362985610961914,
            "y": 17.013689041137695
        },
        {
            "id": "s_412",
            "name": "Xijie Huang",
            "type": "sparse",
            "x": -10.217053413391113,
            "y": 18.488496780395508
        },
        {
            "id": "s_413",
            "name": "Mandi Cai",
            "type": "sparse",
            "x": -3.196571111679077,
            "y": 16.626264572143555
        },
        {
            "id": "s_414",
            "name": "Shuhao Zhang",
            "type": "sparse",
            "x": -11.102662086486816,
            "y": 19.38408660888672
        },
        {
            "id": "s_415",
            "name": "Yuxiao Li",
            "type": "sparse",
            "x": 7.426253795623779,
            "y": 5.348513603210449
        },
        {
            "id": "s_416",
            "name": "Tica Lin",
            "type": "sparse",
            "x": -8.748653411865234,
            "y": 16.54690933227539
        },
        {
            "id": "s_417",
            "name": "Lucas Dixon",
            "type": "sparse",
            "x": 9.923404693603516,
            "y": 10.618555068969727
        },
        {
            "id": "s_418",
            "name": "Helwig Hauser",
            "type": "sparse",
            "x": -6.508821487426758,
            "y": 13.783245086669922
        },
        {
            "id": "s_419",
            "name": "Jiaqi Wang",
            "type": "sparse",
            "x": -10.883256912231445,
            "y": 18.25935173034668
        },
        {
            "id": "s_420",
            "name": "Fraser Anderson",
            "type": "sparse",
            "x": -7.447722434997559,
            "y": 17.851009368896484
        },
        {
            "id": "s_421",
            "name": "Jun Yuan 0003",
            "type": "sparse",
            "x": -9.428032875061035,
            "y": 17.300376892089844
        },
        {
            "id": "s_422",
            "name": "Alexander Straub",
            "type": "sparse",
            "x": -9.80815601348877,
            "y": 16.846097946166992
        },
        {
            "id": "s_423",
            "name": "Kun Zhou 0001",
            "type": "sparse",
            "x": -10.353198051452637,
            "y": 17.18440818786621
        },
        {
            "id": "s_424",
            "name": "Christian Tominski",
            "type": "sparse",
            "x": -5.272059917449951,
            "y": 15.124480247497559
        },
        {
            "id": "s_425",
            "name": "Shouxing Xiang",
            "type": "sparse",
            "x": -9.450898170471191,
            "y": 17.364532470703125
        },
        {
            "id": "s_426",
            "name": "Theophanis Tsandilas",
            "type": "sparse",
            "x": -6.527279853820801,
            "y": 16.644569396972656
        },
        {
            "id": "s_427",
            "name": "Florence Y. Wang",
            "type": "sparse",
            "x": -7.456995487213135,
            "y": 17.852807998657227
        },
        {
            "id": "s_428",
            "name": "Cl\u00e1udio T. Silva",
            "type": "sparse",
            "x": -4.030695915222168,
            "y": 20.24321937561035
        },
        {
            "id": "s_429",
            "name": "Fabian Beck 0001",
            "type": "sparse",
            "x": -10.747048377990723,
            "y": 17.298355102539062
        },
        {
            "id": "s_430",
            "name": "Kaustubh Odak",
            "type": "sparse",
            "x": -4.821314334869385,
            "y": 15.476733207702637
        },
        {
            "id": "s_431",
            "name": "Guoli Yan",
            "type": "sparse",
            "x": -10.995985984802246,
            "y": 16.87628173828125
        },
        {
            "id": "s_432",
            "name": "Hugo Romat",
            "type": "sparse",
            "x": -6.6453166007995605,
            "y": 16.874534606933594
        },
        {
            "id": "s_433",
            "name": "Matthew P. LeGendre",
            "type": "sparse",
            "x": -0.21000507473945618,
            "y": 17.10580062866211
        },
        {
            "id": "s_434",
            "name": "Dirk Streeb",
            "type": "sparse",
            "x": -7.025012016296387,
            "y": 20.880929946899414
        },
        {
            "id": "s_435",
            "name": "Yongfeng Qiu",
            "type": "sparse",
            "x": 7.416275501251221,
            "y": 5.358484268188477
        },
        {
            "id": "s_436",
            "name": "Johannes Fuchs 0001",
            "type": "sparse",
            "x": -6.872532844543457,
            "y": 21.03143310546875
        },
        {
            "id": "s_437",
            "name": "Huawei Zheng",
            "type": "sparse",
            "x": -10.394726753234863,
            "y": 17.209585189819336
        },
        {
            "id": "s_438",
            "name": "Po-Yin Yen",
            "type": "sparse",
            "x": 7.537975311279297,
            "y": 5.237198829650879
        },
        {
            "id": "s_439",
            "name": "Kangping Hu",
            "type": "sparse",
            "x": -10.439682960510254,
            "y": 17.294755935668945
        },
        {
            "id": "s_440",
            "name": "Benjamin Russig",
            "type": "sparse",
            "x": -5.7860331535339355,
            "y": 18.493886947631836
        },
        {
            "id": "s_441",
            "name": "Hanghang Tong",
            "type": "sparse",
            "x": -4.395670413970947,
            "y": 20.93328094482422
        },
        {
            "id": "s_442",
            "name": "Genghuai Bai",
            "type": "sparse",
            "x": -9.234320640563965,
            "y": 17.688030242919922
        },
        {
            "id": "s_443",
            "name": "Mark van de Ruit",
            "type": "sparse",
            "x": -6.874194145202637,
            "y": 14.355083465576172
        },
        {
            "id": "s_444",
            "name": "Brian Summa",
            "type": "sparse",
            "x": -15.073505401611328,
            "y": 7.100839614868164
        },
        {
            "id": "s_445",
            "name": "Weiwen Jiang",
            "type": "sparse",
            "x": -9.820152282714844,
            "y": 18.6337890625
        },
        {
            "id": "s_446",
            "name": "Steffen Koch 0001",
            "type": "sparse",
            "x": -9.795141220092773,
            "y": 17.18658447265625
        },
        {
            "id": "s_447",
            "name": "Susanta Routray",
            "type": "sparse",
            "x": 10.297547340393066,
            "y": 10.250457763671875
        },
        {
            "id": "s_448",
            "name": "Justin Chen",
            "type": "sparse",
            "x": 10.282549858093262,
            "y": 10.265372276306152
        },
        {
            "id": "s_449",
            "name": "Muqiao Yang",
            "type": "sparse",
            "x": -10.192550659179688,
            "y": 18.369176864624023
        },
        {
            "id": "s_450",
            "name": "Udo Schlegel",
            "type": "sparse",
            "x": -7.144602298736572,
            "y": 20.73720359802246
        },
        {
            "id": "s_451",
            "name": "Jie Li 0006",
            "type": "sparse",
            "x": -10.246079444885254,
            "y": 19.643043518066406
        },
        {
            "id": "s_452",
            "name": "Sanghyun Hong 0001",
            "type": "sparse",
            "x": -1.9947530031204224,
            "y": 16.02305793762207
        },
        {
            "id": "s_453",
            "name": "P. Samuel Quinan",
            "type": "sparse",
            "x": -3.3999099731445312,
            "y": 17.590755462646484
        },
        {
            "id": "s_454",
            "name": "Tim J. A. de Jong",
            "type": "sparse",
            "x": -7.273204326629639,
            "y": 16.903255462646484
        },
        {
            "id": "s_455",
            "name": "Danqing Shi",
            "type": "sparse",
            "x": -9.340869903564453,
            "y": 19.730051040649414
        },
        {
            "id": "s_456",
            "name": "Tan Tang",
            "type": "sparse",
            "x": -10.109155654907227,
            "y": 17.304304122924805
        },
        {
            "id": "s_457",
            "name": "Leixian Shen",
            "type": "sparse",
            "x": -10.85231876373291,
            "y": 18.253076553344727
        },
        {
            "id": "s_458",
            "name": "Vivian Pednekar",
            "type": "sparse",
            "x": 10.290043830871582,
            "y": 10.25796127319336
        },
        {
            "id": "s_459",
            "name": "Shunan Guo",
            "type": "sparse",
            "x": -9.287690162658691,
            "y": 19.489545822143555
        },
        {
            "id": "s_460",
            "name": "Steven van den Broek",
            "type": "sparse",
            "x": -5.954538822174072,
            "y": 15.691232681274414
        },
        {
            "id": "s_461",
            "name": "Juncong Lin",
            "type": "sparse",
            "x": -9.250171661376953,
            "y": 17.993003845214844
        },
        {
            "id": "s_462",
            "name": "Hannah K. Bako",
            "type": "sparse",
            "x": -4.272505283355713,
            "y": 16.97903823852539
        },
        {
            "id": "s_463",
            "name": "Hannah Murray",
            "type": "sparse",
            "x": 10.299612045288086,
            "y": 10.248483657836914
        },
        {
            "id": "s_464",
            "name": "Daniel Archambault",
            "type": "sparse",
            "x": -5.711602210998535,
            "y": 15.263940811157227
        },
        {
            "id": "s_465",
            "name": "Zhaowei Wang",
            "type": "sparse",
            "x": -9.011396408081055,
            "y": 16.943016052246094
        },
        {
            "id": "s_466",
            "name": "Yeting Xu",
            "type": "sparse",
            "x": -9.2178373336792,
            "y": 17.687292098999023
        },
        {
            "id": "s_467",
            "name": "Yongheng Wang",
            "type": "sparse",
            "x": -10.300616264343262,
            "y": 17.234201431274414
        },
        {
            "id": "s_468",
            "name": "Arran Zeyu Wang",
            "type": "sparse",
            "x": -2.130122661590576,
            "y": 18.076904296875
        },
        {
            "id": "s_469",
            "name": "Vojtech Reh\u00e1k",
            "type": "sparse",
            "x": -5.7780280113220215,
            "y": 19.47216033935547
        },
        {
            "id": "s_470",
            "name": "Qianwen Wang",
            "type": "sparse",
            "x": -8.548921585083008,
            "y": 17.323963165283203
        },
        {
            "id": "s_471",
            "name": "Simone Scheithauer",
            "type": "sparse",
            "x": -5.838513374328613,
            "y": 15.332216262817383
        },
        {
            "id": "s_472",
            "name": "Vanessa Eichel",
            "type": "sparse",
            "x": -5.8458991050720215,
            "y": 15.31736946105957
        },
        {
            "id": "s_473",
            "name": "Rong Zhang 0011",
            "type": "sparse",
            "x": -10.134061813354492,
            "y": 18.65739631652832
        },
        {
            "id": "s_474",
            "name": "Sarah Sch\u00f6ttler",
            "type": "sparse",
            "x": -6.752090930938721,
            "y": 17.023902893066406
        },
        {
            "id": "s_475",
            "name": "Nicolas Brich",
            "type": "sparse",
            "x": -3.940549850463867,
            "y": 19.787925720214844
        },
        {
            "id": "s_476",
            "name": "Bo Tang 0016",
            "type": "sparse",
            "x": -9.290085792541504,
            "y": 20.254703521728516
        },
        {
            "id": "s_477",
            "name": "Eytan Adar",
            "type": "sparse",
            "x": -3.2648398876190186,
            "y": 16.824020385742188
        },
        {
            "id": "s_478",
            "name": "Anna Vilanova",
            "type": "sparse",
            "x": -8.212425231933594,
            "y": 16.864288330078125
        },
        {
            "id": "s_479",
            "name": "Carolina Veiga Ferreira de Souza",
            "type": "sparse",
            "x": -7.493346214294434,
            "y": -1.1838502883911133
        },
        {
            "id": "s_480",
            "name": "Jambay Kinley",
            "type": "sparse",
            "x": -8.780045509338379,
            "y": 16.51058578491211
        },
        {
            "id": "s_481",
            "name": "Michalis Mamakos",
            "type": "sparse",
            "x": -3.339144229888916,
            "y": 16.830989837646484
        },
        {
            "id": "s_482",
            "name": "Qi'an Chen",
            "type": "sparse",
            "x": -9.256909370422363,
            "y": 17.47037696838379
        },
        {
            "id": "s_483",
            "name": "Matej Lang",
            "type": "sparse",
            "x": -5.749170303344727,
            "y": 19.50078773498535
        },
        {
            "id": "s_484",
            "name": "Pavol Ulbrich",
            "type": "sparse",
            "x": -5.740572452545166,
            "y": 19.505704879760742
        },
        {
            "id": "s_485",
            "name": "Zhen Wen",
            "type": "sparse",
            "x": -9.374597549438477,
            "y": 17.948522567749023
        },
        {
            "id": "s_486",
            "name": "Rico Richter",
            "type": "sparse",
            "x": -8.917670249938965,
            "y": 18.39106559753418
        },
        {
            "id": "s_487",
            "name": "Yongfeng Ji",
            "type": "sparse",
            "x": -6.188525676727295,
            "y": 17.40251922607422
        },
        {
            "id": "s_488",
            "name": "Luke S. Snyder",
            "type": "sparse",
            "x": -3.7782814502716064,
            "y": 17.07843589782715
        },
        {
            "id": "s_489",
            "name": "Shenyu Xu",
            "type": "sparse",
            "x": -4.796384811401367,
            "y": 15.74785041809082
        },
        {
            "id": "s_490",
            "name": "Fan Lei",
            "type": "sparse",
            "x": -4.272980213165283,
            "y": 21.10733985900879
        },
        {
            "id": "s_491",
            "name": "Zhengxin You",
            "type": "sparse",
            "x": -9.264175415039062,
            "y": 20.24895668029785
        },
        {
            "id": "s_492",
            "name": "Md Dilshadur Rahman",
            "type": "sparse",
            "x": -2.0494964122772217,
            "y": 18.02760887145996
        },
        {
            "id": "s_493",
            "name": "Yuhang Zhao 0001",
            "type": "sparse",
            "x": -3.354393720626831,
            "y": 16.41962432861328
        },
        {
            "id": "s_494",
            "name": "Evan Peck",
            "type": "sparse",
            "x": -4.32778263092041,
            "y": 16.047433853149414
        },
        {
            "id": "s_495",
            "name": "Hongye Liang",
            "type": "sparse",
            "x": -9.847557067871094,
            "y": 17.62371826171875
        },
        {
            "id": "s_496",
            "name": "Kyle Wm. Hall",
            "type": "sparse",
            "x": -2.243769884109497,
            "y": 18.00446891784668
        },
        {
            "id": "s_497",
            "name": "Yuhan Guo",
            "type": "sparse",
            "x": -10.47622013092041,
            "y": 19.681447982788086
        },
        {
            "id": "s_498",
            "name": "Wenshuo Zhao",
            "type": "sparse",
            "x": -10.359800338745117,
            "y": 17.372407913208008
        },
        {
            "id": "s_499",
            "name": "Matthias Mitterreiter",
            "type": "sparse",
            "x": -3.52970027923584,
            "y": 19.806236267089844
        },
        {
            "id": "s_500",
            "name": "Myoungkyu Lee",
            "type": "sparse",
            "x": -0.7574728727340698,
            "y": 17.967159271240234
        },
        {
            "id": "s_501",
            "name": "Simon Warchol",
            "type": "sparse",
            "x": -8.750965118408203,
            "y": 16.52971649169922
        },
        {
            "id": "s_502",
            "name": "Sudhanshu Sane",
            "type": "sparse",
            "x": -1.8041741847991943,
            "y": 17.87852668762207
        },
        {
            "id": "s_503",
            "name": "Hana Pokojn\u00e1",
            "type": "sparse",
            "x": -5.8884100914001465,
            "y": 19.294279098510742
        },
        {
            "id": "s_504",
            "name": "David Borland",
            "type": "sparse",
            "x": -9.151019096374512,
            "y": 19.833148956298828
        },
        {
            "id": "s_505",
            "name": "Neha Kumar 0001",
            "type": "sparse",
            "x": -6.677188873291016,
            "y": 21.23165512084961
        },
        {
            "id": "s_506",
            "name": "Peer-Timo Bremer",
            "type": "sparse",
            "x": -15.11274242401123,
            "y": 7.061611175537109
        },
        {
            "id": "s_507",
            "name": "Peter K. Sorger",
            "type": "sparse",
            "x": -8.574909210205078,
            "y": 16.641145706176758
        },
        {
            "id": "s_508",
            "name": "Atharva Kulkarni",
            "type": "sparse",
            "x": -3.401669502258301,
            "y": 16.40964126586914
        },
        {
            "id": "s_509",
            "name": "Fahai Zhong",
            "type": "sparse",
            "x": -8.170942306518555,
            "y": 18.561473846435547
        },
        {
            "id": "s_510",
            "name": "Ivan Viola",
            "type": "sparse",
            "x": -6.193349361419678,
            "y": 18.862407684326172
        },
        {
            "id": "s_511",
            "name": "Tobias Kauer",
            "type": "sparse",
            "x": -6.801074028015137,
            "y": 17.05684471130371
        },
        {
            "id": "s_512",
            "name": "Dongliang Wang",
            "type": "sparse",
            "x": -10.107431411743164,
            "y": 19.585355758666992
        },
        {
            "id": "s_513",
            "name": "Jie Zheng 0002",
            "type": "sparse",
            "x": -11.0949068069458,
            "y": 19.36553382873535
        },
        {
            "id": "s_514",
            "name": "Alexander M. Rush",
            "type": "sparse",
            "x": -8.729491233825684,
            "y": 16.524858474731445
        },
        {
            "id": "s_515",
            "name": "Handi Chen",
            "type": "sparse",
            "x": -9.324723243713379,
            "y": 18.89049530029297
        },
        {
            "id": "s_516",
            "name": "Yumian Cui",
            "type": "sparse",
            "x": -3.405301332473755,
            "y": 16.43229866027832
        },
        {
            "id": "s_517",
            "name": "Chunyao Qian",
            "type": "sparse",
            "x": -10.854547500610352,
            "y": 18.267248153686523
        },
        {
            "id": "s_518",
            "name": "Xiaolong Luke Zhang",
            "type": "sparse",
            "x": -9.401372909545898,
            "y": 18.19166374206543
        },
        {
            "id": "s_519",
            "name": "Yusheng Qi",
            "type": "sparse",
            "x": -9.684982299804688,
            "y": 19.69896125793457
        },
        {
            "id": "s_520",
            "name": "Tobias Klein",
            "type": "sparse",
            "x": -6.257810592651367,
            "y": 19.03653335571289
        },
        {
            "id": "s_521",
            "name": "John Hoffer",
            "type": "sparse",
            "x": -8.626655578613281,
            "y": 16.458839416503906
        },
        {
            "id": "s_522",
            "name": "Aditeya Pandey",
            "type": "sparse",
            "x": -8.2739896774292,
            "y": 17.023305892944336
        },
        {
            "id": "s_523",
            "name": "Ashley Suh 0001",
            "type": "sparse",
            "x": -4.804200649261475,
            "y": 15.949480056762695
        },
        {
            "id": "s_524",
            "name": "Yasmin Reyazuddin",
            "type": "sparse",
            "x": -2.0394067764282227,
            "y": 16.017065048217773
        },
        {
            "id": "s_525",
            "name": "Yanhong Wu",
            "type": "sparse",
            "x": -9.969585418701172,
            "y": 17.224151611328125
        },
        {
            "id": "s_526",
            "name": "Boyd Kenkhuis",
            "type": "sparse",
            "x": -6.797203540802002,
            "y": 14.29612922668457
        },
        {
            "id": "s_527",
            "name": "Ziyue Lin",
            "type": "sparse",
            "x": -10.109185218811035,
            "y": 19.681941986083984
        },
        {
            "id": "s_528",
            "name": "Yifan Qian",
            "type": "sparse",
            "x": -9.312582969665527,
            "y": 19.647449493408203
        },
        {
            "id": "s_529",
            "name": "Kostiantyn Kucher",
            "type": "sparse",
            "x": -3.9384379386901855,
            "y": 19.697208404541016
        },
        {
            "id": "s_530",
            "name": "Willy Scheibel",
            "type": "sparse",
            "x": -8.925466537475586,
            "y": 18.45253562927246
        },
        {
            "id": "s_531",
            "name": "Ziqi Li",
            "type": "sparse",
            "x": -4.270383358001709,
            "y": 21.134363174438477
        },
        {
            "id": "s_532",
            "name": "Thomas Spengler",
            "type": "sparse",
            "x": -6.505026817321777,
            "y": 13.790567398071289
        },
        {
            "id": "s_533",
            "name": "Enrico Bertini",
            "type": "sparse",
            "x": -3.402189254760742,
            "y": 17.61336326599121
        },
        {
            "id": "s_534",
            "name": "Danny Perez",
            "type": "sparse",
            "x": -4.19600248336792,
            "y": 21.021259307861328
        },
        {
            "id": "s_535",
            "name": "Bin Zhu 0008",
            "type": "sparse",
            "x": -10.859342575073242,
            "y": 18.276683807373047
        },
        {
            "id": "s_536",
            "name": "Vidya Setlur",
            "type": "sparse",
            "x": -4.144049644470215,
            "y": 16.250930786132812
        },
        {
            "id": "s_537",
            "name": "Ingrid Zukerman",
            "type": "sparse",
            "x": -7.647334575653076,
            "y": 18.065359115600586
        },
        {
            "id": "s_538",
            "name": "Jianben He",
            "type": "sparse",
            "x": -9.951607704162598,
            "y": 18.48981285095215
        },
        {
            "id": "s_539",
            "name": "Francine Chen 0001",
            "type": "sparse",
            "x": -0.7013787627220154,
            "y": 18.00520133972168
        },
        {
            "id": "s_540",
            "name": "Claudio Delrieux",
            "type": "sparse",
            "x": -6.533764362335205,
            "y": 13.84805965423584
        },
        {
            "id": "s_541",
            "name": "Ignacio Ponzoni",
            "type": "sparse",
            "x": -5.74866247177124,
            "y": 19.50638198852539
        },
        {
            "id": "s_542",
            "name": "Paul Rosen 0001",
            "type": "sparse",
            "x": -1.9206175804138184,
            "y": 17.942209243774414
        },
        {
            "id": "s_543",
            "name": "Jack Pegg",
            "type": "sparse",
            "x": -8.331941604614258,
            "y": 17.34979248046875
        },
        {
            "id": "s_544",
            "name": "Chunggi Lee",
            "type": "sparse",
            "x": -8.733504295349121,
            "y": 16.524816513061523
        },
        {
            "id": "s_545",
            "name": "Wouter Meulemans",
            "type": "sparse",
            "x": -5.9573588371276855,
            "y": 15.64951229095459
        },
        {
            "id": "s_546",
            "name": "Wolfgang Aigner",
            "type": "sparse",
            "x": -6.091065406799316,
            "y": 17.351755142211914
        },
        {
            "id": "s_547",
            "name": "Christine Nothelfer",
            "type": "sparse",
            "x": -2.261063814163208,
            "y": 17.96851348876953
        },
        {
            "id": "s_548",
            "name": "Mengchen Liu",
            "type": "sparse",
            "x": -9.272884368896484,
            "y": 17.25807762145996
        },
        {
            "id": "s_549",
            "name": "Quan Li",
            "type": "sparse",
            "x": -10.946980476379395,
            "y": 19.33216667175293
        },
        {
            "id": "s_550",
            "name": "Magdalena Schwarzl",
            "type": "sparse",
            "x": -8.397322654724121,
            "y": 18.804317474365234
        },
        {
            "id": "s_551",
            "name": "Ao Jiao",
            "type": "sparse",
            "x": -4.305976390838623,
            "y": 21.139619827270508
        },
        {
            "id": "s_552",
            "name": "Smiti Kaul",
            "type": "sparse",
            "x": -9.338784217834473,
            "y": 19.6632080078125
        },
        {
            "id": "s_553",
            "name": "Yamei Tu",
            "type": "sparse",
            "x": 7.590320110321045,
            "y": 5.184680938720703
        },
        {
            "id": "s_554",
            "name": "Gabriela Molina Le\u00f3n",
            "type": "sparse",
            "x": -6.323596954345703,
            "y": 18.41897964477539
        },
        {
            "id": "s_555",
            "name": "Jan Stourac",
            "type": "sparse",
            "x": -5.793778419494629,
            "y": 19.457538604736328
        },
        {
            "id": "s_556",
            "name": "Sunghyo Chung",
            "type": "sparse",
            "x": -1.9921921491622925,
            "y": 16.033761978149414
        },
        {
            "id": "s_557",
            "name": "Haotian Mi",
            "type": "sparse",
            "x": -7.856300354003906,
            "y": 18.882091522216797
        },
        {
            "id": "s_558",
            "name": "Dave Murray-Rust",
            "type": "sparse",
            "x": -6.678660869598389,
            "y": 16.875688552856445
        },
        {
            "id": "s_559",
            "name": "Yifei Li",
            "type": "sparse",
            "x": 10.38487720489502,
            "y": 10.165176391601562
        },
        {
            "id": "s_560",
            "name": "Mahima Pushkarna",
            "type": "sparse",
            "x": 9.877087593078613,
            "y": 10.664929389953613
        },
        {
            "id": "s_561",
            "name": "Holger Steeb",
            "type": "sparse",
            "x": -9.802538871765137,
            "y": 16.80414581298828
        },
        {
            "id": "s_562",
            "name": "Suzanna Maria Bonnet",
            "type": "sparse",
            "x": -7.501627445220947,
            "y": -1.1755471229553223
        },
        {
            "id": "s_563",
            "name": "Michel Wijkstra",
            "type": "sparse",
            "x": -4.380279064178467,
            "y": 15.181581497192383
        },
        {
            "id": "s_564",
            "name": "Hartmut Kaiser",
            "type": "sparse",
            "x": -0.2291490137577057,
            "y": 17.10004997253418
        },
        {
            "id": "s_565",
            "name": "Jimmy Moore",
            "type": "sparse",
            "x": -2.257401943206787,
            "y": 15.365616798400879
        },
        {
            "id": "s_566",
            "name": "Shiqing He",
            "type": "sparse",
            "x": -3.159938335418701,
            "y": 16.882862091064453
        },
        {
            "id": "s_567",
            "name": "Xiaoyu Qi",
            "type": "sparse",
            "x": -9.312220573425293,
            "y": 19.65608787536621
        },
        {
            "id": "s_568",
            "name": "Lin Gao",
            "type": "sparse",
            "x": -10.05766773223877,
            "y": 19.580812454223633
        },
        {
            "id": "s_569",
            "name": "Hayder Mahdi Al-Maneea",
            "type": "sparse",
            "x": -4.940937519073486,
            "y": 16.027767181396484
        },
        {
            "id": "s_570",
            "name": "Yuchen Zhang",
            "type": "sparse",
            "x": -8.571678161621094,
            "y": 18.270910263061523
        },
        {
            "id": "s_571",
            "name": "Christopher R. Johnson 0001",
            "type": "sparse",
            "x": -1.849565029144287,
            "y": 17.934864044189453
        },
        {
            "id": "s_572",
            "name": "Fangfang Sheng",
            "type": "sparse",
            "x": -6.743521690368652,
            "y": 21.155147552490234
        },
        {
            "id": "s_573",
            "name": "Quanxue Guan",
            "type": "sparse",
            "x": -11.046079635620117,
            "y": 19.334590911865234
        },
        {
            "id": "s_574",
            "name": "Xi Chen 0072",
            "type": "sparse",
            "x": -5.001034259796143,
            "y": 16.060251235961914
        },
        {
            "id": "s_575",
            "name": "Henry Reinstein",
            "type": "sparse",
            "x": -15.024615287780762,
            "y": 7.149693012237549
        },
        {
            "id": "s_576",
            "name": "Chang Jiang",
            "type": "sparse",
            "x": -11.022623062133789,
            "y": 19.28498077392578
        },
        {
            "id": "s_577",
            "name": "Tianxiang Chen",
            "type": "sparse",
            "x": -9.312786102294922,
            "y": 18.249675750732422
        },
        {
            "id": "s_578",
            "name": "Tobias Schreck",
            "type": "sparse",
            "x": -8.915781021118164,
            "y": 18.431962966918945
        },
        {
            "id": "s_579",
            "name": "Yun Lin 0005",
            "type": "sparse",
            "x": -9.239740371704102,
            "y": 17.744956970214844
        },
        {
            "id": "s_580",
            "name": "Marc Streit",
            "type": "sparse",
            "x": -2.127157688140869,
            "y": 15.694130897521973
        },
        {
            "id": "s_581",
            "name": "Juraj P\u00e1lenik",
            "type": "sparse",
            "x": -6.504523754119873,
            "y": 13.78901195526123
        },
        {
            "id": "s_582",
            "name": "Jason Dykes",
            "type": "sparse",
            "x": -6.120264053344727,
            "y": 17.322603225708008
        },
        {
            "id": "s_583",
            "name": "Pepe Eulzer",
            "type": "sparse",
            "x": -3.5500149726867676,
            "y": 19.81862449645996
        },
        {
            "id": "s_584",
            "name": "David S. Goodsell",
            "type": "sparse",
            "x": -6.257671356201172,
            "y": 19.03864097595215
        },
        {
            "id": "s_585",
            "name": "Boris Bedic",
            "type": "sparse",
            "x": -6.4855828285217285,
            "y": 13.798066139221191
        },
        {
            "id": "s_586",
            "name": "Jorji Nonaka",
            "type": "sparse",
            "x": -0.7611357569694519,
            "y": 17.98572540283203
        },
        {
            "id": "s_587",
            "name": "Vahan Yoghourdjian",
            "type": "sparse",
            "x": -7.5882887840271,
            "y": 17.792709350585938
        },
        {
            "id": "s_588",
            "name": "Jaakko Peltonen",
            "type": "sparse",
            "x": -5.676260471343994,
            "y": 15.145642280578613
        },
        {
            "id": "s_589",
            "name": "Marianna Farmakis-Serebryakova",
            "type": "sparse",
            "x": -7.175150394439697,
            "y": 18.070554733276367
        },
        {
            "id": "s_590",
            "name": "Ameya Patil",
            "type": "sparse",
            "x": -3.9332892894744873,
            "y": 17.409345626831055
        },
        {
            "id": "s_591",
            "name": "Alexander Wyss",
            "type": "sparse",
            "x": -8.629928588867188,
            "y": 18.646228790283203
        },
        {
            "id": "s_592",
            "name": "Yifang Wang 0001",
            "type": "sparse",
            "x": -10.274357795715332,
            "y": 17.783790588378906
        },
        {
            "id": "s_593",
            "name": "Bhavya Ghai",
            "type": "sparse",
            "x": -9.264512062072754,
            "y": 18.812660217285156
        },
        {
            "id": "s_594",
            "name": "Yiren Ding",
            "type": "sparse",
            "x": -3.1114492416381836,
            "y": 16.541324615478516
        },
        {
            "id": "s_595",
            "name": "Brian Montambault",
            "type": "sparse",
            "x": -4.799209117889404,
            "y": 15.94743537902832
        },
        {
            "id": "s_596",
            "name": "Yanna Lin",
            "type": "sparse",
            "x": -10.100024223327637,
            "y": 18.17445945739746
        },
        {
            "id": "s_597",
            "name": "Chuyi Zhao",
            "type": "sparse",
            "x": -11.056272506713867,
            "y": 19.337907791137695
        },
        {
            "id": "s_598",
            "name": "Sven Mayer",
            "type": "sparse",
            "x": -8.431161880493164,
            "y": 18.788984298706055
        },
        {
            "id": "s_599",
            "name": "Jieyi Chen",
            "type": "sparse",
            "x": -9.514403343200684,
            "y": 17.865039825439453
        },
        {
            "id": "s_600",
            "name": "Lawrence Lee",
            "type": "sparse",
            "x": -7.552596569061279,
            "y": 17.816162109375
        },
        {
            "id": "s_601",
            "name": "Ben Jacobsen",
            "type": "sparse",
            "x": -5.6272430419921875,
            "y": 15.108052253723145
        },
        {
            "id": "s_602",
            "name": "Pei Liu",
            "type": "sparse",
            "x": -9.298507690429688,
            "y": 19.72379493713379
        },
        {
            "id": "s_603",
            "name": "Andreas Kerren",
            "type": "sparse",
            "x": -3.9450368881225586,
            "y": 19.809186935424805
        },
        {
            "id": "s_604",
            "name": "Giorgio Gaglia",
            "type": "sparse",
            "x": -8.62682056427002,
            "y": 16.425065994262695
        },
        {
            "id": "s_605",
            "name": "Racquel Fygenson",
            "type": "sparse",
            "x": -3.2997446060180664,
            "y": 17.342632293701172
        },
        {
            "id": "s_606",
            "name": "Steve Petruzza",
            "type": "sparse",
            "x": -14.96975040435791,
            "y": 7.204563140869141
        },
        {
            "id": "s_607",
            "name": "Fumeng Yang",
            "type": "sparse",
            "x": -3.0223774909973145,
            "y": 16.637134552001953
        },
        {
            "id": "s_608",
            "name": "Foroozan Daneshzand",
            "type": "sparse",
            "x": -6.196457862854004,
            "y": 17.500024795532227
        },
        {
            "id": "s_609",
            "name": "Abdul Rahman Shaikh",
            "type": "sparse",
            "x": -0.5922660827636719,
            "y": 18.176170349121094
        },
        {
            "id": "s_610",
            "name": "He Wang",
            "type": "sparse",
            "x": -10.984352111816406,
            "y": 19.24885368347168
        },
        {
            "id": "s_611",
            "name": "Juntong Luo 0002",
            "type": "sparse",
            "x": -10.15431022644043,
            "y": 17.434757232666016
        },
        {
            "id": "s_612",
            "name": "Thomas Zangle",
            "type": "sparse",
            "x": -2.245281219482422,
            "y": 15.381052017211914
        },
        {
            "id": "s_613",
            "name": "Michael L. Rivera",
            "type": "sparse",
            "x": -2.0983095169067383,
            "y": 18.08280372619629
        },
        {
            "id": "s_614",
            "name": "M. Eduard Gr\u00f6ller",
            "type": "sparse",
            "x": -6.612182140350342,
            "y": 14.05587100982666
        },
        {
            "id": "s_615",
            "name": "Madison A. Elliott",
            "type": "sparse",
            "x": -2.248284101486206,
            "y": 17.973421096801758
        },
        {
            "id": "s_616",
            "name": "Carlos Scheidegger",
            "type": "sparse",
            "x": -0.3735353350639343,
            "y": 17.310012817382812
        },
        {
            "id": "s_617",
            "name": "Hyemi Song",
            "type": "sparse",
            "x": -4.2862701416015625,
            "y": 16.970443725585938
        },
        {
            "id": "s_618",
            "name": "Tak Yeon Lee",
            "type": "sparse",
            "x": -4.1894917488098145,
            "y": 16.86638832092285
        },
        {
            "id": "s_619",
            "name": "Kiran Gadhave",
            "type": "sparse",
            "x": -2.2186756134033203,
            "y": 15.442671775817871
        },
        {
            "id": "s_620",
            "name": "Alessandra Angelucci",
            "type": "sparse",
            "x": -14.967843055725098,
            "y": 7.206468105316162
        },
        {
            "id": "s_621",
            "name": "Oliver van Kaick",
            "type": "sparse",
            "x": -7.973572731018066,
            "y": 18.691116333007812
        },
        {
            "id": "s_622",
            "name": "Christoph Schulz 0001",
            "type": "sparse",
            "x": -8.55041217803955,
            "y": 19.342531204223633
        },
        {
            "id": "s_623",
            "name": "Zezheng Feng",
            "type": "sparse",
            "x": -9.949938774108887,
            "y": 18.13241195678711
        },
        {
            "id": "s_624",
            "name": "Josh Pollock",
            "type": "sparse",
            "x": -4.286224365234375,
            "y": 16.086332321166992
        },
        {
            "id": "s_625",
            "name": "Suyun \"Sandra\" Bae",
            "type": "sparse",
            "x": -0.7131509184837341,
            "y": 17.9348087310791
        },
        {
            "id": "s_626",
            "name": "Shuainan Ye",
            "type": "sparse",
            "x": -10.22610092163086,
            "y": 17.756807327270508
        },
        {
            "id": "s_627",
            "name": "Mingze Ma",
            "type": "sparse",
            "x": -10.255407333374023,
            "y": 17.263303756713867
        },
        {
            "id": "s_628",
            "name": "Golina Hulstein",
            "type": "sparse",
            "x": -6.20862340927124,
            "y": 18.29486846923828
        },
        {
            "id": "s_629",
            "name": "Peter Wonka",
            "type": "sparse",
            "x": -6.247057914733887,
            "y": 19.037099838256836
        },
        {
            "id": "s_630",
            "name": "Xinyi Huang 0003",
            "type": "sparse",
            "x": -0.23122189939022064,
            "y": 17.61739158630371
        },
        {
            "id": "s_631",
            "name": "Xinyi Zhou 0005",
            "type": "sparse",
            "x": -8.718158721923828,
            "y": 16.9830379486084
        },
        {
            "id": "s_632",
            "name": "Michael Behrisch 0001",
            "type": "sparse",
            "x": -4.143703460693359,
            "y": 19.684040069580078
        },
        {
            "id": "s_633",
            "name": "Krystal Kallarackal",
            "type": "sparse",
            "x": 9.919775009155273,
            "y": 10.622162818908691
        },
        {
            "id": "s_634",
            "name": "Naohisa Sakamoto",
            "type": "sparse",
            "x": -0.7025871276855469,
            "y": 17.940509796142578
        },
        {
            "id": "s_635",
            "name": "Jules Vidal",
            "type": "sparse",
            "x": -4.042940616607666,
            "y": 20.698162078857422
        },
        {
            "id": "s_636",
            "name": "He Huang",
            "type": "sparse",
            "x": -10.8375825881958,
            "y": 18.25727081298828
        },
        {
            "id": "s_637",
            "name": "Arjun Choudhry",
            "type": "sparse",
            "x": -2.004289388656616,
            "y": 15.993748664855957
        },
        {
            "id": "s_638",
            "name": "Cong Liu",
            "type": "sparse",
            "x": -4.791744232177734,
            "y": 15.766122817993164
        },
        {
            "id": "s_639",
            "name": "Johanna Hartmann",
            "type": "sparse",
            "x": -6.582104682922363,
            "y": 17.08755874633789
        },
        {
            "id": "s_640",
            "name": "Boyu Wang 0001",
            "type": "sparse",
            "x": -0.19495531916618347,
            "y": 17.61954116821289
        },
        {
            "id": "s_641",
            "name": "Jiehui Zhou",
            "type": "sparse",
            "x": -9.921287536621094,
            "y": 18.33709144592285
        },
        {
            "id": "s_642",
            "name": "Robert S. Laramee",
            "type": "sparse",
            "x": -6.095412254333496,
            "y": 17.35843849182129
        },
        {
            "id": "s_643",
            "name": "Runjin Zhang",
            "type": "sparse",
            "x": -10.28858757019043,
            "y": 17.214820861816406
        },
        {
            "id": "s_644",
            "name": "Angie Boggust",
            "type": "sparse",
            "x": -3.870633125305176,
            "y": 17.3955078125
        },
        {
            "id": "s_645",
            "name": "Guoren Wang",
            "type": "sparse",
            "x": -7.8308210372924805,
            "y": 18.919092178344727
        },
        {
            "id": "s_646",
            "name": "Dominik Moritz",
            "type": "sparse",
            "x": -3.7737865447998047,
            "y": 17.33617401123047
        },
        {
            "id": "s_647",
            "name": "Hui Zhang 0051",
            "type": "sparse",
            "x": -10.407536506652832,
            "y": 17.473602294921875
        },
        {
            "id": "s_648",
            "name": "Dongming Han",
            "type": "sparse",
            "x": -9.484694480895996,
            "y": 17.811655044555664
        },
        {
            "id": "s_649",
            "name": "Hyeon Jeon",
            "type": "sparse",
            "x": -1.2586851119995117,
            "y": 17.996219635009766
        },
        {
            "id": "s_650",
            "name": "R\u00f3bert Zvara",
            "type": "sparse",
            "x": -5.751822471618652,
            "y": 19.495933532714844
        },
        {
            "id": "s_651",
            "name": "Yicheng Hu",
            "type": "sparse",
            "x": -10.926191329956055,
            "y": 18.9807186126709
        },
        {
            "id": "s_652",
            "name": "Hyeshin Chu",
            "type": "sparse",
            "x": -1.704392910003662,
            "y": 16.41983413696289
        },
        {
            "id": "s_653",
            "name": "Alessio Arleo",
            "type": "sparse",
            "x": -5.259125232696533,
            "y": 15.018936157226562
        },
        {
            "id": "s_654",
            "name": "Baicheng Wang",
            "type": "sparse",
            "x": -9.58538818359375,
            "y": 17.99483299255371
        },
        {
            "id": "s_655",
            "name": "Jamal Paden",
            "type": "sparse",
            "x": -4.7478413581848145,
            "y": 15.522064208984375
        },
        {
            "id": "s_656",
            "name": "Lucas Nadolskis",
            "type": "sparse",
            "x": -3.8625519275665283,
            "y": 17.3902530670166
        },
        {
            "id": "s_657",
            "name": "Anna Pia Lohfink",
            "type": "sparse",
            "x": -3.88092041015625,
            "y": 20.855911254882812
        },
        {
            "id": "s_658",
            "name": "Yuanzhe Hu",
            "type": "sparse",
            "x": -9.395759582519531,
            "y": 18.09523582458496
        },
        {
            "id": "s_659",
            "name": "Philipp Meschenmoser",
            "type": "sparse",
            "x": -7.030810356140137,
            "y": 20.87442970275879
        },
        {
            "id": "s_660",
            "name": "Shaohan Shi",
            "type": "sparse",
            "x": -11.070810317993164,
            "y": 19.359344482421875
        },
        {
            "id": "s_661",
            "name": "Seongmin Lee 0007",
            "type": "sparse",
            "x": 10.298052787780762,
            "y": 10.249954223632812
        },
        {
            "id": "s_662",
            "name": "Yuge Zhang",
            "type": "sparse",
            "x": -8.921351432800293,
            "y": 19.952314376831055
        },
        {
            "id": "s_663",
            "name": "Euan Freeman",
            "type": "sparse",
            "x": -7.056784152984619,
            "y": 17.006235122680664
        },
        {
            "id": "s_664",
            "name": "Xiaoli Qiao",
            "type": "sparse",
            "x": -3.4740498065948486,
            "y": 16.880483627319336
        },
        {
            "id": "s_665",
            "name": "Duen Horng (Polo) Chau",
            "type": "sparse",
            "x": 10.030848503112793,
            "y": 10.508420944213867
        },
        {
            "id": "s_666",
            "name": "Dashun Wang",
            "type": "sparse",
            "x": -9.30187702178955,
            "y": 19.66754150390625
        },
        {
            "id": "s_667",
            "name": "Haikuan Zhu",
            "type": "sparse",
            "x": -11.001957893371582,
            "y": 16.869382858276367
        },
        {
            "id": "s_668",
            "name": "Zhongsu Luo",
            "type": "sparse",
            "x": -10.263916969299316,
            "y": 17.336334228515625
        },
        {
            "id": "s_669",
            "name": "Anne-Flore Cabouat",
            "type": "sparse",
            "x": -6.3239312171936035,
            "y": 18.460018157958984
        },
        {
            "id": "s_670",
            "name": "Xinhai Wei",
            "type": "sparse",
            "x": -0.7111281752586365,
            "y": 18.01358413696289
        },
        {
            "id": "s_671",
            "name": "Duong Hoang",
            "type": "sparse",
            "x": -15.071818351745605,
            "y": 7.102410793304443
        },
        {
            "id": "s_672",
            "name": "Qiong Luo 0001",
            "type": "sparse",
            "x": -10.885384559631348,
            "y": 18.419225692749023
        },
        {
            "id": "s_673",
            "name": "Jing Hua 0001",
            "type": "sparse",
            "x": -10.997834205627441,
            "y": 16.873802185058594
        },
        {
            "id": "s_674",
            "name": "Pasindu Tennakoon",
            "type": "sparse",
            "x": -4.005835056304932,
            "y": 20.724910736083984
        },
        {
            "id": "s_675",
            "name": "Edith C. H. Ngai",
            "type": "sparse",
            "x": -9.309708595275879,
            "y": 18.90390396118164
        },
        {
            "id": "s_676",
            "name": "Jincheng Jiang",
            "type": "sparse",
            "x": -9.235111236572266,
            "y": 17.90761947631836
        },
        {
            "id": "s_677",
            "name": "Christophe Hurter",
            "type": "sparse",
            "x": -7.514449119567871,
            "y": 17.908021926879883
        },
        {
            "id": "s_678",
            "name": "Mingxu Zhou",
            "type": "sparse",
            "x": -10.25551700592041,
            "y": 17.354887008666992
        },
        {
            "id": "s_679",
            "name": "Daniel Plakinger",
            "type": "sparse",
            "x": -5.755648136138916,
            "y": 19.501060485839844
        },
        {
            "id": "s_680",
            "name": "Andreas P. Hinterreiter",
            "type": "sparse",
            "x": -2.0809242725372314,
            "y": 15.921492576599121
        },
        {
            "id": "s_681",
            "name": "David Gro\u00df",
            "type": "sparse",
            "x": -5.772097587585449,
            "y": 18.488351821899414
        },
        {
            "id": "s_682",
            "name": "Yihong Wu 0003",
            "type": "sparse",
            "x": -10.116630554199219,
            "y": 17.145496368408203
        },
        {
            "id": "s_683",
            "name": "Shengbin Yue",
            "type": "sparse",
            "x": -10.088927268981934,
            "y": 19.679363250732422
        },
        {
            "id": "s_684",
            "name": "Luiz Morais",
            "type": "sparse",
            "x": -6.102364540100098,
            "y": 17.34092140197754
        },
        {
            "id": "s_685",
            "name": "Jiali Wang",
            "type": "sparse",
            "x": 7.441370964050293,
            "y": 5.3334503173828125
        },
        {
            "id": "s_686",
            "name": "Pascal Aschwanden",
            "type": "sparse",
            "x": -0.19657722115516663,
            "y": 17.090612411499023
        },
        {
            "id": "s_687",
            "name": "Kylie Lin",
            "type": "sparse",
            "x": -4.104088306427002,
            "y": 16.398712158203125
        },
        {
            "id": "s_688",
            "name": "Gu\u00f0bj\u00f6rg Linda Rafnsd\u00f3ttir",
            "type": "sparse",
            "x": -6.307407379150391,
            "y": 18.38471794128418
        },
        {
            "id": "s_689",
            "name": "Johanna Beyer",
            "type": "sparse",
            "x": -8.630010604858398,
            "y": 16.53098487854004
        },
        {
            "id": "s_690",
            "name": "Dylan Cashman",
            "type": "sparse",
            "x": -4.791609287261963,
            "y": 15.765864372253418
        },
        {
            "id": "s_691",
            "name": "James Jackson",
            "type": "sparse",
            "x": -5.993747234344482,
            "y": 17.318933486938477
        },
        {
            "id": "s_692",
            "name": "Johanna Schmidt",
            "type": "sparse",
            "x": -5.4218339920043945,
            "y": 14.872262001037598
        },
        {
            "id": "s_693",
            "name": "Yue Wang 0035",
            "type": "sparse",
            "x": -9.004035949707031,
            "y": 19.82206916809082
        },
        {
            "id": "s_694",
            "name": "Ondrej Strnad",
            "type": "sparse",
            "x": -6.2431321144104,
            "y": 18.919010162353516
        },
        {
            "id": "s_695",
            "name": "Markus Gross 0001",
            "type": "sparse",
            "x": -3.4806439876556396,
            "y": 19.78694725036621
        },
        {
            "id": "s_696",
            "name": "Joyce Ma",
            "type": "sparse",
            "x": -0.7425733804702759,
            "y": 17.996923446655273
        },
        {
            "id": "s_697",
            "name": "Jinhan Choi",
            "type": "sparse",
            "x": -8.870926856994629,
            "y": 16.555723190307617
        },
        {
            "id": "s_698",
            "name": "Jeff M. Phillips",
            "type": "sparse",
            "x": 7.333536624908447,
            "y": 5.4412312507629395
        },
        {
            "id": "s_699",
            "name": "Lincan Zou",
            "type": "sparse",
            "x": -0.3934534192085266,
            "y": 17.540271759033203
        },
        {
            "id": "s_700",
            "name": "Angelos Chatzimparmpas",
            "type": "sparse",
            "x": -3.9566826820373535,
            "y": 19.724597930908203
        },
        {
            "id": "s_701",
            "name": "Nils Rodrigues",
            "type": "sparse",
            "x": -8.56615924835205,
            "y": 19.360868453979492
        },
        {
            "id": "s_702",
            "name": "Patricia Ganea",
            "type": "sparse",
            "x": -6.606717586517334,
            "y": 16.7862491607666
        },
        {
            "id": "s_703",
            "name": "Amitesh Maiti",
            "type": "sparse",
            "x": -15.03667163848877,
            "y": 7.137659072875977
        },
        {
            "id": "s_704",
            "name": "Yuanyuan Chen",
            "type": "sparse",
            "x": -9.27109432220459,
            "y": 17.9311466217041
        },
        {
            "id": "s_705",
            "name": "Laura South",
            "type": "sparse",
            "x": -6.713414192199707,
            "y": 21.177682876586914
        },
        {
            "id": "s_706",
            "name": "Richard Gee",
            "type": "sparse",
            "x": -15.036691665649414,
            "y": 7.137627124786377
        },
        {
            "id": "s_707",
            "name": "Julien Tierny",
            "type": "sparse",
            "x": -4.080753803253174,
            "y": 20.785003662109375
        },
        {
            "id": "s_708",
            "name": "Will Usher 0001",
            "type": "sparse",
            "x": -15.062171936035156,
            "y": 7.111748218536377
        },
        {
            "id": "s_709",
            "name": "Haotian Li 0001",
            "type": "sparse",
            "x": -9.955964088439941,
            "y": 18.190462112426758
        },
        {
            "id": "s_710",
            "name": "Jozef B\u00e1trna",
            "type": "sparse",
            "x": -5.7453999519348145,
            "y": 19.510761260986328
        },
        {
            "id": "s_711",
            "name": "Florent Cabric",
            "type": "sparse",
            "x": -6.297846794128418,
            "y": 18.39122772216797
        },
        {
            "id": "s_712",
            "name": "Huan Song",
            "type": "sparse",
            "x": -0.6706917881965637,
            "y": 17.871963500976562
        },
        {
            "id": "s_713",
            "name": "Maria Luj\u00e1n Ganuza",
            "type": "sparse",
            "x": -5.766964912414551,
            "y": 19.490053176879883
        },
        {
            "id": "s_714",
            "name": "Zhitao Hou",
            "type": "sparse",
            "x": -10.910409927368164,
            "y": 18.221616744995117
        },
        {
            "id": "s_715",
            "name": "Jan-Henrik Haunert",
            "type": "sparse",
            "x": -5.620441913604736,
            "y": 15.080840110778809
        },
        {
            "id": "s_716",
            "name": "Alexandra Voit",
            "type": "sparse",
            "x": -8.40827751159668,
            "y": 18.75929069519043
        },
        {
            "id": "s_717",
            "name": "Axel J. Soto",
            "type": "sparse",
            "x": -5.771137714385986,
            "y": 19.484619140625
        },
        {
            "id": "s_718",
            "name": "Wenyuan Wang",
            "type": "sparse",
            "x": -8.968816757202148,
            "y": 19.848163604736328
        },
        {
            "id": "s_719",
            "name": "Derya Akbaba",
            "type": "sparse",
            "x": -2.3240859508514404,
            "y": 15.368245124816895
        },
        {
            "id": "s_720",
            "name": "Joseph D. Gaggiano",
            "type": "sparse",
            "x": -6.676629543304443,
            "y": 21.232236862182617
        },
        {
            "id": "s_721",
            "name": "Zhuochen Jin",
            "type": "sparse",
            "x": -9.141336441040039,
            "y": 19.86570930480957
        },
        {
            "id": "s_722",
            "name": "Douglas W. Cunningham",
            "type": "sparse",
            "x": -3.643645763397217,
            "y": 19.8533878326416
        },
        {
            "id": "s_723",
            "name": "Deng Luo",
            "type": "sparse",
            "x": -6.226483345031738,
            "y": 18.942991256713867
        },
        {
            "id": "s_724",
            "name": "Yulei Fan",
            "type": "sparse",
            "x": -7.163325786590576,
            "y": 16.962318420410156
        },
        {
            "id": "s_725",
            "name": "Aditya Konduri",
            "type": "sparse",
            "x": -0.7545838952064514,
            "y": 17.982412338256836
        },
        {
            "id": "s_726",
            "name": "Kelei Cao",
            "type": "sparse",
            "x": -9.304853439331055,
            "y": 17.265703201293945
        },
        {
            "id": "s_727",
            "name": "Yansong Huang",
            "type": "sparse",
            "x": -4.276832580566406,
            "y": 21.10464096069336
        },
        {
            "id": "s_728",
            "name": "Juliana Freire",
            "type": "sparse",
            "x": -4.038616180419922,
            "y": 20.105995178222656
        },
        {
            "id": "s_729",
            "name": "Omar Shaikh",
            "type": "sparse",
            "x": 10.053743362426758,
            "y": 10.4856538772583
        },
        {
            "id": "s_730",
            "name": "Chuhan Shi",
            "type": "sparse",
            "x": -10.861876487731934,
            "y": 18.423297882080078
        },
        {
            "id": "s_731",
            "name": "Audrey L. Michal",
            "type": "sparse",
            "x": -3.667937994003296,
            "y": 16.499053955078125
        },
        {
            "id": "s_732",
            "name": "S\u00f8ren Knudsen",
            "type": "sparse",
            "x": -6.109834671020508,
            "y": 17.336219787597656
        },
        {
            "id": "s_733",
            "name": "Harald Steinlechner",
            "type": "sparse",
            "x": -5.587668418884277,
            "y": 14.727384567260742
        },
        {
            "id": "s_734",
            "name": "Yongkang Xiao",
            "type": "sparse",
            "x": -8.452218055725098,
            "y": 17.25356101989746
        },
        {
            "id": "s_735",
            "name": "Di Weng",
            "type": "sparse",
            "x": -10.075018882751465,
            "y": 17.216506958007812
        },
        {
            "id": "s_736",
            "name": "Huisi Wu",
            "type": "sparse",
            "x": -9.682462692260742,
            "y": 18.5732364654541
        },
        {
            "id": "s_737",
            "name": "Yun Wang 0012",
            "type": "sparse",
            "x": -10.605921745300293,
            "y": 18.197080612182617
        },
        {
            "id": "s_738",
            "name": "Shannon Coy",
            "type": "sparse",
            "x": -8.702041625976562,
            "y": 16.495990753173828
        },
        {
            "id": "s_739",
            "name": "Wenkai Song",
            "type": "sparse",
            "x": -11.077435493469238,
            "y": 19.346744537353516
        },
        {
            "id": "s_740",
            "name": "Jie Xu",
            "type": "sparse",
            "x": -10.407838821411133,
            "y": 17.4016170501709
        },
        {
            "id": "s_741",
            "name": "Charles D. Hansen",
            "type": "sparse",
            "x": -4.0298261642456055,
            "y": 20.156126022338867
        },
        {
            "id": "s_742",
            "name": "Yuchao Liu",
            "type": "sparse",
            "x": -9.253331184387207,
            "y": 17.740257263183594
        },
        {
            "id": "s_743",
            "name": "Greg Finak",
            "type": "sparse",
            "x": -8.424046516418457,
            "y": 17.01331329345703
        },
        {
            "id": "s_744",
            "name": "Nivan Ferreira",
            "type": "sparse",
            "x": -7.504749774932861,
            "y": -1.172410011291504
        },
        {
            "id": "s_745",
            "name": "Chi Harold Liu",
            "type": "sparse",
            "x": -7.819571018218994,
            "y": 18.933094024658203
        },
        {
            "id": "s_746",
            "name": "Jason K. Wong",
            "type": "sparse",
            "x": -9.574383735656738,
            "y": 18.238826751708984
        },
        {
            "id": "s_747",
            "name": "Lynn McVey",
            "type": "sparse",
            "x": -6.448915004730225,
            "y": 16.492671966552734
        },
        {
            "id": "s_748",
            "name": "Brian Barr",
            "type": "sparse",
            "x": -4.023610591888428,
            "y": 20.150253295898438
        },
        {
            "id": "s_749",
            "name": "Jonathan Woodring",
            "type": "sparse",
            "x": 7.509633541107178,
            "y": 5.2650957107543945
        },
        {
            "id": "s_750",
            "name": "Xingyu Lan",
            "type": "sparse",
            "x": -9.444934844970703,
            "y": 19.600339889526367
        },
        {
            "id": "s_751",
            "name": "Ahsan Qamar",
            "type": "sparse",
            "x": -4.771408557891846,
            "y": 15.543895721435547
        },
        {
            "id": "s_752",
            "name": "Bingchang Chen",
            "type": "sparse",
            "x": -9.354891777038574,
            "y": 19.702861785888672
        },
        {
            "id": "s_753",
            "name": "Yifan Wang",
            "type": "sparse",
            "x": -10.32337474822998,
            "y": 17.186222076416016
        },
        {
            "id": "s_754",
            "name": "Maneesh Agrawala",
            "type": "sparse",
            "x": -4.113237380981445,
            "y": 16.278268814086914
        },
        {
            "id": "s_755",
            "name": "Tamara Flemisch",
            "type": "sparse",
            "x": -5.787487983703613,
            "y": 18.47785186767578
        },
        {
            "id": "s_756",
            "name": "Rhema Vaithianathan",
            "type": "sparse",
            "x": -10.262652397155762,
            "y": 17.89269256591797
        },
        {
            "id": "s_757",
            "name": "Rory James Zauner",
            "type": "sparse",
            "x": -10.10445499420166,
            "y": 19.67635154724121
        },
        {
            "id": "s_758",
            "name": "Chengbo Zheng",
            "type": "sparse",
            "x": -10.434551239013672,
            "y": 17.41307830810547
        },
        {
            "id": "s_759",
            "name": "Michael E. Papka",
            "type": "sparse",
            "x": -2.2069365978240967,
            "y": 18.014360427856445
        },
        {
            "id": "s_760",
            "name": "Vit\u00f3ria Guardieiro",
            "type": "sparse",
            "x": -3.996777057647705,
            "y": 20.153682708740234
        },
        {
            "id": "s_761",
            "name": "Sarah Bardin",
            "type": "sparse",
            "x": -4.284435272216797,
            "y": 21.12737274169922
        },
        {
            "id": "s_762",
            "name": "Ouxun Jiang",
            "type": "sparse",
            "x": -3.9743406772613525,
            "y": 16.662185668945312
        },
        {
            "id": "s_763",
            "name": "Lu Ying",
            "type": "sparse",
            "x": -10.385638236999512,
            "y": 17.285837173461914
        },
        {
            "id": "s_764",
            "name": "Jing Deng",
            "type": "sparse",
            "x": -9.334280014038086,
            "y": 18.903657913208008
        },
        {
            "id": "s_765",
            "name": "Junxiu Tang",
            "type": "sparse",
            "x": -10.232378959655762,
            "y": 17.691974639892578
        },
        {
            "id": "s_766",
            "name": "Jiacheng Pan",
            "type": "sparse",
            "x": -9.998149871826172,
            "y": 17.576770782470703
        },
        {
            "id": "s_767",
            "name": "Megan L. Burger",
            "type": "sparse",
            "x": -8.682246208190918,
            "y": 16.47057342529297
        },
        {
            "id": "s_768",
            "name": "Joohee Kim",
            "type": "sparse",
            "x": -2.0072362422943115,
            "y": 16.027923583984375
        },
        {
            "id": "s_769",
            "name": "Kevin A. Huck",
            "type": "sparse",
            "x": -0.23096607625484467,
            "y": 17.108972549438477
        },
        {
            "id": "s_770",
            "name": "Huihua Lu",
            "type": "sparse",
            "x": -10.238607406616211,
            "y": 17.68280029296875
        },
        {
            "id": "s_771",
            "name": "Yuanwu Cao",
            "type": "sparse",
            "x": -10.97497844696045,
            "y": 19.244775772094727
        },
        {
            "id": "s_772",
            "name": "Kan Ren",
            "type": "sparse",
            "x": -8.917784690856934,
            "y": 19.954669952392578
        },
        {
            "id": "s_773",
            "name": "Haoyu Li",
            "type": "sparse",
            "x": 7.476876735687256,
            "y": 5.297903060913086
        },
        {
            "id": "s_774",
            "name": "Qiang Wei",
            "type": "sparse",
            "x": -9.254673957824707,
            "y": 17.675373077392578
        },
        {
            "id": "s_775",
            "name": "Andrea Batch",
            "type": "sparse",
            "x": -2.017263889312744,
            "y": 16.014381408691406
        },
        {
            "id": "s_776",
            "name": "Alexander Frings",
            "type": "sparse",
            "x": -6.877583980560303,
            "y": 21.0277156829834
        },
        {
            "id": "s_777",
            "name": "Areti Manataki",
            "type": "sparse",
            "x": -6.0938920974731445,
            "y": 17.37075424194336
        },
        {
            "id": "s_778",
            "name": "Chengshun Wang",
            "type": "sparse",
            "x": -10.111445426940918,
            "y": 19.60177993774414
        },
        {
            "id": "s_779",
            "name": "Ying Wang 0060",
            "type": "sparse",
            "x": -11.006567001342773,
            "y": 16.864355087280273
        },
        {
            "id": "s_780",
            "name": "Andreas Walch",
            "type": "sparse",
            "x": -6.5380096435546875,
            "y": 14.365116119384766
        },
        {
            "id": "s_781",
            "name": "Danielle Albers Szafir",
            "type": "sparse",
            "x": -2.1242947578430176,
            "y": 18.047935485839844
        },
        {
            "id": "s_782",
            "name": "Miguel A. Nacenta",
            "type": "sparse",
            "x": -6.107320308685303,
            "y": 17.29496192932129
        },
        {
            "id": "s_783",
            "name": "Payal Chandak",
            "type": "sparse",
            "x": -8.402694702148438,
            "y": 17.119909286499023
        },
        {
            "id": "s_784",
            "name": "Shuyue Zhou",
            "type": "sparse",
            "x": -9.975894927978516,
            "y": 17.548337936401367
        },
        {
            "id": "s_785",
            "name": "Hamed Alhoori",
            "type": "sparse",
            "x": -0.6083690524101257,
            "y": 18.165359497070312
        },
        {
            "id": "s_786",
            "name": "Marius Horga",
            "type": "sparse",
            "x": -7.489884853363037,
            "y": -1.1873286962509155
        },
        {
            "id": "s_787",
            "name": "Victor S. Bursztyn",
            "type": "sparse",
            "x": -3.9636001586914062,
            "y": 16.726259231567383
        },
        {
            "id": "s_788",
            "name": "Eric Newburger",
            "type": "sparse",
            "x": -2.0139524936676025,
            "y": 16.01156234741211
        },
        {
            "id": "s_789",
            "name": "Kwan-Liu Ma",
            "type": "sparse",
            "x": -0.7596789002418518,
            "y": 17.97035789489746
        },
        {
            "id": "s_790",
            "name": "Ren\u00e9 Cutura",
            "type": "sparse",
            "x": -8.385083198547363,
            "y": 18.748371124267578
        },
        {
            "id": "s_791",
            "name": "Christine Tang",
            "type": "sparse",
            "x": -2.002708673477173,
            "y": 16.020112991333008
        },
        {
            "id": "s_792",
            "name": "Heungseok Park",
            "type": "sparse",
            "x": -1.6703228950500488,
            "y": 16.433422088623047
        },
        {
            "id": "s_793",
            "name": "Jonathan Lazar",
            "type": "sparse",
            "x": -2.0010242462158203,
            "y": 16.031606674194336
        },
        {
            "id": "s_794",
            "name": "Wei Xu 0020",
            "type": "sparse",
            "x": -0.19492940604686737,
            "y": 17.630523681640625
        },
        {
            "id": "s_795",
            "name": "Yang Liu",
            "type": "sparse",
            "x": 7.660970687866211,
            "y": 5.1178178787231445
        },
        {
            "id": "s_796",
            "name": "Rainer Splechtna",
            "type": "sparse",
            "x": -6.508127689361572,
            "y": 13.799703598022461
        },
        {
            "id": "s_797",
            "name": "James Wexler",
            "type": "sparse",
            "x": 9.91833209991455,
            "y": 10.62369441986084
        },
        {
            "id": "s_798",
            "name": "Shishi Xiao",
            "type": "sparse",
            "x": -9.737150192260742,
            "y": 17.90977668762207
        },
        {
            "id": "s_799",
            "name": "Holger Theisel",
            "type": "sparse",
            "x": -3.509281873703003,
            "y": 19.79241180419922
        },
        {
            "id": "s_800",
            "name": "Yuhua Liu",
            "type": "sparse",
            "x": -9.44919204711914,
            "y": 17.97743797302246
        },
        {
            "id": "s_801",
            "name": "Jonathan Zong",
            "type": "sparse",
            "x": -4.297783374786377,
            "y": 16.075082778930664
        },
        {
            "id": "s_802",
            "name": "Sana Malik",
            "type": "sparse",
            "x": -4.164620399475098,
            "y": 16.88210105895996
        },
        {
            "id": "s_803",
            "name": "Changjian Chen",
            "type": "sparse",
            "x": -9.232962608337402,
            "y": 17.092077255249023
        },
        {
            "id": "s_804",
            "name": "Gustavo Moreira",
            "type": "sparse",
            "x": -7.524949073791504,
            "y": -1.152193546295166
        },
        {
            "id": "s_805",
            "name": "Dani Lischinski",
            "type": "sparse",
            "x": -8.194761276245117,
            "y": 18.669586181640625
        },
        {
            "id": "s_806",
            "name": "Norine Coenen",
            "type": "sparse",
            "x": -5.743587017059326,
            "y": 18.46831512451172
        },
        {
            "id": "s_807",
            "name": "Ann Sherlock",
            "type": "sparse",
            "x": -5.951322555541992,
            "y": 17.277002334594727
        },
        {
            "id": "s_808",
            "name": "Lukas Herzberger",
            "type": "sparse",
            "x": -8.841570854187012,
            "y": 16.413036346435547
        },
        {
            "id": "s_809",
            "name": "Mi Feng",
            "type": "sparse",
            "x": -8.187718391418457,
            "y": 18.532733917236328
        },
        {
            "id": "s_810",
            "name": "Daniel Baumgartner",
            "type": "sparse",
            "x": -8.560239791870117,
            "y": 19.350488662719727
        },
        {
            "id": "s_811",
            "name": "Ji Ma",
            "type": "sparse",
            "x": -10.487530708312988,
            "y": 17.368061065673828
        },
        {
            "id": "s_812",
            "name": "Jason Cheung",
            "type": "sparse",
            "x": -3.9869070053100586,
            "y": 16.661670684814453
        },
        {
            "id": "s_813",
            "name": "Keer Lu",
            "type": "sparse",
            "x": -10.455412864685059,
            "y": 19.700759887695312
        },
        {
            "id": "s_814",
            "name": "Nicole Sultanum",
            "type": "sparse",
            "x": -6.379319667816162,
            "y": 16.729461669921875
        },
        {
            "id": "s_815",
            "name": "Ross Maciejewski",
            "type": "sparse",
            "x": -4.166878700256348,
            "y": 20.93424415588379
        },
        {
            "id": "s_816",
            "name": "Jens Schneider 0002",
            "type": "sparse",
            "x": -8.919488906860352,
            "y": 16.107276916503906
        },
        {
            "id": "s_817",
            "name": "Zheng Zhou",
            "type": "sparse",
            "x": -10.529379844665527,
            "y": 17.341720581054688
        },
        {
            "id": "s_818",
            "name": "Haichuan Lin",
            "type": "sparse",
            "x": -9.647886276245117,
            "y": 17.848085403442383
        },
        {
            "id": "s_819",
            "name": "Nico T. Mutters",
            "type": "sparse",
            "x": -5.851348876953125,
            "y": 15.339242935180664
        },
        {
            "id": "s_820",
            "name": "Jessica K. Witt",
            "type": "sparse",
            "x": -2.1867964267730713,
            "y": 18.060466766357422
        },
        {
            "id": "s_821",
            "name": "Arun Reddy Nelakurthi",
            "type": "sparse",
            "x": -4.268473148345947,
            "y": 21.090425491333008
        },
        {
            "id": "s_822",
            "name": "Helia Hosseinpour",
            "type": "sparse",
            "x": -3.396606683731079,
            "y": 17.592966079711914
        },
        {
            "id": "s_823",
            "name": "Qinghua Zheng",
            "type": "sparse",
            "x": -9.977354049682617,
            "y": 18.566404342651367
        },
        {
            "id": "s_824",
            "name": "Kalyan Veeramachaneni",
            "type": "sparse",
            "x": -10.183547019958496,
            "y": 18.402339935302734
        },
        {
            "id": "s_825",
            "name": "Shuang-Hua Yang",
            "type": "sparse",
            "x": -9.999902725219727,
            "y": 18.195049285888672
        },
        {
            "id": "s_826",
            "name": "Zhu-Tian Chen",
            "type": "sparse",
            "x": -8.751983642578125,
            "y": 16.544462203979492
        },
        {
            "id": "s_827",
            "name": "Sam Yu-Te Lee",
            "type": "sparse",
            "x": -0.707705020904541,
            "y": 17.970096588134766
        },
        {
            "id": "s_828",
            "name": "Yihan Liu",
            "type": "sparse",
            "x": -9.688215255737305,
            "y": 17.802461624145508
        },
        {
            "id": "s_829",
            "name": "Young-Ho Kim",
            "type": "sparse",
            "x": -1.3748817443847656,
            "y": 18.064762115478516
        },
        {
            "id": "s_830",
            "name": "Jeffery Chieh Liu",
            "type": "sparse",
            "x": -7.229886054992676,
            "y": 18.013973236083984
        },
        {
            "id": "s_831",
            "name": "Jinrui Wang",
            "type": "sparse",
            "x": -6.89166259765625,
            "y": 17.085988998413086
        },
        {
            "id": "s_832",
            "name": "Mengyu Chen",
            "type": "sparse",
            "x": -4.762172222137451,
            "y": 15.493599891662598
        },
        {
            "id": "s_833",
            "name": "Aimen Gaba",
            "type": "sparse",
            "x": -3.8911211490631104,
            "y": 16.49579620361328
        },
        {
            "id": "s_834",
            "name": "Youcheng Gong",
            "type": "sparse",
            "x": -9.79176139831543,
            "y": 18.26522445678711
        },
        {
            "id": "s_835",
            "name": "Micha\u00ebl Aupetit 0001",
            "type": "sparse",
            "x": -0.863434374332428,
            "y": 18.036527633666992
        },
        {
            "id": "s_836",
            "name": "Bei Wang 0001",
            "type": "sparse",
            "x": 7.386489391326904,
            "y": 5.388542175292969
        },
        {
            "id": "s_837",
            "name": "Vaishali Dhanoa",
            "type": "sparse",
            "x": -2.1821401119232178,
            "y": 15.564592361450195
        },
        {
            "id": "s_838",
            "name": "Yu Zhang 0043",
            "type": "sparse",
            "x": -10.293741226196289,
            "y": 19.604705810546875
        },
        {
            "id": "s_839",
            "name": "Takayuki Itoh",
            "type": "sparse",
            "x": -7.854986667633057,
            "y": 18.856281280517578
        },
        {
            "id": "s_840",
            "name": "Katrin Angerbauer",
            "type": "sparse",
            "x": -8.469603538513184,
            "y": 19.097951889038086
        },
        {
            "id": "s_841",
            "name": "Tom Peterka",
            "type": "sparse",
            "x": 7.461470127105713,
            "y": 5.313377857208252
        },
        {
            "id": "s_842",
            "name": "Lyn Bartram",
            "type": "sparse",
            "x": -4.225686073303223,
            "y": 16.118228912353516
        },
        {
            "id": "s_843",
            "name": "Tali Mazor",
            "type": "sparse",
            "x": -8.441413879394531,
            "y": 17.073841094970703
        },
        {
            "id": "s_844",
            "name": "Ziqi Ye",
            "type": "sparse",
            "x": -7.981767177581787,
            "y": 18.678586959838867
        },
        {
            "id": "s_845",
            "name": "Jiacheng Yu",
            "type": "sparse",
            "x": -10.405036926269531,
            "y": 19.644489288330078
        },
        {
            "id": "s_846",
            "name": "Guido Reina",
            "type": "sparse",
            "x": -8.965470314025879,
            "y": 16.222801208496094
        },
        {
            "id": "s_847",
            "name": "Michael Brudno",
            "type": "sparse",
            "x": -6.6047492027282715,
            "y": 16.813859939575195
        },
        {
            "id": "s_848",
            "name": "Yiping Sun",
            "type": "sparse",
            "x": -9.26197624206543,
            "y": 18.144826889038086
        },
        {
            "id": "s_849",
            "name": "Haoxuan Wang",
            "type": "sparse",
            "x": -9.319059371948242,
            "y": 17.759675979614258
        },
        {
            "id": "s_850",
            "name": "John T. Stasko",
            "type": "sparse",
            "x": -4.604275226593018,
            "y": 15.425344467163086
        },
        {
            "id": "s_851",
            "name": "Victor Sanh",
            "type": "sparse",
            "x": -8.815694808959961,
            "y": 16.460813522338867
        },
        {
            "id": "s_852",
            "name": "Jui-Hsien Wang",
            "type": "sparse",
            "x": -8.798980712890625,
            "y": 16.60369873046875
        },
        {
            "id": "s_853",
            "name": "Manlio Massiris Fern\u00e1ndez",
            "type": "sparse",
            "x": -6.544055938720703,
            "y": 13.857746124267578
        },
        {
            "id": "s_854",
            "name": "Heidrun Schumann",
            "type": "sparse",
            "x": -5.780698776245117,
            "y": 18.424068450927734
        },
        {
            "id": "s_855",
            "name": "Mandar Sharma",
            "type": "sparse",
            "x": -2.0030903816223145,
            "y": 15.990057945251465
        },
        {
            "id": "s_856",
            "name": "Roxana Bujack",
            "type": "sparse",
            "x": -7.2712931632995605,
            "y": 16.91916847229004
        },
        {
            "id": "s_857",
            "name": "Chenhao Yu",
            "type": "sparse",
            "x": -9.39246654510498,
            "y": 18.080453872680664
        },
        {
            "id": "s_858",
            "name": "Younghoon Kim",
            "type": "sparse",
            "x": -3.7886407375335693,
            "y": 17.101598739624023
        },
        {
            "id": "s_859",
            "name": "Jordan Matelsky",
            "type": "sparse",
            "x": -8.894730567932129,
            "y": 16.57579803466797
        },
        {
            "id": "s_860",
            "name": "Klaus Nordhausen",
            "type": "sparse",
            "x": -5.390473365783691,
            "y": 14.856634140014648
        },
        {
            "id": "s_861",
            "name": "Qian Gong",
            "type": "sparse",
            "x": -1.8465168476104736,
            "y": 17.9735164642334
        },
        {
            "id": "s_862",
            "name": "Kecheng Lu",
            "type": "sparse",
            "x": -8.186297416687012,
            "y": 18.678417205810547
        },
        {
            "id": "s_863",
            "name": "Vaishnavi Gorantla",
            "type": "sparse",
            "x": -3.845506429672241,
            "y": 17.41609001159668
        },
        {
            "id": "s_864",
            "name": "Olivier Gladin",
            "type": "sparse",
            "x": -6.285941123962402,
            "y": 18.371414184570312
        },
        {
            "id": "s_865",
            "name": "Hai-Ning Liang",
            "type": "sparse",
            "x": -6.4491987228393555,
            "y": 18.743927001953125
        },
        {
            "id": "s_866",
            "name": "Tao Lu",
            "type": "sparse",
            "x": -10.725005149841309,
            "y": 18.989168167114258
        },
        {
            "id": "s_867",
            "name": "Christina Stoiber",
            "type": "sparse",
            "x": -6.062784671783447,
            "y": 17.320091247558594
        },
        {
            "id": "s_868",
            "name": "Martin Rieth",
            "type": "sparse",
            "x": -0.7790719866752625,
            "y": 17.93819808959961
        },
        {
            "id": "s_869",
            "name": "Qi Wu 0015",
            "type": "sparse",
            "x": -0.7389376759529114,
            "y": 17.96623420715332
        },
        {
            "id": "s_870",
            "name": "Karsten Klein 0001",
            "type": "sparse",
            "x": -3.926170587539673,
            "y": 19.80088996887207
        },
        {
            "id": "s_871",
            "name": "Rostyslav Hnatyshyn",
            "type": "sparse",
            "x": -4.204840183258057,
            "y": 21.035430908203125
        },
        {
            "id": "s_872",
            "name": "Jianwei Yin",
            "type": "sparse",
            "x": -9.533666610717773,
            "y": 17.762388229370117
        },
        {
            "id": "s_873",
            "name": "Oliver R\u00fcbel",
            "type": "sparse",
            "x": 7.303729057312012,
            "y": 5.4714765548706055
        },
        {
            "id": "s_874",
            "name": "Bohyoung Kim",
            "type": "sparse",
            "x": -1.3553314208984375,
            "y": 18.077341079711914
        },
        {
            "id": "s_875",
            "name": "Alex Kale",
            "type": "sparse",
            "x": -3.427480697631836,
            "y": 16.91968536376953
        },
        {
            "id": "s_876",
            "name": "Weifeng Chen 0003",
            "type": "sparse",
            "x": -9.342232704162598,
            "y": 18.070472717285156
        },
        {
            "id": "s_877",
            "name": "Aoyu Wu",
            "type": "sparse",
            "x": -8.232725143432617,
            "y": 17.927610397338867
        },
        {
            "id": "s_878",
            "name": "Reihaneh Rabbany",
            "type": "sparse",
            "x": 10.374675750732422,
            "y": 10.175518035888672
        },
        {
            "id": "s_879",
            "name": "Yixuan Zhang 0001",
            "type": "sparse",
            "x": -6.754563808441162,
            "y": 21.144676208496094
        },
        {
            "id": "s_880",
            "name": "Arlen Fan",
            "type": "sparse",
            "x": -4.282907009124756,
            "y": 21.10702896118164
        },
        {
            "id": "s_881",
            "name": "Christopher Jermaine",
            "type": "sparse",
            "x": -3.9217851161956787,
            "y": 17.39754295349121
        },
        {
            "id": "s_882",
            "name": "Jiajing Guo",
            "type": "sparse",
            "x": -0.35189369320869446,
            "y": 17.6314640045166
        },
        {
            "id": "s_883",
            "name": "Junpeng Wang 0001",
            "type": "sparse",
            "x": 7.592033386230469,
            "y": 5.183665752410889
        },
        {
            "id": "s_884",
            "name": "Jie Liu 0046",
            "type": "sparse",
            "x": -7.51871395111084,
            "y": 17.87839126586914
        },
        {
            "id": "s_885",
            "name": "Minsuk Chang",
            "type": "sparse",
            "x": 9.921062469482422,
            "y": 10.62087631225586
        },
        {
            "id": "s_886",
            "name": "Shaun J. Canavan",
            "type": "sparse",
            "x": -1.8682833909988403,
            "y": 17.932666778564453
        },
        {
            "id": "s_887",
            "name": "Jiachen Wang",
            "type": "sparse",
            "x": -10.256482124328613,
            "y": 17.556751251220703
        },
        {
            "id": "s_888",
            "name": "Mar\u00eda Virginia Sabando",
            "type": "sparse",
            "x": -5.728916168212891,
            "y": 19.529178619384766
        },
        {
            "id": "s_889",
            "name": "Roberta C. Ramos Mota",
            "type": "sparse",
            "x": -7.501023769378662,
            "y": -1.1761349439620972
        },
        {
            "id": "s_890",
            "name": "Scott Klasky",
            "type": "sparse",
            "x": -1.8688675165176392,
            "y": 17.963056564331055
        },
        {
            "id": "s_891",
            "name": "Mennatallah El-Assady",
            "type": "sparse",
            "x": -4.977645397186279,
            "y": 15.330939292907715
        },
        {
            "id": "s_892",
            "name": "Yifan Sun 0002",
            "type": "sparse",
            "x": -6.6701202392578125,
            "y": 21.238149642944336
        },
        {
            "id": "s_893",
            "name": "Ehud Sharlin",
            "type": "sparse",
            "x": -7.504776954650879,
            "y": -1.172392725944519
        },
        {
            "id": "s_894",
            "name": "Yafeng Lu",
            "type": "sparse",
            "x": -9.286263465881348,
            "y": 17.220386505126953
        },
        {
            "id": "s_895",
            "name": "David Bauer",
            "type": "sparse",
            "x": -0.7302836179733276,
            "y": 17.987770080566406
        },
        {
            "id": "s_896",
            "name": "Michael Will",
            "type": "sparse",
            "x": -3.946135997772217,
            "y": 20.851778030395508
        },
        {
            "id": "s_897",
            "name": "Furui Cheng",
            "type": "sparse",
            "x": -10.26854133605957,
            "y": 17.634262084960938
        },
        {
            "id": "s_898",
            "name": "Davide Ceneda",
            "type": "sparse",
            "x": -5.230187892913818,
            "y": 15.04636287689209
        },
        {
            "id": "s_899",
            "name": "Michael Sedlmair",
            "type": "sparse",
            "x": -8.39600658416748,
            "y": 18.82501792907715
        },
        {
            "id": "s_900",
            "name": "Yizhi Zhang",
            "type": "sparse",
            "x": -10.823287963867188,
            "y": 18.2213191986084
        },
        {
            "id": "s_901",
            "name": "Steven R. Brandt",
            "type": "sparse",
            "x": -0.23109371960163116,
            "y": 17.094738006591797
        },
        {
            "id": "s_902",
            "name": "Kevin G. Yager",
            "type": "sparse",
            "x": -0.21092596650123596,
            "y": 17.620559692382812
        },
        {
            "id": "s_903",
            "name": "Xueying Wang",
            "type": "sparse",
            "x": -8.877176284790039,
            "y": 16.562171936035156
        },
        {
            "id": "s_904",
            "name": "Qi Sun 0003",
            "type": "sparse",
            "x": -4.0314788818359375,
            "y": 20.069955825805664
        },
        {
            "id": "s_905",
            "name": "Connor Guerin",
            "type": "sparse",
            "x": 10.291916847229004,
            "y": 10.256114959716797
        },
        {
            "id": "s_906",
            "name": "Shilong Liu",
            "type": "sparse",
            "x": -8.994132041931152,
            "y": 16.893381118774414
        },
        {
            "id": "s_907",
            "name": "A. Stewart Fotheringham",
            "type": "sparse",
            "x": -4.268448352813721,
            "y": 21.105592727661133
        },
        {
            "id": "s_908",
            "name": "Chen Chen 0080",
            "type": "sparse",
            "x": -7.994290828704834,
            "y": 18.50909423828125
        },
        {
            "id": "s_909",
            "name": "Jagoda Walny",
            "type": "sparse",
            "x": -6.20914888381958,
            "y": 17.657947540283203
        },
        {
            "id": "s_910",
            "name": "Ayan Biswas",
            "type": "sparse",
            "x": 7.5522685050964355,
            "y": 5.222675323486328
        },
        {
            "id": "s_911",
            "name": "Xinke Wu",
            "type": "sparse",
            "x": -10.229264259338379,
            "y": 17.217052459716797
        },
        {
            "id": "s_912",
            "name": "Chen Shi",
            "type": "sparse",
            "x": -9.389076232910156,
            "y": 17.794673919677734
        },
        {
            "id": "s_913",
            "name": "Jiayun Fu",
            "type": "sparse",
            "x": -10.851838111877441,
            "y": 18.26422691345215
        },
        {
            "id": "s_914",
            "name": "Julio Daniel Silva",
            "type": "sparse",
            "x": -7.498447895050049,
            "y": -1.1787818670272827
        },
        {
            "id": "s_915",
            "name": "Zicheng Wang",
            "type": "sparse",
            "x": -7.821394920349121,
            "y": 18.956331253051758
        },
        {
            "id": "s_916",
            "name": "Peter Rottmann",
            "type": "sparse",
            "x": -5.586359977722168,
            "y": 15.069317817687988
        },
        {
            "id": "s_917",
            "name": "Doris Kosminsky",
            "type": "sparse",
            "x": -6.083252906799316,
            "y": 17.367725372314453
        },
        {
            "id": "s_918",
            "name": "Kun-Ting Chen",
            "type": "sparse",
            "x": -7.255850315093994,
            "y": 17.51283836364746
        },
        {
            "id": "s_919",
            "name": "Connor Wilson",
            "type": "sparse",
            "x": -6.755430698394775,
            "y": 21.13630485534668
        },
        {
            "id": "s_920",
            "name": "Arvind Satyanarayan",
            "type": "sparse",
            "x": -4.316587448120117,
            "y": 16.09906005859375
        },
        {
            "id": "s_921",
            "name": "Daniel Atzberger",
            "type": "sparse",
            "x": -8.930935859680176,
            "y": 18.418153762817383
        },
        {
            "id": "s_922",
            "name": "Ziqin Luo",
            "type": "sparse",
            "x": -9.994213104248047,
            "y": 19.544984817504883
        },
        {
            "id": "s_923",
            "name": "Tianhong Ding",
            "type": "sparse",
            "x": -10.279363632202148,
            "y": 19.620222091674805
        },
        {
            "id": "s_924",
            "name": "S\u00e9rgio M. Marques",
            "type": "sparse",
            "x": -5.773497104644775,
            "y": 19.477746963500977
        },
        {
            "id": "s_925",
            "name": "Jiaying Lu 0005",
            "type": "sparse",
            "x": -9.356849670410156,
            "y": 18.049663543701172
        },
        {
            "id": "s_926",
            "name": "Sebastian Mazza",
            "type": "sparse",
            "x": -6.241617679595947,
            "y": 18.99557876586914
        },
        {
            "id": "s_927",
            "name": "Spencer C. Castro",
            "type": "sparse",
            "x": -3.3907642364501953,
            "y": 17.574241638183594
        },
        {
            "id": "s_928",
            "name": "Tianqi Song",
            "type": "sparse",
            "x": -10.579242706298828,
            "y": 17.67232894897461
        },
        {
            "id": "s_929",
            "name": "Xiyao Wang",
            "type": "sparse",
            "x": -6.170058250427246,
            "y": 17.421009063720703
        },
        {
            "id": "s_930",
            "name": "Elham Sakhaee",
            "type": "sparse",
            "x": -1.8344606161117554,
            "y": 17.915407180786133
        },
        {
            "id": "s_931",
            "name": "Yuchen Wu",
            "type": "sparse",
            "x": -10.969093322753906,
            "y": 19.239049911499023
        },
        {
            "id": "s_932",
            "name": "Peiran Ren",
            "type": "sparse",
            "x": -10.294900894165039,
            "y": 17.12957191467285
        },
        {
            "id": "s_933",
            "name": "Lixia Jin",
            "type": "sparse",
            "x": -11.031846046447754,
            "y": 19.26256561279297
        },
        {
            "id": "s_934",
            "name": "Frank Elavsky",
            "type": "sparse",
            "x": -3.843191385269165,
            "y": 17.38551139831543
        },
        {
            "id": "s_935",
            "name": "Jinlu Yu",
            "type": "sparse",
            "x": -8.91883659362793,
            "y": 19.531658172607422
        },
        {
            "id": "s_936",
            "name": "Monique Meuschke",
            "type": "sparse",
            "x": -3.532221555709839,
            "y": 19.80658721923828
        },
        {
            "id": "s_937",
            "name": "Thibaud Porphyre",
            "type": "sparse",
            "x": -7.290333271026611,
            "y": 16.86314582824707
        },
        {
            "id": "s_938",
            "name": "Ewart Mark Haacke",
            "type": "sparse",
            "x": -11.003124237060547,
            "y": 16.86897850036621
        },
        {
            "id": "s_939",
            "name": "Hongguang Xiao",
            "type": "sparse",
            "x": -10.427313804626465,
            "y": 17.319326400756836
        },
        {
            "id": "s_940",
            "name": "Abhraneel Sarma",
            "type": "sparse",
            "x": -3.3513388633728027,
            "y": 16.81730079650879
        },
        {
            "id": "s_941",
            "name": "Yuxuan Hou",
            "type": "sparse",
            "x": -9.402530670166016,
            "y": 18.080066680908203
        },
        {
            "id": "s_942",
            "name": "Steven Franconeri",
            "type": "sparse",
            "x": -3.320467710494995,
            "y": 16.54072380065918
        },
        {
            "id": "s_943",
            "name": "Zhen Li 0044",
            "type": "sparse",
            "x": -9.260612487792969,
            "y": 17.233572006225586
        },
        {
            "id": "s_944",
            "name": "Noel F. C. C. de Miranda",
            "type": "sparse",
            "x": -6.797321796417236,
            "y": 14.296780586242676
        },
        {
            "id": "s_945",
            "name": "Robert Judson-Torres",
            "type": "sparse",
            "x": -2.2465174198150635,
            "y": 15.38310718536377
        },
        {
            "id": "s_946",
            "name": "Kresimir Matkovic",
            "type": "sparse",
            "x": -6.530787944793701,
            "y": 13.84750747680664
        },
        {
            "id": "s_947",
            "name": "Youngtaek Kim",
            "type": "sparse",
            "x": -1.3634858131408691,
            "y": 18.06663703918457
        },
        {
            "id": "s_948",
            "name": "Sagar Buch",
            "type": "sparse",
            "x": -11.002050399780273,
            "y": 16.86958122253418
        },
        {
            "id": "s_949",
            "name": "Stefan Zellmann",
            "type": "sparse",
            "x": -15.019813537597656,
            "y": 7.15451192855835
        },
        {
            "id": "s_950",
            "name": "Francesca Morini",
            "type": "sparse",
            "x": -6.581318378448486,
            "y": 17.090076446533203
        },
        {
            "id": "s_951",
            "name": "Bhavana Doppalapudi",
            "type": "sparse",
            "x": -2.001880407333374,
            "y": 18.021408081054688
        },
        {
            "id": "s_952",
            "name": "Jaeyoung Kim",
            "type": "sparse",
            "x": -1.3857319355010986,
            "y": 18.054899215698242
        },
        {
            "id": "s_953",
            "name": "Maximilian T. Fischer",
            "type": "sparse",
            "x": -7.025144100189209,
            "y": 20.881271362304688
        },
        {
            "id": "s_954",
            "name": "Xinli Hou",
            "type": "sparse",
            "x": -4.691896438598633,
            "y": 16.206737518310547
        },
        {
            "id": "s_955",
            "name": "Yida Chen",
            "type": "sparse",
            "x": -10.473893165588379,
            "y": 17.86050033569336
        },
        {
            "id": "s_956",
            "name": "Prateek Mantri",
            "type": "sparse",
            "x": -3.6908209323883057,
            "y": 16.50154685974121
        },
        {
            "id": "s_957",
            "name": "Rita Sevastjanova",
            "type": "sparse",
            "x": -5.126667022705078,
            "y": 15.112526893615723
        },
        {
            "id": "s_958",
            "name": "Pascal Goffin",
            "type": "sparse",
            "x": -2.2810611724853516,
            "y": 15.338166236877441
        },
        {
            "id": "s_959",
            "name": "Sonia Castelo",
            "type": "sparse",
            "x": -4.084847927093506,
            "y": 20.04085922241211
        },
        {
            "id": "s_960",
            "name": "Lorenz Hurni",
            "type": "sparse",
            "x": -7.1903300285339355,
            "y": 18.037052154541016
        },
        {
            "id": "s_961",
            "name": "Wai Tong",
            "type": "sparse",
            "x": -7.6730875968933105,
            "y": 17.954906463623047
        },
        {
            "id": "s_962",
            "name": "Ghulam Jilani Quadri",
            "type": "sparse",
            "x": -1.9230878353118896,
            "y": 18.020376205444336
        },
        {
            "id": "s_963",
            "name": "Andr\u00e9s Lalama",
            "type": "sparse",
            "x": -9.869441986083984,
            "y": 16.95114517211914
        },
        {
            "id": "s_964",
            "name": "Mingwei Li",
            "type": "sparse",
            "x": -4.75797700881958,
            "y": 15.985830307006836
        },
        {
            "id": "s_965",
            "name": "Chenyang Zhang 0002",
            "type": "sparse",
            "x": -10.885151863098145,
            "y": 19.10888671875
        },
        {
            "id": "s_966",
            "name": "Rahul Duggal",
            "type": "sparse",
            "x": 10.018819808959961,
            "y": 10.518260955810547
        },
        {
            "id": "s_967",
            "name": "Danni Liu",
            "type": "sparse",
            "x": -3.407642364501953,
            "y": 16.931093215942383
        },
        {
            "id": "s_968",
            "name": "Gerik Scheuermann",
            "type": "sparse",
            "x": -7.247078895568848,
            "y": 16.90873146057129
        },
        {
            "id": "s_969",
            "name": "Cristina Dondi",
            "type": "sparse",
            "x": -7.096705436706543,
            "y": 16.83481788635254
        },
        {
            "id": "s_970",
            "name": "Nathalie Henry Riche",
            "type": "sparse",
            "x": -6.618730068206787,
            "y": 16.888065338134766
        },
        {
            "id": "s_971",
            "name": "Fan Yang",
            "type": "sparse",
            "x": -4.299758434295654,
            "y": 21.08617401123047
        },
        {
            "id": "s_972",
            "name": "Yiwen Xing",
            "type": "sparse",
            "x": -7.084519386291504,
            "y": 16.81534767150879
        },
        {
            "id": "s_973",
            "name": "Christoph Muehlmann",
            "type": "sparse",
            "x": -5.410940170288086,
            "y": 14.853527069091797
        },
        {
            "id": "s_974",
            "name": "Chengqiao Lin",
            "type": "sparse",
            "x": -9.287650108337402,
            "y": 17.986328125
        },
        {
            "id": "s_975",
            "name": "Liwen Xu",
            "type": "sparse",
            "x": -11.078909873962402,
            "y": 19.365869522094727
        },
        {
            "id": "s_976",
            "name": "David Gotz",
            "type": "sparse",
            "x": -8.96853256225586,
            "y": 19.80609703063965
        },
        {
            "id": "s_977",
            "name": "Jakob Troidl",
            "type": "sparse",
            "x": -8.719244003295898,
            "y": 16.444658279418945
        },
        {
            "id": "s_978",
            "name": "Sheelagh Carpendale",
            "type": "sparse",
            "x": -6.350558757781982,
            "y": 17.638059616088867
        },
        {
            "id": "s_979",
            "name": "Zengsheng Zhong",
            "type": "sparse",
            "x": -9.198824882507324,
            "y": 17.68614959716797
        },
        {
            "id": "s_980",
            "name": "Sanjin Rados",
            "type": "sparse",
            "x": -6.532253265380859,
            "y": 13.845574378967285
        },
        {
            "id": "s_981",
            "name": "Boyang Xie",
            "type": "sparse",
            "x": -10.202784538269043,
            "y": 17.18288803100586
        },
        {
            "id": "s_982",
            "name": "Jeffrey Riedmiller",
            "type": "sparse",
            "x": -3.9823951721191406,
            "y": 16.668607711791992
        },
        {
            "id": "s_983",
            "name": "Cristina Morariu",
            "type": "sparse",
            "x": -8.378267288208008,
            "y": 18.79620933532715
        },
        {
            "id": "s_984",
            "name": "Paula Kayongo",
            "type": "sparse",
            "x": -3.362746477127075,
            "y": 16.79219627380371
        },
        {
            "id": "s_985",
            "name": "Samuel Huron",
            "type": "sparse",
            "x": -6.070545673370361,
            "y": 17.34415626525879
        },
        {
            "id": "s_986",
            "name": "Hanqi Guo 0001",
            "type": "sparse",
            "x": 7.433459281921387,
            "y": 5.341292381286621
        },
        {
            "id": "s_987",
            "name": "Theresa Anisja Harbig",
            "type": "sparse",
            "x": -8.43828010559082,
            "y": 17.04737663269043
        },
        {
            "id": "s_988",
            "name": "Panagiotis D. Ritsos",
            "type": "sparse",
            "x": -2.06898832321167,
            "y": 16.06472396850586
        },
        {
            "id": "s_989",
            "name": "Nan Cao 0001",
            "type": "sparse",
            "x": -9.193487167358398,
            "y": 19.71310806274414
        },
        {
            "id": "s_990",
            "name": "J\u00fcrgen Bernard",
            "type": "sparse",
            "x": -8.59127140045166,
            "y": 18.6314754486084
        },
        {
            "id": "s_991",
            "name": "Jiajun Zhu",
            "type": "sparse",
            "x": -10.139730453491211,
            "y": 17.204265594482422
        },
        {
            "id": "s_992",
            "name": "Yuanyuan Tang",
            "type": "sparse",
            "x": -10.881905555725098,
            "y": 18.291955947875977
        },
        {
            "id": "s_993",
            "name": "Meng Ling",
            "type": "sparse",
            "x": -6.290825843811035,
            "y": 18.380596160888672
        },
        {
            "id": "s_994",
            "name": "Graham Kosiba",
            "type": "sparse",
            "x": -15.027462005615234,
            "y": 7.146845817565918
        },
        {
            "id": "s_995",
            "name": "Zherui Zhang",
            "type": "sparse",
            "x": -4.280335903167725,
            "y": 21.110708236694336
        },
        {
            "id": "s_996",
            "name": "Hendrik Strobelt",
            "type": "sparse",
            "x": -8.813722610473633,
            "y": 16.482526779174805
        },
        {
            "id": "s_997",
            "name": "Zhonghua Lu",
            "type": "sparse",
            "x": 7.562093734741211,
            "y": 5.21349573135376
        },
        {
            "id": "s_998",
            "name": "Sarah Nason",
            "type": "sparse",
            "x": -5.9507737159729,
            "y": 17.276994705200195
        },
        {
            "id": "s_999",
            "name": "Peter Gyory",
            "type": "sparse",
            "x": -2.1690523624420166,
            "y": 18.045560836791992
        },
        {
            "id": "s_1000",
            "name": "Yu Liu",
            "type": "sparse",
            "x": -9.380688667297363,
            "y": 19.6751708984375
        },
        {
            "id": "s_1001",
            "name": "Jan Byska",
            "type": "sparse",
            "x": -5.7382683753967285,
            "y": 19.513647079467773
        },
        {
            "id": "s_1002",
            "name": "Kexin Huang",
            "type": "sparse",
            "x": -8.407233238220215,
            "y": 17.092575073242188
        },
        {
            "id": "s_1003",
            "name": "Peter Rautek",
            "type": "sparse",
            "x": -8.853841781616211,
            "y": 16.40580940246582
        },
        {
            "id": "s_1004",
            "name": "Xiaomeng Fan",
            "type": "sparse",
            "x": -10.936405181884766,
            "y": 19.194181442260742
        },
        {
            "id": "s_1005",
            "name": "Tom Horak",
            "type": "sparse",
            "x": -5.773400783538818,
            "y": 18.46847152709961
        },
        {
            "id": "s_1006",
            "name": "Cody Dunne",
            "type": "sparse",
            "x": -6.736438751220703,
            "y": 21.160825729370117
        },
        {
            "id": "s_1007",
            "name": "Xiaoyu Zhang 0014",
            "type": "sparse",
            "x": -0.609981894493103,
            "y": 17.798912048339844
        },
        {
            "id": "s_1008",
            "name": "Magnus Heitzler",
            "type": "sparse",
            "x": -7.224942207336426,
            "y": 18.022653579711914
        },
        {
            "id": "s_1009",
            "name": "Weixing Lin",
            "type": "sparse",
            "x": -9.420028686523438,
            "y": 17.844587326049805
        },
        {
            "id": "s_1010",
            "name": "Weiwei Cui",
            "type": "sparse",
            "x": -10.554813385009766,
            "y": 17.659029006958008
        },
        {
            "id": "s_1011",
            "name": "Zac Lucarelli",
            "type": "sparse",
            "x": -7.451108932495117,
            "y": 17.847728729248047
        },
        {
            "id": "s_1012",
            "name": "Md Montaser Hamid",
            "type": "sparse",
            "x": 9.971508026123047,
            "y": 10.56988525390625
        },
        {
            "id": "s_1013",
            "name": "Stefan Bruckner",
            "type": "sparse",
            "x": -5.912600994110107,
            "y": 19.274126052856445
        },
        {
            "id": "s_1014",
            "name": "Hang Su 0006",
            "type": "sparse",
            "x": -9.039877891540527,
            "y": 16.952985763549805
        },
        {
            "id": "s_1015",
            "name": "Marcos Lage",
            "type": "sparse",
            "x": -7.507899761199951,
            "y": -1.1693058013916016
        },
        {
            "id": "s_1016",
            "name": "Tatiana Losev",
            "type": "sparse",
            "x": -6.080733299255371,
            "y": 17.35956382751465
        },
        {
            "id": "s_1017",
            "name": "Bo Ma 0002",
            "type": "sparse",
            "x": -1.8128607273101807,
            "y": 17.889450073242188
        },
        {
            "id": "s_1018",
            "name": "Hamza Afzaal",
            "type": "sparse",
            "x": -7.508872032165527,
            "y": -1.1683030128479004
        },
        {
            "id": "s_1019",
            "name": "Haoran Jiang",
            "type": "sparse",
            "x": -11.088623046875,
            "y": 19.35671615600586
        },
        {
            "id": "s_1020",
            "name": "Rui Zhang",
            "type": "sparse",
            "x": -8.469386100769043,
            "y": 17.271446228027344
        },
        {
            "id": "s_1021",
            "name": "Barrett Ens",
            "type": "sparse",
            "x": -7.464419364929199,
            "y": 17.827678680419922
        },
        {
            "id": "s_1022",
            "name": "Annemieke Verbraeck",
            "type": "sparse",
            "x": -6.872686386108398,
            "y": 14.374115943908691
        },
        {
            "id": "s_1023",
            "name": "Johannes Ellemose",
            "type": "sparse",
            "x": -2.023926019668579,
            "y": 15.992884635925293
        },
        {
            "id": "s_1024",
            "name": "Alexander Bendeck",
            "type": "sparse",
            "x": -4.511322975158691,
            "y": 15.333175659179688
        },
        {
            "id": "s_1025",
            "name": "Leilani Battle",
            "type": "sparse",
            "x": -4.00829553604126,
            "y": 17.026119232177734
        },
        {
            "id": "s_1026",
            "name": "Steven L. Franconeri",
            "type": "sparse",
            "x": -3.5328080654144287,
            "y": 17.32074546813965
        },
        {
            "id": "s_1027",
            "name": "Arpit Narechania",
            "type": "sparse",
            "x": -4.690298557281494,
            "y": 15.35843563079834
        },
        {
            "id": "s_1028",
            "name": "Xiaoran Yan",
            "type": "sparse",
            "x": -9.849148750305176,
            "y": 18.411666870117188
        },
        {
            "id": "s_1029",
            "name": "My T. Thai",
            "type": "sparse",
            "x": -4.325807571411133,
            "y": 21.124147415161133
        },
        {
            "id": "s_1030",
            "name": "Douglas Markant",
            "type": "sparse",
            "x": -4.717983722686768,
            "y": 15.39806842803955
        },
        {
            "id": "s_1031",
            "name": "Matthew Kay 0001",
            "type": "sparse",
            "x": -3.1653590202331543,
            "y": 16.62369155883789
        },
        {
            "id": "s_1032",
            "name": "Marcel Wunderlich",
            "type": "sparse",
            "x": -5.845585823059082,
            "y": 15.33279800415039
        },
        {
            "id": "s_1033",
            "name": "Eduard Gr\u00f6ller",
            "type": "sparse",
            "x": -8.591498374938965,
            "y": 16.26411247253418
        },
        {
            "id": "s_1034",
            "name": "Lijie Yao",
            "type": "sparse",
            "x": -6.286887168884277,
            "y": 18.3721923828125
        },
        {
            "id": "s_1035",
            "name": "Ronghua Shi",
            "type": "sparse",
            "x": -9.2078857421875,
            "y": 17.67098045349121
        },
        {
            "id": "s_1036",
            "name": "Le Liu",
            "type": "sparse",
            "x": -10.40074634552002,
            "y": 19.66013526916504
        },
        {
            "id": "s_1037",
            "name": "Michael Krone",
            "type": "sparse",
            "x": -3.9320409297943115,
            "y": 19.969980239868164
        },
        {
            "id": "s_1038",
            "name": "Alex Bigelow",
            "type": "sparse",
            "x": -0.25023993849754333,
            "y": 17.144601821899414
        },
        {
            "id": "s_1039",
            "name": "Stefan Gumhold",
            "type": "sparse",
            "x": -5.772637367248535,
            "y": 18.48914337158203
        },
        {
            "id": "s_1040",
            "name": "Adam Stefkovics",
            "type": "sparse",
            "x": -8.756529808044434,
            "y": 16.44931411743164
        },
        {
            "id": "s_1041",
            "name": "Gunther H. Weber",
            "type": "sparse",
            "x": 7.313407897949219,
            "y": 5.463132381439209
        },
        {
            "id": "s_1042",
            "name": "Marinka Zitnik",
            "type": "sparse",
            "x": -8.400671005249023,
            "y": 17.151945114135742
        },
        {
            "id": "s_1043",
            "name": "Nanxiang Li",
            "type": "sparse",
            "x": -0.3820408880710602,
            "y": 17.541778564453125
        },
        {
            "id": "s_1044",
            "name": "Scott Davidoff",
            "type": "sparse",
            "x": -0.723167896270752,
            "y": 17.944580078125
        },
        {
            "id": "s_1045",
            "name": "Zhenhua Xu",
            "type": "sparse",
            "x": -9.481412887573242,
            "y": 17.289928436279297
        },
        {
            "id": "s_1046",
            "name": "Yi Sun",
            "type": "sparse",
            "x": -10.120345115661621,
            "y": 19.709739685058594
        },
        {
            "id": "s_1047",
            "name": "Li Chen 0031",
            "type": "sparse",
            "x": -10.951629638671875,
            "y": 19.412033081054688
        },
        {
            "id": "s_1048",
            "name": "Xiaonan Luo",
            "type": "sparse",
            "x": -9.76258659362793,
            "y": 17.140453338623047
        },
        {
            "id": "s_1049",
            "name": "Huixuan Xie",
            "type": "sparse",
            "x": -9.184131622314453,
            "y": 17.536436080932617
        },
        {
            "id": "s_1050",
            "name": "Goran Todorovic",
            "type": "sparse",
            "x": -6.524535179138184,
            "y": 13.820906639099121
        },
        {
            "id": "s_1051",
            "name": "Daniel Patel",
            "type": "sparse",
            "x": -6.235623836517334,
            "y": 18.993988037109375
        },
        {
            "id": "s_1052",
            "name": "Katy Williams",
            "type": "sparse",
            "x": -0.18562760949134827,
            "y": 17.08089256286621
        },
        {
            "id": "s_1053",
            "name": "Ye Zhao 0003",
            "type": "sparse",
            "x": -0.22373057901859283,
            "y": 17.61713218688965
        },
        {
            "id": "s_1054",
            "name": "Linhao Meng",
            "type": "sparse",
            "x": -6.972940444946289,
            "y": 14.457308769226074
        },
        {
            "id": "s_1055",
            "name": "Xinyi He",
            "type": "sparse",
            "x": -10.49372386932373,
            "y": 18.34697723388672
        },
        {
            "id": "s_1056",
            "name": "Matt-Heun Hong",
            "type": "sparse",
            "x": -2.1474270820617676,
            "y": 18.064579010009766
        },
        {
            "id": "s_1057",
            "name": "Margr\u00e9t Vilborg Bjarnad\u00f3ttir",
            "type": "sparse",
            "x": -6.308945655822754,
            "y": 18.378318786621094
        },
        {
            "id": "s_1058",
            "name": "Yuan Cui",
            "type": "sparse",
            "x": -3.0702109336853027,
            "y": 16.545682907104492
        },
        {
            "id": "s_1059",
            "name": "Guido Tack",
            "type": "sparse",
            "x": -7.554339408874512,
            "y": 17.925874710083008
        },
        {
            "id": "s_1060",
            "name": "Khaled A. Al-Thelaya",
            "type": "sparse",
            "x": -8.912983894348145,
            "y": 16.133142471313477
        },
        {
            "id": "s_1061",
            "name": "Philip Berger",
            "type": "sparse",
            "x": -5.760557651519775,
            "y": 18.46953582763672
        },
        {
            "id": "s_1062",
            "name": "Denis Gracanin",
            "type": "sparse",
            "x": -6.513533592224121,
            "y": 13.804177284240723
        },
        {
            "id": "s_1063",
            "name": "Jianping Kelvin Li",
            "type": "sparse",
            "x": -0.781344473361969,
            "y": 17.945505142211914
        },
        {
            "id": "s_1064",
            "name": "Aniketh Venkat",
            "type": "sparse",
            "x": -15.035354614257812,
            "y": 7.138950824737549
        },
        {
            "id": "s_1065",
            "name": "Mu Fan",
            "type": "sparse",
            "x": -10.331381797790527,
            "y": 17.213804244995117
        },
        {
            "id": "s_1066",
            "name": "Ratanond Koonchanok",
            "type": "sparse",
            "x": -2.1947779655456543,
            "y": 18.031055450439453
        },
        {
            "id": "s_1067",
            "name": "Julie Delon",
            "type": "sparse",
            "x": -4.035627365112305,
            "y": 20.69490623474121
        },
        {
            "id": "s_1068",
            "name": "Christoph Garth",
            "type": "sparse",
            "x": -4.054912567138672,
            "y": 20.883195877075195
        },
        {
            "id": "s_1069",
            "name": "Haochao Ying",
            "type": "sparse",
            "x": -9.838674545288086,
            "y": 18.423315048217773
        },
        {
            "id": "s_1070",
            "name": "Kai Xiong",
            "type": "sparse",
            "x": -10.298222541809082,
            "y": 17.334793090820312
        },
        {
            "id": "s_1071",
            "name": "Michail Schwab",
            "type": "sparse",
            "x": -6.730096817016602,
            "y": 21.168636322021484
        },
        {
            "id": "s_1072",
            "name": "Minfeng Zhu 0001",
            "type": "sparse",
            "x": -9.972212791442871,
            "y": 17.776002883911133
        },
        {
            "id": "s_1073",
            "name": "Zeng Dai",
            "type": "sparse",
            "x": -0.3699510395526886,
            "y": 17.54452133178711
        },
        {
            "id": "s_1074",
            "name": "Nicola Pezzotti",
            "type": "sparse",
            "x": -6.978562831878662,
            "y": 14.461797714233398
        },
        {
            "id": "s_1075",
            "name": "Meng Xia 0002",
            "type": "sparse",
            "x": -10.159101486206055,
            "y": 18.2906494140625
        },
        {
            "id": "s_1076",
            "name": "Hui Huang 0004",
            "type": "sparse",
            "x": -8.069344520568848,
            "y": 18.69196128845215
        },
        {
            "id": "s_1077",
            "name": "Tarik Crnovrsanin",
            "type": "sparse",
            "x": -0.8258193135261536,
            "y": 17.894298553466797
        },
        {
            "id": "s_1078",
            "name": "Max Mahdi Roozbahani",
            "type": "sparse",
            "x": 10.285959243774414,
            "y": 10.262150764465332
        },
        {
            "id": "s_1079",
            "name": "Mandy Keck",
            "type": "sparse",
            "x": -6.079066276550293,
            "y": 17.345111846923828
        },
        {
            "id": "s_1080",
            "name": "Khairi Reda",
            "type": "sparse",
            "x": -2.21368670463562,
            "y": 18.03677749633789
        },
        {
            "id": "s_1081",
            "name": "Max Sondag",
            "type": "sparse",
            "x": -5.960926532745361,
            "y": 15.661661148071289
        },
        {
            "id": "s_1082",
            "name": "Xinyue Xu",
            "type": "sparse",
            "x": -9.340376853942871,
            "y": 19.724803924560547
        },
        {
            "id": "s_1083",
            "name": "Nathan van Beusekom",
            "type": "sparse",
            "x": -5.946002960205078,
            "y": 15.679059028625488
        },
        {
            "id": "s_1084",
            "name": "Usman R. Alim",
            "type": "sparse",
            "x": -7.535691738128662,
            "y": -1.1414257287979126
        },
        {
            "id": "s_1085",
            "name": "Lanxi Xiao",
            "type": "sparse",
            "x": -9.326310157775879,
            "y": 17.237693786621094
        },
        {
            "id": "s_1086",
            "name": "Daniel de Oliveira 0001",
            "type": "sparse",
            "x": -7.503316879272461,
            "y": -1.1738789081573486
        },
        {
            "id": "s_1087",
            "name": "Jiang Wu",
            "type": "sparse",
            "x": -10.058709144592285,
            "y": 17.315996170043945
        },
        {
            "id": "s_1088",
            "name": "Xiaohan Jiao",
            "type": "sparse",
            "x": -9.295489311218262,
            "y": 19.79011344909668
        },
        {
            "id": "s_1089",
            "name": "Chuhan Zhang",
            "type": "sparse",
            "x": -10.405718803405762,
            "y": 17.150562286376953
        },
        {
            "id": "s_1090",
            "name": "Juho Kim",
            "type": "sparse",
            "x": -4.1073784828186035,
            "y": 16.229312896728516
        },
        {
            "id": "s_1091",
            "name": "Keshav Dasu",
            "type": "sparse",
            "x": -0.7451569437980652,
            "y": 17.960569381713867
        },
        {
            "id": "s_1092",
            "name": "Kento Shigyo",
            "type": "sparse",
            "x": -10.162421226501465,
            "y": 18.365835189819336
        },
        {
            "id": "s_1093",
            "name": "Peter Xenopoulos",
            "type": "sparse",
            "x": -4.014463901519775,
            "y": 20.133638381958008
        },
        {
            "id": "s_1094",
            "name": "Conny Walchshofer",
            "type": "sparse",
            "x": -2.2252469062805176,
            "y": 15.432352066040039
        },
        {
            "id": "s_1095",
            "name": "Wenbin He",
            "type": "sparse",
            "x": -0.3329527676105499,
            "y": 17.56449317932129
        },
        {
            "id": "s_1096",
            "name": "Eddie Polanco",
            "type": "sparse",
            "x": -2.230541706085205,
            "y": 15.402384757995605
        },
        {
            "id": "s_1097",
            "name": "Jieqiong Zhao",
            "type": "sparse",
            "x": -4.246099948883057,
            "y": 21.07543182373047
        },
        {
            "id": "s_1098",
            "name": "Sijia Wang",
            "type": "sparse",
            "x": -9.587471008300781,
            "y": 18.007911682128906
        },
        {
            "id": "s_1099",
            "name": "Jim Smiley",
            "type": "sparse",
            "x": -7.439794540405273,
            "y": 17.87795639038086
        },
        {
            "id": "s_1100",
            "name": "Daniel A. Keim",
            "type": "sparse",
            "x": -6.8900604248046875,
            "y": 21.007492065429688
        },
        {
            "id": "s_1101",
            "name": "Jan Aerts",
            "type": "sparse",
            "x": -6.067625522613525,
            "y": 17.366289138793945
        },
        {
            "id": "s_1102",
            "name": "Fei Nie",
            "type": "sparse",
            "x": -10.928980827331543,
            "y": 18.97296142578125
        },
        {
            "id": "s_1103",
            "name": "Md. Naimul Hoque",
            "type": "sparse",
            "x": -1.8694349527359009,
            "y": 16.125198364257812
        },
        {
            "id": "s_1104",
            "name": "Fred Hohman",
            "type": "sparse",
            "x": -3.86318302154541,
            "y": 17.415218353271484
        },
        {
            "id": "s_1105",
            "name": "Niklas Metzger 0001",
            "type": "sparse",
            "x": -5.740808010101318,
            "y": 18.470794677734375
        },
        {
            "id": "s_1106",
            "name": "Peter Lindstrom 0001",
            "type": "sparse",
            "x": -15.078049659729004,
            "y": 7.096274375915527
        },
        {
            "id": "s_1107",
            "name": "Zhihan Jiang",
            "type": "sparse",
            "x": -9.340656280517578,
            "y": 18.908063888549805
        },
        {
            "id": "s_1108",
            "name": "Haijun Xia",
            "type": "sparse",
            "x": -9.174410820007324,
            "y": 16.68987274169922
        },
        {
            "id": "s_1109",
            "name": "Patrick Paetzold",
            "type": "sparse",
            "x": -8.106677055358887,
            "y": 18.655040740966797
        },
        {
            "id": "s_1110",
            "name": "Ran Cheng",
            "type": "sparse",
            "x": -4.2592597007751465,
            "y": 21.112163543701172
        },
        {
            "id": "s_1111",
            "name": "Tongshuang Wu",
            "type": "sparse",
            "x": -10.86319637298584,
            "y": 18.27054214477539
        },
        {
            "id": "s_1112",
            "name": "Wei Zhang 0219",
            "type": "sparse",
            "x": -9.788735389709473,
            "y": 18.167797088623047
        },
        {
            "id": "s_1113",
            "name": "Ulrich Lang 0002",
            "type": "sparse",
            "x": -15.027063369750977,
            "y": 7.147265911102295
        },
        {
            "id": "s_1114",
            "name": "Chris P. Gale",
            "type": "sparse",
            "x": -6.435336589813232,
            "y": 16.470481872558594
        },
        {
            "id": "s_1115",
            "name": "Xian Xu",
            "type": "sparse",
            "x": -9.554560661315918,
            "y": 19.322256088256836
        },
        {
            "id": "s_1116",
            "name": "Stanislaw Nowak",
            "type": "sparse",
            "x": -4.196431636810303,
            "y": 16.07634925842285
        },
        {
            "id": "s_1117",
            "name": "R. Tohid",
            "type": "sparse",
            "x": -0.22983476519584656,
            "y": 17.09531021118164
        },
        {
            "id": "s_1118",
            "name": "Raphael Buchm\u00fcller",
            "type": "sparse",
            "x": -7.04925012588501,
            "y": 20.858505249023438
        },
        {
            "id": "s_1119",
            "name": "Reshika Palaniyappan Velumani",
            "type": "sparse",
            "x": -10.131927490234375,
            "y": 18.6522216796875
        },
        {
            "id": "s_1120",
            "name": "Matias Nicol\u00e1s Selzer",
            "type": "sparse",
            "x": -5.774681568145752,
            "y": 19.48044204711914
        },
        {
            "id": "s_1121",
            "name": "Zhiguang Zhou",
            "type": "sparse",
            "x": -9.352557182312012,
            "y": 17.55404281616211
        },
        {
            "id": "s_1122",
            "name": "Yu Fu",
            "type": "sparse",
            "x": -4.522456169128418,
            "y": 15.331584930419922
        },
        {
            "id": "s_1123",
            "name": "Yang Chen",
            "type": "sparse",
            "x": -9.106324195861816,
            "y": 18.401742935180664
        },
        {
            "id": "s_1124",
            "name": "Petra Isenberg",
            "type": "sparse",
            "x": -6.388592720031738,
            "y": 18.277847290039062
        },
        {
            "id": "s_1125",
            "name": "Alexander Vieth",
            "type": "sparse",
            "x": -6.804708480834961,
            "y": 14.28958797454834
        },
        {
            "id": "s_1126",
            "name": "Steven Mark Drucker",
            "type": "sparse",
            "x": -7.567201614379883,
            "y": 17.976457595825195
        },
        {
            "id": "s_1127",
            "name": "Emily Kuang",
            "type": "sparse",
            "x": -0.5544378161430359,
            "y": 18.21550178527832
        },
        {
            "id": "s_1128",
            "name": "Shouling Ji",
            "type": "sparse",
            "x": -10.437884330749512,
            "y": 17.103605270385742
        },
        {
            "id": "s_1129",
            "name": "Jeff W. Lichtman",
            "type": "sparse",
            "x": -8.875916481018066,
            "y": 16.5394287109375
        },
        {
            "id": "s_1130",
            "name": "Kevin Sidak",
            "type": "sparse",
            "x": -8.511391639709473,
            "y": 16.745004653930664
        },
        {
            "id": "s_1131",
            "name": "Yanqiu Wu 0001",
            "type": "sparse",
            "x": -9.376312255859375,
            "y": 19.660152435302734
        },
        {
            "id": "s_1132",
            "name": "Elsie Lee-Robbins",
            "type": "sparse",
            "x": -3.1594414710998535,
            "y": 16.854759216308594
        },
        {
            "id": "s_1133",
            "name": "Tianyu Zhang",
            "type": "sparse",
            "x": -10.979545593261719,
            "y": 19.225065231323242
        },
        {
            "id": "s_1134",
            "name": "Yuansong Xu",
            "type": "sparse",
            "x": -11.065895080566406,
            "y": 19.344633102416992
        },
        {
            "id": "s_1135",
            "name": "Felipe Inagaki de Oliveira",
            "type": "sparse",
            "x": -4.026177406311035,
            "y": 20.123994827270508
        },
        {
            "id": "s_1136",
            "name": "Nimisha Roy",
            "type": "sparse",
            "x": 10.300928115844727,
            "y": 10.247509002685547
        },
        {
            "id": "s_1137",
            "name": "Yngve Sekse Kristiansen",
            "type": "sparse",
            "x": -5.886399269104004,
            "y": 19.326087951660156
        },
        {
            "id": "s_1138",
            "name": "Annika Bonerath",
            "type": "sparse",
            "x": -5.618027210235596,
            "y": 15.07261848449707
        },
        {
            "id": "s_1139",
            "name": "Cristina Nita-Rotaru",
            "type": "sparse",
            "x": -6.718262195587158,
            "y": 21.184059143066406
        },
        {
            "id": "s_1140",
            "name": "Yuhong Li",
            "type": "sparse",
            "x": -9.963203430175781,
            "y": 17.225238800048828
        },
        {
            "id": "s_1141",
            "name": "Kai Liu",
            "type": "sparse",
            "x": -9.832393646240234,
            "y": 18.190614700317383
        },
        {
            "id": "s_1142",
            "name": "Boudewijn P. F. Lelieveldt",
            "type": "sparse",
            "x": -6.8935112953186035,
            "y": 14.372384071350098
        },
        {
            "id": "s_1143",
            "name": "Min Zhu",
            "type": "sparse",
            "x": -10.184301376342773,
            "y": 18.462615966796875
        },
        {
            "id": "s_1144",
            "name": "Jinpeng Wang 0001",
            "type": "sparse",
            "x": -10.858573913574219,
            "y": 18.263038635253906
        },
        {
            "id": "s_1145",
            "name": "Rongzheng Bian",
            "type": "sparse",
            "x": -8.303385734558105,
            "y": 18.99061393737793
        },
        {
            "id": "s_1146",
            "name": "Yunhai Wang",
            "type": "sparse",
            "x": -8.06623649597168,
            "y": 18.562602996826172
        },
        {
            "id": "s_1147",
            "name": "Yuxin Tian",
            "type": "sparse",
            "x": -10.337045669555664,
            "y": 17.219890594482422
        },
        {
            "id": "s_1148",
            "name": "Ryan Wesslen",
            "type": "sparse",
            "x": -4.7458391189575195,
            "y": 15.451166152954102
        },
        {
            "id": "s_1149",
            "name": "Will Epperson",
            "type": "sparse",
            "x": -3.846788167953491,
            "y": 17.404991149902344
        },
        {
            "id": "s_1150",
            "name": "Highmed Consortium",
            "type": "sparse",
            "x": -5.846201419830322,
            "y": 15.334893226623535
        },
        {
            "id": "s_1151",
            "name": "Ambre Assor",
            "type": "sparse",
            "x": -7.182371139526367,
            "y": 17.94027328491211
        },
        {
            "id": "s_1152",
            "name": "Yitao Wu",
            "type": "sparse",
            "x": -9.25825309753418,
            "y": 17.47172737121582
        },
        {
            "id": "s_1153",
            "name": "Ben Swallow",
            "type": "sparse",
            "x": -7.315457820892334,
            "y": 16.86161994934082
        },
        {
            "id": "s_1154",
            "name": "Amanda Hirsch-H\u00fcsler",
            "type": "sparse",
            "x": -8.637360572814941,
            "y": 18.6469669342041
        },
        {
            "id": "s_1155",
            "name": "Hong Zhou 0004",
            "type": "sparse",
            "x": -10.500120162963867,
            "y": 17.656328201293945
        },
        {
            "id": "s_1156",
            "name": "Rafael Messias Martins",
            "type": "sparse",
            "x": -3.9383559226989746,
            "y": 19.717226028442383
        },
        {
            "id": "s_1157",
            "name": "Eugene Wu 0002",
            "type": "sparse",
            "x": -4.607156753540039,
            "y": 15.990559577941895
        },
        {
            "id": "s_1158",
            "name": "Yuchen Yang",
            "type": "sparse",
            "x": -10.30992317199707,
            "y": 17.476301193237305
        },
        {
            "id": "s_1159",
            "name": "Mashael AlKadi",
            "type": "sparse",
            "x": -6.490146160125732,
            "y": 17.23322868347168
        },
        {
            "id": "s_1160",
            "name": "Dongyu Liu",
            "type": "sparse",
            "x": -10.248188972473145,
            "y": 17.885282516479492
        },
        {
            "id": "s_1161",
            "name": "Chase Stokes",
            "type": "sparse",
            "x": -3.756664276123047,
            "y": 16.325925827026367
        },
        {
            "id": "s_1162",
            "name": "Yaqi Qin",
            "type": "sparse",
            "x": -9.246356964111328,
            "y": 17.489925384521484
        },
        {
            "id": "s_1163",
            "name": "Michelle A. Borkin",
            "type": "sparse",
            "x": -8.373533248901367,
            "y": 17.169437408447266
        },
        {
            "id": "s_1164",
            "name": "Yukai Guo",
            "type": "sparse",
            "x": -8.966821670532227,
            "y": 16.880271911621094
        },
        {
            "id": "s_1165",
            "name": "Ignacio P\u00e9rez-Messina",
            "type": "sparse",
            "x": -5.336940765380859,
            "y": 14.92864990234375
        },
        {
            "id": "s_1166",
            "name": "James P. Ahrens",
            "type": "sparse",
            "x": -4.220578193664551,
            "y": 21.0560245513916
        },
        {
            "id": "s_1167",
            "name": "Zeqing Yuan",
            "type": "sparse",
            "x": -10.346992492675781,
            "y": 17.358705520629883
        },
        {
            "id": "s_1168",
            "name": "Brendan Moyle",
            "type": "sparse",
            "x": -7.453673839569092,
            "y": 17.851055145263672
        },
        {
            "id": "s_1169",
            "name": "Linping Yuan",
            "type": "sparse",
            "x": -10.093771934509277,
            "y": 18.267648696899414
        },
        {
            "id": "s_1170",
            "name": "Jan Mican",
            "type": "sparse",
            "x": -5.75709342956543,
            "y": 19.50130844116211
        },
        {
            "id": "s_1171",
            "name": "Yijing Ren",
            "type": "sparse",
            "x": -11.011123657226562,
            "y": 19.28965187072754
        },
        {
            "id": "s_1172",
            "name": "Pascal Nardini",
            "type": "sparse",
            "x": -7.263421535491943,
            "y": 16.913196563720703
        },
        {
            "id": "s_1173",
            "name": "Xinchen Zhang",
            "type": "sparse",
            "x": -9.321276664733887,
            "y": 18.894914627075195
        },
        {
            "id": "s_1174",
            "name": "Fengyuan Tian",
            "type": "sparse",
            "x": -9.005059242248535,
            "y": 16.837665557861328
        },
        {
            "id": "s_1175",
            "name": "Jan Zah\u00e1lka",
            "type": "sparse",
            "x": -6.995812892913818,
            "y": 20.90435218811035
        },
        {
            "id": "s_1176",
            "name": "Jeroen Eggermont",
            "type": "sparse",
            "x": -6.874845027923584,
            "y": 14.342353820800781
        },
        {
            "id": "s_1177",
            "name": "Zhe Xu 0007",
            "type": "sparse",
            "x": -9.265056610107422,
            "y": 19.75649642944336
        },
        {
            "id": "s_1178",
            "name": "Songwen Hu",
            "type": "sparse",
            "x": -3.9912123680114746,
            "y": 16.626882553100586
        },
        {
            "id": "s_1179",
            "name": "Buwei Zhou",
            "type": "sparse",
            "x": -10.339402198791504,
            "y": 17.27062225341797
        },
        {
            "id": "s_1180",
            "name": "Arvind Kumar Shekar",
            "type": "sparse",
            "x": -0.41558516025543213,
            "y": 17.56848907470703
        },
        {
            "id": "s_1181",
            "name": "Zichun Zhong",
            "type": "sparse",
            "x": -11.003897666931152,
            "y": 16.868629455566406
        },
        {
            "id": "s_1182",
            "name": "Wei Shuai",
            "type": "sparse",
            "x": -9.265825271606445,
            "y": 19.76325798034668
        },
        {
            "id": "s_1183",
            "name": "Xiangyuan Chen",
            "type": "sparse",
            "x": -9.723884582519531,
            "y": 18.55529022216797
        },
        {
            "id": "s_1184",
            "name": "Cindy Xiong Bearfield",
            "type": "sparse",
            "x": -4.031999588012695,
            "y": 16.70789337158203
        },
        {
            "id": "s_1185",
            "name": "Zhicheng Liu 0001",
            "type": "sparse",
            "x": -4.497593402862549,
            "y": 17.0808048248291
        },
        {
            "id": "s_1186",
            "name": "Robert Kr\u00fcger",
            "type": "sparse",
            "x": -8.706467628479004,
            "y": 16.479522705078125
        },
        {
            "id": "s_1187",
            "name": "David Saffo",
            "type": "sparse",
            "x": -6.719100475311279,
            "y": 21.15475845336914
        },
        {
            "id": "s_1188",
            "name": "Bryan Triana",
            "type": "sparse",
            "x": -1.8892006874084473,
            "y": 17.9597110748291
        },
        {
            "id": "s_1189",
            "name": "Jie Bao 0003",
            "type": "sparse",
            "x": -10.107878684997559,
            "y": 17.321508407592773
        },
        {
            "id": "s_1190",
            "name": "Madeleine Grunde-McLaughlin",
            "type": "sparse",
            "x": -3.3221490383148193,
            "y": 16.74342155456543
        },
        {
            "id": "s_1191",
            "name": "Ayse D. Lokmanoglu",
            "type": "sparse",
            "x": -3.1438167095184326,
            "y": 16.662376403808594
        },
        {
            "id": "s_1192",
            "name": "Xinlong Zhang",
            "type": "sparse",
            "x": -9.276224136352539,
            "y": 17.889522552490234
        },
        {
            "id": "s_1193",
            "name": "Yilin Ye",
            "type": "sparse",
            "x": -9.684903144836426,
            "y": 17.935510635375977
        },
        {
            "id": "s_1194",
            "name": "Wen Zhang",
            "type": "sparse",
            "x": -8.048645973205566,
            "y": 18.706645965576172
        },
        {
            "id": "s_1195",
            "name": "Silvia Miksch",
            "type": "sparse",
            "x": -5.329970836639404,
            "y": 15.006878852844238
        },
        {
            "id": "s_1196",
            "name": "Weiran Lyu",
            "type": "sparse",
            "x": 7.311459541320801,
            "y": 5.463476181030273
        },
        {
            "id": "s_1197",
            "name": "Linquan Huang",
            "type": "sparse",
            "x": -9.39871597290039,
            "y": 17.706037521362305
        },
        {
            "id": "s_1198",
            "name": "Bo Hyoung Kim",
            "type": "sparse",
            "x": -1.372320294380188,
            "y": 18.073179244995117
        },
        {
            "id": "s_1199",
            "name": "Ko-Chih Wang",
            "type": "sparse",
            "x": 7.707251071929932,
            "y": 5.072447299957275
        },
        {
            "id": "s_1200",
            "name": "Tabitha C. Peck",
            "type": "sparse",
            "x": -8.98729133605957,
            "y": 19.842744827270508
        },
        {
            "id": "s_1201",
            "name": "Gi-Yeul Bae",
            "type": "sparse",
            "x": -3.3762154579162598,
            "y": 17.58768081665039
        },
        {
            "id": "s_1202",
            "name": "Xiangyang Wu",
            "type": "sparse",
            "x": -9.397157669067383,
            "y": 17.91257095336914
        },
        {
            "id": "s_1203",
            "name": "Feng Luo 0002",
            "type": "sparse",
            "x": -9.197144508361816,
            "y": 17.691661834716797
        },
        {
            "id": "s_1204",
            "name": "Panpan Xu",
            "type": "sparse",
            "x": -0.3840155303478241,
            "y": 17.49223518371582
        },
        {
            "id": "s_1205",
            "name": "Robert Turko",
            "type": "sparse",
            "x": 10.025531768798828,
            "y": 10.513174057006836
        },
        {
            "id": "s_1206",
            "name": "Carmen Hull",
            "type": "sparse",
            "x": -6.4724040031433105,
            "y": 16.549423217773438
        },
        {
            "id": "s_1207",
            "name": "Zainab Alsuwaykit",
            "type": "sparse",
            "x": -6.2403998374938965,
            "y": 18.915851593017578
        },
        {
            "id": "s_1208",
            "name": "Erez Zadok",
            "type": "sparse",
            "x": -9.247405052185059,
            "y": 18.790969848632812
        },
        {
            "id": "s_1209",
            "name": "Caitlyn M. McColeman",
            "type": "sparse",
            "x": -3.1659655570983887,
            "y": 16.504207611083984
        },
        {
            "id": "s_1210",
            "name": "Yong Wang 0021",
            "type": "sparse",
            "x": -9.8589448928833,
            "y": 18.597009658813477
        },
        {
            "id": "s_1211",
            "name": "Yanwei Huang",
            "type": "sparse",
            "x": -10.355905532836914,
            "y": 17.267955780029297
        },
        {
            "id": "s_1212",
            "name": "Zhenhui Peng",
            "type": "sparse",
            "x": -11.022603988647461,
            "y": 19.300832748413086
        },
        {
            "id": "s_1213",
            "name": "Suizi Huang",
            "type": "sparse",
            "x": -9.707880973815918,
            "y": 17.91424560546875
        },
        {
            "id": "s_1214",
            "name": "Matthew Butler 0002",
            "type": "sparse",
            "x": -7.6406378746032715,
            "y": 18.03247833251953
        },
        {
            "id": "s_1215",
            "name": "Cindy Xiong",
            "type": "sparse",
            "x": -3.791440486907959,
            "y": 16.45296859741211
        },
        {
            "id": "s_1216",
            "name": "Jeremy Muhlich",
            "type": "sparse",
            "x": -8.69844913482666,
            "y": 16.466060638427734
        },
        {
            "id": "s_1217",
            "name": "Yijie Hou",
            "type": "sparse",
            "x": -10.096966743469238,
            "y": 19.588647842407227
        },
        {
            "id": "s_1218",
            "name": "Jihoon Kim 0001",
            "type": "sparse",
            "x": -1.6711381673812866,
            "y": 16.435869216918945
        },
        {
            "id": "s_1219",
            "name": "Shaozhang Dai",
            "type": "sparse",
            "x": -7.503246307373047,
            "y": 17.843198776245117
        },
        {
            "id": "s_1220",
            "name": "Magdalena Boucher",
            "type": "sparse",
            "x": -6.073223114013672,
            "y": 17.346330642700195
        },
        {
            "id": "s_1221",
            "name": "Benjamin Bach",
            "type": "sparse",
            "x": -6.819238185882568,
            "y": 17.151273727416992
        },
        {
            "id": "s_1222",
            "name": "Eli Holder",
            "type": "sparse",
            "x": -3.84541916847229,
            "y": 16.541837692260742
        },
        {
            "id": "s_1223",
            "name": "Jian Chen 0006",
            "type": "sparse",
            "x": -9.998971939086914,
            "y": 17.603151321411133
        },
        {
            "id": "s_1224",
            "name": "Nikolaos Karadimitriou",
            "type": "sparse",
            "x": -9.805859565734863,
            "y": 16.827356338500977
        },
        {
            "id": "s_1225",
            "name": "Bridget Cogley",
            "type": "sparse",
            "x": -4.1415839195251465,
            "y": 16.2016544342041
        },
        {
            "id": "s_1226",
            "name": "Fan Du",
            "type": "sparse",
            "x": -3.905578136444092,
            "y": 16.942895889282227
        },
        {
            "id": "s_1227",
            "name": "Jinwook Seo",
            "type": "sparse",
            "x": -1.3124263286590576,
            "y": 18.022727966308594
        },
        {
            "id": "s_1228",
            "name": "Nagaraju Dhanyasi",
            "type": "sparse",
            "x": -8.86609935760498,
            "y": 16.544404983520508
        },
        {
            "id": "s_1229",
            "name": "Adam Perer",
            "type": "sparse",
            "x": -3.8426239490509033,
            "y": 17.407955169677734
        },
        {
            "id": "s_1230",
            "name": "Mehak Sachdeva",
            "type": "sparse",
            "x": -4.284819602966309,
            "y": 21.119548797607422
        },
        {
            "id": "s_1231",
            "name": "Meng-Chieh Lee",
            "type": "sparse",
            "x": 10.380304336547852,
            "y": 10.169671058654785
        },
        {
            "id": "s_1232",
            "name": "Jingrui He",
            "type": "sparse",
            "x": -4.278588771820068,
            "y": 21.091928482055664
        },
        {
            "id": "s_1233",
            "name": "Paola Valdivia",
            "type": "sparse",
            "x": -6.451669692993164,
            "y": 17.019155502319336
        },
        {
            "id": "s_1234",
            "name": "Helen Ai He",
            "type": "sparse",
            "x": -6.192897796630859,
            "y": 17.607789993286133
        },
        {
            "id": "s_1235",
            "name": "Xingbo Wang 0001",
            "type": "sparse",
            "x": -10.139918327331543,
            "y": 18.353485107421875
        },
        {
            "id": "s_1236",
            "name": "Albert Webson",
            "type": "sparse",
            "x": -8.803406715393066,
            "y": 16.47862434387207
        },
        {
            "id": "s_1237",
            "name": "Niklas Elmqvist",
            "type": "sparse",
            "x": -2.1784393787384033,
            "y": 16.059892654418945
        },
        {
            "id": "s_1238",
            "name": "Carolina Nobre",
            "type": "sparse",
            "x": -8.757172584533691,
            "y": 16.48414421081543
        },
        {
            "id": "s_1239",
            "name": "Rebecca Randell",
            "type": "sparse",
            "x": -6.441144943237305,
            "y": 16.49129867553711
        },
        {
            "id": "s_1240",
            "name": "Cecily C. Ritch",
            "type": "sparse",
            "x": -8.65553092956543,
            "y": 16.456510543823242
        },
        {
            "id": "s_1241",
            "name": "Yifan Cao",
            "type": "sparse",
            "x": -10.044816017150879,
            "y": 18.387956619262695
        },
        {
            "id": "s_1242",
            "name": "Wolfgang Gatterbauer",
            "type": "sparse",
            "x": -6.705533027648926,
            "y": 21.192415237426758
        },
        {
            "id": "s_1243",
            "name": "Gennady L. Andrienko",
            "type": "sparse",
            "x": -10.233859062194824,
            "y": 19.66407585144043
        },
        {
            "id": "s_1244",
            "name": "Raghavendra Sridharamurthy",
            "type": "sparse",
            "x": 7.3330793380737305,
            "y": 5.441738128662109
        },
        {
            "id": "s_1245",
            "name": "Yuqing Yang 0001",
            "type": "sparse",
            "x": -8.93126106262207,
            "y": 19.944814682006836
        },
        {
            "id": "s_1246",
            "name": "Zoltan Maliga",
            "type": "sparse",
            "x": -8.571935653686523,
            "y": 16.68111801147461
        },
        {
            "id": "s_1247",
            "name": "Brian Bollen",
            "type": "sparse",
            "x": -4.0158610343933105,
            "y": 20.725000381469727
        },
        {
            "id": "s_1248",
            "name": "Yinqiao Wang",
            "type": "sparse",
            "x": -8.063214302062988,
            "y": 18.60970687866211
        },
        {
            "id": "s_1249",
            "name": "Lejun Shen",
            "type": "sparse",
            "x": -10.390961647033691,
            "y": 17.218669891357422
        },
        {
            "id": "s_1250",
            "name": "Matej Mlejnek",
            "type": "sparse",
            "x": -8.75124740600586,
            "y": 16.43606948852539
        },
        {
            "id": "s_1251",
            "name": "Xiaojing Ma 0002",
            "type": "sparse",
            "x": -10.869125366210938,
            "y": 18.284791946411133
        },
        {
            "id": "s_1252",
            "name": "Christos Faloutsos",
            "type": "sparse",
            "x": 10.37822151184082,
            "y": 10.17180347442627
        },
        {
            "id": "s_1253",
            "name": "Thomas H\u00f6llt",
            "type": "sparse",
            "x": -6.7633185386657715,
            "y": 14.306272506713867
        },
        {
            "id": "s_1254",
            "name": "Marcel Worring",
            "type": "sparse",
            "x": -7.014517307281494,
            "y": 20.89041519165039
        },
        {
            "id": "s_1255",
            "name": "Rebecca Mackenzie",
            "type": "sparse",
            "x": 10.37695598602295,
            "y": 10.173086166381836
        },
        {
            "id": "s_1256",
            "name": "Jacqueline H. Chen",
            "type": "sparse",
            "x": -0.776049792766571,
            "y": 17.950027465820312
        },
        {
            "id": "s_1257",
            "name": "Yumeng Xue",
            "type": "sparse",
            "x": -8.25503158569336,
            "y": 18.965564727783203
        },
        {
            "id": "s_1258",
            "name": "Fabian Sperrle",
            "type": "sparse",
            "x": -5.148189067840576,
            "y": 15.082869529724121
        },
        {
            "id": "s_1259",
            "name": "Baoquan Chen",
            "type": "sparse",
            "x": -8.276715278625488,
            "y": 18.961008071899414
        },
        {
            "id": "s_1260",
            "name": "Zhongyu Wei",
            "type": "sparse",
            "x": -10.038476943969727,
            "y": 19.656484603881836
        },
        {
            "id": "s_1261",
            "name": "Zhirui Wang",
            "type": "sparse",
            "x": -8.095151901245117,
            "y": 18.669124603271484
        },
        {
            "id": "s_1262",
            "name": "Donghao Ren",
            "type": "sparse",
            "x": -3.8662185668945312,
            "y": 17.3792781829834
        },
        {
            "id": "s_1263",
            "name": "Michael Middleton",
            "type": "sparse",
            "x": -4.06439733505249,
            "y": 20.150028228759766
        },
        {
            "id": "s_1264",
            "name": "Donald Bertucci",
            "type": "sparse",
            "x": 9.955756187438965,
            "y": 10.585609436035156
        },
        {
            "id": "s_1265",
            "name": "Jiayi Xu 0001",
            "type": "sparse",
            "x": 7.5087890625,
            "y": 5.26605224609375
        },
        {
            "id": "s_1266",
            "name": "Sehi L'Yi",
            "type": "sparse",
            "x": -8.352128982543945,
            "y": 17.038373947143555
        },
        {
            "id": "s_1267",
            "name": "Lvkeshen Shen",
            "type": "sparse",
            "x": -10.194436073303223,
            "y": 17.268861770629883
        },
        {
            "id": "s_1268",
            "name": "Gromit Yeuk-Yin Chan",
            "type": "sparse",
            "x": -3.9120755195617676,
            "y": 16.823068618774414
        },
        {
            "id": "s_1269",
            "name": "Zhida Sun",
            "type": "sparse",
            "x": -9.713203430175781,
            "y": 18.525529861450195
        },
        {
            "id": "s_1270",
            "name": "Xin Liang 0001",
            "type": "sparse",
            "x": 7.385069847106934,
            "y": 5.389693737030029
        },
        {
            "id": "s_1271",
            "name": "Elizabeth A. Mack",
            "type": "sparse",
            "x": -4.2578840255737305,
            "y": 21.12769889831543
        },
        {
            "id": "s_1272",
            "name": "Zhihua Jin",
            "type": "sparse",
            "x": -10.100279808044434,
            "y": 18.45136070251465
        },
        {
            "id": "s_1273",
            "name": "Alexandra Zytek",
            "type": "sparse",
            "x": -10.210524559020996,
            "y": 18.3541202545166
        },
        {
            "id": "s_1274",
            "name": "Phoebe Moh",
            "type": "sparse",
            "x": -4.172336578369141,
            "y": 16.8784122467041
        },
        {
            "id": "s_1275",
            "name": "Thomas Kapler",
            "type": "sparse",
            "x": -2.002507448196411,
            "y": 16.016481399536133
        },
        {
            "id": "s_1276",
            "name": "Yunjeong Chang",
            "type": "sparse",
            "x": -7.925715446472168,
            "y": 18.445314407348633
        },
        {
            "id": "s_1277",
            "name": "Michael Xieyang Liu",
            "type": "sparse",
            "x": 9.928740501403809,
            "y": 10.612991333007812
        },
        {
            "id": "s_1278",
            "name": "Marta Koc-Januchta",
            "type": "sparse",
            "x": -7.461137294769287,
            "y": 17.74297523498535
        },
        {
            "id": "s_1279",
            "name": "Benjamin Niedermann",
            "type": "sparse",
            "x": -5.605041980743408,
            "y": 15.067261695861816
        },
        {
            "id": "s_1280",
            "name": "Lingyun Yu 0001",
            "type": "sparse",
            "x": -10.30655288696289,
            "y": 17.295530319213867
        },
        {
            "id": "s_1281",
            "name": "Arvind Srinivasan 0001",
            "type": "sparse",
            "x": -2.001222610473633,
            "y": 16.008930206298828
        },
        {
            "id": "s_1282",
            "name": "Rita Borgo",
            "type": "sparse",
            "x": -7.244770526885986,
            "y": 16.81715202331543
        },
        {
            "id": "s_1283",
            "name": "Federico Rossi 0001",
            "type": "sparse",
            "x": -0.6959089636802673,
            "y": 17.92713737487793
        },
        {
            "id": "s_1284",
            "name": "Jie Song",
            "type": "sparse",
            "x": -8.630620002746582,
            "y": 18.223848342895508
        },
        {
            "id": "s_1285",
            "name": "Ran Chen",
            "type": "sparse",
            "x": -10.38479232788086,
            "y": 17.196500778198242
        },
        {
            "id": "s_1286",
            "name": "Juan Pablo Bello",
            "type": "sparse",
            "x": -4.060301780700684,
            "y": 20.061145782470703
        },
        {
            "id": "s_1287",
            "name": "Yang Ouyang",
            "type": "sparse",
            "x": -11.003975868225098,
            "y": 19.311975479125977
        },
        {
            "id": "s_1288",
            "name": "Yang Liu 0136",
            "type": "sparse",
            "x": -3.640472412109375,
            "y": 17.05105972290039
        },
        {
            "id": "s_1289",
            "name": "Lixiang Zhao",
            "type": "sparse",
            "x": -6.452560901641846,
            "y": 18.76093864440918
        },
        {
            "id": "s_1290",
            "name": "Catherine Yeh",
            "type": "sparse",
            "x": -10.472951889038086,
            "y": 17.942296981811523
        },
        {
            "id": "s_1291",
            "name": "Jennifer Frazier",
            "type": "sparse",
            "x": -0.7273049354553223,
            "y": 17.96986961364746
        },
        {
            "id": "s_1292",
            "name": "Bum Chul Kwon",
            "type": "sparse",
            "x": -2.0650923252105713,
            "y": 16.119049072265625
        },
        {
            "id": "s_1293",
            "name": "Andrew M. McNutt",
            "type": "sparse",
            "x": -3.7460274696350098,
            "y": 17.107168197631836
        },
        {
            "id": "s_1294",
            "name": "Dhiraj Barnwal",
            "type": "sparse",
            "x": -4.309266567230225,
            "y": 16.06330680847168
        },
        {
            "id": "s_1295",
            "name": "Sungahn Ko",
            "type": "sparse",
            "x": -1.9643287658691406,
            "y": 16.114748001098633
        },
        {
            "id": "s_1296",
            "name": "Yifan Wu",
            "type": "sparse",
            "x": -4.538579940795898,
            "y": 16.027753829956055
        },
        {
            "id": "s_1297",
            "name": "Zijie J. Wang",
            "type": "sparse",
            "x": 10.067285537719727,
            "y": 10.472894668579102
        },
        {
            "id": "s_1298",
            "name": "Daniele Chiappalupi",
            "type": "sparse",
            "x": -8.808658599853516,
            "y": 16.692636489868164
        },
        {
            "id": "s_1299",
            "name": "Zui Chen",
            "type": "sparse",
            "x": -9.325299263000488,
            "y": 19.702680587768555
        },
        {
            "id": "s_1300",
            "name": "Adam Sah",
            "type": "sparse",
            "x": -4.698257923126221,
            "y": 16.2304630279541
        },
        {
            "id": "s_1301",
            "name": "Luke Harmon",
            "type": "sparse",
            "x": -2.2741870880126953,
            "y": 15.393957138061523
        },
        {
            "id": "s_1302",
            "name": "Tim Krake",
            "type": "sparse",
            "x": -8.54920482635498,
            "y": 19.331336975097656
        },
        {
            "id": "s_1303",
            "name": "Jonathan C. Roberts",
            "type": "sparse",
            "x": -6.362865924835205,
            "y": 17.306983947753906
        },
        {
            "id": "s_1304",
            "name": "Zhihua Zhu",
            "type": "sparse",
            "x": -11.019444465637207,
            "y": 19.22075843811035
        },
        {
            "id": "s_1305",
            "name": "Kim Marriott",
            "type": "sparse",
            "x": -7.254931926727295,
            "y": 17.523548126220703
        },
        {
            "id": "s_1306",
            "name": "Jing Lu",
            "type": "sparse",
            "x": -10.122502326965332,
            "y": 19.691070556640625
        },
        {
            "id": "s_1307",
            "name": "Zhongwei Wang",
            "type": "sparse",
            "x": -9.914422988891602,
            "y": 18.315654754638672
        },
        {
            "id": "s_1308",
            "name": "Zikun Deng",
            "type": "sparse",
            "x": -9.705275535583496,
            "y": 17.47173500061035
        },
        {
            "id": "s_1309",
            "name": "Wenbo Tao",
            "type": "sparse",
            "x": -4.704818248748779,
            "y": 16.234851837158203
        },
        {
            "id": "s_1310",
            "name": "Shilpika",
            "type": "sparse",
            "x": -0.7584438920021057,
            "y": 17.987165451049805
        },
        {
            "id": "s_1311",
            "name": "Leni Yang",
            "type": "sparse",
            "x": -9.594548225402832,
            "y": 19.330801010131836
        },
        {
            "id": "s_1312",
            "name": "Juli\u00e1n M\u00e9ndez 0001",
            "type": "sparse",
            "x": -5.736471176147461,
            "y": 18.469558715820312
        },
        {
            "id": "s_1313",
            "name": "Yating Lin",
            "type": "sparse",
            "x": -9.928750038146973,
            "y": 18.61966896057129
        },
        {
            "id": "s_1314",
            "name": "Daniel Braun 0010",
            "type": "sparse",
            "x": -4.939118385314941,
            "y": 15.945503234863281
        },
        {
            "id": "s_1315",
            "name": "Tian Gao",
            "type": "sparse",
            "x": -9.278613090515137,
            "y": 19.78287696838379
        },
        {
            "id": "s_1316",
            "name": "Yutian Zhang",
            "type": "sparse",
            "x": -11.057818412780762,
            "y": 19.325475692749023
        },
        {
            "id": "s_1317",
            "name": "Takanori Fujiwara",
            "type": "sparse",
            "x": -0.8719204664230347,
            "y": 18.013532638549805
        },
        {
            "id": "s_1318",
            "name": "Maryam Hedayati",
            "type": "sparse",
            "x": -3.1774826049804688,
            "y": 16.597986221313477
        },
        {
            "id": "s_1319",
            "name": "Sihyeon Lee",
            "type": "sparse",
            "x": -1.359510898590088,
            "y": 18.07345962524414
        },
        {
            "id": "s_1320",
            "name": "Fabio Miranda 0001",
            "type": "sparse",
            "x": -7.507392406463623,
            "y": -1.1697969436645508
        },
        {
            "id": "s_1321",
            "name": "Maxime Cordeil",
            "type": "sparse",
            "x": -7.525042533874512,
            "y": 17.75238800048828
        },
        {
            "id": "s_1322",
            "name": "Vanessa Serrano",
            "type": "sparse",
            "x": -6.674644470214844,
            "y": 17.1263484954834
        },
        {
            "id": "s_1323",
            "name": "Tobias Isenberg 0001",
            "type": "sparse",
            "x": -6.159990310668945,
            "y": 18.854093551635742
        },
        {
            "id": "s_1324",
            "name": "Yuriy Brun",
            "type": "sparse",
            "x": -3.9806816577911377,
            "y": 16.65677261352539
        },
        {
            "id": "s_1325",
            "name": "Crescentia Jung",
            "type": "sparse",
            "x": -3.395695209503174,
            "y": 16.40166473388672
        },
        {
            "id": "s_1326",
            "name": "Thomas Kroes",
            "type": "sparse",
            "x": -6.8921990394592285,
            "y": 14.320096969604492
        },
        {
            "id": "s_1327",
            "name": "Xiaodong Zhao",
            "type": "sparse",
            "x": -9.967987060546875,
            "y": 17.6473331451416
        },
        {
            "id": "s_1328",
            "name": "Cristina R. Ceja",
            "type": "sparse",
            "x": -3.5713746547698975,
            "y": 16.51019287109375
        },
        {
            "id": "s_1329",
            "name": "Yoonsoo Nam",
            "type": "sparse",
            "x": -1.6537566184997559,
            "y": 16.447025299072266
        },
        {
            "id": "s_1330",
            "name": "S. Sandra Bae",
            "type": "sparse",
            "x": -2.153019428253174,
            "y": 18.1021785736084
        },
        {
            "id": "s_1331",
            "name": "Melanie Tory",
            "type": "sparse",
            "x": -4.221306324005127,
            "y": 16.1362247467041
        },
        {
            "id": "s_1332",
            "name": "Gaoqi He",
            "type": "sparse",
            "x": -8.081360816955566,
            "y": 18.691059112548828
        },
        {
            "id": "s_1333",
            "name": "Vanessa Fediuk",
            "type": "sparse",
            "x": -2.09344220161438,
            "y": 15.908543586730957
        },
        {
            "id": "s_1334",
            "name": "Chenglong Wang",
            "type": "sparse",
            "x": -7.642424583435059,
            "y": 18.146087646484375
        },
        {
            "id": "s_1335",
            "name": "Yoon Kim",
            "type": "sparse",
            "x": -10.863635063171387,
            "y": 17.26551628112793
        },
        {
            "id": "s_1336",
            "name": "Raymundo Navarrete",
            "type": "sparse",
            "x": -3.9646332263946533,
            "y": 19.69342613220215
        },
        {
            "id": "s_1337",
            "name": "Fernanda B. Vi\u00e9gas",
            "type": "sparse",
            "x": -10.495383262634277,
            "y": 17.918001174926758
        },
        {
            "id": "s_1338",
            "name": "Xiaoru Yuan",
            "type": "sparse",
            "x": -10.593152046203613,
            "y": 19.551576614379883
        },
        {
            "id": "s_1339",
            "name": "Hariharan Subramonyam",
            "type": "sparse",
            "x": -3.4793732166290283,
            "y": 16.745582580566406
        },
        {
            "id": "s_1340",
            "name": "Michael Correll",
            "type": "sparse",
            "x": -4.25051212310791,
            "y": 16.109006881713867
        },
        {
            "id": "s_1341",
            "name": "Dazhen Deng",
            "type": "sparse",
            "x": -10.095970153808594,
            "y": 17.561277389526367
        },
        {
            "id": "s_1342",
            "name": "Xiao Yan 0002",
            "type": "sparse",
            "x": -9.301690101623535,
            "y": 20.26869773864746
        },
        {
            "id": "s_1343",
            "name": "Simon Duque Ant\u00f3n",
            "type": "sparse",
            "x": -3.908261775970459,
            "y": 20.869346618652344
        },
        {
            "id": "s_1344",
            "name": "Fritz Lekschas",
            "type": "sparse",
            "x": -8.802640914916992,
            "y": 17.07512855529785
        },
        {
            "id": "s_1345",
            "name": "Oliver Deussen",
            "type": "sparse",
            "x": -8.03954029083252,
            "y": 18.711286544799805
        },
        {
            "id": "s_1346",
            "name": "Francis Kilian",
            "type": "sparse",
            "x": -3.5494539737701416,
            "y": 19.848134994506836
        },
        {
            "id": "s_1347",
            "name": "Roque Lopez",
            "type": "sparse",
            "x": -4.0584492683410645,
            "y": 20.01323890686035
        },
        {
            "id": "s_1348",
            "name": "Fateme Rajabiyazdi",
            "type": "sparse",
            "x": -6.080713748931885,
            "y": 17.358266830444336
        },
        {
            "id": "s_1349",
            "name": "Tobias G\u00fcnther",
            "type": "sparse",
            "x": -3.5001156330108643,
            "y": 19.78601837158203
        },
        {
            "id": "s_1350",
            "name": "Zhe Wang",
            "type": "sparse",
            "x": -1.8654437065124512,
            "y": 17.956043243408203
        },
        {
            "id": "s_1351",
            "name": "Antonios Somarakis",
            "type": "sparse",
            "x": -6.799899101257324,
            "y": 14.298995018005371
        },
        {
            "id": "s_1352",
            "name": "Marko Reh\u00e1cek",
            "type": "sparse",
            "x": -5.78003454208374,
            "y": 19.47618293762207
        },
        {
            "id": "s_1353",
            "name": "Hamish A. Carr",
            "type": "sparse",
            "x": 7.3117146492004395,
            "y": 5.463586807250977
        },
        {
            "id": "s_1354",
            "name": "Barbora Kozl\u00edkov\u00e1",
            "type": "sparse",
            "x": -5.9298095703125,
            "y": 19.260469436645508
        },
        {
            "id": "s_1355",
            "name": "Harper Schwab",
            "type": "sparse",
            "x": -3.425644874572754,
            "y": 16.9384765625
        },
        {
            "id": "s_1356",
            "name": "Austin H. Patton",
            "type": "sparse",
            "x": -2.2762672901153564,
            "y": 15.349945068359375
        },
        {
            "id": "s_1357",
            "name": "Yuanzhe Jin",
            "type": "sparse",
            "x": -7.276676177978516,
            "y": 16.893924713134766
        },
        {
            "id": "s_1358",
            "name": "Ziyan Liu",
            "type": "sparse",
            "x": -9.92343521118164,
            "y": 18.842941284179688
        },
        {
            "id": "s_1359",
            "name": "Lalitha Sankar",
            "type": "sparse",
            "x": -3.27577543258667,
            "y": 17.650562286376953
        },
        {
            "id": "s_1360",
            "name": "Shah Rukh Humayoun",
            "type": "sparse",
            "x": -4.790092945098877,
            "y": 15.758539199829102
        },
        {
            "id": "s_1361",
            "name": "Junhai Yong",
            "type": "sparse",
            "x": -10.902400970458984,
            "y": 19.466623306274414
        },
        {
            "id": "s_1362",
            "name": "Kamkwai Wong",
            "type": "sparse",
            "x": -9.711771011352539,
            "y": 18.39276695251465
        },
        {
            "id": "s_1363",
            "name": "Alexander Bock 0002",
            "type": "sparse",
            "x": -4.028604507446289,
            "y": 20.17606544494629
        },
        {
            "id": "s_1364",
            "name": "Emily Reif",
            "type": "sparse",
            "x": 9.913849830627441,
            "y": 10.62818717956543
        },
        {
            "id": "s_1365",
            "name": "Mohamed Kissi",
            "type": "sparse",
            "x": -4.056864261627197,
            "y": 20.67831802368164
        },
        {
            "id": "s_1366",
            "name": "Lu Chen",
            "type": "sparse",
            "x": -8.065051078796387,
            "y": 18.611536026000977
        },
        {
            "id": "s_1367",
            "name": "Shuirun Wei",
            "type": "sparse",
            "x": -9.20926284790039,
            "y": 17.68190574645996
        },
        {
            "id": "s_1368",
            "name": "Maureen C. Stone",
            "type": "sparse",
            "x": -3.797422409057617,
            "y": 17.162256240844727
        },
        {
            "id": "s_1369",
            "name": "Stef van den Elzen",
            "type": "sparse",
            "x": -6.971498012542725,
            "y": 14.453225135803223
        },
        {
            "id": "s_1370",
            "name": "Kyunghyun Cho",
            "type": "sparse",
            "x": -3.9582395553588867,
            "y": 20.042240142822266
        },
        {
            "id": "s_1371",
            "name": "Isabel Meirelles",
            "type": "sparse",
            "x": -6.07983922958374,
            "y": 17.355552673339844
        },
        {
            "id": "s_1372",
            "name": "Cynthia Chen",
            "type": "sparse",
            "x": -10.494970321655273,
            "y": 17.92532730102539
        },
        {
            "id": "s_1373",
            "name": "Shauli Ravfogel",
            "type": "sparse",
            "x": -5.085165977478027,
            "y": 15.144813537597656
        },
        {
            "id": "s_1374",
            "name": "Siji Chen",
            "type": "sparse",
            "x": -9.290997505187988,
            "y": 19.767091751098633
        },
        {
            "id": "s_1375",
            "name": "Kenneth Moreland",
            "type": "sparse",
            "x": -1.8510164022445679,
            "y": 17.949974060058594
        },
        {
            "id": "s_1376",
            "name": "J\u00fcrgen D\u00f6llner",
            "type": "sparse",
            "x": -8.905159950256348,
            "y": 18.444589614868164
        },
        {
            "id": "s_1377",
            "name": "Luoxuan Weng",
            "type": "sparse",
            "x": -9.78874683380127,
            "y": 17.73747444152832
        },
        {
            "id": "s_1378",
            "name": "Joan Planas-Iglesias",
            "type": "sparse",
            "x": -5.801751613616943,
            "y": 19.451190948486328
        },
        {
            "id": "s_1379",
            "name": "Mario Jelovic",
            "type": "sparse",
            "x": -6.507861137390137,
            "y": 13.799270629882812
        },
        {
            "id": "s_1380",
            "name": "Johannes Knittel",
            "type": "sparse",
            "x": -9.827157974243164,
            "y": 17.14974021911621
        },
        {
            "id": "s_1381",
            "name": "Anthony Kouroupis",
            "type": "sparse",
            "x": -2.265030860900879,
            "y": 18.068666458129883
        },
        {
            "id": "s_1382",
            "name": "Jian-Guang Lou",
            "type": "sparse",
            "x": -10.862922668457031,
            "y": 18.267759323120117
        },
        {
            "id": "s_1383",
            "name": "Nicholas Diakopoulos",
            "type": "sparse",
            "x": -3.202441930770874,
            "y": 16.63640785217285
        },
        {
            "id": "s_1384",
            "name": "Raimund Dachselt",
            "type": "sparse",
            "x": -5.941182613372803,
            "y": 18.51721954345703
        },
        {
            "id": "s_1385",
            "name": "Wei Chen 0001",
            "type": "sparse",
            "x": -9.498659133911133,
            "y": 17.939367294311523
        },
        {
            "id": "s_1386",
            "name": "Jeffrey Heer",
            "type": "sparse",
            "x": -3.7265448570251465,
            "y": 17.05756378173828
        },
        {
            "id": "s_1387",
            "name": "Nan Chen",
            "type": "sparse",
            "x": -8.944972038269043,
            "y": 19.706268310546875
        },
        {
            "id": "s_1388",
            "name": "Bon Adriel Aseniero",
            "type": "sparse",
            "x": -6.3516387939453125,
            "y": 18.25102996826172
        },
        {
            "id": "s_1389",
            "name": "Dilpreet Singh",
            "type": "sparse",
            "x": -7.225720405578613,
            "y": 18.017934799194336
        },
        {
            "id": "s_1390",
            "name": "Zheng Wei",
            "type": "sparse",
            "x": -10.162585258483887,
            "y": 18.336105346679688
        },
        {
            "id": "s_1391",
            "name": "Konrad J. Sch\u00f6nborn",
            "type": "sparse",
            "x": -7.44346284866333,
            "y": 17.74235725402832
        },
        {
            "id": "s_1392",
            "name": "Haibo Hu 0002",
            "type": "sparse",
            "x": -10.0013427734375,
            "y": 19.554658889770508
        },
        {
            "id": "s_1393",
            "name": "Yue Zhao",
            "type": "sparse",
            "x": -4.055538654327393,
            "y": 17.53238868713379
        },
        {
            "id": "s_1394",
            "name": "Lei Chen 0002",
            "type": "sparse",
            "x": -10.918895721435547,
            "y": 18.980239868164062
        },
        {
            "id": "s_1395",
            "name": "Cong Xie",
            "type": "sparse",
            "x": -9.289539337158203,
            "y": 18.333934783935547
        },
        {
            "id": "s_1396",
            "name": "Yuhan Duan",
            "type": "sparse",
            "x": 7.532076835632324,
            "y": 5.242880344390869
        },
        {
            "id": "s_1397",
            "name": "Chin-Yew Lin",
            "type": "sparse",
            "x": -10.867069244384766,
            "y": 18.27009391784668
        },
        {
            "id": "s_1398",
            "name": "Tyson Neuroth",
            "type": "sparse",
            "x": -0.7577449679374695,
            "y": 17.96700096130371
        },
        {
            "id": "s_1399",
            "name": "David Pugmire",
            "type": "sparse",
            "x": -1.8759621381759644,
            "y": 17.945924758911133
        },
        {
            "id": "s_1400",
            "name": "Zehua Zeng",
            "type": "sparse",
            "x": -3.9376635551452637,
            "y": 17.180307388305664
        },
        {
            "id": "s_1401",
            "name": "Nam Wook Kim",
            "type": "sparse",
            "x": -10.848999977111816,
            "y": 17.26551055908203
        },
        {
            "id": "s_1402",
            "name": "Astrid van den Brandt",
            "type": "sparse",
            "x": -8.427996635437012,
            "y": 17.03369903564453
        },
        {
            "id": "s_1403",
            "name": "Aida Nordman",
            "type": "sparse",
            "x": -7.459988594055176,
            "y": 17.746410369873047
        },
        {
            "id": "s_1404",
            "name": "Qiang Guan",
            "type": "sparse",
            "x": -9.833139419555664,
            "y": 18.62320899963379
        },
        {
            "id": "s_1405",
            "name": "Guande Wu",
            "type": "sparse",
            "x": -4.257150173187256,
            "y": 20.004161834716797
        },
        {
            "id": "s_1406",
            "name": "David H\u00e4gele",
            "type": "sparse",
            "x": -8.573248863220215,
            "y": 19.35514259338379
        },
        {
            "id": "s_1407",
            "name": "Michael B\u00f6ttinger",
            "type": "sparse",
            "x": -7.266229152679443,
            "y": 16.912445068359375
        },
        {
            "id": "s_1408",
            "name": "William Alexander",
            "type": "sparse",
            "x": -8.425069808959961,
            "y": 17.397314071655273
        },
        {
            "id": "s_1409",
            "name": "Mingxuan Li",
            "type": "sparse",
            "x": -10.389484405517578,
            "y": 19.65302085876465
        },
        {
            "id": "s_1410",
            "name": "Emre Oral",
            "type": "sparse",
            "x": -4.394093990325928,
            "y": 15.194056510925293
        },
        {
            "id": "s_1411",
            "name": "Jessica Hullman",
            "type": "sparse",
            "x": -3.3397817611694336,
            "y": 16.87760353088379
        },
        {
            "id": "s_1412",
            "name": "Rui Zhou",
            "type": "sparse",
            "x": -9.323772430419922,
            "y": 18.89349365234375
        },
        {
            "id": "s_1413",
            "name": "Xiangyu Zhu",
            "type": "sparse",
            "x": -10.377129554748535,
            "y": 17.132118225097656
        },
        {
            "id": "s_1414",
            "name": "Mark S. Keller",
            "type": "sparse",
            "x": -10.112691879272461,
            "y": 18.07598114013672
        },
        {
            "id": "s_1415",
            "name": "Yingcai Wu",
            "type": "sparse",
            "x": -10.360095024108887,
            "y": 17.364093780517578
        },
        {
            "id": "s_1416",
            "name": "Marieke E. Ijsselsteijn",
            "type": "sparse",
            "x": -6.796704292297363,
            "y": 14.295219421386719
        },
        {
            "id": "s_1417",
            "name": "Attila Szab\u00f3",
            "type": "sparse",
            "x": -5.561643600463867,
            "y": 14.724047660827637
        },
        {
            "id": "s_1418",
            "name": "Clara Hu",
            "type": "sparse",
            "x": -3.9291534423828125,
            "y": 16.199403762817383
        },
        {
            "id": "s_1419",
            "name": "Emmanuel Tung",
            "type": "sparse",
            "x": 10.281071662902832,
            "y": 10.266935348510742
        },
        {
            "id": "s_1420",
            "name": "Weiqun Cao",
            "type": "sparse",
            "x": 7.696887493133545,
            "y": 5.082406520843506
        },
        {
            "id": "s_1421",
            "name": "Sandro Santagata",
            "type": "sparse",
            "x": -8.651471138000488,
            "y": 16.440292358398438
        },
        {
            "id": "s_1422",
            "name": "Sergej Stoppel",
            "type": "sparse",
            "x": -5.772004127502441,
            "y": 19.483654022216797
        },
        {
            "id": "s_1423",
            "name": "Ehsan Jahangirzadeh Soure",
            "type": "sparse",
            "x": -0.610762894153595,
            "y": 18.159976959228516
        },
        {
            "id": "s_1424",
            "name": "Jianing Yin",
            "type": "sparse",
            "x": -10.369193077087402,
            "y": 17.282867431640625
        },
        {
            "id": "s_1425",
            "name": "Jian Zhao 0010",
            "type": "sparse",
            "x": -0.7466683983802795,
            "y": 18.00955581665039
        },
        {
            "id": "s_1426",
            "name": "Alexandre Aouididi",
            "type": "sparse",
            "x": -8.830255508422852,
            "y": 16.61018180847168
        },
        {
            "id": "s_1427",
            "name": "Bernd Finkbeiner",
            "type": "sparse",
            "x": -5.73888635635376,
            "y": 18.47238540649414
        },
        {
            "id": "s_1428",
            "name": "Chaozu Zhang",
            "type": "sparse",
            "x": -9.296472549438477,
            "y": 20.27349281311035
        },
        {
            "id": "s_1429",
            "name": "Ying Zhao 0001",
            "type": "sparse",
            "x": -9.298829078674316,
            "y": 17.46539306640625
        },
        {
            "id": "s_1430",
            "name": "Hee-Joon Bae",
            "type": "sparse",
            "x": -1.3736401796340942,
            "y": 18.080923080444336
        },
        {
            "id": "s_1431",
            "name": "Yuzhe Luo",
            "type": "sparse",
            "x": -10.088346481323242,
            "y": 17.153072357177734
        },
        {
            "id": "s_1432",
            "name": "Honglei Yin",
            "type": "sparse",
            "x": -10.343965530395508,
            "y": 17.169288635253906
        },
        {
            "id": "s_1433",
            "name": "Skylar W. Wurster",
            "type": "sparse",
            "x": 7.444360733032227,
            "y": 5.330343723297119
        },
        {
            "id": "s_1434",
            "name": "Xiaoyun Hu",
            "type": "sparse",
            "x": -7.448946475982666,
            "y": 17.814943313598633
        },
        {
            "id": "s_1435",
            "name": "Haizheng Yang",
            "type": "sparse",
            "x": -10.433403968811035,
            "y": 19.68848419189453
        },
        {
            "id": "s_1436",
            "name": "Chengzhong Liu",
            "type": "sparse",
            "x": -10.934815406799316,
            "y": 18.4733829498291
        },
        {
            "id": "s_1437",
            "name": "Neng Shi",
            "type": "sparse",
            "x": 7.5097575187683105,
            "y": 5.264976978302002
        },
        {
            "id": "s_1438",
            "name": "Wei Zeng 0004",
            "type": "sparse",
            "x": -9.665609359741211,
            "y": 18.019716262817383
        },
        {
            "id": "s_1439",
            "name": "Shizhao Sun",
            "type": "sparse",
            "x": -10.891494750976562,
            "y": 18.244760513305664
        },
        {
            "id": "s_1440",
            "name": "Ryan A. Rossi",
            "type": "sparse",
            "x": -3.3884594440460205,
            "y": 16.741296768188477
        },
        {
            "id": "s_1441",
            "name": "Jian Zhang 0070",
            "type": "sparse",
            "x": -8.279269218444824,
            "y": 18.43860626220703
        },
        {
            "id": "s_1442",
            "name": "Jaemin Jo",
            "type": "sparse",
            "x": -7.887273788452148,
            "y": 18.464242935180664
        },
        {
            "id": "s_1443",
            "name": "Chun-qi Zhou",
            "type": "sparse",
            "x": -10.178022384643555,
            "y": 19.6855411529541
        },
        {
            "id": "s_1444",
            "name": "Eric M\u00f6rth",
            "type": "sparse",
            "x": -8.578374862670898,
            "y": 16.575843811035156
        },
        {
            "id": "s_1445",
            "name": "Haoran Xu 0003",
            "type": "sparse",
            "x": -10.366226196289062,
            "y": 17.167325973510742
        },
        {
            "id": "s_1446",
            "name": "Eren Cakmak",
            "type": "sparse",
            "x": -7.16310977935791,
            "y": 20.654579162597656
        },
        {
            "id": "s_1447",
            "name": "Zhiheng Nie",
            "type": "sparse",
            "x": -11.05566120147705,
            "y": 19.321285247802734
        },
        {
            "id": "s_1448",
            "name": "Yuanyang Zhong",
            "type": "sparse",
            "x": -6.277156352996826,
            "y": 18.47176742553711
        },
        {
            "id": "s_1449",
            "name": "Kaixin Chen 0004",
            "type": "sparse",
            "x": 7.550203800201416,
            "y": 5.224980354309082
        },
        {
            "id": "s_1450",
            "name": "Maximilian Wei\u00df",
            "type": "sparse",
            "x": -8.398694038391113,
            "y": 18.811208724975586
        },
        {
            "id": "s_1451",
            "name": "Zexian Chen",
            "type": "sparse",
            "x": -9.376718521118164,
            "y": 17.905637741088867
        },
        {
            "id": "s_1452",
            "name": "Bo Pan",
            "type": "sparse",
            "x": -9.397351264953613,
            "y": 18.05739974975586
        },
        {
            "id": "s_1453",
            "name": "Huichen Will Wang",
            "type": "sparse",
            "x": -3.9908883571624756,
            "y": 16.890151977539062
        },
        {
            "id": "s_1454",
            "name": "Kang Li 0005",
            "type": "sparse",
            "x": -10.272268295288086,
            "y": 17.191953659057617
        },
        {
            "id": "s_1455",
            "name": "Pratheeksha Nair",
            "type": "sparse",
            "x": 10.37873649597168,
            "y": 10.171326637268066
        },
        {
            "id": "s_1456",
            "name": "Junran Yang",
            "type": "sparse",
            "x": -3.9031875133514404,
            "y": 17.2432918548584
        },
        {
            "id": "s_1457",
            "name": "Derek W. S. Gray",
            "type": "sparse",
            "x": -1.9776270389556885,
            "y": 16.036338806152344
        },
        {
            "id": "s_1458",
            "name": "Xinyi Liu",
            "type": "sparse",
            "x": -4.267306804656982,
            "y": 16.97099494934082
        },
        {
            "id": "s_1459",
            "name": "Stephen G. Kobourov",
            "type": "sparse",
            "x": -3.994755983352661,
            "y": 19.703283309936523
        },
        {
            "id": "s_1460",
            "name": "Xiyuan Wang",
            "type": "sparse",
            "x": -11.022522926330566,
            "y": 19.26909828186035
        },
        {
            "id": "s_1461",
            "name": "Cheonbok Park",
            "type": "sparse",
            "x": -1.6999964714050293,
            "y": 16.42570686340332
        },
        {
            "id": "s_1462",
            "name": "Mathieu Pont",
            "type": "sparse",
            "x": -4.017386436462402,
            "y": 20.720020294189453
        },
        {
            "id": "s_1463",
            "name": "Shash Sinha",
            "type": "sparse",
            "x": -6.7302117347717285,
            "y": 21.16805648803711
        },
        {
            "id": "s_1464",
            "name": "Bradley Feest",
            "type": "sparse",
            "x": -4.077473163604736,
            "y": 20.168479919433594
        },
        {
            "id": "s_1465",
            "name": "Jana Wilms",
            "type": "sparse",
            "x": -3.675590991973877,
            "y": 19.89765739440918
        },
        {
            "id": "s_1466",
            "name": "Yukun Ren",
            "type": "sparse",
            "x": -11.00350570678711,
            "y": 19.2391357421875
        },
        {
            "id": "s_1467",
            "name": "Keiji Yamamoto",
            "type": "sparse",
            "x": -0.7683465480804443,
            "y": 17.96619415283203
        },
        {
            "id": "s_1468",
            "name": "Thomas Peterka",
            "type": "sparse",
            "x": 7.445589065551758,
            "y": 5.329070568084717
        },
        {
            "id": "s_1469",
            "name": "Zelin Ye",
            "type": "sparse",
            "x": -7.268019676208496,
            "y": 16.90699577331543
        },
        {
            "id": "s_1470",
            "name": "Hanze Jia",
            "type": "sparse",
            "x": -10.417634010314941,
            "y": 17.21779441833496
        },
        {
            "id": "s_1471",
            "name": "Zekai Shao",
            "type": "sparse",
            "x": -10.106639862060547,
            "y": 19.628948211669922
        },
        {
            "id": "s_1472",
            "name": "Can Liu 0004",
            "type": "sparse",
            "x": -10.307785987854004,
            "y": 19.634429931640625
        },
        {
            "id": "s_1473",
            "name": "Rui Qiu",
            "type": "sparse",
            "x": 7.535368919372559,
            "y": 5.239513874053955
        },
        {
            "id": "s_1474",
            "name": "Liu Ren",
            "type": "sparse",
            "x": -0.3933456242084503,
            "y": 17.653057098388672
        },
        {
            "id": "s_1475",
            "name": "Michael Wybrow",
            "type": "sparse",
            "x": -7.61427640914917,
            "y": 17.842355728149414
        },
        {
            "id": "s_1476",
            "name": "Donglai Wei 0001",
            "type": "sparse",
            "x": -8.867674827575684,
            "y": 16.563825607299805
        },
        {
            "id": "s_1477",
            "name": "Mengdi Sun",
            "type": "sparse",
            "x": -9.319121360778809,
            "y": 19.724449157714844
        },
        {
            "id": "s_1478",
            "name": "Benjamin Lee",
            "type": "sparse",
            "x": -7.592768669128418,
            "y": 17.94832992553711
        },
        {
            "id": "s_1479",
            "name": "Chi-Wing Fu",
            "type": "sparse",
            "x": -7.957512855529785,
            "y": 18.567398071289062
        },
        {
            "id": "s_1480",
            "name": "Catherine Plaisant",
            "type": "sparse",
            "x": -6.717489242553711,
            "y": 17.09869956970215
        },
        {
            "id": "s_1481",
            "name": "Yue Lin",
            "type": "sparse",
            "x": -9.73572063446045,
            "y": 17.956165313720703
        },
        {
            "id": "s_1482",
            "name": "Lily W. Ge",
            "type": "sparse",
            "x": -3.109827995300293,
            "y": 16.55494499206543
        },
        {
            "id": "s_1483",
            "name": "Tanner Finken",
            "type": "sparse",
            "x": -4.053013324737549,
            "y": 20.705419540405273
        },
        {
            "id": "s_1484",
            "name": "Venkatesh Sivaraman",
            "type": "sparse",
            "x": -3.8767361640930176,
            "y": 17.456418991088867
        },
        {
            "id": "s_1485",
            "name": "Jianing Hao",
            "type": "sparse",
            "x": -9.697150230407715,
            "y": 17.89105224609375
        },
        {
            "id": "s_1486",
            "name": "Hanspeter Pfister",
            "type": "sparse",
            "x": -8.495206832885742,
            "y": 16.570751190185547
        },
        {
            "id": "s_1487",
            "name": "Yvonne Jansen",
            "type": "sparse",
            "x": -6.3696064949035645,
            "y": 18.221940994262695
        },
        {
            "id": "s_1488",
            "name": "Marti A. Hearst",
            "type": "sparse",
            "x": -4.153527736663818,
            "y": 16.199932098388672
        },
        {
            "id": "s_1489",
            "name": "Thomas Torsney-Weir",
            "type": "sparse",
            "x": -7.311152935028076,
            "y": 16.825504302978516
        },
        {
            "id": "s_1490",
            "name": "Alfie Abdul-Rahman",
            "type": "sparse",
            "x": -6.992219924926758,
            "y": 16.99896240234375
        },
        {
            "id": "s_1491",
            "name": "Erik Rydow",
            "type": "sparse",
            "x": -7.328635215759277,
            "y": 16.87313461303711
        },
        {
            "id": "s_1492",
            "name": "Jared Jessup",
            "type": "sparse",
            "x": -8.648380279541016,
            "y": 16.4050350189209
        },
        {
            "id": "s_1493",
            "name": "James Tompkin 0001",
            "type": "sparse",
            "x": -6.711729526519775,
            "y": 21.187196731567383
        },
        {
            "id": "s_1494",
            "name": "Hoda Fakhari",
            "type": "sparse",
            "x": -3.1396644115448,
            "y": 16.666622161865234
        },
        {
            "id": "s_1495",
            "name": "Leo Yu-Ho Lo",
            "type": "sparse",
            "x": -6.798700332641602,
            "y": 17.173839569091797
        },
        {
            "id": "s_1496",
            "name": "Thomas Theu\u00dfl",
            "type": "sparse",
            "x": -8.684812545776367,
            "y": 16.368148803710938
        },
        {
            "id": "s_1497",
            "name": "Tanmay Kotha",
            "type": "sparse",
            "x": -1.8854765892028809,
            "y": 17.961185455322266
        },
        {
            "id": "s_1498",
            "name": "Zhiyang Shen",
            "type": "sparse",
            "x": -9.755993843078613,
            "y": 17.13572120666504
        },
        {
            "id": "s_1499",
            "name": "David Bedn\u00e1r",
            "type": "sparse",
            "x": -5.850201606750488,
            "y": 19.40745735168457
        },
        {
            "id": "s_1500",
            "name": "Nicolas Kruchten",
            "type": "sparse",
            "x": -3.721250057220459,
            "y": 17.122024536132812
        },
        {
            "id": "s_1501",
            "name": "Joshua A. Levine",
            "type": "sparse",
            "x": -4.045373916625977,
            "y": 20.701269149780273
        },
        {
            "id": "s_1502",
            "name": "Naiyu Wang",
            "type": "sparse",
            "x": -11.08207893371582,
            "y": 19.365097045898438
        },
        {
            "id": "s_1503",
            "name": "Marco Agus",
            "type": "sparse",
            "x": -8.883201599121094,
            "y": 16.188711166381836
        },
        {
            "id": "s_1504",
            "name": "Trevor Manz",
            "type": "sparse",
            "x": -8.44587516784668,
            "y": 16.993728637695312
        },
        {
            "id": "s_1505",
            "name": "Tianqing Fang",
            "type": "sparse",
            "x": -10.104231834411621,
            "y": 18.279314041137695
        },
        {
            "id": "s_1506",
            "name": "Michael Marschollek",
            "type": "sparse",
            "x": -5.844120025634766,
            "y": 15.330683708190918
        },
        {
            "id": "s_1507",
            "name": "Manuela Waldner",
            "type": "sparse",
            "x": -5.7896409034729,
            "y": 19.459659576416016
        },
        {
            "id": "s_1508",
            "name": "Mitchell Gordon",
            "type": "sparse",
            "x": -3.9729678630828857,
            "y": 16.97198486328125
        },
        {
            "id": "s_1509",
            "name": "Kai Fischbach",
            "type": "sparse",
            "x": -8.600748062133789,
            "y": 18.668048858642578
        },
        {
            "id": "s_1510",
            "name": "David Auber",
            "type": "sparse",
            "x": -5.672873497009277,
            "y": 15.148053169250488
        },
        {
            "id": "s_1511",
            "name": "Yao Ming",
            "type": "sparse",
            "x": -10.179506301879883,
            "y": 18.36944580078125
        },
        {
            "id": "s_1512",
            "name": "Yang Zhou",
            "type": "sparse",
            "x": -11.121076583862305,
            "y": 19.393980026245117
        },
        {
            "id": "s_1513",
            "name": "No\u00eblle Rakotondravony",
            "type": "sparse",
            "x": -3.08304762840271,
            "y": 16.519943237304688
        },
        {
            "id": "s_1514",
            "name": "Yuxing Zhou",
            "type": "sparse",
            "x": -9.677336692810059,
            "y": 17.13136863708496
        },
        {
            "id": "s_1515",
            "name": "Dieter Schmalstieg",
            "type": "sparse",
            "x": -8.408310890197754,
            "y": 18.734580993652344
        },
        {
            "id": "s_1516",
            "name": "Xinhuan Shu",
            "type": "sparse",
            "x": -10.034660339355469,
            "y": 16.98845100402832
        },
        {
            "id": "s_1517",
            "name": "Miriah Meyer",
            "type": "sparse",
            "x": -2.2564282417297363,
            "y": 15.420127868652344
        },
        {
            "id": "s_1518",
            "name": "Shaoyu Chen",
            "type": "sparse",
            "x": -3.9699246883392334,
            "y": 20.12213134765625
        },
        {
            "id": "s_1519",
            "name": "Xingdi Zhang",
            "type": "sparse",
            "x": -8.835102081298828,
            "y": 16.212642669677734
        },
        {
            "id": "s_1520",
            "name": "Xilong Shen",
            "type": "sparse",
            "x": -9.347790718078613,
            "y": 17.749141693115234
        },
        {
            "id": "s_1521",
            "name": "Yu Hou",
            "type": "sparse",
            "x": -8.457611083984375,
            "y": 17.25001335144043
        },
        {
            "id": "s_1522",
            "name": "Rebecca Kehlbeck",
            "type": "sparse",
            "x": -8.06950569152832,
            "y": 18.614540100097656
        },
        {
            "id": "s_1523",
            "name": "Jing Wu 0004",
            "type": "sparse",
            "x": -9.356537818908691,
            "y": 17.35885238647461
        },
        {
            "id": "s_1524",
            "name": "Zhilan Zhou",
            "type": "sparse",
            "x": -8.959749221801758,
            "y": 19.855480194091797
        },
        {
            "id": "s_1525",
            "name": "Jiahang Xu",
            "type": "sparse",
            "x": -8.912409782409668,
            "y": 19.962968826293945
        },
        {
            "id": "s_1526",
            "name": "Andreas M. Olligschlaeger",
            "type": "sparse",
            "x": 10.368907928466797,
            "y": 10.181319236755371
        },
        {
            "id": "s_1527",
            "name": "Haihan Lin",
            "type": "sparse",
            "x": -2.255525588989258,
            "y": 15.384383201599121
        },
        {
            "id": "s_1528",
            "name": "Zhanna Kaufman",
            "type": "sparse",
            "x": -3.9943246841430664,
            "y": 16.619277954101562
        },
        {
            "id": "s_1529",
            "name": "Matthias Trapp 0001",
            "type": "sparse",
            "x": -8.93542766571045,
            "y": 18.401790618896484
        },
        {
            "id": "s_1530",
            "name": "Peter Filzmoser",
            "type": "sparse",
            "x": -5.404230117797852,
            "y": 14.87740707397461
        },
        {
            "id": "s_1531",
            "name": "Xumeng Wang",
            "type": "sparse",
            "x": -9.894903182983398,
            "y": 18.77325439453125
        },
        {
            "id": "s_1532",
            "name": "Kazi Tasnim Zinat",
            "type": "sparse",
            "x": -4.379519462585449,
            "y": 17.012916564941406
        },
        {
            "id": "s_1533",
            "name": "Jonathan Roberts 0002",
            "type": "sparse",
            "x": -4.916120529174805,
            "y": 16.011247634887695
        },
        {
            "id": "s_1534",
            "name": "Hyunjoo Song",
            "type": "sparse",
            "x": -1.36797297000885,
            "y": 18.070335388183594
        },
        {
            "id": "s_1535",
            "name": "Roy A. Ruddle",
            "type": "sparse",
            "x": -6.444340229034424,
            "y": 16.49191665649414
        },
        {
            "id": "s_1536",
            "name": "Yuan Tian",
            "type": "sparse",
            "x": -10.353653907775879,
            "y": 17.085824966430664
        },
        {
            "id": "s_1537",
            "name": "Ryan McKendrick",
            "type": "sparse",
            "x": -4.077607154846191,
            "y": 20.16655921936035
        },
        {
            "id": "s_1538",
            "name": "James Scott-Brown",
            "type": "sparse",
            "x": -6.712653636932373,
            "y": 17.103872299194336
        },
        {
            "id": "s_1539",
            "name": "Kaiyuan Zhang 0002",
            "type": "sparse",
            "x": -9.353632926940918,
            "y": 18.054176330566406
        },
        {
            "id": "s_1540",
            "name": "Michaela Schmitt",
            "type": "sparse",
            "x": -3.734907388687134,
            "y": 20.943391799926758
        },
        {
            "id": "s_1541",
            "name": "Tyler Estro",
            "type": "sparse",
            "x": -9.259185791015625,
            "y": 18.812528610229492
        },
        {
            "id": "s_1542",
            "name": "Saiful Khan",
            "type": "sparse",
            "x": -7.076756954193115,
            "y": 16.993837356567383
        },
        {
            "id": "s_1543",
            "name": "Fuqi Xie",
            "type": "sparse",
            "x": -6.425889492034912,
            "y": 18.719938278198242
        },
        {
            "id": "s_1544",
            "name": "Adrien Bibal",
            "type": "sparse",
            "x": -8.342657089233398,
            "y": 18.79205894470215
        },
        {
            "id": "s_1545",
            "name": "Nicolas Heulot",
            "type": "sparse",
            "x": -6.32719612121582,
            "y": 18.383583068847656
        },
        {
            "id": "s_1546",
            "name": "M. Lieser",
            "type": "sparse",
            "x": -5.841914653778076,
            "y": 15.329421043395996
        },
        {
            "id": "s_1547",
            "name": "Uta Hinrichs",
            "type": "sparse",
            "x": -6.518893718719482,
            "y": 17.164384841918945
        },
        {
            "id": "s_1548",
            "name": "Hamza Elhamdadi",
            "type": "sparse",
            "x": -2.2140488624572754,
            "y": 17.78638458251953
        },
        {
            "id": "s_1549",
            "name": "Naren Ramakrishnan",
            "type": "sparse",
            "x": -1.9891523122787476,
            "y": 16.007986068725586
        },
        {
            "id": "s_1550",
            "name": "Anna Sterzik",
            "type": "sparse",
            "x": -3.6773271560668945,
            "y": 19.884061813354492
        },
        {
            "id": "s_1551",
            "name": "Moataz Abdelaal",
            "type": "sparse",
            "x": -8.44299030303955,
            "y": 19.168216705322266
        },
        {
            "id": "s_1552",
            "name": "Qiangqiang Liu",
            "type": "sparse",
            "x": -11.074881553649902,
            "y": 19.327585220336914
        },
        {
            "id": "s_1553",
            "name": "Jin Wen",
            "type": "sparse",
            "x": -9.326093673706055,
            "y": 17.920475006103516
        },
        {
            "id": "s_1554",
            "name": "Minsuk Kahng",
            "type": "sparse",
            "x": 10.039114952087402,
            "y": 10.500033378601074
        },
        {
            "id": "s_1555",
            "name": "Carter Emmart",
            "type": "sparse",
            "x": -4.046583652496338,
            "y": 20.160768508911133
        },
        {
            "id": "s_1556",
            "name": "Xiaojuan Ma",
            "type": "sparse",
            "x": -10.833168029785156,
            "y": 18.586801528930664
        },
        {
            "id": "s_1557",
            "name": "Min Lu 0002",
            "type": "sparse",
            "x": -7.993812561035156,
            "y": 18.758071899414062
        },
        {
            "id": "s_1558",
            "name": "Peter Mindek",
            "type": "sparse",
            "x": -6.229496479034424,
            "y": 19.019474029541016
        },
        {
            "id": "s_1559",
            "name": "Qing Shi",
            "type": "sparse",
            "x": -9.719649314880371,
            "y": 17.89365005493164
        },
        {
            "id": "s_1560",
            "name": "Paolo Buono",
            "type": "sparse",
            "x": -6.453737258911133,
            "y": 17.020761489868164
        },
        {
            "id": "s_1561",
            "name": "Qiaomu Shen",
            "type": "sparse",
            "x": -9.290950775146484,
            "y": 20.274900436401367
        },
        {
            "id": "s_1562",
            "name": "Jo\u00e3o Rulff",
            "type": "sparse",
            "x": -4.05144739151001,
            "y": 20.091293334960938
        },
        {
            "id": "s_1563",
            "name": "Hyunwook Lee",
            "type": "sparse",
            "x": -1.857552170753479,
            "y": 17.663297653198242
        },
        {
            "id": "s_1564",
            "name": "Tim Cech",
            "type": "sparse",
            "x": -8.940621376037598,
            "y": 18.418588638305664
        },
        {
            "id": "s_1565",
            "name": "Soumyadeep Basu",
            "type": "sparse",
            "x": -6.861433029174805,
            "y": 14.333842277526855
        },
        {
            "id": "s_1566",
            "name": "Ingo Wald",
            "type": "sparse",
            "x": -15.054770469665527,
            "y": 7.119626045227051
        },
        {
            "id": "s_1567",
            "name": "Longfei Chen",
            "type": "sparse",
            "x": -11.072134017944336,
            "y": 19.356746673583984
        },
        {
            "id": "s_1568",
            "name": "Lucas Alexandre",
            "type": "sparse",
            "x": -7.508215427398682,
            "y": -1.1689794063568115
        },
        {
            "id": "s_1569",
            "name": "Yuwen Pu",
            "type": "sparse",
            "x": -10.402508735656738,
            "y": 17.09678077697754
        },
        {
            "id": "s_1570",
            "name": "Matthew Hull",
            "type": "sparse",
            "x": 10.294321060180664,
            "y": 10.253989219665527
        },
        {
            "id": "s_1571",
            "name": "Angela Nyhout",
            "type": "sparse",
            "x": -6.604841709136963,
            "y": 16.792936325073242
        },
        {
            "id": "s_1572",
            "name": "Axel Wendt",
            "type": "sparse",
            "x": -0.39257675409317017,
            "y": 17.57428741455078
        },
        {
            "id": "s_1573",
            "name": "Torsten M\u00f6ller",
            "type": "sparse",
            "x": -8.52580451965332,
            "y": 16.71403694152832
        },
        {
            "id": "s_1574",
            "name": "Xiao Xie",
            "type": "sparse",
            "x": -10.630865097045898,
            "y": 17.514497756958008
        },
        {
            "id": "s_1575",
            "name": "Huy T. Vo",
            "type": "sparse",
            "x": -3.9696807861328125,
            "y": 20.056554794311523
        },
        {
            "id": "s_1576",
            "name": "Nils Lichtenberg",
            "type": "sparse",
            "x": -3.683694362640381,
            "y": 19.905879974365234
        },
        {
            "id": "s_1577",
            "name": "Joshua Vander Hook",
            "type": "sparse",
            "x": -0.7234196066856384,
            "y": 17.928709030151367
        },
        {
            "id": "s_1578",
            "name": "Narges Mahyar",
            "type": "sparse",
            "x": -4.3950581550598145,
            "y": 15.192594528198242
        },
        {
            "id": "s_1579",
            "name": "Samuel Reinders",
            "type": "sparse",
            "x": -7.673722267150879,
            "y": 18.09122657775879
        },
        {
            "id": "s_1580",
            "name": "Matthias Miller",
            "type": "sparse",
            "x": -7.043935298919678,
            "y": 20.863574981689453
        },
        {
            "id": "s_1581",
            "name": "Lin Yan 0003",
            "type": "sparse",
            "x": 7.450819492340088,
            "y": 5.323950290679932
        },
        {
            "id": "s_1582",
            "name": "Nicola Colaninno",
            "type": "sparse",
            "x": -7.4780731201171875,
            "y": -1.1991655826568604
        },
        {
            "id": "s_1583",
            "name": "Youfu Yan",
            "type": "sparse",
            "x": -8.486122131347656,
            "y": 17.296466827392578
        },
        {
            "id": "s_1584",
            "name": "Juri F. Buchm\u00fcller",
            "type": "sparse",
            "x": -7.0657639503479,
            "y": 20.839933395385742
        },
        {
            "id": "s_1585",
            "name": "Jiayi Zhou",
            "type": "sparse",
            "x": -10.415562629699707,
            "y": 17.255462646484375
        },
        {
            "id": "s_1586",
            "name": "Jonathas Costa",
            "type": "sparse",
            "x": -4.037154197692871,
            "y": 20.155906677246094
        },
        {
            "id": "s_1587",
            "name": "Bernhard Eberhardt",
            "type": "sparse",
            "x": -8.543477058410645,
            "y": 19.328170776367188
        },
        {
            "id": "s_1588",
            "name": "Tingying He",
            "type": "sparse",
            "x": -6.052093505859375,
            "y": 18.560632705688477
        },
        {
            "id": "s_1589",
            "name": "Zhutian Chen",
            "type": "sparse",
            "x": -9.531813621520996,
            "y": 16.904354095458984
        },
        {
            "id": "s_1590",
            "name": "Adam Coscia",
            "type": "sparse",
            "x": -4.756389141082764,
            "y": 15.47297191619873
        },
        {
            "id": "s_1591",
            "name": "Michael Stonebraker",
            "type": "sparse",
            "x": -4.715044975280762,
            "y": 16.240610122680664
        },
        {
            "id": "s_1592",
            "name": "Saimadhav Naga Sakhamuri",
            "type": "sparse",
            "x": -4.3789520263671875,
            "y": 17.015316009521484
        },
        {
            "id": "s_1593",
            "name": "Devanshu Arya",
            "type": "sparse",
            "x": -7.036038875579834,
            "y": 20.871631622314453
        },
        {
            "id": "s_1594",
            "name": "Farooq Naeem",
            "type": "sparse",
            "x": -6.596611976623535,
            "y": 16.81273651123047
        },
        {
            "id": "s_1595",
            "name": "Alexandra Lee",
            "type": "sparse",
            "x": -5.853999614715576,
            "y": 15.357603073120117
        },
        {
            "id": "s_1596",
            "name": "Kuno Kurzhals",
            "type": "sparse",
            "x": -8.460583686828613,
            "y": 19.191970825195312
        },
        {
            "id": "s_1597",
            "name": "Yiyao Wang",
            "type": "sparse",
            "x": -9.352527618408203,
            "y": 18.079967498779297
        },
        {
            "id": "s_1598",
            "name": "Kai Lawonn",
            "type": "sparse",
            "x": -3.595963716506958,
            "y": 19.84259605407715
        },
        {
            "id": "s_1599",
            "name": "Alex Endert",
            "type": "sparse",
            "x": -4.8283586502075195,
            "y": 15.629190444946289
        },
        {
            "id": "s_1600",
            "name": "Suting Hong",
            "type": "sparse",
            "x": -11.00661563873291,
            "y": 19.32915687561035
        },
        {
            "id": "s_1601",
            "name": "Jason Wiese",
            "type": "sparse",
            "x": -2.25361704826355,
            "y": 15.369855880737305
        },
        {
            "id": "s_1602",
            "name": "Attila Gyulassy",
            "type": "sparse",
            "x": -15.04552173614502,
            "y": 7.128807544708252
        },
        {
            "id": "s_1603",
            "name": "Siwei Tan",
            "type": "sparse",
            "x": -10.142122268676758,
            "y": 18.646146774291992
        },
        {
            "id": "s_1604",
            "name": "Hui Fang 0003",
            "type": "sparse",
            "x": -7.3528008460998535,
            "y": 16.82513999938965
        },
        {
            "id": "s_1605",
            "name": "Yangqiu Song",
            "type": "sparse",
            "x": -10.117268562316895,
            "y": 18.616804122924805
        },
        {
            "id": "s_1606",
            "name": "Dawar Khan",
            "type": "sparse",
            "x": -6.249751091003418,
            "y": 18.89618492126465
        },
        {
            "id": "s_1607",
            "name": "Rishi Vanukuru",
            "type": "sparse",
            "x": -2.170273780822754,
            "y": 18.062236785888672
        },
        {
            "id": "s_1608",
            "name": "Aryaman Bahukhandi",
            "type": "sparse",
            "x": -0.6952912211418152,
            "y": 17.977462768554688
        },
        {
            "id": "s_1609",
            "name": "Christopher Collins 0001",
            "type": "sparse",
            "x": -2.4551427364349365,
            "y": 17.91096305847168
        },
        {
            "id": "s_1610",
            "name": "Stanislav Goja",
            "type": "sparse",
            "x": -6.485772609710693,
            "y": 13.773940086364746
        },
        {
            "id": "s_1611",
            "name": "Yuhua Zhou",
            "type": "sparse",
            "x": -10.150810241699219,
            "y": 17.068288803100586
        },
        {
            "id": "s_1612",
            "name": "Zhenge Zhao",
            "type": "sparse",
            "x": -0.3844202756881714,
            "y": 17.481143951416016
        },
        {
            "id": "s_1613",
            "name": "Hans Hasse",
            "type": "sparse",
            "x": -3.72723388671875,
            "y": 20.949045181274414
        },
        {
            "id": "s_1614",
            "name": "Chiokit Leong",
            "type": "sparse",
            "x": -10.093785285949707,
            "y": 19.678823471069336
        },
        {
            "id": "s_1615",
            "name": "Suphanut Jamonnak",
            "type": "sparse",
            "x": -0.3443552851676941,
            "y": 17.56300163269043
        },
        {
            "id": "s_1616",
            "name": "Renfei Huang",
            "type": "sparse",
            "x": -10.114692687988281,
            "y": 18.26224136352539
        },
        {
            "id": "s_1617",
            "name": "Tushar M. Athawale",
            "type": "sparse",
            "x": -1.8926960229873657,
            "y": 17.965805053710938
        }
    ],
    "links": [
        {
            "source": "r_0",
            "target": "s_1297"
        },
        {
            "source": "r_0",
            "target": "s_1205"
        },
        {
            "source": "r_0",
            "target": "s_729"
        },
        {
            "source": "r_0",
            "target": "s_26"
        },
        {
            "source": "r_0",
            "target": "s_244"
        },
        {
            "source": "r_0",
            "target": "s_1104"
        },
        {
            "source": "r_0",
            "target": "s_1554"
        },
        {
            "source": "r_0",
            "target": "s_665"
        },
        {
            "source": "r_1",
            "target": "s_257"
        },
        {
            "source": "r_1",
            "target": "s_755"
        },
        {
            "source": "r_1",
            "target": "s_1384"
        },
        {
            "source": "r_2",
            "target": "s_1478"
        },
        {
            "source": "r_2",
            "target": "s_1434"
        },
        {
            "source": "r_2",
            "target": "s_1321"
        },
        {
            "source": "r_2",
            "target": "s_269"
        },
        {
            "source": "r_2",
            "target": "s_55"
        },
        {
            "source": "r_2",
            "target": "s_210"
        },
        {
            "source": "r_3",
            "target": "s_455"
        },
        {
            "source": "r_3",
            "target": "s_1082"
        },
        {
            "source": "r_3",
            "target": "s_226"
        },
        {
            "source": "r_3",
            "target": "s_184"
        },
        {
            "source": "r_3",
            "target": "s_989"
        },
        {
            "source": "r_4",
            "target": "s_1027"
        },
        {
            "source": "r_4",
            "target": "s_27"
        },
        {
            "source": "r_4",
            "target": "s_850"
        },
        {
            "source": "r_5",
            "target": "s_996"
        },
        {
            "source": "r_5",
            "target": "s_1236"
        },
        {
            "source": "r_5",
            "target": "s_851"
        },
        {
            "source": "r_5",
            "target": "s_251"
        },
        {
            "source": "r_5",
            "target": "s_689"
        },
        {
            "source": "r_5",
            "target": "s_1486"
        },
        {
            "source": "r_5",
            "target": "s_514"
        },
        {
            "source": "r_6",
            "target": "s_709"
        },
        {
            "source": "r_6",
            "target": "s_1210"
        },
        {
            "source": "r_6",
            "target": "s_353"
        },
        {
            "source": "r_6",
            "target": "s_1605"
        },
        {
            "source": "r_6",
            "target": "s_156"
        },
        {
            "source": "r_7",
            "target": "s_1429"
        },
        {
            "source": "r_7",
            "target": "s_361"
        },
        {
            "source": "r_7",
            "target": "s_482"
        },
        {
            "source": "r_7",
            "target": "s_1162"
        },
        {
            "source": "r_7",
            "target": "s_1049"
        },
        {
            "source": "r_7",
            "target": "s_1152"
        },
        {
            "source": "r_7",
            "target": "s_348"
        },
        {
            "source": "r_7",
            "target": "s_1121"
        },
        {
            "source": "r_7",
            "target": "s_308"
        },
        {
            "source": "r_7",
            "target": "s_134"
        },
        {
            "source": "r_8",
            "target": "s_297"
        },
        {
            "source": "r_8",
            "target": "s_920"
        },
        {
            "source": "r_9",
            "target": "s_626"
        },
        {
            "source": "r_9",
            "target": "s_1589"
        },
        {
            "source": "r_9",
            "target": "s_152"
        },
        {
            "source": "r_9",
            "target": "s_753"
        },
        {
            "source": "r_9",
            "target": "s_237"
        },
        {
            "source": "r_9",
            "target": "s_1249"
        },
        {
            "source": "r_9",
            "target": "s_423"
        },
        {
            "source": "r_9",
            "target": "s_1415"
        },
        {
            "source": "r_10",
            "target": "s_875"
        },
        {
            "source": "r_10",
            "target": "s_1031"
        },
        {
            "source": "r_10",
            "target": "s_1411"
        },
        {
            "source": "r_11",
            "target": "s_1021"
        },
        {
            "source": "r_11",
            "target": "s_100"
        },
        {
            "source": "r_11",
            "target": "s_269"
        },
        {
            "source": "r_11",
            "target": "s_420"
        },
        {
            "source": "r_11",
            "target": "s_427"
        },
        {
            "source": "r_11",
            "target": "s_83"
        },
        {
            "source": "r_11",
            "target": "s_1011"
        },
        {
            "source": "r_11",
            "target": "s_1168"
        },
        {
            "source": "r_11",
            "target": "s_1099"
        },
        {
            "source": "r_11",
            "target": "s_210"
        },
        {
            "source": "r_12",
            "target": "s_574"
        },
        {
            "source": "r_12",
            "target": "s_1438"
        },
        {
            "source": "r_12",
            "target": "s_596"
        },
        {
            "source": "r_12",
            "target": "s_569"
        },
        {
            "source": "r_12",
            "target": "s_1533"
        },
        {
            "source": "r_12",
            "target": "s_366"
        },
        {
            "source": "r_13",
            "target": "s_700"
        },
        {
            "source": "r_13",
            "target": "s_1156"
        },
        {
            "source": "r_13",
            "target": "s_529"
        },
        {
            "source": "r_13",
            "target": "s_603"
        },
        {
            "source": "r_14",
            "target": "s_897"
        },
        {
            "source": "r_14",
            "target": "s_1511"
        },
        {
            "source": "r_14",
            "target": "s_156"
        },
        {
            "source": "r_15",
            "target": "s_1221"
        },
        {
            "source": "r_15",
            "target": "s_663"
        },
        {
            "source": "r_15",
            "target": "s_1490"
        },
        {
            "source": "r_15",
            "target": "s_96"
        },
        {
            "source": "r_15",
            "target": "s_1542"
        },
        {
            "source": "r_15",
            "target": "s_724"
        },
        {
            "source": "r_15",
            "target": "s_298"
        },
        {
            "source": "r_16",
            "target": "s_735"
        },
        {
            "source": "r_16",
            "target": "s_758"
        },
        {
            "source": "r_16",
            "target": "s_1308"
        },
        {
            "source": "r_16",
            "target": "s_627"
        },
        {
            "source": "r_16",
            "target": "s_1189"
        },
        {
            "source": "r_16",
            "target": "s_105"
        },
        {
            "source": "r_16",
            "target": "s_79"
        },
        {
            "source": "r_16",
            "target": "s_1415"
        },
        {
            "source": "r_17",
            "target": "s_152"
        },
        {
            "source": "r_17",
            "target": "s_1574"
        },
        {
            "source": "r_17",
            "target": "s_626"
        },
        {
            "source": "r_17",
            "target": "s_336"
        },
        {
            "source": "r_17",
            "target": "s_939"
        },
        {
            "source": "r_17",
            "target": "s_1167"
        },
        {
            "source": "r_17",
            "target": "s_1589"
        },
        {
            "source": "r_17",
            "target": "s_647"
        },
        {
            "source": "r_17",
            "target": "s_1415"
        },
        {
            "source": "r_18",
            "target": "s_141"
        },
        {
            "source": "r_18",
            "target": "s_699"
        },
        {
            "source": "r_18",
            "target": "s_1043"
        },
        {
            "source": "r_18",
            "target": "s_254"
        },
        {
            "source": "r_18",
            "target": "s_1180"
        },
        {
            "source": "r_18",
            "target": "s_1572"
        },
        {
            "source": "r_18",
            "target": "s_1474"
        },
        {
            "source": "r_19",
            "target": "s_1235"
        },
        {
            "source": "r_19",
            "target": "s_538"
        },
        {
            "source": "r_19",
            "target": "s_1272"
        },
        {
            "source": "r_19",
            "target": "s_449"
        },
        {
            "source": "r_19",
            "target": "s_1210"
        },
        {
            "source": "r_19",
            "target": "s_156"
        },
        {
            "source": "r_20",
            "target": "s_1589"
        },
        {
            "source": "r_20",
            "target": "s_626"
        },
        {
            "source": "r_20",
            "target": "s_152"
        },
        {
            "source": "r_20",
            "target": "s_1108"
        },
        {
            "source": "r_20",
            "target": "s_647"
        },
        {
            "source": "r_20",
            "target": "s_156"
        },
        {
            "source": "r_20",
            "target": "s_1415"
        },
        {
            "source": "r_21",
            "target": "s_1574"
        },
        {
            "source": "r_21",
            "target": "s_887"
        },
        {
            "source": "r_21",
            "target": "s_495"
        },
        {
            "source": "r_21",
            "target": "s_1341"
        },
        {
            "source": "r_21",
            "target": "s_409"
        },
        {
            "source": "r_21",
            "target": "s_647"
        },
        {
            "source": "r_21",
            "target": "s_1385"
        },
        {
            "source": "r_21",
            "target": "s_1415"
        },
        {
            "source": "r_22",
            "target": "s_1478"
        },
        {
            "source": "r_22",
            "target": "s_84"
        },
        {
            "source": "r_22",
            "target": "s_274"
        },
        {
            "source": "r_22",
            "target": "s_677"
        },
        {
            "source": "r_22",
            "target": "s_1126"
        },
        {
            "source": "r_22",
            "target": "s_210"
        },
        {
            "source": "r_23",
            "target": "s_308"
        },
        {
            "source": "r_23",
            "target": "s_570"
        },
        {
            "source": "r_23",
            "target": "s_1284"
        },
        {
            "source": "r_23",
            "target": "s_1123"
        },
        {
            "source": "r_23",
            "target": "s_1146"
        },
        {
            "source": "r_23",
            "target": "s_348"
        },
        {
            "source": "r_24",
            "target": "s_470"
        },
        {
            "source": "r_24",
            "target": "s_1002"
        },
        {
            "source": "r_24",
            "target": "s_783"
        },
        {
            "source": "r_24",
            "target": "s_1042"
        },
        {
            "source": "r_24",
            "target": "s_393"
        },
        {
            "source": "r_25",
            "target": "s_1266"
        },
        {
            "source": "r_25",
            "target": "s_470"
        },
        {
            "source": "r_25",
            "target": "s_1344"
        },
        {
            "source": "r_25",
            "target": "s_393"
        },
        {
            "source": "r_26",
            "target": "s_41"
        },
        {
            "source": "r_26",
            "target": "s_1321"
        },
        {
            "source": "r_26",
            "target": "s_689"
        },
        {
            "source": "r_26",
            "target": "s_210"
        },
        {
            "source": "r_26",
            "target": "s_1305"
        },
        {
            "source": "r_26",
            "target": "s_1486"
        },
        {
            "source": "r_27",
            "target": "s_1325"
        },
        {
            "source": "r_27",
            "target": "s_163"
        },
        {
            "source": "r_27",
            "target": "s_508"
        },
        {
            "source": "r_27",
            "target": "s_493"
        },
        {
            "source": "r_27",
            "target": "s_173"
        },
        {
            "source": "r_28",
            "target": "s_456"
        },
        {
            "source": "r_28",
            "target": "s_62"
        },
        {
            "source": "r_28",
            "target": "s_911"
        },
        {
            "source": "r_28",
            "target": "s_311"
        },
        {
            "source": "r_28",
            "target": "s_1380"
        },
        {
            "source": "r_28",
            "target": "s_446"
        },
        {
            "source": "r_28",
            "target": "s_1280"
        },
        {
            "source": "r_28",
            "target": "s_932"
        },
        {
            "source": "r_28",
            "target": "s_346"
        },
        {
            "source": "r_28",
            "target": "s_1415"
        },
        {
            "source": "r_29",
            "target": "s_1288"
        },
        {
            "source": "r_29",
            "target": "s_875"
        },
        {
            "source": "r_29",
            "target": "s_72"
        },
        {
            "source": "r_29",
            "target": "s_1386"
        },
        {
            "source": "r_30",
            "target": "s_14"
        },
        {
            "source": "r_30",
            "target": "s_817"
        },
        {
            "source": "r_30",
            "target": "s_1335"
        },
        {
            "source": "r_30",
            "target": "s_429"
        },
        {
            "source": "r_30",
            "target": "s_1401"
        },
        {
            "source": "r_31",
            "target": "s_183"
        },
        {
            "source": "r_31",
            "target": "s_883"
        },
        {
            "source": "r_31",
            "target": "s_405"
        },
        {
            "source": "r_31",
            "target": "s_1449"
        },
        {
            "source": "r_31",
            "target": "s_58"
        },
        {
            "source": "r_31",
            "target": "s_997"
        },
        {
            "source": "r_32",
            "target": "s_1308"
        },
        {
            "source": "r_32",
            "target": "s_735"
        },
        {
            "source": "r_32",
            "target": "s_1574"
        },
        {
            "source": "r_32",
            "target": "s_1189"
        },
        {
            "source": "r_32",
            "target": "s_105"
        },
        {
            "source": "r_32",
            "target": "s_79"
        },
        {
            "source": "r_32",
            "target": "s_1385"
        },
        {
            "source": "r_32",
            "target": "s_1415"
        },
        {
            "source": "r_33",
            "target": "s_1087"
        },
        {
            "source": "r_33",
            "target": "s_1160"
        },
        {
            "source": "r_33",
            "target": "s_270"
        },
        {
            "source": "r_33",
            "target": "s_246"
        },
        {
            "source": "r_33",
            "target": "s_1415"
        },
        {
            "source": "r_34",
            "target": "s_1132"
        },
        {
            "source": "r_34",
            "target": "s_477"
        },
        {
            "source": "r_35",
            "target": "s_293"
        },
        {
            "source": "r_35",
            "target": "s_226"
        },
        {
            "source": "r_35",
            "target": "s_1082"
        },
        {
            "source": "r_35",
            "target": "s_1299"
        },
        {
            "source": "r_35",
            "target": "s_351"
        },
        {
            "source": "r_35",
            "target": "s_989"
        },
        {
            "source": "r_36",
            "target": "s_1516"
        },
        {
            "source": "r_36",
            "target": "s_877"
        },
        {
            "source": "r_36",
            "target": "s_765"
        },
        {
            "source": "r_36",
            "target": "s_1221"
        },
        {
            "source": "r_36",
            "target": "s_1415"
        },
        {
            "source": "r_36",
            "target": "s_156"
        },
        {
            "source": "r_37",
            "target": "s_101"
        },
        {
            "source": "r_37",
            "target": "s_959"
        },
        {
            "source": "r_37",
            "target": "s_1347"
        },
        {
            "source": "r_37",
            "target": "s_533"
        },
        {
            "source": "r_37",
            "target": "s_728"
        },
        {
            "source": "r_37",
            "target": "s_428"
        },
        {
            "source": "r_38",
            "target": "s_877"
        },
        {
            "source": "r_38",
            "target": "s_737"
        },
        {
            "source": "r_38",
            "target": "s_367"
        },
        {
            "source": "r_38",
            "target": "s_1055"
        },
        {
            "source": "r_38",
            "target": "s_404"
        },
        {
            "source": "r_38",
            "target": "s_156"
        },
        {
            "source": "r_38",
            "target": "s_155"
        },
        {
            "source": "r_39",
            "target": "s_721"
        },
        {
            "source": "r_39",
            "target": "s_459"
        },
        {
            "source": "r_39",
            "target": "s_1387"
        },
        {
            "source": "r_39",
            "target": "s_157"
        },
        {
            "source": "r_39",
            "target": "s_976"
        },
        {
            "source": "r_39",
            "target": "s_989"
        },
        {
            "source": "r_40",
            "target": "s_55"
        },
        {
            "source": "r_40",
            "target": "s_1008"
        },
        {
            "source": "r_40",
            "target": "s_1389"
        },
        {
            "source": "r_40",
            "target": "s_589"
        },
        {
            "source": "r_40",
            "target": "s_830"
        },
        {
            "source": "r_40",
            "target": "s_960"
        },
        {
            "source": "r_41",
            "target": "s_517"
        },
        {
            "source": "r_41",
            "target": "s_1439"
        },
        {
            "source": "r_41",
            "target": "s_1010"
        },
        {
            "source": "r_41",
            "target": "s_1382"
        },
        {
            "source": "r_41",
            "target": "s_404"
        },
        {
            "source": "r_41",
            "target": "s_155"
        },
        {
            "source": "r_42",
            "target": "s_421"
        },
        {
            "source": "r_42",
            "target": "s_425"
        },
        {
            "source": "r_42",
            "target": "s_308"
        },
        {
            "source": "r_42",
            "target": "s_1280"
        },
        {
            "source": "r_42",
            "target": "s_348"
        },
        {
            "source": "r_43",
            "target": "s_368"
        },
        {
            "source": "r_43",
            "target": "s_1239"
        },
        {
            "source": "r_43",
            "target": "s_12"
        },
        {
            "source": "r_43",
            "target": "s_747"
        },
        {
            "source": "r_43",
            "target": "s_176"
        },
        {
            "source": "r_43",
            "target": "s_1114"
        },
        {
            "source": "r_43",
            "target": "s_1535"
        },
        {
            "source": "r_44",
            "target": "s_1317"
        },
        {
            "source": "r_44",
            "target": "s_1310"
        },
        {
            "source": "r_44",
            "target": "s_634"
        },
        {
            "source": "r_44",
            "target": "s_586"
        },
        {
            "source": "r_44",
            "target": "s_1467"
        },
        {
            "source": "r_44",
            "target": "s_789"
        },
        {
            "source": "r_45",
            "target": "s_1393"
        },
        {
            "source": "r_45",
            "target": "s_1146"
        },
        {
            "source": "r_45",
            "target": "s_1441"
        },
        {
            "source": "r_45",
            "target": "s_1479"
        },
        {
            "source": "r_45",
            "target": "s_93"
        },
        {
            "source": "r_45",
            "target": "s_646"
        },
        {
            "source": "r_46",
            "target": "s_1311"
        },
        {
            "source": "r_46",
            "target": "s_1115"
        },
        {
            "source": "r_46",
            "target": "s_750"
        },
        {
            "source": "r_46",
            "target": "s_1358"
        },
        {
            "source": "r_46",
            "target": "s_459"
        },
        {
            "source": "r_46",
            "target": "s_184"
        },
        {
            "source": "r_46",
            "target": "s_156"
        },
        {
            "source": "r_46",
            "target": "s_989"
        },
        {
            "source": "r_47",
            "target": "s_858"
        },
        {
            "source": "r_47",
            "target": "s_1386"
        },
        {
            "source": "r_48",
            "target": "s_842"
        },
        {
            "source": "r_48",
            "target": "s_1340"
        },
        {
            "source": "r_48",
            "target": "s_1331"
        },
        {
            "source": "r_49",
            "target": "s_1121"
        },
        {
            "source": "r_49",
            "target": "s_912"
        },
        {
            "source": "r_49",
            "target": "s_1520"
        },
        {
            "source": "r_49",
            "target": "s_142"
        },
        {
            "source": "r_49",
            "target": "s_849"
        },
        {
            "source": "r_49",
            "target": "s_800"
        },
        {
            "source": "r_49",
            "target": "s_1429"
        },
        {
            "source": "r_49",
            "target": "s_1385"
        },
        {
            "source": "r_50",
            "target": "s_1080"
        },
        {
            "source": "r_50",
            "target": "s_781"
        },
        {
            "source": "r_51",
            "target": "s_1531"
        },
        {
            "source": "r_51",
            "target": "s_1385"
        },
        {
            "source": "r_51",
            "target": "s_308"
        },
        {
            "source": "r_51",
            "target": "s_1451"
        },
        {
            "source": "r_51",
            "target": "s_287"
        },
        {
            "source": "r_51",
            "target": "s_1202"
        },
        {
            "source": "r_51",
            "target": "s_79"
        },
        {
            "source": "r_51",
            "target": "s_578"
        },
        {
            "source": "r_52",
            "target": "s_1612"
        },
        {
            "source": "r_52",
            "target": "s_1204"
        },
        {
            "source": "r_52",
            "target": "s_616"
        },
        {
            "source": "r_52",
            "target": "s_1474"
        },
        {
            "source": "r_53",
            "target": "s_897"
        },
        {
            "source": "r_53",
            "target": "s_1160"
        },
        {
            "source": "r_53",
            "target": "s_1226"
        },
        {
            "source": "r_53",
            "target": "s_596"
        },
        {
            "source": "r_53",
            "target": "s_1273"
        },
        {
            "source": "r_53",
            "target": "s_61"
        },
        {
            "source": "r_53",
            "target": "s_156"
        },
        {
            "source": "r_53",
            "target": "s_824"
        },
        {
            "source": "r_54",
            "target": "s_1317"
        },
        {
            "source": "r_54",
            "target": "s_670"
        },
        {
            "source": "r_54",
            "target": "s_1425"
        },
        {
            "source": "r_54",
            "target": "s_789"
        },
        {
            "source": "r_55",
            "target": "s_750"
        },
        {
            "source": "r_55",
            "target": "s_184"
        },
        {
            "source": "r_55",
            "target": "s_1131"
        },
        {
            "source": "r_55",
            "target": "s_1088"
        },
        {
            "source": "r_55",
            "target": "s_989"
        },
        {
            "source": "r_56",
            "target": "s_1438"
        },
        {
            "source": "r_56",
            "target": "s_974"
        },
        {
            "source": "r_56",
            "target": "s_461"
        },
        {
            "source": "r_56",
            "target": "s_676"
        },
        {
            "source": "r_56",
            "target": "s_308"
        },
        {
            "source": "r_56",
            "target": "s_96"
        },
        {
            "source": "r_56",
            "target": "s_1385"
        },
        {
            "source": "r_57",
            "target": "s_753"
        },
        {
            "source": "r_57",
            "target": "s_431"
        },
        {
            "source": "r_57",
            "target": "s_667"
        },
        {
            "source": "r_57",
            "target": "s_948"
        },
        {
            "source": "r_57",
            "target": "s_779"
        },
        {
            "source": "r_57",
            "target": "s_938"
        },
        {
            "source": "r_57",
            "target": "s_673"
        },
        {
            "source": "r_57",
            "target": "s_1181"
        },
        {
            "source": "r_58",
            "target": "s_1478"
        },
        {
            "source": "r_58",
            "target": "s_899"
        },
        {
            "source": "r_58",
            "target": "s_1515"
        },
        {
            "source": "r_59",
            "target": "s_601"
        },
        {
            "source": "r_59",
            "target": "s_262"
        },
        {
            "source": "r_59",
            "target": "s_1459"
        },
        {
            "source": "r_59",
            "target": "s_124"
        },
        {
            "source": "r_60",
            "target": "s_587"
        },
        {
            "source": "r_60",
            "target": "s_41"
        },
        {
            "source": "r_60",
            "target": "s_210"
        },
        {
            "source": "r_60",
            "target": "s_600"
        },
        {
            "source": "r_60",
            "target": "s_1475"
        },
        {
            "source": "r_60",
            "target": "s_1305"
        },
        {
            "source": "r_61",
            "target": "s_1072"
        },
        {
            "source": "r_61",
            "target": "s_1385"
        },
        {
            "source": "r_61",
            "target": "s_658"
        },
        {
            "source": "r_61",
            "target": "s_941"
        },
        {
            "source": "r_61",
            "target": "s_410"
        },
        {
            "source": "r_61",
            "target": "s_1539"
        },
        {
            "source": "r_62",
            "target": "s_877"
        },
        {
            "source": "r_62",
            "target": "s_961"
        },
        {
            "source": "r_62",
            "target": "s_210"
        },
        {
            "source": "r_62",
            "target": "s_274"
        },
        {
            "source": "r_62",
            "target": "s_1124"
        },
        {
            "source": "r_62",
            "target": "s_156"
        },
        {
            "source": "r_63",
            "target": "s_91"
        },
        {
            "source": "r_63",
            "target": "s_1235"
        },
        {
            "source": "r_63",
            "target": "s_1362"
        },
        {
            "source": "r_63",
            "target": "s_1098"
        },
        {
            "source": "r_63",
            "target": "s_224"
        },
        {
            "source": "r_63",
            "target": "s_1072"
        },
        {
            "source": "r_63",
            "target": "s_654"
        },
        {
            "source": "r_63",
            "target": "s_1385"
        },
        {
            "source": "r_64",
            "target": "s_737"
        },
        {
            "source": "r_64",
            "target": "s_714"
        },
        {
            "source": "r_64",
            "target": "s_457"
        },
        {
            "source": "r_64",
            "target": "s_1111"
        },
        {
            "source": "r_64",
            "target": "s_419"
        },
        {
            "source": "r_64",
            "target": "s_636"
        },
        {
            "source": "r_64",
            "target": "s_404"
        },
        {
            "source": "r_64",
            "target": "s_155"
        },
        {
            "source": "r_65",
            "target": "s_1095"
        },
        {
            "source": "r_65",
            "target": "s_699"
        },
        {
            "source": "r_65",
            "target": "s_1180"
        },
        {
            "source": "r_65",
            "target": "s_141"
        },
        {
            "source": "r_65",
            "target": "s_1474"
        },
        {
            "source": "r_66",
            "target": "s_623"
        },
        {
            "source": "r_66",
            "target": "s_709"
        },
        {
            "source": "r_66",
            "target": "s_1438"
        },
        {
            "source": "r_66",
            "target": "s_825"
        },
        {
            "source": "r_66",
            "target": "s_156"
        },
        {
            "source": "r_67",
            "target": "s_416"
        },
        {
            "source": "r_67",
            "target": "s_1589"
        },
        {
            "source": "r_67",
            "target": "s_41"
        },
        {
            "source": "r_67",
            "target": "s_1298"
        },
        {
            "source": "r_67",
            "target": "s_689"
        },
        {
            "source": "r_67",
            "target": "s_1486"
        },
        {
            "source": "r_68",
            "target": "s_125"
        },
        {
            "source": "r_68",
            "target": "s_187"
        },
        {
            "source": "r_68",
            "target": "s_524"
        },
        {
            "source": "r_68",
            "target": "s_791"
        },
        {
            "source": "r_68",
            "target": "s_793"
        },
        {
            "source": "r_68",
            "target": "s_1237"
        },
        {
            "source": "r_69",
            "target": "s_1400"
        },
        {
            "source": "r_69",
            "target": "s_1274"
        },
        {
            "source": "r_69",
            "target": "s_1226"
        },
        {
            "source": "r_69",
            "target": "s_69"
        },
        {
            "source": "r_69",
            "target": "s_618"
        },
        {
            "source": "r_69",
            "target": "s_802"
        },
        {
            "source": "r_69",
            "target": "s_110"
        },
        {
            "source": "r_69",
            "target": "s_1025"
        },
        {
            "source": "r_70",
            "target": "s_312"
        },
        {
            "source": "r_70",
            "target": "s_1571"
        },
        {
            "source": "r_70",
            "target": "s_702"
        },
        {
            "source": "r_70",
            "target": "s_198"
        },
        {
            "source": "r_71",
            "target": "s_1273"
        },
        {
            "source": "r_71",
            "target": "s_1160"
        },
        {
            "source": "r_71",
            "target": "s_756"
        },
        {
            "source": "r_71",
            "target": "s_824"
        },
        {
            "source": "r_72",
            "target": "s_371"
        },
        {
            "source": "r_72",
            "target": "s_694"
        },
        {
            "source": "r_72",
            "target": "s_520"
        },
        {
            "source": "r_72",
            "target": "s_723"
        },
        {
            "source": "r_72",
            "target": "s_222"
        },
        {
            "source": "r_72",
            "target": "s_629"
        },
        {
            "source": "r_72",
            "target": "s_185"
        },
        {
            "source": "r_72",
            "target": "s_1558"
        },
        {
            "source": "r_72",
            "target": "s_355"
        },
        {
            "source": "r_72",
            "target": "s_584"
        },
        {
            "source": "r_72",
            "target": "s_510"
        },
        {
            "source": "r_73",
            "target": "s_801"
        },
        {
            "source": "r_73",
            "target": "s_1294"
        },
        {
            "source": "r_73",
            "target": "s_313"
        },
        {
            "source": "r_73",
            "target": "s_920"
        },
        {
            "source": "r_74",
            "target": "s_1574"
        },
        {
            "source": "r_74",
            "target": "s_1226"
        },
        {
            "source": "r_74",
            "target": "s_1415"
        },
        {
            "source": "r_75",
            "target": "s_389"
        },
        {
            "source": "r_75",
            "target": "s_695"
        },
        {
            "source": "r_75",
            "target": "s_1349"
        },
        {
            "source": "r_76",
            "target": "s_203"
        },
        {
            "source": "r_76",
            "target": "s_850"
        },
        {
            "source": "r_77",
            "target": "s_477"
        },
        {
            "source": "r_77",
            "target": "s_75"
        },
        {
            "source": "r_78",
            "target": "s_48"
        },
        {
            "source": "r_78",
            "target": "s_880"
        },
        {
            "source": "r_78",
            "target": "s_1232"
        },
        {
            "source": "r_78",
            "target": "s_821"
        },
        {
            "source": "r_78",
            "target": "s_815"
        },
        {
            "source": "r_79",
            "target": "s_470"
        },
        {
            "source": "r_79",
            "target": "s_1045"
        },
        {
            "source": "r_79",
            "target": "s_1589"
        },
        {
            "source": "r_79",
            "target": "s_1210"
        },
        {
            "source": "r_79",
            "target": "s_348"
        },
        {
            "source": "r_79",
            "target": "s_156"
        },
        {
            "source": "r_80",
            "target": "s_1266"
        },
        {
            "source": "r_80",
            "target": "s_1442"
        },
        {
            "source": "r_80",
            "target": "s_1227"
        },
        {
            "source": "r_81",
            "target": "s_862"
        },
        {
            "source": "r_81",
            "target": "s_809"
        },
        {
            "source": "r_81",
            "target": "s_363"
        },
        {
            "source": "r_81",
            "target": "s_899"
        },
        {
            "source": "r_81",
            "target": "s_1345"
        },
        {
            "source": "r_81",
            "target": "s_805"
        },
        {
            "source": "r_81",
            "target": "s_53"
        },
        {
            "source": "r_81",
            "target": "s_1146"
        },
        {
            "source": "r_82",
            "target": "s_148"
        },
        {
            "source": "r_82",
            "target": "s_943"
        },
        {
            "source": "r_82",
            "target": "s_548"
        },
        {
            "source": "r_82",
            "target": "s_894"
        },
        {
            "source": "r_82",
            "target": "s_726"
        },
        {
            "source": "r_82",
            "target": "s_815"
        },
        {
            "source": "r_82",
            "target": "s_348"
        },
        {
            "source": "r_83",
            "target": "s_1031"
        },
        {
            "source": "r_84",
            "target": "s_294"
        },
        {
            "source": "r_84",
            "target": "s_23"
        },
        {
            "source": "r_84",
            "target": "s_42"
        },
        {
            "source": "r_84",
            "target": "s_899"
        },
        {
            "source": "r_84",
            "target": "s_103"
        },
        {
            "source": "r_84",
            "target": "s_1509"
        },
        {
            "source": "r_84",
            "target": "s_578"
        },
        {
            "source": "r_84",
            "target": "s_990"
        },
        {
            "source": "r_85",
            "target": "s_286"
        },
        {
            "source": "r_85",
            "target": "s_879"
        },
        {
            "source": "r_85",
            "target": "s_572"
        },
        {
            "source": "r_85",
            "target": "s_1006"
        },
        {
            "source": "r_86",
            "target": "s_1087"
        },
        {
            "source": "r_86",
            "target": "s_270"
        },
        {
            "source": "r_86",
            "target": "s_56"
        },
        {
            "source": "r_86",
            "target": "s_246"
        },
        {
            "source": "r_86",
            "target": "s_1415"
        },
        {
            "source": "r_87",
            "target": "s_392"
        },
        {
            "source": "r_87",
            "target": "s_1388"
        },
        {
            "source": "r_87",
            "target": "s_978"
        },
        {
            "source": "r_87",
            "target": "s_245"
        },
        {
            "source": "r_87",
            "target": "s_1487"
        },
        {
            "source": "r_87",
            "target": "s_326"
        },
        {
            "source": "r_87",
            "target": "s_1124"
        },
        {
            "source": "r_88",
            "target": "s_1462"
        },
        {
            "source": "r_88",
            "target": "s_635"
        },
        {
            "source": "r_88",
            "target": "s_1067"
        },
        {
            "source": "r_88",
            "target": "s_707"
        },
        {
            "source": "r_89",
            "target": "s_927"
        },
        {
            "source": "r_89",
            "target": "s_453"
        },
        {
            "source": "r_89",
            "target": "s_822"
        },
        {
            "source": "r_89",
            "target": "s_229"
        },
        {
            "source": "r_90",
            "target": "s_256"
        },
        {
            "source": "r_90",
            "target": "s_379"
        },
        {
            "source": "r_90",
            "target": "s_1032"
        },
        {
            "source": "r_90",
            "target": "s_95"
        },
        {
            "source": "r_90",
            "target": "s_464"
        },
        {
            "source": "r_90",
            "target": "s_1546"
        },
        {
            "source": "r_90",
            "target": "s_221"
        },
        {
            "source": "r_90",
            "target": "s_471"
        },
        {
            "source": "r_90",
            "target": "s_1506"
        },
        {
            "source": "r_90",
            "target": "s_472"
        },
        {
            "source": "r_90",
            "target": "s_819"
        },
        {
            "source": "r_90",
            "target": "s_1150"
        },
        {
            "source": "r_90",
            "target": "s_267"
        },
        {
            "source": "r_91",
            "target": "s_214"
        },
        {
            "source": "r_91",
            "target": "s_1"
        },
        {
            "source": "r_91",
            "target": "s_123"
        },
        {
            "source": "r_91",
            "target": "s_198"
        },
        {
            "source": "r_91",
            "target": "s_1221"
        },
        {
            "source": "r_92",
            "target": "s_763"
        },
        {
            "source": "r_92",
            "target": "s_456"
        },
        {
            "source": "r_92",
            "target": "s_1431"
        },
        {
            "source": "r_92",
            "target": "s_1267"
        },
        {
            "source": "r_92",
            "target": "s_1574"
        },
        {
            "source": "r_92",
            "target": "s_1280"
        },
        {
            "source": "r_92",
            "target": "s_1415"
        },
        {
            "source": "r_93",
            "target": "s_6"
        },
        {
            "source": "r_93",
            "target": "s_1027"
        },
        {
            "source": "r_93",
            "target": "s_1590"
        },
        {
            "source": "r_93",
            "target": "s_655"
        },
        {
            "source": "r_93",
            "target": "s_1599"
        },
        {
            "source": "r_94",
            "target": "s_1215"
        },
        {
            "source": "r_94",
            "target": "s_536"
        },
        {
            "source": "r_94",
            "target": "s_1221"
        },
        {
            "source": "r_94",
            "target": "s_110"
        },
        {
            "source": "r_94",
            "target": "s_687"
        },
        {
            "source": "r_94",
            "target": "s_942"
        },
        {
            "source": "r_95",
            "target": "s_552"
        },
        {
            "source": "r_95",
            "target": "s_504"
        },
        {
            "source": "r_95",
            "target": "s_989"
        },
        {
            "source": "r_95",
            "target": "s_976"
        },
        {
            "source": "r_96",
            "target": "s_1313"
        },
        {
            "source": "r_96",
            "target": "s_1362"
        },
        {
            "source": "r_96",
            "target": "s_1210"
        },
        {
            "source": "r_96",
            "target": "s_473"
        },
        {
            "source": "r_96",
            "target": "s_309"
        },
        {
            "source": "r_96",
            "target": "s_156"
        },
        {
            "source": "r_96",
            "target": "s_823"
        },
        {
            "source": "r_97",
            "target": "s_913"
        },
        {
            "source": "r_97",
            "target": "s_535"
        },
        {
            "source": "r_97",
            "target": "s_1010"
        },
        {
            "source": "r_97",
            "target": "s_177"
        },
        {
            "source": "r_97",
            "target": "s_737"
        },
        {
            "source": "r_97",
            "target": "s_404"
        },
        {
            "source": "r_97",
            "target": "s_636"
        },
        {
            "source": "r_97",
            "target": "s_992"
        },
        {
            "source": "r_97",
            "target": "s_155"
        },
        {
            "source": "r_97",
            "target": "s_1251"
        },
        {
            "source": "r_98",
            "target": "s_615"
        },
        {
            "source": "r_98",
            "target": "s_547"
        },
        {
            "source": "r_98",
            "target": "s_1215"
        },
        {
            "source": "r_98",
            "target": "s_781"
        },
        {
            "source": "r_99",
            "target": "s_173"
        },
        {
            "source": "r_99",
            "target": "s_984"
        },
        {
            "source": "r_99",
            "target": "s_1190"
        },
        {
            "source": "r_99",
            "target": "s_1411"
        },
        {
            "source": "r_100",
            "target": "s_750"
        },
        {
            "source": "r_100",
            "target": "s_1131"
        },
        {
            "source": "r_100",
            "target": "s_989"
        },
        {
            "source": "r_101",
            "target": "s_593"
        },
        {
            "source": "r_101",
            "target": "s_277"
        },
        {
            "source": "r_102",
            "target": "s_1007"
        },
        {
            "source": "r_102",
            "target": "s_101"
        },
        {
            "source": "r_102",
            "target": "s_712"
        },
        {
            "source": "r_102",
            "target": "s_141"
        },
        {
            "source": "r_102",
            "target": "s_789"
        },
        {
            "source": "r_102",
            "target": "s_1474"
        },
        {
            "source": "r_103",
            "target": "s_1527"
        },
        {
            "source": "r_103",
            "target": "s_719"
        },
        {
            "source": "r_103",
            "target": "s_1517"
        },
        {
            "source": "r_103",
            "target": "s_215"
        },
        {
            "source": "r_104",
            "target": "s_763"
        },
        {
            "source": "r_104",
            "target": "s_1516"
        },
        {
            "source": "r_104",
            "target": "s_1341"
        },
        {
            "source": "r_104",
            "target": "s_1158"
        },
        {
            "source": "r_104",
            "target": "s_456"
        },
        {
            "source": "r_104",
            "target": "s_1280"
        },
        {
            "source": "r_104",
            "target": "s_1415"
        },
        {
            "source": "r_105",
            "target": "s_877"
        },
        {
            "source": "r_105",
            "target": "s_1341"
        },
        {
            "source": "r_105",
            "target": "s_897"
        },
        {
            "source": "r_105",
            "target": "s_1415"
        },
        {
            "source": "r_105",
            "target": "s_348"
        },
        {
            "source": "r_105",
            "target": "s_156"
        },
        {
            "source": "r_106",
            "target": "s_456"
        },
        {
            "source": "r_106",
            "target": "s_525"
        },
        {
            "source": "r_106",
            "target": "s_1415"
        },
        {
            "source": "r_106",
            "target": "s_1280"
        },
        {
            "source": "r_106",
            "target": "s_1140"
        },
        {
            "source": "r_107",
            "target": "s_953"
        },
        {
            "source": "r_107",
            "target": "s_1593"
        },
        {
            "source": "r_107",
            "target": "s_434"
        },
        {
            "source": "r_107",
            "target": "s_272"
        },
        {
            "source": "r_107",
            "target": "s_1100"
        },
        {
            "source": "r_107",
            "target": "s_1254"
        },
        {
            "source": "r_108",
            "target": "s_671"
        },
        {
            "source": "r_108",
            "target": "s_444"
        },
        {
            "source": "r_108",
            "target": "s_218"
        },
        {
            "source": "r_108",
            "target": "s_1106"
        },
        {
            "source": "r_108",
            "target": "s_94"
        },
        {
            "source": "r_108",
            "target": "s_708"
        },
        {
            "source": "r_108",
            "target": "s_506"
        },
        {
            "source": "r_108",
            "target": "s_343"
        },
        {
            "source": "r_109",
            "target": "s_1344"
        },
        {
            "source": "r_109",
            "target": "s_631"
        },
        {
            "source": "r_109",
            "target": "s_1385"
        },
        {
            "source": "r_109",
            "target": "s_393"
        },
        {
            "source": "r_109",
            "target": "s_1221"
        },
        {
            "source": "r_109",
            "target": "s_1486"
        },
        {
            "source": "r_110",
            "target": "s_1221"
        },
        {
            "source": "r_110",
            "target": "s_1079"
        },
        {
            "source": "r_110",
            "target": "s_1348"
        },
        {
            "source": "r_110",
            "target": "s_1016"
        },
        {
            "source": "r_110",
            "target": "s_1371"
        },
        {
            "source": "r_110",
            "target": "s_582"
        },
        {
            "source": "r_110",
            "target": "s_642"
        },
        {
            "source": "r_110",
            "target": "s_1159"
        },
        {
            "source": "r_110",
            "target": "s_867"
        },
        {
            "source": "r_110",
            "target": "s_985"
        },
        {
            "source": "r_110",
            "target": "s_386"
        },
        {
            "source": "r_110",
            "target": "s_684"
        },
        {
            "source": "r_110",
            "target": "s_546"
        },
        {
            "source": "r_110",
            "target": "s_917"
        },
        {
            "source": "r_110",
            "target": "s_1220"
        },
        {
            "source": "r_110",
            "target": "s_732"
        },
        {
            "source": "r_110",
            "target": "s_777"
        },
        {
            "source": "r_110",
            "target": "s_1101"
        },
        {
            "source": "r_110",
            "target": "s_1547"
        },
        {
            "source": "r_110",
            "target": "s_1303"
        },
        {
            "source": "r_110",
            "target": "s_978"
        },
        {
            "source": "r_111",
            "target": "s_1429"
        },
        {
            "source": "r_111",
            "target": "s_57"
        },
        {
            "source": "r_111",
            "target": "s_1049"
        },
        {
            "source": "r_111",
            "target": "s_442"
        },
        {
            "source": "r_111",
            "target": "s_116"
        },
        {
            "source": "r_111",
            "target": "s_774"
        },
        {
            "source": "r_111",
            "target": "s_579"
        },
        {
            "source": "r_111",
            "target": "s_742"
        },
        {
            "source": "r_111",
            "target": "s_134"
        },
        {
            "source": "r_112",
            "target": "s_196"
        },
        {
            "source": "r_112",
            "target": "s_1540"
        },
        {
            "source": "r_112",
            "target": "s_8"
        },
        {
            "source": "r_112",
            "target": "s_1613"
        },
        {
            "source": "r_112",
            "target": "s_40"
        },
        {
            "source": "r_113",
            "target": "s_1615"
        },
        {
            "source": "r_113",
            "target": "s_1053"
        },
        {
            "source": "r_113",
            "target": "s_630"
        },
        {
            "source": "r_113",
            "target": "s_242"
        },
        {
            "source": "r_114",
            "target": "s_1209"
        },
        {
            "source": "r_114",
            "target": "s_607"
        },
        {
            "source": "r_114",
            "target": "s_126"
        },
        {
            "source": "r_114",
            "target": "s_942"
        },
        {
            "source": "r_115",
            "target": "s_219"
        },
        {
            "source": "r_115",
            "target": "s_395"
        },
        {
            "source": "r_115",
            "target": "s_157"
        },
        {
            "source": "r_116",
            "target": "s_35"
        },
        {
            "source": "r_116",
            "target": "s_1068"
        },
        {
            "source": "r_116",
            "target": "s_815"
        },
        {
            "source": "r_116",
            "target": "s_707"
        },
        {
            "source": "r_117",
            "target": "s_318"
        },
        {
            "source": "r_117",
            "target": "s_1560"
        },
        {
            "source": "r_117",
            "target": "s_107"
        },
        {
            "source": "r_117",
            "target": "s_1480"
        },
        {
            "source": "r_117",
            "target": "s_1233"
        },
        {
            "source": "r_118",
            "target": "s_979"
        },
        {
            "source": "r_118",
            "target": "s_1367"
        },
        {
            "source": "r_118",
            "target": "s_466"
        },
        {
            "source": "r_118",
            "target": "s_1429"
        },
        {
            "source": "r_118",
            "target": "s_134"
        },
        {
            "source": "r_118",
            "target": "s_1203"
        },
        {
            "source": "r_118",
            "target": "s_1035"
        },
        {
            "source": "r_119",
            "target": "s_592"
        },
        {
            "source": "r_119",
            "target": "s_46"
        },
        {
            "source": "r_119",
            "target": "s_770"
        },
        {
            "source": "r_119",
            "target": "s_60"
        },
        {
            "source": "r_119",
            "target": "s_1574"
        },
        {
            "source": "r_119",
            "target": "s_156"
        },
        {
            "source": "r_119",
            "target": "s_1415"
        },
        {
            "source": "r_120",
            "target": "s_286"
        },
        {
            "source": "r_120",
            "target": "s_10"
        },
        {
            "source": "r_120",
            "target": "s_1242"
        },
        {
            "source": "r_120",
            "target": "s_1006"
        },
        {
            "source": "r_121",
            "target": "s_690"
        },
        {
            "source": "r_121",
            "target": "s_489"
        },
        {
            "source": "r_121",
            "target": "s_24"
        },
        {
            "source": "r_121",
            "target": "s_76"
        },
        {
            "source": "r_121",
            "target": "s_638"
        },
        {
            "source": "r_121",
            "target": "s_1360"
        },
        {
            "source": "r_121",
            "target": "s_179"
        },
        {
            "source": "r_121",
            "target": "s_1599"
        },
        {
            "source": "r_121",
            "target": "s_366"
        },
        {
            "source": "r_122",
            "target": "s_792"
        },
        {
            "source": "r_122",
            "target": "s_1329"
        },
        {
            "source": "r_122",
            "target": "s_1218"
        },
        {
            "source": "r_122",
            "target": "s_364"
        },
        {
            "source": "r_123",
            "target": "s_962"
        },
        {
            "source": "r_123",
            "target": "s_542"
        },
        {
            "source": "r_124",
            "target": "s_1330"
        },
        {
            "source": "r_124",
            "target": "s_1607"
        },
        {
            "source": "r_124",
            "target": "s_319"
        },
        {
            "source": "r_124",
            "target": "s_999"
        },
        {
            "source": "r_124",
            "target": "s_213"
        },
        {
            "source": "r_124",
            "target": "s_82"
        },
        {
            "source": "r_124",
            "target": "s_781"
        },
        {
            "source": "r_125",
            "target": "s_302"
        },
        {
            "source": "r_125",
            "target": "s_111"
        },
        {
            "source": "r_125",
            "target": "s_1292"
        },
        {
            "source": "r_125",
            "target": "s_394"
        },
        {
            "source": "r_126",
            "target": "s_1161"
        },
        {
            "source": "r_126",
            "target": "s_536"
        },
        {
            "source": "r_126",
            "target": "s_1225"
        },
        {
            "source": "r_126",
            "target": "s_920"
        },
        {
            "source": "r_126",
            "target": "s_1488"
        },
        {
            "source": "r_127",
            "target": "s_801"
        },
        {
            "source": "r_127",
            "target": "s_624"
        },
        {
            "source": "r_127",
            "target": "s_130"
        },
        {
            "source": "r_127",
            "target": "s_920"
        },
        {
            "source": "r_128",
            "target": "s_1557"
        },
        {
            "source": "r_128",
            "target": "s_230"
        },
        {
            "source": "r_128",
            "target": "s_331"
        },
        {
            "source": "r_128",
            "target": "s_407"
        },
        {
            "source": "r_128",
            "target": "s_1194"
        },
        {
            "source": "r_128",
            "target": "s_1345"
        },
        {
            "source": "r_128",
            "target": "s_1076"
        },
        {
            "source": "r_129",
            "target": "s_26"
        },
        {
            "source": "r_129",
            "target": "s_244"
        },
        {
            "source": "r_129",
            "target": "s_966"
        },
        {
            "source": "r_129",
            "target": "s_350"
        },
        {
            "source": "r_129",
            "target": "s_729"
        },
        {
            "source": "r_129",
            "target": "s_1104"
        },
        {
            "source": "r_129",
            "target": "s_665"
        },
        {
            "source": "r_130",
            "target": "s_1027"
        },
        {
            "source": "r_130",
            "target": "s_384"
        },
        {
            "source": "r_130",
            "target": "s_1148"
        },
        {
            "source": "r_130",
            "target": "s_6"
        },
        {
            "source": "r_131",
            "target": "s_1617"
        },
        {
            "source": "r_131",
            "target": "s_1017"
        },
        {
            "source": "r_131",
            "target": "s_930"
        },
        {
            "source": "r_131",
            "target": "s_395"
        },
        {
            "source": "r_131",
            "target": "s_181"
        },
        {
            "source": "r_132",
            "target": "s_1025"
        },
        {
            "source": "r_132",
            "target": "s_616"
        },
        {
            "source": "r_133",
            "target": "s_1589"
        },
        {
            "source": "r_133",
            "target": "s_18"
        },
        {
            "source": "r_133",
            "target": "s_1574"
        },
        {
            "source": "r_133",
            "target": "s_689"
        },
        {
            "source": "r_133",
            "target": "s_1108"
        },
        {
            "source": "r_133",
            "target": "s_1415"
        },
        {
            "source": "r_133",
            "target": "s_1486"
        },
        {
            "source": "r_134",
            "target": "s_1341"
        },
        {
            "source": "r_134",
            "target": "s_877"
        },
        {
            "source": "r_134",
            "target": "s_156"
        },
        {
            "source": "r_134",
            "target": "s_1415"
        },
        {
            "source": "r_135",
            "target": "s_765"
        },
        {
            "source": "r_135",
            "target": "s_1611"
        },
        {
            "source": "r_135",
            "target": "s_456"
        },
        {
            "source": "r_135",
            "target": "s_735"
        },
        {
            "source": "r_135",
            "target": "s_981"
        },
        {
            "source": "r_135",
            "target": "s_1280"
        },
        {
            "source": "r_135",
            "target": "s_204"
        },
        {
            "source": "r_135",
            "target": "s_1415"
        },
        {
            "source": "r_136",
            "target": "s_214"
        },
        {
            "source": "r_136",
            "target": "s_432"
        },
        {
            "source": "r_136",
            "target": "s_198"
        },
        {
            "source": "r_136",
            "target": "s_970"
        },
        {
            "source": "r_136",
            "target": "s_558"
        },
        {
            "source": "r_136",
            "target": "s_1221"
        },
        {
            "source": "r_137",
            "target": "s_875"
        },
        {
            "source": "r_137",
            "target": "s_408"
        },
        {
            "source": "r_137",
            "target": "s_1411"
        },
        {
            "source": "r_138",
            "target": "s_1423"
        },
        {
            "source": "r_138",
            "target": "s_1127"
        },
        {
            "source": "r_138",
            "target": "s_285"
        },
        {
            "source": "r_138",
            "target": "s_1425"
        },
        {
            "source": "r_139",
            "target": "s_1217"
        },
        {
            "source": "r_139",
            "target": "s_778"
        },
        {
            "source": "r_139",
            "target": "s_136"
        },
        {
            "source": "r_139",
            "target": "s_345"
        },
        {
            "source": "r_139",
            "target": "s_518"
        },
        {
            "source": "r_139",
            "target": "s_109"
        },
        {
            "source": "r_139",
            "target": "s_512"
        },
        {
            "source": "r_139",
            "target": "s_360"
        },
        {
            "source": "r_140",
            "target": "s_1248"
        },
        {
            "source": "r_140",
            "target": "s_1366"
        },
        {
            "source": "r_140",
            "target": "s_1442"
        },
        {
            "source": "r_140",
            "target": "s_1146"
        },
        {
            "source": "r_141",
            "target": "s_12"
        },
        {
            "source": "r_141",
            "target": "s_80"
        },
        {
            "source": "r_142",
            "target": "s_1005"
        },
        {
            "source": "r_142",
            "target": "s_1061"
        },
        {
            "source": "r_142",
            "target": "s_854"
        },
        {
            "source": "r_142",
            "target": "s_1384"
        },
        {
            "source": "r_142",
            "target": "s_424"
        },
        {
            "source": "r_143",
            "target": "s_1075"
        },
        {
            "source": "r_143",
            "target": "s_1119"
        },
        {
            "source": "r_143",
            "target": "s_1210"
        },
        {
            "source": "r_143",
            "target": "s_156"
        },
        {
            "source": "r_143",
            "target": "s_1556"
        },
        {
            "source": "r_144",
            "target": "s_384"
        },
        {
            "source": "r_144",
            "target": "s_1030"
        },
        {
            "source": "r_144",
            "target": "s_1148"
        },
        {
            "source": "r_144",
            "target": "s_335"
        },
        {
            "source": "r_145",
            "target": "s_879"
        },
        {
            "source": "r_145",
            "target": "s_892"
        },
        {
            "source": "r_145",
            "target": "s_720"
        },
        {
            "source": "r_145",
            "target": "s_505"
        },
        {
            "source": "r_145",
            "target": "s_359"
        },
        {
            "source": "r_145",
            "target": "s_122"
        },
        {
            "source": "r_146",
            "target": "s_308"
        },
        {
            "source": "r_146",
            "target": "s_1197"
        },
        {
            "source": "r_146",
            "target": "s_1009"
        },
        {
            "source": "r_146",
            "target": "s_19"
        },
        {
            "source": "r_146",
            "target": "s_1523"
        },
        {
            "source": "r_146",
            "target": "s_1123"
        },
        {
            "source": "r_146",
            "target": "s_1429"
        },
        {
            "source": "r_146",
            "target": "s_1385"
        },
        {
            "source": "r_147",
            "target": "s_1215"
        },
        {
            "source": "r_147",
            "target": "s_1161"
        },
        {
            "source": "r_147",
            "target": "s_173"
        },
        {
            "source": "r_147",
            "target": "s_942"
        },
        {
            "source": "r_148",
            "target": "s_961"
        },
        {
            "source": "r_148",
            "target": "s_1589"
        },
        {
            "source": "r_148",
            "target": "s_1075"
        },
        {
            "source": "r_148",
            "target": "s_1495"
        },
        {
            "source": "r_148",
            "target": "s_1169"
        },
        {
            "source": "r_148",
            "target": "s_1221"
        },
        {
            "source": "r_148",
            "target": "s_156"
        },
        {
            "source": "r_149",
            "target": "s_1010"
        },
        {
            "source": "r_149",
            "target": "s_1144"
        },
        {
            "source": "r_149",
            "target": "s_636"
        },
        {
            "source": "r_149",
            "target": "s_737"
        },
        {
            "source": "r_149",
            "target": "s_1397"
        },
        {
            "source": "r_149",
            "target": "s_404"
        },
        {
            "source": "r_149",
            "target": "s_155"
        },
        {
            "source": "r_150",
            "target": "s_1003"
        },
        {
            "source": "r_150",
            "target": "s_1250"
        },
        {
            "source": "r_150",
            "target": "s_689"
        },
        {
            "source": "r_150",
            "target": "s_977"
        },
        {
            "source": "r_150",
            "target": "s_1486"
        },
        {
            "source": "r_150",
            "target": "s_1496"
        },
        {
            "source": "r_150",
            "target": "s_388"
        },
        {
            "source": "r_151",
            "target": "s_28"
        },
        {
            "source": "r_151",
            "target": "s_707"
        },
        {
            "source": "r_151",
            "target": "s_231"
        },
        {
            "source": "r_151",
            "target": "s_402"
        },
        {
            "source": "r_151",
            "target": "s_428"
        },
        {
            "source": "r_152",
            "target": "s_637"
        },
        {
            "source": "r_152",
            "target": "s_855"
        },
        {
            "source": "r_152",
            "target": "s_125"
        },
        {
            "source": "r_152",
            "target": "s_1275"
        },
        {
            "source": "r_152",
            "target": "s_1457"
        },
        {
            "source": "r_152",
            "target": "s_1549"
        },
        {
            "source": "r_152",
            "target": "s_1237"
        },
        {
            "source": "r_153",
            "target": "s_1351"
        },
        {
            "source": "r_153",
            "target": "s_1416"
        },
        {
            "source": "r_153",
            "target": "s_271"
        },
        {
            "source": "r_153",
            "target": "s_526"
        },
        {
            "source": "r_153",
            "target": "s_944"
        },
        {
            "source": "r_153",
            "target": "s_1142"
        },
        {
            "source": "r_153",
            "target": "s_1253"
        },
        {
            "source": "r_154",
            "target": "s_485"
        },
        {
            "source": "r_154",
            "target": "s_1438"
        },
        {
            "source": "r_154",
            "target": "s_1377"
        },
        {
            "source": "r_154",
            "target": "s_828"
        },
        {
            "source": "r_154",
            "target": "s_79"
        },
        {
            "source": "r_154",
            "target": "s_1385"
        },
        {
            "source": "r_155",
            "target": "s_229"
        },
        {
            "source": "r_155",
            "target": "s_605"
        },
        {
            "source": "r_155",
            "target": "s_927"
        },
        {
            "source": "r_155",
            "target": "s_533"
        },
        {
            "source": "r_156",
            "target": "s_1264"
        },
        {
            "source": "r_156",
            "target": "s_1012"
        },
        {
            "source": "r_156",
            "target": "s_273"
        },
        {
            "source": "r_156",
            "target": "s_352"
        },
        {
            "source": "r_156",
            "target": "s_253"
        },
        {
            "source": "r_156",
            "target": "s_67"
        },
        {
            "source": "r_156",
            "target": "s_1554"
        },
        {
            "source": "r_157",
            "target": "s_682"
        },
        {
            "source": "r_157",
            "target": "s_1341"
        },
        {
            "source": "r_157",
            "target": "s_1574"
        },
        {
            "source": "r_157",
            "target": "s_144"
        },
        {
            "source": "r_157",
            "target": "s_740"
        },
        {
            "source": "r_157",
            "target": "s_225"
        },
        {
            "source": "r_157",
            "target": "s_647"
        },
        {
            "source": "r_157",
            "target": "s_1415"
        },
        {
            "source": "r_158",
            "target": "s_202"
        },
        {
            "source": "r_158",
            "target": "s_48"
        },
        {
            "source": "r_158",
            "target": "s_310"
        },
        {
            "source": "r_158",
            "target": "s_441"
        },
        {
            "source": "r_158",
            "target": "s_815"
        },
        {
            "source": "r_159",
            "target": "s_947"
        },
        {
            "source": "r_159",
            "target": "s_952"
        },
        {
            "source": "r_159",
            "target": "s_649"
        },
        {
            "source": "r_159",
            "target": "s_829"
        },
        {
            "source": "r_159",
            "target": "s_1534"
        },
        {
            "source": "r_159",
            "target": "s_1198"
        },
        {
            "source": "r_159",
            "target": "s_1227"
        },
        {
            "source": "r_160",
            "target": "s_360"
        },
        {
            "source": "r_160",
            "target": "s_11"
        },
        {
            "source": "r_160",
            "target": "s_1243"
        },
        {
            "source": "r_160",
            "target": "s_451"
        },
        {
            "source": "r_160",
            "target": "s_1338"
        },
        {
            "source": "r_161",
            "target": "s_766"
        },
        {
            "source": "r_161",
            "target": "s_1385"
        },
        {
            "source": "r_161",
            "target": "s_1327"
        },
        {
            "source": "r_161",
            "target": "s_784"
        },
        {
            "source": "r_161",
            "target": "s_1438"
        },
        {
            "source": "r_161",
            "target": "s_1072"
        },
        {
            "source": "r_161",
            "target": "s_1223"
        },
        {
            "source": "r_161",
            "target": "s_237"
        },
        {
            "source": "r_161",
            "target": "s_1415"
        },
        {
            "source": "r_162",
            "target": "s_457"
        },
        {
            "source": "r_162",
            "target": "s_900"
        },
        {
            "source": "r_162",
            "target": "s_404"
        },
        {
            "source": "r_162",
            "target": "s_737"
        },
        {
            "source": "r_163",
            "target": "s_470"
        },
        {
            "source": "r_163",
            "target": "s_843"
        },
        {
            "source": "r_163",
            "target": "s_987"
        },
        {
            "source": "r_163",
            "target": "s_4"
        },
        {
            "source": "r_163",
            "target": "s_393"
        },
        {
            "source": "r_164",
            "target": "s_649"
        },
        {
            "source": "r_164",
            "target": "s_261"
        },
        {
            "source": "r_164",
            "target": "s_1442"
        },
        {
            "source": "r_164",
            "target": "s_947"
        },
        {
            "source": "r_164",
            "target": "s_1227"
        },
        {
            "source": "r_165",
            "target": "s_337"
        },
        {
            "source": "r_165",
            "target": "s_252"
        },
        {
            "source": "r_165",
            "target": "s_1545"
        },
        {
            "source": "r_165",
            "target": "s_1124"
        },
        {
            "source": "r_166",
            "target": "s_1380"
        },
        {
            "source": "r_166",
            "target": "s_446"
        },
        {
            "source": "r_166",
            "target": "s_456"
        },
        {
            "source": "r_166",
            "target": "s_1385"
        },
        {
            "source": "r_166",
            "target": "s_1415"
        },
        {
            "source": "r_166",
            "target": "s_348"
        },
        {
            "source": "r_166",
            "target": "s_346"
        },
        {
            "source": "r_167",
            "target": "s_1492"
        },
        {
            "source": "r_167",
            "target": "s_1186"
        },
        {
            "source": "r_167",
            "target": "s_501"
        },
        {
            "source": "r_167",
            "target": "s_521"
        },
        {
            "source": "r_167",
            "target": "s_1216"
        },
        {
            "source": "r_167",
            "target": "s_1240"
        },
        {
            "source": "r_167",
            "target": "s_604"
        },
        {
            "source": "r_167",
            "target": "s_738"
        },
        {
            "source": "r_167",
            "target": "s_399"
        },
        {
            "source": "r_167",
            "target": "s_68"
        },
        {
            "source": "r_167",
            "target": "s_1421"
        },
        {
            "source": "r_167",
            "target": "s_507"
        },
        {
            "source": "r_167",
            "target": "s_1486"
        },
        {
            "source": "r_168",
            "target": "s_1328"
        },
        {
            "source": "r_168",
            "target": "s_1209"
        },
        {
            "source": "r_168",
            "target": "s_1215"
        },
        {
            "source": "r_168",
            "target": "s_1026"
        },
        {
            "source": "r_169",
            "target": "s_1586"
        },
        {
            "source": "r_169",
            "target": "s_1363"
        },
        {
            "source": "r_169",
            "target": "s_1555"
        },
        {
            "source": "r_169",
            "target": "s_741"
        },
        {
            "source": "r_169",
            "target": "s_292"
        },
        {
            "source": "r_169",
            "target": "s_428"
        },
        {
            "source": "r_170",
            "target": "s_545"
        },
        {
            "source": "r_170",
            "target": "s_1081"
        },
        {
            "source": "r_170",
            "target": "s_347"
        },
        {
            "source": "r_171",
            "target": "s_308"
        },
        {
            "source": "r_171",
            "target": "s_577"
        },
        {
            "source": "r_171",
            "target": "s_284"
        },
        {
            "source": "r_171",
            "target": "s_1385"
        },
        {
            "source": "r_171",
            "target": "s_1123"
        },
        {
            "source": "r_171",
            "target": "s_283"
        },
        {
            "source": "r_171",
            "target": "s_1395"
        },
        {
            "source": "r_171",
            "target": "s_578"
        },
        {
            "source": "r_172",
            "target": "s_959"
        },
        {
            "source": "r_172",
            "target": "s_1562"
        },
        {
            "source": "r_172",
            "target": "s_304"
        },
        {
            "source": "r_172",
            "target": "s_240"
        },
        {
            "source": "r_172",
            "target": "s_1405"
        },
        {
            "source": "r_172",
            "target": "s_1518"
        },
        {
            "source": "r_172",
            "target": "s_406"
        },
        {
            "source": "r_172",
            "target": "s_1347"
        },
        {
            "source": "r_172",
            "target": "s_158"
        },
        {
            "source": "r_172",
            "target": "s_3"
        },
        {
            "source": "r_172",
            "target": "s_113"
        },
        {
            "source": "r_172",
            "target": "s_1370"
        },
        {
            "source": "r_172",
            "target": "s_97"
        },
        {
            "source": "r_172",
            "target": "s_904"
        },
        {
            "source": "r_172",
            "target": "s_1575"
        },
        {
            "source": "r_172",
            "target": "s_1286"
        },
        {
            "source": "r_172",
            "target": "s_1037"
        },
        {
            "source": "r_172",
            "target": "s_428"
        },
        {
            "source": "r_173",
            "target": "s_934"
        },
        {
            "source": "r_173",
            "target": "s_656"
        },
        {
            "source": "r_173",
            "target": "s_646"
        },
        {
            "source": "r_174",
            "target": "s_1187"
        },
        {
            "source": "r_174",
            "target": "s_286"
        },
        {
            "source": "r_174",
            "target": "s_1077"
        },
        {
            "source": "r_174",
            "target": "s_705"
        },
        {
            "source": "r_174",
            "target": "s_390"
        },
        {
            "source": "r_174",
            "target": "s_377"
        },
        {
            "source": "r_174",
            "target": "s_1006"
        },
        {
            "source": "r_175",
            "target": "s_184"
        },
        {
            "source": "r_175",
            "target": "s_602"
        },
        {
            "source": "r_175",
            "target": "s_1374"
        },
        {
            "source": "r_175",
            "target": "s_1477"
        },
        {
            "source": "r_175",
            "target": "s_989"
        },
        {
            "source": "r_176",
            "target": "s_1103"
        },
        {
            "source": "r_176",
            "target": "s_1095"
        },
        {
            "source": "r_176",
            "target": "s_1180"
        },
        {
            "source": "r_176",
            "target": "s_141"
        },
        {
            "source": "r_176",
            "target": "s_1474"
        },
        {
            "source": "r_177",
            "target": "s_814"
        },
        {
            "source": "r_177",
            "target": "s_1594"
        },
        {
            "source": "r_177",
            "target": "s_847"
        },
        {
            "source": "r_177",
            "target": "s_198"
        },
        {
            "source": "r_178",
            "target": "s_1551"
        },
        {
            "source": "r_178",
            "target": "s_45"
        },
        {
            "source": "r_178",
            "target": "s_840"
        },
        {
            "source": "r_178",
            "target": "s_1596"
        },
        {
            "source": "r_178",
            "target": "s_899"
        },
        {
            "source": "r_178",
            "target": "s_157"
        },
        {
            "source": "r_179",
            "target": "s_1258"
        },
        {
            "source": "r_179",
            "target": "s_898"
        },
        {
            "source": "r_179",
            "target": "s_891"
        },
        {
            "source": "r_180",
            "target": "s_887"
        },
        {
            "source": "r_180",
            "target": "s_811"
        },
        {
            "source": "r_180",
            "target": "s_439"
        },
        {
            "source": "r_180",
            "target": "s_817"
        },
        {
            "source": "r_180",
            "target": "s_647"
        },
        {
            "source": "r_180",
            "target": "s_1574"
        },
        {
            "source": "r_180",
            "target": "s_1415"
        },
        {
            "source": "r_181",
            "target": "s_1531"
        },
        {
            "source": "r_181",
            "target": "s_1385"
        },
        {
            "source": "r_181",
            "target": "s_308"
        },
        {
            "source": "r_181",
            "target": "s_485"
        },
        {
            "source": "r_181",
            "target": "s_190"
        },
        {
            "source": "r_181",
            "target": "s_578"
        },
        {
            "source": "r_182",
            "target": "s_712"
        },
        {
            "source": "r_182",
            "target": "s_1073"
        },
        {
            "source": "r_182",
            "target": "s_1204"
        },
        {
            "source": "r_182",
            "target": "s_1474"
        },
        {
            "source": "r_183",
            "target": "s_1542"
        },
        {
            "source": "r_183",
            "target": "s_65"
        },
        {
            "source": "r_183",
            "target": "s_1490"
        },
        {
            "source": "r_183",
            "target": "s_1221"
        },
        {
            "source": "r_183",
            "target": "s_298"
        },
        {
            "source": "r_183",
            "target": "s_663"
        },
        {
            "source": "r_183",
            "target": "s_96"
        },
        {
            "source": "r_184",
            "target": "s_1027"
        },
        {
            "source": "r_184",
            "target": "s_1590"
        },
        {
            "source": "r_184",
            "target": "s_6"
        },
        {
            "source": "r_184",
            "target": "s_1599"
        },
        {
            "source": "r_185",
            "target": "s_496"
        },
        {
            "source": "r_185",
            "target": "s_1381"
        },
        {
            "source": "r_185",
            "target": "s_182"
        },
        {
            "source": "r_185",
            "target": "s_781"
        },
        {
            "source": "r_185",
            "target": "s_1609"
        },
        {
            "source": "r_186",
            "target": "s_127"
        },
        {
            "source": "r_186",
            "target": "s_609"
        },
        {
            "source": "r_186",
            "target": "s_785"
        },
        {
            "source": "r_186",
            "target": "s_1425"
        },
        {
            "source": "r_187",
            "target": "s_262"
        },
        {
            "source": "r_187",
            "target": "s_464"
        },
        {
            "source": "r_187",
            "target": "s_1510"
        },
        {
            "source": "r_187",
            "target": "s_124"
        },
        {
            "source": "r_187",
            "target": "s_588"
        },
        {
            "source": "r_188",
            "target": "s_337"
        },
        {
            "source": "r_188",
            "target": "s_245"
        },
        {
            "source": "r_188",
            "target": "s_1124"
        },
        {
            "source": "r_189",
            "target": "s_1056"
        },
        {
            "source": "r_189",
            "target": "s_820"
        },
        {
            "source": "r_189",
            "target": "s_781"
        },
        {
            "source": "r_190",
            "target": "s_504"
        },
        {
            "source": "r_190",
            "target": "s_139"
        },
        {
            "source": "r_190",
            "target": "s_552"
        },
        {
            "source": "r_190",
            "target": "s_976"
        },
        {
            "source": "r_191",
            "target": "s_1595"
        },
        {
            "source": "r_191",
            "target": "s_464"
        },
        {
            "source": "r_191",
            "target": "s_782"
        },
        {
            "source": "r_192",
            "target": "s_362"
        },
        {
            "source": "r_192",
            "target": "s_607"
        },
        {
            "source": "r_192",
            "target": "s_1031"
        },
        {
            "source": "r_192",
            "target": "s_1237"
        },
        {
            "source": "r_192",
            "target": "s_942"
        },
        {
            "source": "r_193",
            "target": "s_98"
        },
        {
            "source": "r_193",
            "target": "s_1356"
        },
        {
            "source": "r_193",
            "target": "s_1301"
        },
        {
            "source": "r_193",
            "target": "s_215"
        },
        {
            "source": "r_193",
            "target": "s_1517"
        },
        {
            "source": "r_194",
            "target": "s_9"
        },
        {
            "source": "r_194",
            "target": "s_138"
        },
        {
            "source": "r_194",
            "target": "s_470"
        },
        {
            "source": "r_194",
            "target": "s_1441"
        },
        {
            "source": "r_194",
            "target": "s_899"
        },
        {
            "source": "r_194",
            "target": "s_1345"
        },
        {
            "source": "r_194",
            "target": "s_1146"
        },
        {
            "source": "r_195",
            "target": "s_387"
        },
        {
            "source": "r_195",
            "target": "s_708"
        },
        {
            "source": "r_195",
            "target": "s_121"
        },
        {
            "source": "r_195",
            "target": "s_1602"
        },
        {
            "source": "r_195",
            "target": "s_606"
        },
        {
            "source": "r_195",
            "target": "s_342"
        },
        {
            "source": "r_195",
            "target": "s_620"
        },
        {
            "source": "r_195",
            "target": "s_343"
        },
        {
            "source": "r_196",
            "target": "s_1121"
        },
        {
            "source": "r_196",
            "target": "s_1192"
        },
        {
            "source": "r_196",
            "target": "s_37"
        },
        {
            "source": "r_196",
            "target": "s_704"
        },
        {
            "source": "r_196",
            "target": "s_800"
        },
        {
            "source": "r_196",
            "target": "s_1553"
        },
        {
            "source": "r_196",
            "target": "s_78"
        },
        {
            "source": "r_196",
            "target": "s_1429"
        },
        {
            "source": "r_196",
            "target": "s_1385"
        },
        {
            "source": "r_197",
            "target": "s_1317"
        },
        {
            "source": "r_197",
            "target": "s_1425"
        },
        {
            "source": "r_197",
            "target": "s_539"
        },
        {
            "source": "r_197",
            "target": "s_789"
        },
        {
            "source": "r_198",
            "target": "s_43"
        },
        {
            "source": "r_198",
            "target": "s_99"
        },
        {
            "source": "r_198",
            "target": "s_197"
        },
        {
            "source": "r_198",
            "target": "s_366"
        },
        {
            "source": "r_198",
            "target": "s_523"
        },
        {
            "source": "r_199",
            "target": "s_1093"
        },
        {
            "source": "r_199",
            "target": "s_1562"
        },
        {
            "source": "r_199",
            "target": "s_402"
        },
        {
            "source": "r_199",
            "target": "s_748"
        },
        {
            "source": "r_199",
            "target": "s_428"
        },
        {
            "source": "r_200",
            "target": "s_1112"
        },
        {
            "source": "r_200",
            "target": "s_746"
        },
        {
            "source": "r_200",
            "target": "s_1531"
        },
        {
            "source": "r_200",
            "target": "s_834"
        },
        {
            "source": "r_200",
            "target": "s_190"
        },
        {
            "source": "r_200",
            "target": "s_1141"
        },
        {
            "source": "r_200",
            "target": "s_249"
        },
        {
            "source": "r_200",
            "target": "s_1603"
        },
        {
            "source": "r_200",
            "target": "s_156"
        },
        {
            "source": "r_200",
            "target": "s_360"
        },
        {
            "source": "r_200",
            "target": "s_1385"
        },
        {
            "source": "r_201",
            "target": "s_965"
        },
        {
            "source": "r_201",
            "target": "s_1460"
        },
        {
            "source": "r_201",
            "target": "s_597"
        },
        {
            "source": "r_201",
            "target": "s_1171"
        },
        {
            "source": "r_201",
            "target": "s_1133"
        },
        {
            "source": "r_201",
            "target": "s_1212"
        },
        {
            "source": "r_201",
            "target": "s_1004"
        },
        {
            "source": "r_201",
            "target": "s_1556"
        },
        {
            "source": "r_201",
            "target": "s_549"
        },
        {
            "source": "r_202",
            "target": "s_641"
        },
        {
            "source": "r_202",
            "target": "s_1531"
        },
        {
            "source": "r_202",
            "target": "s_746"
        },
        {
            "source": "r_202",
            "target": "s_192"
        },
        {
            "source": "r_202",
            "target": "s_1307"
        },
        {
            "source": "r_202",
            "target": "s_51"
        },
        {
            "source": "r_202",
            "target": "s_1028"
        },
        {
            "source": "r_202",
            "target": "s_162"
        },
        {
            "source": "r_202",
            "target": "s_156"
        },
        {
            "source": "r_202",
            "target": "s_1069"
        },
        {
            "source": "r_202",
            "target": "s_1385"
        },
        {
            "source": "r_203",
            "target": "s_1548"
        },
        {
            "source": "r_203",
            "target": "s_886"
        },
        {
            "source": "r_203",
            "target": "s_542"
        },
        {
            "source": "r_204",
            "target": "s_64"
        },
        {
            "source": "r_204",
            "target": "s_1440"
        },
        {
            "source": "r_204",
            "target": "s_940"
        },
        {
            "source": "r_204",
            "target": "s_646"
        },
        {
            "source": "r_204",
            "target": "s_1411"
        },
        {
            "source": "r_205",
            "target": "s_159"
        },
        {
            "source": "r_205",
            "target": "s_477"
        },
        {
            "source": "r_205",
            "target": "s_1411"
        },
        {
            "source": "r_206",
            "target": "s_426"
        },
        {
            "source": "r_207",
            "target": "s_1063"
        },
        {
            "source": "r_207",
            "target": "s_789"
        },
        {
            "source": "r_208",
            "target": "s_888"
        },
        {
            "source": "r_208",
            "target": "s_484"
        },
        {
            "source": "r_208",
            "target": "s_1120"
        },
        {
            "source": "r_208",
            "target": "s_1001"
        },
        {
            "source": "r_208",
            "target": "s_1170"
        },
        {
            "source": "r_208",
            "target": "s_541"
        },
        {
            "source": "r_208",
            "target": "s_717"
        },
        {
            "source": "r_208",
            "target": "s_713"
        },
        {
            "source": "r_208",
            "target": "s_1354"
        },
        {
            "source": "r_209",
            "target": "s_1077"
        },
        {
            "source": "r_209",
            "target": "s_1310"
        },
        {
            "source": "r_209",
            "target": "s_208"
        },
        {
            "source": "r_209",
            "target": "s_789"
        },
        {
            "source": "r_210",
            "target": "s_1175"
        },
        {
            "source": "r_210",
            "target": "s_1254"
        },
        {
            "source": "r_210",
            "target": "s_74"
        },
        {
            "source": "r_211",
            "target": "s_649"
        },
        {
            "source": "r_211",
            "target": "s_962"
        },
        {
            "source": "r_211",
            "target": "s_1563"
        },
        {
            "source": "r_211",
            "target": "s_542"
        },
        {
            "source": "r_211",
            "target": "s_781"
        },
        {
            "source": "r_211",
            "target": "s_1227"
        },
        {
            "source": "r_212",
            "target": "s_1290"
        },
        {
            "source": "r_212",
            "target": "s_955"
        },
        {
            "source": "r_212",
            "target": "s_877"
        },
        {
            "source": "r_212",
            "target": "s_1372"
        },
        {
            "source": "r_212",
            "target": "s_1337"
        },
        {
            "source": "r_212",
            "target": "s_180"
        },
        {
            "source": "r_213",
            "target": "s_416"
        },
        {
            "source": "r_213",
            "target": "s_1426"
        },
        {
            "source": "r_213",
            "target": "s_1589"
        },
        {
            "source": "r_213",
            "target": "s_689"
        },
        {
            "source": "r_213",
            "target": "s_1486"
        },
        {
            "source": "r_213",
            "target": "s_852"
        },
        {
            "source": "r_214",
            "target": "s_1289"
        },
        {
            "source": "r_214",
            "target": "s_1323"
        },
        {
            "source": "r_214",
            "target": "s_1543"
        },
        {
            "source": "r_214",
            "target": "s_865"
        },
        {
            "source": "r_214",
            "target": "s_1280"
        },
        {
            "source": "r_215",
            "target": "s_895"
        },
        {
            "source": "r_215",
            "target": "s_869"
        },
        {
            "source": "r_215",
            "target": "s_789"
        },
        {
            "source": "r_216",
            "target": "s_462"
        },
        {
            "source": "r_216",
            "target": "s_1458"
        },
        {
            "source": "r_216",
            "target": "s_1025"
        },
        {
            "source": "r_216",
            "target": "s_1185"
        },
        {
            "source": "r_217",
            "target": "s_47"
        },
        {
            "source": "r_217",
            "target": "s_519"
        },
        {
            "source": "r_217",
            "target": "s_184"
        },
        {
            "source": "r_217",
            "target": "s_293"
        },
        {
            "source": "r_217",
            "target": "s_989"
        },
        {
            "source": "r_217",
            "target": "s_360"
        },
        {
            "source": "r_218",
            "target": "s_1293"
        },
        {
            "source": "r_219",
            "target": "s_833"
        },
        {
            "source": "r_219",
            "target": "s_536"
        },
        {
            "source": "r_219",
            "target": "s_27"
        },
        {
            "source": "r_219",
            "target": "s_69"
        },
        {
            "source": "r_219",
            "target": "s_1215"
        },
        {
            "source": "r_220",
            "target": "s_522"
        },
        {
            "source": "r_220",
            "target": "s_27"
        },
        {
            "source": "r_220",
            "target": "s_536"
        },
        {
            "source": "r_221",
            "target": "s_1285"
        },
        {
            "source": "r_221",
            "target": "s_735"
        },
        {
            "source": "r_221",
            "target": "s_1211"
        },
        {
            "source": "r_221",
            "target": "s_1516"
        },
        {
            "source": "r_221",
            "target": "s_1585"
        },
        {
            "source": "r_221",
            "target": "s_300"
        },
        {
            "source": "r_221",
            "target": "s_1415"
        },
        {
            "source": "r_222",
            "target": "s_501"
        },
        {
            "source": "r_222",
            "target": "s_1186"
        },
        {
            "source": "r_222",
            "target": "s_320"
        },
        {
            "source": "r_222",
            "target": "s_604"
        },
        {
            "source": "r_222",
            "target": "s_1492"
        },
        {
            "source": "r_222",
            "target": "s_1240"
        },
        {
            "source": "r_222",
            "target": "s_521"
        },
        {
            "source": "r_222",
            "target": "s_1216"
        },
        {
            "source": "r_222",
            "target": "s_767"
        },
        {
            "source": "r_222",
            "target": "s_382"
        },
        {
            "source": "r_222",
            "target": "s_1421"
        },
        {
            "source": "r_222",
            "target": "s_507"
        },
        {
            "source": "r_222",
            "target": "s_1486"
        },
        {
            "source": "r_223",
            "target": "s_1566"
        },
        {
            "source": "r_223",
            "target": "s_121"
        },
        {
            "source": "r_223",
            "target": "s_949"
        },
        {
            "source": "r_224",
            "target": "s_996"
        },
        {
            "source": "r_224",
            "target": "s_480"
        },
        {
            "source": "r_224",
            "target": "s_1186"
        },
        {
            "source": "r_224",
            "target": "s_689"
        },
        {
            "source": "r_224",
            "target": "s_1486"
        },
        {
            "source": "r_224",
            "target": "s_514"
        },
        {
            "source": "r_225",
            "target": "s_1522"
        },
        {
            "source": "r_225",
            "target": "s_247"
        },
        {
            "source": "r_225",
            "target": "s_1146"
        },
        {
            "source": "r_225",
            "target": "s_1345"
        },
        {
            "source": "r_226",
            "target": "s_443"
        },
        {
            "source": "r_226",
            "target": "s_227"
        },
        {
            "source": "r_226",
            "target": "s_167"
        },
        {
            "source": "r_227",
            "target": "s_1083"
        },
        {
            "source": "r_227",
            "target": "s_545"
        },
        {
            "source": "r_227",
            "target": "s_347"
        },
        {
            "source": "r_228",
            "target": "s_1566"
        },
        {
            "source": "r_228",
            "target": "s_949"
        },
        {
            "source": "r_228",
            "target": "s_708"
        },
        {
            "source": "r_228",
            "target": "s_121"
        },
        {
            "source": "r_228",
            "target": "s_1113"
        },
        {
            "source": "r_228",
            "target": "s_343"
        },
        {
            "source": "r_229",
            "target": "s_1380"
        },
        {
            "source": "r_229",
            "target": "s_963"
        },
        {
            "source": "r_229",
            "target": "s_446"
        },
        {
            "source": "r_229",
            "target": "s_346"
        },
        {
            "source": "r_230",
            "target": "s_89"
        },
        {
            "source": "r_230",
            "target": "s_1138"
        },
        {
            "source": "r_230",
            "target": "s_1279"
        },
        {
            "source": "r_230",
            "target": "s_715"
        },
        {
            "source": "r_231",
            "target": "s_1446"
        },
        {
            "source": "r_231",
            "target": "s_450"
        },
        {
            "source": "r_231",
            "target": "s_42"
        },
        {
            "source": "r_231",
            "target": "s_1100"
        },
        {
            "source": "r_231",
            "target": "s_578"
        },
        {
            "source": "r_232",
            "target": "s_39"
        },
        {
            "source": "r_232",
            "target": "s_1047"
        },
        {
            "source": "r_232",
            "target": "s_549"
        },
        {
            "source": "r_232",
            "target": "s_1338"
        },
        {
            "source": "r_232",
            "target": "s_1361"
        },
        {
            "source": "r_233",
            "target": "s_1071"
        },
        {
            "source": "r_233",
            "target": "s_1187"
        },
        {
            "source": "r_233",
            "target": "s_879"
        },
        {
            "source": "r_233",
            "target": "s_1463"
        },
        {
            "source": "r_233",
            "target": "s_1139"
        },
        {
            "source": "r_233",
            "target": "s_1493"
        },
        {
            "source": "r_233",
            "target": "s_1006"
        },
        {
            "source": "r_233",
            "target": "s_1163"
        },
        {
            "source": "r_234",
            "target": "s_1149"
        },
        {
            "source": "r_234",
            "target": "s_863"
        },
        {
            "source": "r_234",
            "target": "s_646"
        },
        {
            "source": "r_234",
            "target": "s_1229"
        },
        {
            "source": "r_235",
            "target": "s_1405"
        },
        {
            "source": "r_235",
            "target": "s_459"
        },
        {
            "source": "r_235",
            "target": "s_69"
        },
        {
            "source": "r_235",
            "target": "s_1268"
        },
        {
            "source": "r_235",
            "target": "s_1440"
        },
        {
            "source": "r_235",
            "target": "s_110"
        },
        {
            "source": "r_236",
            "target": "s_956"
        },
        {
            "source": "r_236",
            "target": "s_1339"
        },
        {
            "source": "r_236",
            "target": "s_731"
        },
        {
            "source": "r_236",
            "target": "s_1215"
        },
        {
            "source": "r_237",
            "target": "s_522"
        },
        {
            "source": "r_237",
            "target": "s_1266"
        },
        {
            "source": "r_237",
            "target": "s_470"
        },
        {
            "source": "r_237",
            "target": "s_1163"
        },
        {
            "source": "r_237",
            "target": "s_393"
        },
        {
            "source": "r_238",
            "target": "s_1588"
        },
        {
            "source": "r_238",
            "target": "s_1124"
        },
        {
            "source": "r_238",
            "target": "s_1384"
        },
        {
            "source": "r_238",
            "target": "s_1323"
        },
        {
            "source": "r_239",
            "target": "s_358"
        },
        {
            "source": "r_239",
            "target": "s_1038"
        },
        {
            "source": "r_239",
            "target": "s_1117"
        },
        {
            "source": "r_239",
            "target": "s_87"
        },
        {
            "source": "r_239",
            "target": "s_616"
        },
        {
            "source": "r_239",
            "target": "s_901"
        },
        {
            "source": "r_239",
            "target": "s_223"
        },
        {
            "source": "r_239",
            "target": "s_769"
        },
        {
            "source": "r_239",
            "target": "s_564"
        },
        {
            "source": "r_239",
            "target": "s_174"
        },
        {
            "source": "r_240",
            "target": "s_385"
        },
        {
            "source": "r_240",
            "target": "s_77"
        },
        {
            "source": "r_240",
            "target": "s_915"
        },
        {
            "source": "r_240",
            "target": "s_745"
        },
        {
            "source": "r_240",
            "target": "s_1557"
        },
        {
            "source": "r_240",
            "target": "s_645"
        },
        {
            "source": "r_241",
            "target": "s_940"
        },
        {
            "source": "r_241",
            "target": "s_459"
        },
        {
            "source": "r_241",
            "target": "s_69"
        },
        {
            "source": "r_241",
            "target": "s_1440"
        },
        {
            "source": "r_241",
            "target": "s_1226"
        },
        {
            "source": "r_241",
            "target": "s_110"
        },
        {
            "source": "r_241",
            "target": "s_1031"
        },
        {
            "source": "r_242",
            "target": "s_1005"
        },
        {
            "source": "r_242",
            "target": "s_806"
        },
        {
            "source": "r_242",
            "target": "s_1105"
        },
        {
            "source": "r_242",
            "target": "s_276"
        },
        {
            "source": "r_242",
            "target": "s_755"
        },
        {
            "source": "r_242",
            "target": "s_1312"
        },
        {
            "source": "r_242",
            "target": "s_401"
        },
        {
            "source": "r_242",
            "target": "s_1427"
        },
        {
            "source": "r_242",
            "target": "s_1384"
        },
        {
            "source": "r_243",
            "target": "s_1137"
        },
        {
            "source": "r_243",
            "target": "s_66"
        },
        {
            "source": "r_243",
            "target": "s_1013"
        },
        {
            "source": "r_244",
            "target": "s_160"
        },
        {
            "source": "r_244",
            "target": "s_1096"
        },
        {
            "source": "r_244",
            "target": "s_945"
        },
        {
            "source": "r_244",
            "target": "s_612"
        },
        {
            "source": "r_244",
            "target": "s_215"
        },
        {
            "source": "r_245",
            "target": "s_1148"
        },
        {
            "source": "r_245",
            "target": "s_384"
        },
        {
            "source": "r_245",
            "target": "s_1030"
        },
        {
            "source": "r_245",
            "target": "s_335"
        },
        {
            "source": "r_246",
            "target": "s_1091"
        },
        {
            "source": "r_246",
            "target": "s_789"
        },
        {
            "source": "r_246",
            "target": "s_696"
        },
        {
            "source": "r_246",
            "target": "s_1291"
        },
        {
            "source": "r_247",
            "target": "s_1450"
        },
        {
            "source": "r_247",
            "target": "s_840"
        },
        {
            "source": "r_247",
            "target": "s_716"
        },
        {
            "source": "r_247",
            "target": "s_550"
        },
        {
            "source": "r_247",
            "target": "s_899"
        },
        {
            "source": "r_247",
            "target": "s_598"
        },
        {
            "source": "r_248",
            "target": "s_1309"
        },
        {
            "source": "r_248",
            "target": "s_954"
        },
        {
            "source": "r_248",
            "target": "s_1300"
        },
        {
            "source": "r_248",
            "target": "s_1025"
        },
        {
            "source": "r_248",
            "target": "s_366"
        },
        {
            "source": "r_248",
            "target": "s_1591"
        },
        {
            "source": "r_249",
            "target": "s_884"
        },
        {
            "source": "r_249",
            "target": "s_210"
        },
        {
            "source": "r_249",
            "target": "s_1059"
        },
        {
            "source": "r_249",
            "target": "s_83"
        },
        {
            "source": "r_249",
            "target": "s_1305"
        },
        {
            "source": "r_250",
            "target": "s_1287"
        },
        {
            "source": "r_250",
            "target": "s_931"
        },
        {
            "source": "r_250",
            "target": "s_610"
        },
        {
            "source": "r_250",
            "target": "s_965"
        },
        {
            "source": "r_250",
            "target": "s_897"
        },
        {
            "source": "r_250",
            "target": "s_576"
        },
        {
            "source": "r_250",
            "target": "s_933"
        },
        {
            "source": "r_250",
            "target": "s_771"
        },
        {
            "source": "r_250",
            "target": "s_549"
        },
        {
            "source": "r_251",
            "target": "s_804"
        },
        {
            "source": "r_251",
            "target": "s_22"
        },
        {
            "source": "r_251",
            "target": "s_329"
        },
        {
            "source": "r_251",
            "target": "s_1015"
        },
        {
            "source": "r_251",
            "target": "s_744"
        },
        {
            "source": "r_251",
            "target": "s_1320"
        },
        {
            "source": "r_252",
            "target": "s_538"
        },
        {
            "source": "r_252",
            "target": "s_1235"
        },
        {
            "source": "r_252",
            "target": "s_1362"
        },
        {
            "source": "r_252",
            "target": "s_412"
        },
        {
            "source": "r_252",
            "target": "s_803"
        },
        {
            "source": "r_252",
            "target": "s_369"
        },
        {
            "source": "r_252",
            "target": "s_50"
        },
        {
            "source": "r_252",
            "target": "s_1143"
        },
        {
            "source": "r_252",
            "target": "s_156"
        },
        {
            "source": "r_253",
            "target": "s_1070"
        },
        {
            "source": "r_253",
            "target": "s_668"
        },
        {
            "source": "r_253",
            "target": "s_237"
        },
        {
            "source": "r_253",
            "target": "s_467"
        },
        {
            "source": "r_253",
            "target": "s_79"
        },
        {
            "source": "r_253",
            "target": "s_1415"
        },
        {
            "source": "r_254",
            "target": "s_239"
        },
        {
            "source": "r_254",
            "target": "s_1563"
        },
        {
            "source": "r_254",
            "target": "s_1461"
        },
        {
            "source": "r_254",
            "target": "s_652"
        },
        {
            "source": "r_254",
            "target": "s_81"
        },
        {
            "source": "r_254",
            "target": "s_364"
        },
        {
            "source": "r_254",
            "target": "s_1295"
        },
        {
            "source": "r_255",
            "target": "s_957"
        },
        {
            "source": "r_255",
            "target": "s_1446"
        },
        {
            "source": "r_255",
            "target": "s_1373"
        },
        {
            "source": "r_255",
            "target": "s_288"
        },
        {
            "source": "r_255",
            "target": "s_891"
        },
        {
            "source": "r_256",
            "target": "s_32"
        },
        {
            "source": "r_256",
            "target": "s_1210"
        },
        {
            "source": "r_256",
            "target": "s_445"
        },
        {
            "source": "r_256",
            "target": "s_133"
        },
        {
            "source": "r_256",
            "target": "s_1404"
        },
        {
            "source": "r_257",
            "target": "s_1087"
        },
        {
            "source": "r_257",
            "target": "s_1160"
        },
        {
            "source": "r_257",
            "target": "s_270"
        },
        {
            "source": "r_257",
            "target": "s_1415"
        },
        {
            "source": "r_258",
            "target": "s_1524"
        },
        {
            "source": "r_258",
            "target": "s_718"
        },
        {
            "source": "r_258",
            "target": "s_278"
        },
        {
            "source": "r_258",
            "target": "s_693"
        },
        {
            "source": "r_258",
            "target": "s_976"
        },
        {
            "source": "r_259",
            "target": "s_1477"
        },
        {
            "source": "r_259",
            "target": "s_306"
        },
        {
            "source": "r_259",
            "target": "s_1010"
        },
        {
            "source": "r_259",
            "target": "s_1131"
        },
        {
            "source": "r_259",
            "target": "s_184"
        },
        {
            "source": "r_259",
            "target": "s_989"
        },
        {
            "source": "r_260",
            "target": "s_590"
        },
        {
            "source": "r_260",
            "target": "s_38"
        },
        {
            "source": "r_260",
            "target": "s_881"
        },
        {
            "source": "r_260",
            "target": "s_646"
        },
        {
            "source": "r_260",
            "target": "s_107"
        },
        {
            "source": "r_261",
            "target": "s_184"
        },
        {
            "source": "r_261",
            "target": "s_1315"
        },
        {
            "source": "r_261",
            "target": "s_1088"
        },
        {
            "source": "r_261",
            "target": "s_989"
        },
        {
            "source": "r_262",
            "target": "s_1266"
        },
        {
            "source": "r_262",
            "target": "s_393"
        },
        {
            "source": "r_263",
            "target": "s_451"
        },
        {
            "source": "r_263",
            "target": "s_1443"
        },
        {
            "source": "r_264",
            "target": "s_1222"
        },
        {
            "source": "r_264",
            "target": "s_1215"
        },
        {
            "source": "r_265",
            "target": "s_363"
        },
        {
            "source": "r_265",
            "target": "s_1441"
        },
        {
            "source": "r_265",
            "target": "s_1479"
        },
        {
            "source": "r_265",
            "target": "s_107"
        },
        {
            "source": "r_265",
            "target": "s_1146"
        },
        {
            "source": "r_266",
            "target": "s_565"
        },
        {
            "source": "r_266",
            "target": "s_958"
        },
        {
            "source": "r_266",
            "target": "s_1601"
        },
        {
            "source": "r_266",
            "target": "s_1517"
        },
        {
            "source": "r_267",
            "target": "s_12"
        },
        {
            "source": "r_267",
            "target": "s_80"
        },
        {
            "source": "r_267",
            "target": "s_1206"
        },
        {
            "source": "r_268",
            "target": "s_1060"
        },
        {
            "source": "r_268",
            "target": "s_1503"
        },
        {
            "source": "r_268",
            "target": "s_816"
        },
        {
            "source": "r_269",
            "target": "s_470"
        },
        {
            "source": "r_269",
            "target": "s_1408"
        },
        {
            "source": "r_269",
            "target": "s_543"
        },
        {
            "source": "r_269",
            "target": "s_156"
        },
        {
            "source": "r_269",
            "target": "s_298"
        },
        {
            "source": "r_270",
            "target": "s_583"
        },
        {
            "source": "r_270",
            "target": "s_112"
        },
        {
            "source": "r_270",
            "target": "s_1346"
        },
        {
            "source": "r_270",
            "target": "s_1598"
        },
        {
            "source": "r_271",
            "target": "s_1209"
        },
        {
            "source": "r_271",
            "target": "s_145"
        },
        {
            "source": "r_271",
            "target": "s_809"
        },
        {
            "source": "r_271",
            "target": "s_942"
        },
        {
            "source": "r_272",
            "target": "s_596"
        },
        {
            "source": "r_272",
            "target": "s_709"
        },
        {
            "source": "r_272",
            "target": "s_1311"
        },
        {
            "source": "r_272",
            "target": "s_877"
        },
        {
            "source": "r_272",
            "target": "s_156"
        },
        {
            "source": "r_273",
            "target": "s_1308"
        },
        {
            "source": "r_273",
            "target": "s_264"
        },
        {
            "source": "r_273",
            "target": "s_578"
        },
        {
            "source": "r_273",
            "target": "s_1341"
        },
        {
            "source": "r_273",
            "target": "s_456"
        },
        {
            "source": "r_273",
            "target": "s_79"
        },
        {
            "source": "r_273",
            "target": "s_735"
        },
        {
            "source": "r_273",
            "target": "s_1415"
        },
        {
            "source": "r_274",
            "target": "s_798"
        },
        {
            "source": "r_274",
            "target": "s_1213"
        },
        {
            "source": "r_274",
            "target": "s_1481"
        },
        {
            "source": "r_274",
            "target": "s_1193"
        },
        {
            "source": "r_274",
            "target": "s_1438"
        },
        {
            "source": "r_275",
            "target": "s_1122"
        },
        {
            "source": "r_275",
            "target": "s_850"
        },
        {
            "source": "r_276",
            "target": "s_1094"
        },
        {
            "source": "r_276",
            "target": "s_837"
        },
        {
            "source": "r_276",
            "target": "s_580"
        },
        {
            "source": "r_276",
            "target": "s_1517"
        },
        {
            "source": "r_277",
            "target": "s_1159"
        },
        {
            "source": "r_277",
            "target": "s_1322"
        },
        {
            "source": "r_277",
            "target": "s_1538"
        },
        {
            "source": "r_277",
            "target": "s_1480"
        },
        {
            "source": "r_277",
            "target": "s_107"
        },
        {
            "source": "r_277",
            "target": "s_1547"
        },
        {
            "source": "r_277",
            "target": "s_1221"
        },
        {
            "source": "r_278",
            "target": "s_889"
        },
        {
            "source": "r_278",
            "target": "s_744"
        },
        {
            "source": "r_278",
            "target": "s_914"
        },
        {
            "source": "r_278",
            "target": "s_786"
        },
        {
            "source": "r_278",
            "target": "s_1015"
        },
        {
            "source": "r_278",
            "target": "s_54"
        },
        {
            "source": "r_278",
            "target": "s_1084"
        },
        {
            "source": "r_278",
            "target": "s_893"
        },
        {
            "source": "r_278",
            "target": "s_1320"
        },
        {
            "source": "r_279",
            "target": "s_15"
        },
        {
            "source": "r_279",
            "target": "s_1003"
        },
        {
            "source": "r_279",
            "target": "s_846"
        },
        {
            "source": "r_279",
            "target": "s_1503"
        },
        {
            "source": "r_279",
            "target": "s_388"
        },
        {
            "source": "r_280",
            "target": "s_926"
        },
        {
            "source": "r_280",
            "target": "s_1051"
        },
        {
            "source": "r_280",
            "target": "s_510"
        },
        {
            "source": "r_281",
            "target": "s_542"
        },
        {
            "source": "r_281",
            "target": "s_962"
        },
        {
            "source": "r_282",
            "target": "s_659"
        },
        {
            "source": "r_282",
            "target": "s_1584"
        },
        {
            "source": "r_282",
            "target": "s_272"
        },
        {
            "source": "r_282",
            "target": "s_372"
        },
        {
            "source": "r_282",
            "target": "s_1100"
        },
        {
            "source": "r_283",
            "target": "s_630"
        },
        {
            "source": "r_283",
            "target": "s_1615"
        },
        {
            "source": "r_283",
            "target": "s_1053"
        },
        {
            "source": "r_283",
            "target": "s_640"
        },
        {
            "source": "r_283",
            "target": "s_234"
        },
        {
            "source": "r_283",
            "target": "s_902"
        },
        {
            "source": "r_283",
            "target": "s_794"
        },
        {
            "source": "r_284",
            "target": "s_147"
        },
        {
            "source": "r_284",
            "target": "s_686"
        },
        {
            "source": "r_284",
            "target": "s_44"
        },
        {
            "source": "r_284",
            "target": "s_433"
        },
        {
            "source": "r_284",
            "target": "s_174"
        },
        {
            "source": "r_285",
            "target": "s_1172"
        },
        {
            "source": "r_285",
            "target": "s_298"
        },
        {
            "source": "r_285",
            "target": "s_856"
        },
        {
            "source": "r_285",
            "target": "s_1407"
        },
        {
            "source": "r_285",
            "target": "s_968"
        },
        {
            "source": "r_286",
            "target": "s_625"
        },
        {
            "source": "r_286",
            "target": "s_1283"
        },
        {
            "source": "r_286",
            "target": "s_1577"
        },
        {
            "source": "r_286",
            "target": "s_1044"
        },
        {
            "source": "r_286",
            "target": "s_789"
        },
        {
            "source": "r_287",
            "target": "s_1514"
        },
        {
            "source": "r_287",
            "target": "s_148"
        },
        {
            "source": "r_287",
            "target": "s_250"
        },
        {
            "source": "r_287",
            "target": "s_803"
        },
        {
            "source": "r_287",
            "target": "s_1498"
        },
        {
            "source": "r_287",
            "target": "s_1048"
        },
        {
            "source": "r_287",
            "target": "s_1280"
        },
        {
            "source": "r_287",
            "target": "s_348"
        },
        {
            "source": "r_288",
            "target": "s_916"
        },
        {
            "source": "r_288",
            "target": "s_262"
        },
        {
            "source": "r_288",
            "target": "s_1138"
        },
        {
            "source": "r_288",
            "target": "s_89"
        },
        {
            "source": "r_288",
            "target": "s_124"
        },
        {
            "source": "r_288",
            "target": "s_715"
        },
        {
            "source": "r_289",
            "target": "s_983"
        },
        {
            "source": "r_289",
            "target": "s_1544"
        },
        {
            "source": "r_289",
            "target": "s_790"
        },
        {
            "source": "r_289",
            "target": "s_391"
        },
        {
            "source": "r_289",
            "target": "s_899"
        },
        {
            "source": "r_290",
            "target": "s_1219"
        },
        {
            "source": "r_290",
            "target": "s_1099"
        },
        {
            "source": "r_290",
            "target": "s_210"
        },
        {
            "source": "r_290",
            "target": "s_1021"
        },
        {
            "source": "r_290",
            "target": "s_73"
        },
        {
            "source": "r_291",
            "target": "s_241"
        },
        {
            "source": "r_291",
            "target": "s_516"
        },
        {
            "source": "r_291",
            "target": "s_354"
        },
        {
            "source": "r_291",
            "target": "s_173"
        },
        {
            "source": "r_292",
            "target": "s_484"
        },
        {
            "source": "r_292",
            "target": "s_1507"
        },
        {
            "source": "r_292",
            "target": "s_333"
        },
        {
            "source": "r_292",
            "target": "s_924"
        },
        {
            "source": "r_292",
            "target": "s_1499"
        },
        {
            "source": "r_292",
            "target": "s_1354"
        },
        {
            "source": "r_292",
            "target": "s_1001"
        },
        {
            "source": "r_293",
            "target": "s_421"
        },
        {
            "source": "r_293",
            "target": "s_548"
        },
        {
            "source": "r_293",
            "target": "s_1174"
        },
        {
            "source": "r_293",
            "target": "s_348"
        },
        {
            "source": "r_294",
            "target": "s_1247"
        },
        {
            "source": "r_294",
            "target": "s_674"
        },
        {
            "source": "r_294",
            "target": "s_1501"
        },
        {
            "source": "r_295",
            "target": "s_608"
        },
        {
            "source": "r_295",
            "target": "s_386"
        },
        {
            "source": "r_295",
            "target": "s_978"
        },
        {
            "source": "r_296",
            "target": "s_265"
        },
        {
            "source": "r_296",
            "target": "s_1395"
        },
        {
            "source": "r_296",
            "target": "s_277"
        },
        {
            "source": "r_297",
            "target": "s_705"
        },
        {
            "source": "r_297",
            "target": "s_1163"
        },
        {
            "source": "r_298",
            "target": "s_1519"
        },
        {
            "source": "r_298",
            "target": "s_388"
        },
        {
            "source": "r_298",
            "target": "s_1496"
        },
        {
            "source": "r_298",
            "target": "s_1003"
        },
        {
            "source": "r_299",
            "target": "s_1303"
        },
        {
            "source": "r_299",
            "target": "s_49"
        },
        {
            "source": "r_299",
            "target": "s_807"
        },
        {
            "source": "r_299",
            "target": "s_998"
        },
        {
            "source": "r_300",
            "target": "s_1296"
        },
        {
            "source": "r_300",
            "target": "s_366"
        },
        {
            "source": "r_300",
            "target": "s_327"
        },
        {
            "source": "r_300",
            "target": "s_920"
        },
        {
            "source": "r_300",
            "target": "s_1157"
        },
        {
            "source": "r_301",
            "target": "s_657"
        },
        {
            "source": "r_301",
            "target": "s_1343"
        },
        {
            "source": "r_301",
            "target": "s_40"
        },
        {
            "source": "r_301",
            "target": "s_1068"
        },
        {
            "source": "r_302",
            "target": "s_1022"
        },
        {
            "source": "r_302",
            "target": "s_167"
        },
        {
            "source": "r_303",
            "target": "s_1145"
        },
        {
            "source": "r_303",
            "target": "s_1257"
        },
        {
            "source": "r_303",
            "target": "s_219"
        },
        {
            "source": "r_303",
            "target": "s_1441"
        },
        {
            "source": "r_303",
            "target": "s_1259"
        },
        {
            "source": "r_303",
            "target": "s_157"
        },
        {
            "source": "r_303",
            "target": "s_1146"
        },
        {
            "source": "r_304",
            "target": "s_1038"
        },
        {
            "source": "r_304",
            "target": "s_1052"
        },
        {
            "source": "r_304",
            "target": "s_174"
        },
        {
            "source": "r_305",
            "target": "s_681"
        },
        {
            "source": "r_305",
            "target": "s_1039"
        },
        {
            "source": "r_306",
            "target": "s_301"
        },
        {
            "source": "r_306",
            "target": "s_63"
        },
        {
            "source": "r_306",
            "target": "s_1459"
        },
        {
            "source": "r_306",
            "target": "s_475"
        },
        {
            "source": "r_306",
            "target": "s_1037"
        },
        {
            "source": "r_306",
            "target": "s_603"
        },
        {
            "source": "r_306",
            "target": "s_632"
        },
        {
            "source": "r_306",
            "target": "s_380"
        },
        {
            "source": "r_306",
            "target": "s_870"
        },
        {
            "source": "r_307",
            "target": "s_1452"
        },
        {
            "source": "r_307",
            "target": "s_925"
        },
        {
            "source": "r_307",
            "target": "s_373"
        },
        {
            "source": "r_307",
            "target": "s_876"
        },
        {
            "source": "r_307",
            "target": "s_1597"
        },
        {
            "source": "r_307",
            "target": "s_1072"
        },
        {
            "source": "r_307",
            "target": "s_857"
        },
        {
            "source": "r_307",
            "target": "s_1385"
        },
        {
            "source": "r_308",
            "target": "s_607"
        },
        {
            "source": "r_308",
            "target": "s_413"
        },
        {
            "source": "r_308",
            "target": "s_193"
        },
        {
            "source": "r_308",
            "target": "s_1494"
        },
        {
            "source": "r_308",
            "target": "s_1191"
        },
        {
            "source": "r_308",
            "target": "s_1411"
        },
        {
            "source": "r_308",
            "target": "s_942"
        },
        {
            "source": "r_308",
            "target": "s_1383"
        },
        {
            "source": "r_308",
            "target": "s_131"
        },
        {
            "source": "r_308",
            "target": "s_1031"
        },
        {
            "source": "r_309",
            "target": "s_808"
        },
        {
            "source": "r_309",
            "target": "s_388"
        },
        {
            "source": "r_309",
            "target": "s_1186"
        },
        {
            "source": "r_309",
            "target": "s_507"
        },
        {
            "source": "r_309",
            "target": "s_1486"
        },
        {
            "source": "r_309",
            "target": "s_1033"
        },
        {
            "source": "r_309",
            "target": "s_689"
        },
        {
            "source": "r_310",
            "target": "s_1386"
        },
        {
            "source": "r_310",
            "target": "s_646"
        },
        {
            "source": "r_311",
            "target": "s_1570"
        },
        {
            "source": "r_311",
            "target": "s_458"
        },
        {
            "source": "r_311",
            "target": "s_463"
        },
        {
            "source": "r_311",
            "target": "s_1136"
        },
        {
            "source": "r_311",
            "target": "s_1419"
        },
        {
            "source": "r_311",
            "target": "s_447"
        },
        {
            "source": "r_311",
            "target": "s_905"
        },
        {
            "source": "r_311",
            "target": "s_448"
        },
        {
            "source": "r_311",
            "target": "s_1297"
        },
        {
            "source": "r_311",
            "target": "s_661"
        },
        {
            "source": "r_311",
            "target": "s_1078"
        },
        {
            "source": "r_311",
            "target": "s_341"
        },
        {
            "source": "r_312",
            "target": "s_536"
        },
        {
            "source": "r_312",
            "target": "s_1340"
        },
        {
            "source": "r_312",
            "target": "s_920"
        },
        {
            "source": "r_312",
            "target": "s_1331"
        },
        {
            "source": "r_313",
            "target": "s_898"
        },
        {
            "source": "r_313",
            "target": "s_1609"
        },
        {
            "source": "r_313",
            "target": "s_891"
        },
        {
            "source": "r_313",
            "target": "s_1195"
        },
        {
            "source": "r_313",
            "target": "s_424"
        },
        {
            "source": "r_313",
            "target": "s_653"
        },
        {
            "source": "r_314",
            "target": "s_1257"
        },
        {
            "source": "r_314",
            "target": "s_1109"
        },
        {
            "source": "r_314",
            "target": "s_1522"
        },
        {
            "source": "r_314",
            "target": "s_314"
        },
        {
            "source": "r_314",
            "target": "s_161"
        },
        {
            "source": "r_314",
            "target": "s_1146"
        },
        {
            "source": "r_314",
            "target": "s_1345"
        },
        {
            "source": "r_315",
            "target": "s_1410"
        },
        {
            "source": "r_315",
            "target": "s_207"
        },
        {
            "source": "r_315",
            "target": "s_563"
        },
        {
            "source": "r_315",
            "target": "s_1578"
        },
        {
            "source": "r_315",
            "target": "s_203"
        },
        {
            "source": "r_316",
            "target": "s_775"
        },
        {
            "source": "r_316",
            "target": "s_49"
        },
        {
            "source": "r_316",
            "target": "s_988"
        },
        {
            "source": "r_316",
            "target": "s_1237"
        },
        {
            "source": "r_317",
            "target": "s_1548"
        },
        {
            "source": "r_317",
            "target": "s_1040"
        },
        {
            "source": "r_317",
            "target": "s_689"
        },
        {
            "source": "r_317",
            "target": "s_1444"
        },
        {
            "source": "r_317",
            "target": "s_1486"
        },
        {
            "source": "r_317",
            "target": "s_1184"
        },
        {
            "source": "r_317",
            "target": "s_1238"
        },
        {
            "source": "r_318",
            "target": "s_921"
        },
        {
            "source": "r_318",
            "target": "s_1564"
        },
        {
            "source": "r_318",
            "target": "s_1529"
        },
        {
            "source": "r_318",
            "target": "s_486"
        },
        {
            "source": "r_318",
            "target": "s_530"
        },
        {
            "source": "r_318",
            "target": "s_1376"
        },
        {
            "source": "r_318",
            "target": "s_578"
        },
        {
            "source": "r_319",
            "target": "s_875"
        },
        {
            "source": "r_319",
            "target": "s_270"
        },
        {
            "source": "r_319",
            "target": "s_664"
        },
        {
            "source": "r_319",
            "target": "s_1386"
        },
        {
            "source": "r_319",
            "target": "s_1411"
        },
        {
            "source": "r_320",
            "target": "s_1222"
        },
        {
            "source": "r_320",
            "target": "s_1184"
        },
        {
            "source": "r_321",
            "target": "s_311"
        },
        {
            "source": "r_321",
            "target": "s_735"
        },
        {
            "source": "r_321",
            "target": "s_1536"
        },
        {
            "source": "r_321",
            "target": "s_1308"
        },
        {
            "source": "r_321",
            "target": "s_1445"
        },
        {
            "source": "r_321",
            "target": "s_1413"
        },
        {
            "source": "r_321",
            "target": "s_1432"
        },
        {
            "source": "r_321",
            "target": "s_268"
        },
        {
            "source": "r_321",
            "target": "s_1415"
        },
        {
            "source": "r_322",
            "target": "s_1617"
        },
        {
            "source": "r_322",
            "target": "s_571"
        },
        {
            "source": "r_322",
            "target": "s_502"
        },
        {
            "source": "r_322",
            "target": "s_1399"
        },
        {
            "source": "r_323",
            "target": "s_121"
        },
        {
            "source": "r_323",
            "target": "s_20"
        },
        {
            "source": "r_323",
            "target": "s_295"
        },
        {
            "source": "r_323",
            "target": "s_1566"
        },
        {
            "source": "r_323",
            "target": "s_343"
        },
        {
            "source": "r_324",
            "target": "s_390"
        },
        {
            "source": "r_324",
            "target": "s_1077"
        },
        {
            "source": "r_324",
            "target": "s_286"
        },
        {
            "source": "r_324",
            "target": "s_705"
        },
        {
            "source": "r_324",
            "target": "s_1187"
        },
        {
            "source": "r_324",
            "target": "s_1006"
        },
        {
            "source": "r_325",
            "target": "s_897"
        },
        {
            "source": "r_325",
            "target": "s_1414"
        },
        {
            "source": "r_325",
            "target": "s_156"
        },
        {
            "source": "r_325",
            "target": "s_393"
        },
        {
            "source": "r_325",
            "target": "s_470"
        },
        {
            "source": "r_326",
            "target": "s_626"
        },
        {
            "source": "r_326",
            "target": "s_1589"
        },
        {
            "source": "r_326",
            "target": "s_152"
        },
        {
            "source": "r_326",
            "target": "s_1454"
        },
        {
            "source": "r_326",
            "target": "s_611"
        },
        {
            "source": "r_326",
            "target": "s_120"
        },
        {
            "source": "r_326",
            "target": "s_325"
        },
        {
            "source": "r_326",
            "target": "s_1415"
        },
        {
            "source": "r_327",
            "target": "s_195"
        },
        {
            "source": "r_327",
            "target": "s_357"
        },
        {
            "source": "r_327",
            "target": "s_509"
        },
        {
            "source": "r_327",
            "target": "s_1210"
        },
        {
            "source": "r_327",
            "target": "s_93"
        },
        {
            "source": "r_327",
            "target": "s_1345"
        },
        {
            "source": "r_327",
            "target": "s_1146"
        },
        {
            "source": "r_328",
            "target": "s_1567"
        },
        {
            "source": "r_328",
            "target": "s_1287"
        },
        {
            "source": "r_328",
            "target": "s_200"
        },
        {
            "source": "r_328",
            "target": "s_1600"
        },
        {
            "source": "r_328",
            "target": "s_549"
        },
        {
            "source": "r_329",
            "target": "s_1064"
        },
        {
            "source": "r_329",
            "target": "s_1602"
        },
        {
            "source": "r_329",
            "target": "s_994"
        },
        {
            "source": "r_329",
            "target": "s_703"
        },
        {
            "source": "r_329",
            "target": "s_575"
        },
        {
            "source": "r_329",
            "target": "s_706"
        },
        {
            "source": "r_329",
            "target": "s_506"
        },
        {
            "source": "r_329",
            "target": "s_343"
        },
        {
            "source": "r_330",
            "target": "s_918"
        },
        {
            "source": "r_330",
            "target": "s_210"
        },
        {
            "source": "r_330",
            "target": "s_1221"
        },
        {
            "source": "r_330",
            "target": "s_1305"
        },
        {
            "source": "r_331",
            "target": "s_186"
        },
        {
            "source": "r_331",
            "target": "s_334"
        },
        {
            "source": "r_331",
            "target": "s_1459"
        },
        {
            "source": "r_331",
            "target": "s_1336"
        },
        {
            "source": "r_332",
            "target": "s_202"
        },
        {
            "source": "r_332",
            "target": "s_48"
        },
        {
            "source": "r_332",
            "target": "s_441"
        },
        {
            "source": "r_332",
            "target": "s_1029"
        },
        {
            "source": "r_332",
            "target": "s_815"
        },
        {
            "source": "r_333",
            "target": "s_796"
        },
        {
            "source": "r_333",
            "target": "s_263"
        },
        {
            "source": "r_333",
            "target": "s_1379"
        },
        {
            "source": "r_333",
            "target": "s_1062"
        },
        {
            "source": "r_333",
            "target": "s_418"
        },
        {
            "source": "r_333",
            "target": "s_946"
        },
        {
            "source": "r_334",
            "target": "s_1485"
        },
        {
            "source": "r_334",
            "target": "s_1559"
        },
        {
            "source": "r_334",
            "target": "s_1193"
        },
        {
            "source": "r_334",
            "target": "s_1438"
        },
        {
            "source": "r_335",
            "target": "s_977"
        },
        {
            "source": "r_335",
            "target": "s_501"
        },
        {
            "source": "r_335",
            "target": "s_697"
        },
        {
            "source": "r_335",
            "target": "s_859"
        },
        {
            "source": "r_335",
            "target": "s_1228"
        },
        {
            "source": "r_335",
            "target": "s_903"
        },
        {
            "source": "r_335",
            "target": "s_296"
        },
        {
            "source": "r_335",
            "target": "s_1476"
        },
        {
            "source": "r_335",
            "target": "s_1129"
        },
        {
            "source": "r_335",
            "target": "s_1486"
        },
        {
            "source": "r_335",
            "target": "s_689"
        },
        {
            "source": "r_336",
            "target": "s_1151"
        },
        {
            "source": "r_336",
            "target": "s_269"
        },
        {
            "source": "r_336",
            "target": "s_30"
        },
        {
            "source": "r_336",
            "target": "s_245"
        },
        {
            "source": "r_337",
            "target": "s_490"
        },
        {
            "source": "r_337",
            "target": "s_48"
        },
        {
            "source": "r_337",
            "target": "s_907"
        },
        {
            "source": "r_337",
            "target": "s_1271"
        },
        {
            "source": "r_337",
            "target": "s_531"
        },
        {
            "source": "r_337",
            "target": "s_1230"
        },
        {
            "source": "r_337",
            "target": "s_761"
        },
        {
            "source": "r_337",
            "target": "s_815"
        },
        {
            "source": "r_338",
            "target": "s_908"
        },
        {
            "source": "r_338",
            "target": "s_274"
        },
        {
            "source": "r_338",
            "target": "s_1146"
        },
        {
            "source": "r_338",
            "target": "s_1276"
        },
        {
            "source": "r_338",
            "target": "s_1185"
        },
        {
            "source": "r_339",
            "target": "s_184"
        },
        {
            "source": "r_339",
            "target": "s_752"
        },
        {
            "source": "r_339",
            "target": "s_365"
        },
        {
            "source": "r_339",
            "target": "s_721"
        },
        {
            "source": "r_339",
            "target": "s_25"
        },
        {
            "source": "r_339",
            "target": "s_1088"
        },
        {
            "source": "r_339",
            "target": "s_1315"
        },
        {
            "source": "r_339",
            "target": "s_989"
        },
        {
            "source": "r_340",
            "target": "s_1330"
        },
        {
            "source": "r_340",
            "target": "s_1317"
        },
        {
            "source": "r_340",
            "target": "s_292"
        },
        {
            "source": "r_340",
            "target": "s_82"
        },
        {
            "source": "r_340",
            "target": "s_613"
        },
        {
            "source": "r_340",
            "target": "s_781"
        },
        {
            "source": "r_341",
            "target": "s_649"
        },
        {
            "source": "r_341",
            "target": "s_233"
        },
        {
            "source": "r_341",
            "target": "s_835"
        },
        {
            "source": "r_341",
            "target": "s_789"
        },
        {
            "source": "r_341",
            "target": "s_1227"
        },
        {
            "source": "r_342",
            "target": "s_950"
        },
        {
            "source": "r_342",
            "target": "s_291"
        },
        {
            "source": "r_342",
            "target": "s_639"
        },
        {
            "source": "r_342",
            "target": "s_188"
        },
        {
            "source": "r_343",
            "target": "s_485"
        },
        {
            "source": "r_343",
            "target": "s_828"
        },
        {
            "source": "r_343",
            "target": "s_1603"
        },
        {
            "source": "r_343",
            "target": "s_599"
        },
        {
            "source": "r_343",
            "target": "s_1072"
        },
        {
            "source": "r_343",
            "target": "s_648"
        },
        {
            "source": "r_343",
            "target": "s_872"
        },
        {
            "source": "r_343",
            "target": "s_79"
        },
        {
            "source": "r_343",
            "target": "s_1385"
        },
        {
            "source": "r_344",
            "target": "s_1567"
        },
        {
            "source": "r_344",
            "target": "s_610"
        },
        {
            "source": "r_344",
            "target": "s_1287"
        },
        {
            "source": "r_344",
            "target": "s_1512"
        },
        {
            "source": "r_344",
            "target": "s_1502"
        },
        {
            "source": "r_344",
            "target": "s_549"
        },
        {
            "source": "r_345",
            "target": "s_497"
        },
        {
            "source": "r_345",
            "target": "s_153"
        },
        {
            "source": "r_345",
            "target": "s_813"
        },
        {
            "source": "r_345",
            "target": "s_52"
        },
        {
            "source": "r_345",
            "target": "s_1435"
        },
        {
            "source": "r_345",
            "target": "s_1338"
        },
        {
            "source": "r_346",
            "target": "s_1107"
        },
        {
            "source": "r_346",
            "target": "s_515"
        },
        {
            "source": "r_346",
            "target": "s_1412"
        },
        {
            "source": "r_346",
            "target": "s_764"
        },
        {
            "source": "r_346",
            "target": "s_1173"
        },
        {
            "source": "r_346",
            "target": "s_168"
        },
        {
            "source": "r_346",
            "target": "s_1395"
        },
        {
            "source": "r_346",
            "target": "s_592"
        },
        {
            "source": "r_346",
            "target": "s_675"
        },
        {
            "source": "r_347",
            "target": "s_125"
        },
        {
            "source": "r_347",
            "target": "s_524"
        },
        {
            "source": "r_347",
            "target": "s_275"
        },
        {
            "source": "r_347",
            "target": "s_793"
        },
        {
            "source": "r_347",
            "target": "s_1237"
        },
        {
            "source": "r_348",
            "target": "s_727"
        },
        {
            "source": "r_348",
            "target": "s_995"
        },
        {
            "source": "r_348",
            "target": "s_551"
        },
        {
            "source": "r_348",
            "target": "s_48"
        },
        {
            "source": "r_348",
            "target": "s_1110"
        },
        {
            "source": "r_349",
            "target": "s_1581"
        },
        {
            "source": "r_349",
            "target": "s_1270"
        },
        {
            "source": "r_349",
            "target": "s_986"
        },
        {
            "source": "r_349",
            "target": "s_836"
        },
        {
            "source": "r_350",
            "target": "s_1003"
        },
        {
            "source": "r_350",
            "target": "s_1519"
        },
        {
            "source": "r_350",
            "target": "s_205"
        },
        {
            "source": "r_350",
            "target": "s_1496"
        },
        {
            "source": "r_350",
            "target": "s_388"
        },
        {
            "source": "r_351",
            "target": "s_931"
        },
        {
            "source": "r_351",
            "target": "s_1134"
        },
        {
            "source": "r_351",
            "target": "s_13"
        },
        {
            "source": "r_351",
            "target": "s_1235"
        },
        {
            "source": "r_351",
            "target": "s_739"
        },
        {
            "source": "r_351",
            "target": "s_1447"
        },
        {
            "source": "r_351",
            "target": "s_1004"
        },
        {
            "source": "r_351",
            "target": "s_549"
        },
        {
            "source": "r_352",
            "target": "s_279"
        },
        {
            "source": "r_352",
            "target": "s_1462"
        },
        {
            "source": "r_352",
            "target": "s_707"
        },
        {
            "source": "r_352",
            "target": "s_1068"
        },
        {
            "source": "r_353",
            "target": "s_1054"
        },
        {
            "source": "r_353",
            "target": "s_1369"
        },
        {
            "source": "r_353",
            "target": "s_1074"
        },
        {
            "source": "r_353",
            "target": "s_478"
        },
        {
            "source": "r_354",
            "target": "s_1103"
        },
        {
            "source": "r_354",
            "target": "s_1237"
        },
        {
            "source": "r_355",
            "target": "s_646"
        },
        {
            "source": "r_355",
            "target": "s_229"
        },
        {
            "source": "r_355",
            "target": "s_71"
        },
        {
            "source": "r_355",
            "target": "s_1026"
        },
        {
            "source": "r_356",
            "target": "s_1125"
        },
        {
            "source": "r_356",
            "target": "s_1326"
        },
        {
            "source": "r_356",
            "target": "s_211"
        },
        {
            "source": "r_356",
            "target": "s_212"
        },
        {
            "source": "r_356",
            "target": "s_1176"
        },
        {
            "source": "r_356",
            "target": "s_1565"
        },
        {
            "source": "r_356",
            "target": "s_167"
        },
        {
            "source": "r_356",
            "target": "s_478"
        },
        {
            "source": "r_356",
            "target": "s_1253"
        },
        {
            "source": "r_356",
            "target": "s_1142"
        },
        {
            "source": "r_357",
            "target": "s_1091"
        },
        {
            "source": "r_357",
            "target": "s_233"
        },
        {
            "source": "r_357",
            "target": "s_789"
        },
        {
            "source": "r_358",
            "target": "s_1314"
        },
        {
            "source": "r_358",
            "target": "s_1282"
        },
        {
            "source": "r_358",
            "target": "s_1081"
        },
        {
            "source": "r_358",
            "target": "s_267"
        },
        {
            "source": "r_359",
            "target": "s_1400"
        },
        {
            "source": "r_359",
            "target": "s_1456"
        },
        {
            "source": "r_359",
            "target": "s_646"
        },
        {
            "source": "r_359",
            "target": "s_1386"
        },
        {
            "source": "r_359",
            "target": "s_1025"
        },
        {
            "source": "r_360",
            "target": "s_143"
        },
        {
            "source": "r_360",
            "target": "s_12"
        },
        {
            "source": "r_360",
            "target": "s_198"
        },
        {
            "source": "r_360",
            "target": "s_426"
        },
        {
            "source": "r_361",
            "target": "s_408"
        },
        {
            "source": "r_361",
            "target": "s_270"
        },
        {
            "source": "r_361",
            "target": "s_481"
        },
        {
            "source": "r_361",
            "target": "s_135"
        },
        {
            "source": "r_361",
            "target": "s_1411"
        },
        {
            "source": "r_362",
            "target": "s_175"
        },
        {
            "source": "r_362",
            "target": "s_844"
        },
        {
            "source": "r_362",
            "target": "s_314"
        },
        {
            "source": "r_362",
            "target": "s_621"
        },
        {
            "source": "r_362",
            "target": "s_1076"
        },
        {
            "source": "r_363",
            "target": "s_628"
        },
        {
            "source": "r_363",
            "target": "s_305"
        },
        {
            "source": "r_363",
            "target": "s_182"
        },
        {
            "source": "r_364",
            "target": "s_796"
        },
        {
            "source": "r_364",
            "target": "s_1062"
        },
        {
            "source": "r_364",
            "target": "s_1050"
        },
        {
            "source": "r_364",
            "target": "s_1610"
        },
        {
            "source": "r_364",
            "target": "s_585"
        },
        {
            "source": "r_364",
            "target": "s_418"
        },
        {
            "source": "r_364",
            "target": "s_946"
        },
        {
            "source": "r_365",
            "target": "s_1302"
        },
        {
            "source": "r_365",
            "target": "s_344"
        },
        {
            "source": "r_365",
            "target": "s_1587"
        },
        {
            "source": "r_365",
            "target": "s_157"
        },
        {
            "source": "r_366",
            "target": "s_1406"
        },
        {
            "source": "r_366",
            "target": "s_1302"
        },
        {
            "source": "r_366",
            "target": "s_157"
        },
        {
            "source": "r_367",
            "target": "s_477"
        },
        {
            "source": "r_367",
            "target": "s_1132"
        },
        {
            "source": "r_368",
            "target": "s_440"
        },
        {
            "source": "r_368",
            "target": "s_681"
        },
        {
            "source": "r_368",
            "target": "s_1384"
        },
        {
            "source": "r_368",
            "target": "s_1039"
        },
        {
            "source": "r_369",
            "target": "s_236"
        },
        {
            "source": "r_369",
            "target": "s_85"
        },
        {
            "source": "r_369",
            "target": "s_1359"
        },
        {
            "source": "r_369",
            "target": "s_394"
        },
        {
            "source": "r_370",
            "target": "s_1469"
        },
        {
            "source": "r_370",
            "target": "s_298"
        },
        {
            "source": "r_371",
            "target": "s_1513"
        },
        {
            "source": "r_371",
            "target": "s_594"
        },
        {
            "source": "r_371",
            "target": "s_145"
        },
        {
            "source": "r_372",
            "target": "s_370"
        },
        {
            "source": "r_372",
            "target": "s_29"
        },
        {
            "source": "r_372",
            "target": "s_102"
        },
        {
            "source": "r_372",
            "target": "s_444"
        },
        {
            "source": "r_373",
            "target": "s_984"
        },
        {
            "source": "r_373",
            "target": "s_166"
        },
        {
            "source": "r_373",
            "target": "s_135"
        },
        {
            "source": "r_373",
            "target": "s_1411"
        },
        {
            "source": "r_374",
            "target": "s_1132"
        },
        {
            "source": "r_374",
            "target": "s_566"
        },
        {
            "source": "r_374",
            "target": "s_477"
        },
        {
            "source": "r_375",
            "target": "s_1146"
        },
        {
            "source": "r_375",
            "target": "s_194"
        },
        {
            "source": "r_375",
            "target": "s_1261"
        },
        {
            "source": "r_375",
            "target": "s_1441"
        },
        {
            "source": "r_375",
            "target": "s_219"
        },
        {
            "source": "r_375",
            "target": "s_1332"
        },
        {
            "source": "r_375",
            "target": "s_1345"
        },
        {
            "source": "r_376",
            "target": "s_581"
        },
        {
            "source": "r_376",
            "target": "s_532"
        },
        {
            "source": "r_376",
            "target": "s_418"
        },
        {
            "source": "r_377",
            "target": "s_356"
        },
        {
            "source": "r_377",
            "target": "s_1352"
        },
        {
            "source": "r_377",
            "target": "s_710"
        },
        {
            "source": "r_377",
            "target": "s_679"
        },
        {
            "source": "r_377",
            "target": "s_1422"
        },
        {
            "source": "r_377",
            "target": "s_1354"
        },
        {
            "source": "r_378",
            "target": "s_827"
        },
        {
            "source": "r_378",
            "target": "s_1608"
        },
        {
            "source": "r_378",
            "target": "s_1160"
        },
        {
            "source": "r_378",
            "target": "s_789"
        },
        {
            "source": "r_379",
            "target": "s_1583"
        },
        {
            "source": "r_379",
            "target": "s_1521"
        },
        {
            "source": "r_379",
            "target": "s_734"
        },
        {
            "source": "r_379",
            "target": "s_1020"
        },
        {
            "source": "r_379",
            "target": "s_470"
        },
        {
            "source": "r_380",
            "target": "s_1387"
        },
        {
            "source": "r_380",
            "target": "s_662"
        },
        {
            "source": "r_380",
            "target": "s_1525"
        },
        {
            "source": "r_380",
            "target": "s_772"
        },
        {
            "source": "r_380",
            "target": "s_1245"
        },
        {
            "source": "r_381",
            "target": "s_429"
        },
        {
            "source": "r_382",
            "target": "s_592"
        },
        {
            "source": "r_382",
            "target": "s_528"
        },
        {
            "source": "r_382",
            "target": "s_567"
        },
        {
            "source": "r_382",
            "target": "s_989"
        },
        {
            "source": "r_382",
            "target": "s_666"
        },
        {
            "source": "r_383",
            "target": "s_121"
        },
        {
            "source": "r_383",
            "target": "s_949"
        },
        {
            "source": "r_383",
            "target": "s_20"
        },
        {
            "source": "r_383",
            "target": "s_339"
        },
        {
            "source": "r_383",
            "target": "s_343"
        },
        {
            "source": "r_384",
            "target": "s_1433"
        },
        {
            "source": "r_384",
            "target": "s_322"
        },
        {
            "source": "r_384",
            "target": "s_405"
        },
        {
            "source": "r_384",
            "target": "s_986"
        },
        {
            "source": "r_384",
            "target": "s_841"
        },
        {
            "source": "r_385",
            "target": "s_833"
        },
        {
            "source": "r_385",
            "target": "s_1528"
        },
        {
            "source": "r_385",
            "target": "s_812"
        },
        {
            "source": "r_385",
            "target": "s_128"
        },
        {
            "source": "r_385",
            "target": "s_496"
        },
        {
            "source": "r_385",
            "target": "s_1324"
        },
        {
            "source": "r_385",
            "target": "s_1184"
        },
        {
            "source": "r_386",
            "target": "s_165"
        },
        {
            "source": "r_386",
            "target": "s_1106"
        },
        {
            "source": "r_387",
            "target": "s_172"
        },
        {
            "source": "r_387",
            "target": "s_405"
        },
        {
            "source": "r_388",
            "target": "s_1058"
        },
        {
            "source": "r_388",
            "target": "s_1482"
        },
        {
            "source": "r_388",
            "target": "s_594"
        },
        {
            "source": "r_388",
            "target": "s_607"
        },
        {
            "source": "r_388",
            "target": "s_145"
        },
        {
            "source": "r_388",
            "target": "s_1031"
        },
        {
            "source": "r_389",
            "target": "s_1235"
        },
        {
            "source": "r_389",
            "target": "s_1616"
        },
        {
            "source": "r_389",
            "target": "s_1272"
        },
        {
            "source": "r_389",
            "target": "s_1505"
        },
        {
            "source": "r_389",
            "target": "s_156"
        },
        {
            "source": "r_390",
            "target": "s_7"
        },
        {
            "source": "r_390",
            "target": "s_398"
        },
        {
            "source": "r_390",
            "target": "s_1090"
        },
        {
            "source": "r_390",
            "target": "s_536"
        },
        {
            "source": "r_390",
            "target": "s_754"
        },
        {
            "source": "r_391",
            "target": "s_293"
        },
        {
            "source": "r_391",
            "target": "s_1387"
        },
        {
            "source": "r_391",
            "target": "s_1182"
        },
        {
            "source": "r_391",
            "target": "s_1405"
        },
        {
            "source": "r_391",
            "target": "s_1177"
        },
        {
            "source": "r_391",
            "target": "s_441"
        },
        {
            "source": "r_391",
            "target": "s_989"
        },
        {
            "source": "r_392",
            "target": "s_236"
        },
        {
            "source": "r_392",
            "target": "s_229"
        },
        {
            "source": "r_392",
            "target": "s_1201"
        },
        {
            "source": "r_392",
            "target": "s_394"
        },
        {
            "source": "r_393",
            "target": "s_1234"
        },
        {
            "source": "r_393",
            "target": "s_909"
        },
        {
            "source": "r_393",
            "target": "s_375"
        },
        {
            "source": "r_393",
            "target": "s_978"
        },
        {
            "source": "r_393",
            "target": "s_392"
        },
        {
            "source": "r_394",
            "target": "s_838"
        },
        {
            "source": "r_394",
            "target": "s_118"
        },
        {
            "source": "r_394",
            "target": "s_376"
        },
        {
            "source": "r_394",
            "target": "s_332"
        },
        {
            "source": "r_394",
            "target": "s_1472"
        },
        {
            "source": "r_394",
            "target": "s_923"
        },
        {
            "source": "r_394",
            "target": "s_360"
        },
        {
            "source": "r_394",
            "target": "s_1338"
        },
        {
            "source": "r_395",
            "target": "s_1581"
        },
        {
            "source": "r_395",
            "target": "s_986"
        },
        {
            "source": "r_395",
            "target": "s_1468"
        },
        {
            "source": "r_395",
            "target": "s_836"
        },
        {
            "source": "r_395",
            "target": "s_685"
        },
        {
            "source": "r_396",
            "target": "s_1339"
        },
        {
            "source": "r_396",
            "target": "s_1411"
        },
        {
            "source": "r_397",
            "target": "s_803"
        },
        {
            "source": "r_397",
            "target": "s_1164"
        },
        {
            "source": "r_397",
            "target": "s_1174"
        },
        {
            "source": "r_397",
            "target": "s_906"
        },
        {
            "source": "r_397",
            "target": "s_148"
        },
        {
            "source": "r_397",
            "target": "s_465"
        },
        {
            "source": "r_397",
            "target": "s_1523"
        },
        {
            "source": "r_397",
            "target": "s_1014"
        },
        {
            "source": "r_397",
            "target": "s_1486"
        },
        {
            "source": "r_397",
            "target": "s_348"
        },
        {
            "source": "r_398",
            "target": "s_871"
        },
        {
            "source": "r_398",
            "target": "s_1097"
        },
        {
            "source": "r_398",
            "target": "s_534"
        },
        {
            "source": "r_398",
            "target": "s_1166"
        },
        {
            "source": "r_398",
            "target": "s_815"
        },
        {
            "source": "r_399",
            "target": "s_64"
        },
        {
            "source": "r_399",
            "target": "s_1440"
        },
        {
            "source": "r_399",
            "target": "s_1411"
        },
        {
            "source": "r_399",
            "target": "s_69"
        },
        {
            "source": "r_400",
            "target": "s_1589"
        },
        {
            "source": "r_400",
            "target": "s_1298"
        },
        {
            "source": "r_400",
            "target": "s_416"
        },
        {
            "source": "r_400",
            "target": "s_41"
        },
        {
            "source": "r_400",
            "target": "s_689"
        },
        {
            "source": "r_400",
            "target": "s_1486"
        },
        {
            "source": "r_401",
            "target": "s_1550"
        },
        {
            "source": "r_401",
            "target": "s_1576"
        },
        {
            "source": "r_401",
            "target": "s_1465"
        },
        {
            "source": "r_401",
            "target": "s_1037"
        },
        {
            "source": "r_401",
            "target": "s_722"
        },
        {
            "source": "r_401",
            "target": "s_1598"
        },
        {
            "source": "r_402",
            "target": "s_730"
        },
        {
            "source": "r_402",
            "target": "s_1010"
        },
        {
            "source": "r_402",
            "target": "s_1436"
        },
        {
            "source": "r_402",
            "target": "s_758"
        },
        {
            "source": "r_402",
            "target": "s_404"
        },
        {
            "source": "r_402",
            "target": "s_672"
        },
        {
            "source": "r_402",
            "target": "s_1556"
        },
        {
            "source": "r_403",
            "target": "s_171"
        },
        {
            "source": "r_403",
            "target": "s_1490"
        },
        {
            "source": "r_403",
            "target": "s_1282"
        },
        {
            "source": "r_404",
            "target": "s_349"
        },
        {
            "source": "r_404",
            "target": "s_845"
        },
        {
            "source": "r_404",
            "target": "s_1409"
        },
        {
            "source": "r_404",
            "target": "s_1036"
        },
        {
            "source": "r_404",
            "target": "s_518"
        },
        {
            "source": "r_404",
            "target": "s_1338"
        },
        {
            "source": "r_405",
            "target": "s_1398"
        },
        {
            "source": "r_405",
            "target": "s_868"
        },
        {
            "source": "r_405",
            "target": "s_725"
        },
        {
            "source": "r_405",
            "target": "s_500"
        },
        {
            "source": "r_405",
            "target": "s_1256"
        },
        {
            "source": "r_405",
            "target": "s_789"
        },
        {
            "source": "r_406",
            "target": "s_209"
        },
        {
            "source": "r_406",
            "target": "s_556"
        },
        {
            "source": "r_406",
            "target": "s_452"
        },
        {
            "source": "r_406",
            "target": "s_1237"
        },
        {
            "source": "r_407",
            "target": "s_1491"
        },
        {
            "source": "r_407",
            "target": "s_1282"
        },
        {
            "source": "r_407",
            "target": "s_1604"
        },
        {
            "source": "r_407",
            "target": "s_1489"
        },
        {
            "source": "r_407",
            "target": "s_1153"
        },
        {
            "source": "r_407",
            "target": "s_937"
        },
        {
            "source": "r_407",
            "target": "s_96"
        },
        {
            "source": "r_407",
            "target": "s_298"
        },
        {
            "source": "r_408",
            "target": "s_1552"
        },
        {
            "source": "r_408",
            "target": "s_1466"
        },
        {
            "source": "r_408",
            "target": "s_1304"
        },
        {
            "source": "r_408",
            "target": "s_115"
        },
        {
            "source": "r_408",
            "target": "s_1556"
        },
        {
            "source": "r_408",
            "target": "s_549"
        },
        {
            "source": "r_409",
            "target": "s_33"
        },
        {
            "source": "r_409",
            "target": "s_232"
        },
        {
            "source": "r_409",
            "target": "s_315"
        },
        {
            "source": "r_409",
            "target": "s_289"
        },
        {
            "source": "r_409",
            "target": "s_1003"
        },
        {
            "source": "r_409",
            "target": "s_388"
        },
        {
            "source": "r_410",
            "target": "s_172"
        },
        {
            "source": "r_410",
            "target": "s_773"
        },
        {
            "source": "r_410",
            "target": "s_1265"
        },
        {
            "source": "r_410",
            "target": "s_910"
        },
        {
            "source": "r_410",
            "target": "s_405"
        },
        {
            "source": "r_411",
            "target": "s_1437"
        },
        {
            "source": "r_411",
            "target": "s_1265"
        },
        {
            "source": "r_411",
            "target": "s_773"
        },
        {
            "source": "r_411",
            "target": "s_986"
        },
        {
            "source": "r_411",
            "target": "s_749"
        },
        {
            "source": "r_411",
            "target": "s_405"
        },
        {
            "source": "r_412",
            "target": "s_1598"
        },
        {
            "source": "r_412",
            "target": "s_936"
        },
        {
            "source": "r_412",
            "target": "s_583"
        },
        {
            "source": "r_412",
            "target": "s_499"
        },
        {
            "source": "r_412",
            "target": "s_328"
        },
        {
            "source": "r_412",
            "target": "s_1349"
        },
        {
            "source": "r_413",
            "target": "s_108"
        },
        {
            "source": "r_413",
            "target": "s_780"
        },
        {
            "source": "r_413",
            "target": "s_340"
        },
        {
            "source": "r_413",
            "target": "s_16"
        },
        {
            "source": "r_413",
            "target": "s_1253"
        },
        {
            "source": "r_413",
            "target": "s_614"
        },
        {
            "source": "r_414",
            "target": "s_1027"
        },
        {
            "source": "r_414",
            "target": "s_751"
        },
        {
            "source": "r_414",
            "target": "s_1599"
        },
        {
            "source": "r_415",
            "target": "s_814"
        },
        {
            "source": "r_415",
            "target": "s_536"
        },
        {
            "source": "r_416",
            "target": "s_1266"
        },
        {
            "source": "r_416",
            "target": "s_1402"
        },
        {
            "source": "r_416",
            "target": "s_178"
        },
        {
            "source": "r_416",
            "target": "s_260"
        },
        {
            "source": "r_416",
            "target": "s_393"
        },
        {
            "source": "r_417",
            "target": "s_1532"
        },
        {
            "source": "r_417",
            "target": "s_1592"
        },
        {
            "source": "r_417",
            "target": "s_411"
        },
        {
            "source": "r_417",
            "target": "s_1185"
        },
        {
            "source": "r_418",
            "target": "s_462"
        },
        {
            "source": "r_418",
            "target": "s_1458"
        },
        {
            "source": "r_418",
            "target": "s_119"
        },
        {
            "source": "r_418",
            "target": "s_617"
        },
        {
            "source": "r_418",
            "target": "s_1025"
        },
        {
            "source": "r_418",
            "target": "s_1185"
        },
        {
            "source": "r_419",
            "target": "s_1444"
        },
        {
            "source": "r_419",
            "target": "s_1130"
        },
        {
            "source": "r_419",
            "target": "s_1246"
        },
        {
            "source": "r_419",
            "target": "s_1573"
        },
        {
            "source": "r_419",
            "target": "s_393"
        },
        {
            "source": "r_419",
            "target": "s_507"
        },
        {
            "source": "r_419",
            "target": "s_1486"
        },
        {
            "source": "r_419",
            "target": "s_689"
        },
        {
            "source": "r_419",
            "target": "s_1186"
        },
        {
            "source": "r_420",
            "target": "s_1178"
        },
        {
            "source": "r_420",
            "target": "s_762"
        },
        {
            "source": "r_420",
            "target": "s_982"
        },
        {
            "source": "r_420",
            "target": "s_1184"
        },
        {
            "source": "r_421",
            "target": "s_723"
        },
        {
            "source": "r_421",
            "target": "s_1207"
        },
        {
            "source": "r_421",
            "target": "s_1606"
        },
        {
            "source": "r_421",
            "target": "s_694"
        },
        {
            "source": "r_421",
            "target": "s_1323"
        },
        {
            "source": "r_421",
            "target": "s_510"
        },
        {
            "source": "r_422",
            "target": "s_487"
        },
        {
            "source": "r_422",
            "target": "s_386"
        },
        {
            "source": "r_422",
            "target": "s_782"
        },
        {
            "source": "r_423",
            "target": "s_270"
        },
        {
            "source": "r_423",
            "target": "s_875"
        },
        {
            "source": "r_423",
            "target": "s_1031"
        },
        {
            "source": "r_423",
            "target": "s_1411"
        },
        {
            "source": "r_424",
            "target": "s_290"
        },
        {
            "source": "r_424",
            "target": "s_484"
        },
        {
            "source": "r_424",
            "target": "s_1378"
        },
        {
            "source": "r_424",
            "target": "s_1001"
        },
        {
            "source": "r_424",
            "target": "s_555"
        },
        {
            "source": "r_424",
            "target": "s_1499"
        },
        {
            "source": "r_424",
            "target": "s_333"
        },
        {
            "source": "r_424",
            "target": "s_1354"
        },
        {
            "source": "r_425",
            "target": "s_1219"
        },
        {
            "source": "r_425",
            "target": "s_120"
        },
        {
            "source": "r_425",
            "target": "s_1021"
        },
        {
            "source": "r_425",
            "target": "s_73"
        },
        {
            "source": "r_425",
            "target": "s_210"
        },
        {
            "source": "r_426",
            "target": "s_605"
        },
        {
            "source": "r_426",
            "target": "s_229"
        },
        {
            "source": "r_427",
            "target": "s_370"
        },
        {
            "source": "r_427",
            "target": "s_29"
        },
        {
            "source": "r_427",
            "target": "s_102"
        },
        {
            "source": "r_427",
            "target": "s_444"
        },
        {
            "source": "r_428",
            "target": "s_1617"
        },
        {
            "source": "r_428",
            "target": "s_1350"
        },
        {
            "source": "r_428",
            "target": "s_1399"
        },
        {
            "source": "r_428",
            "target": "s_1375"
        },
        {
            "source": "r_428",
            "target": "s_861"
        },
        {
            "source": "r_428",
            "target": "s_890"
        },
        {
            "source": "r_428",
            "target": "s_395"
        },
        {
            "source": "r_428",
            "target": "s_542"
        },
        {
            "source": "r_429",
            "target": "s_1316"
        },
        {
            "source": "r_429",
            "target": "s_975"
        },
        {
            "source": "r_429",
            "target": "s_216"
        },
        {
            "source": "r_429",
            "target": "s_573"
        },
        {
            "source": "r_429",
            "target": "s_549"
        },
        {
            "source": "r_429",
            "target": "s_378"
        },
        {
            "source": "r_430",
            "target": "s_595"
        },
        {
            "source": "r_430",
            "target": "s_99"
        },
        {
            "source": "r_430",
            "target": "s_98"
        },
        {
            "source": "r_430",
            "target": "s_316"
        },
        {
            "source": "r_430",
            "target": "s_964"
        },
        {
            "source": "r_430",
            "target": "s_366"
        },
        {
            "source": "r_431",
            "target": "s_385"
        },
        {
            "source": "r_431",
            "target": "s_557"
        },
        {
            "source": "r_431",
            "target": "s_745"
        },
        {
            "source": "r_431",
            "target": "s_839"
        },
        {
            "source": "r_431",
            "target": "s_645"
        },
        {
            "source": "r_432",
            "target": "s_959"
        },
        {
            "source": "r_432",
            "target": "s_1562"
        },
        {
            "source": "r_432",
            "target": "s_303"
        },
        {
            "source": "r_432",
            "target": "s_304"
        },
        {
            "source": "r_432",
            "target": "s_1405"
        },
        {
            "source": "r_432",
            "target": "s_406"
        },
        {
            "source": "r_432",
            "target": "s_1347"
        },
        {
            "source": "r_432",
            "target": "s_240"
        },
        {
            "source": "r_432",
            "target": "s_904"
        },
        {
            "source": "r_432",
            "target": "s_1286"
        },
        {
            "source": "r_432",
            "target": "s_1464"
        },
        {
            "source": "r_432",
            "target": "s_1263"
        },
        {
            "source": "r_432",
            "target": "s_1537"
        },
        {
            "source": "r_432",
            "target": "s_428"
        },
        {
            "source": "r_433",
            "target": "s_1193"
        },
        {
            "source": "r_433",
            "target": "s_798"
        },
        {
            "source": "r_433",
            "target": "s_220"
        },
        {
            "source": "r_433",
            "target": "s_1438"
        },
        {
            "source": "r_434",
            "target": "s_250"
        },
        {
            "source": "r_434",
            "target": "s_148"
        },
        {
            "source": "r_434",
            "target": "s_282"
        },
        {
            "source": "r_434",
            "target": "s_1085"
        },
        {
            "source": "r_434",
            "target": "s_348"
        },
        {
            "source": "r_435",
            "target": "s_36"
        },
        {
            "source": "r_435",
            "target": "s_468"
        },
        {
            "source": "r_435",
            "target": "s_962"
        },
        {
            "source": "r_435",
            "target": "s_781"
        },
        {
            "source": "r_436",
            "target": "s_1349"
        },
        {
            "source": "r_436",
            "target": "s_799"
        },
        {
            "source": "r_437",
            "target": "s_1196"
        },
        {
            "source": "r_437",
            "target": "s_1244"
        },
        {
            "source": "r_437",
            "target": "s_698"
        },
        {
            "source": "r_437",
            "target": "s_836"
        },
        {
            "source": "r_438",
            "target": "s_468"
        },
        {
            "source": "r_438",
            "target": "s_504"
        },
        {
            "source": "r_438",
            "target": "s_1200"
        },
        {
            "source": "r_438",
            "target": "s_718"
        },
        {
            "source": "r_438",
            "target": "s_976"
        },
        {
            "source": "r_439",
            "target": "s_86"
        },
        {
            "source": "r_439",
            "target": "s_1338"
        },
        {
            "source": "r_440",
            "target": "s_1453"
        },
        {
            "source": "r_440",
            "target": "s_69"
        },
        {
            "source": "r_440",
            "target": "s_169"
        },
        {
            "source": "r_440",
            "target": "s_787"
        },
        {
            "source": "r_440",
            "target": "s_1184"
        },
        {
            "source": "r_441",
            "target": "s_233"
        },
        {
            "source": "r_441",
            "target": "s_1160"
        },
        {
            "source": "r_441",
            "target": "s_789"
        },
        {
            "source": "r_442",
            "target": "s_172"
        },
        {
            "source": "r_442",
            "target": "s_1396"
        },
        {
            "source": "r_442",
            "target": "s_405"
        },
        {
            "source": "r_443",
            "target": "s_644"
        },
        {
            "source": "r_443",
            "target": "s_1484"
        },
        {
            "source": "r_443",
            "target": "s_0"
        },
        {
            "source": "r_443",
            "target": "s_1262"
        },
        {
            "source": "r_443",
            "target": "s_646"
        },
        {
            "source": "r_443",
            "target": "s_1104"
        },
        {
            "source": "r_444",
            "target": "s_1504"
        },
        {
            "source": "r_444",
            "target": "s_1344"
        },
        {
            "source": "r_444",
            "target": "s_199"
        },
        {
            "source": "r_444",
            "target": "s_743"
        },
        {
            "source": "r_444",
            "target": "s_393"
        },
        {
            "source": "r_445",
            "target": "s_468"
        },
        {
            "source": "r_445",
            "target": "s_504"
        },
        {
            "source": "r_445",
            "target": "s_976"
        },
        {
            "source": "r_446",
            "target": "s_1289"
        },
        {
            "source": "r_446",
            "target": "s_1323"
        },
        {
            "source": "r_446",
            "target": "s_1543"
        },
        {
            "source": "r_446",
            "target": "s_865"
        },
        {
            "source": "r_446",
            "target": "s_1280"
        },
        {
            "source": "r_447",
            "target": "s_607"
        },
        {
            "source": "r_447",
            "target": "s_413"
        },
        {
            "source": "r_447",
            "target": "s_193"
        },
        {
            "source": "r_447",
            "target": "s_1494"
        },
        {
            "source": "r_447",
            "target": "s_1191"
        },
        {
            "source": "r_447",
            "target": "s_1383"
        },
        {
            "source": "r_447",
            "target": "s_131"
        },
        {
            "source": "r_447",
            "target": "s_1031"
        },
        {
            "source": "r_448",
            "target": "s_760"
        },
        {
            "source": "r_448",
            "target": "s_1135"
        },
        {
            "source": "r_448",
            "target": "s_28"
        },
        {
            "source": "r_448",
            "target": "s_402"
        },
        {
            "source": "r_448",
            "target": "s_428"
        },
        {
            "source": "r_449",
            "target": "s_369"
        },
        {
            "source": "r_449",
            "target": "s_887"
        },
        {
            "source": "r_449",
            "target": "s_1075"
        },
        {
            "source": "r_449",
            "target": "s_1092"
        },
        {
            "source": "r_449",
            "target": "s_137"
        },
        {
            "source": "r_449",
            "target": "s_473"
        },
        {
            "source": "r_449",
            "target": "s_156"
        },
        {
            "source": "r_450",
            "target": "s_228"
        },
        {
            "source": "r_450",
            "target": "s_929"
        },
        {
            "source": "r_450",
            "target": "s_386"
        },
        {
            "source": "r_451",
            "target": "s_492"
        },
        {
            "source": "r_451",
            "target": "s_962"
        },
        {
            "source": "r_451",
            "target": "s_951"
        },
        {
            "source": "r_451",
            "target": "s_781"
        },
        {
            "source": "r_451",
            "target": "s_542"
        },
        {
            "source": "r_452",
            "target": "s_1579"
        },
        {
            "source": "r_452",
            "target": "s_1214"
        },
        {
            "source": "r_452",
            "target": "s_537"
        },
        {
            "source": "r_452",
            "target": "s_274"
        },
        {
            "source": "r_452",
            "target": "s_5"
        },
        {
            "source": "r_452",
            "target": "s_1305"
        },
        {
            "source": "r_453",
            "target": "s_322"
        },
        {
            "source": "r_453",
            "target": "s_1433"
        },
        {
            "source": "r_453",
            "target": "s_986"
        },
        {
            "source": "r_453",
            "target": "s_841"
        },
        {
            "source": "r_453",
            "target": "s_405"
        },
        {
            "source": "r_454",
            "target": "s_1483"
        },
        {
            "source": "r_454",
            "target": "s_707"
        },
        {
            "source": "r_454",
            "target": "s_1501"
        },
        {
            "source": "r_455",
            "target": "s_1554"
        },
        {
            "source": "r_455",
            "target": "s_243"
        },
        {
            "source": "r_455",
            "target": "s_560"
        },
        {
            "source": "r_455",
            "target": "s_1277"
        },
        {
            "source": "r_455",
            "target": "s_797"
        },
        {
            "source": "r_455",
            "target": "s_1364"
        },
        {
            "source": "r_455",
            "target": "s_633"
        },
        {
            "source": "r_455",
            "target": "s_885"
        },
        {
            "source": "r_455",
            "target": "s_31"
        },
        {
            "source": "r_455",
            "target": "s_417"
        },
        {
            "source": "r_456",
            "target": "s_804"
        },
        {
            "source": "r_456",
            "target": "s_22"
        },
        {
            "source": "r_456",
            "target": "s_479"
        },
        {
            "source": "r_456",
            "target": "s_1568"
        },
        {
            "source": "r_456",
            "target": "s_1582"
        },
        {
            "source": "r_456",
            "target": "s_1086"
        },
        {
            "source": "r_456",
            "target": "s_744"
        },
        {
            "source": "r_456",
            "target": "s_1015"
        },
        {
            "source": "r_456",
            "target": "s_1320"
        },
        {
            "source": "r_457",
            "target": "s_474"
        },
        {
            "source": "r_457",
            "target": "s_1547"
        },
        {
            "source": "r_457",
            "target": "s_1221"
        },
        {
            "source": "r_458",
            "target": "s_151"
        },
        {
            "source": "r_458",
            "target": "s_174"
        },
        {
            "source": "r_459",
            "target": "s_1453"
        },
        {
            "source": "r_459",
            "target": "s_1508"
        },
        {
            "source": "r_459",
            "target": "s_1025"
        },
        {
            "source": "r_459",
            "target": "s_1386"
        },
        {
            "source": "r_460",
            "target": "s_919"
        },
        {
            "source": "r_460",
            "target": "s_255"
        },
        {
            "source": "r_460",
            "target": "s_1077"
        },
        {
            "source": "r_460",
            "target": "s_286"
        },
        {
            "source": "r_460",
            "target": "s_1006"
        },
        {
            "source": "r_461",
            "target": "s_837"
        },
        {
            "source": "r_461",
            "target": "s_680"
        },
        {
            "source": "r_461",
            "target": "s_1333"
        },
        {
            "source": "r_461",
            "target": "s_1237"
        },
        {
            "source": "r_461",
            "target": "s_1033"
        },
        {
            "source": "r_461",
            "target": "s_580"
        },
        {
            "source": "r_462",
            "target": "s_149"
        },
        {
            "source": "r_462",
            "target": "s_935"
        },
        {
            "source": "r_462",
            "target": "s_157"
        },
        {
            "source": "r_462",
            "target": "s_989"
        },
        {
            "source": "r_462",
            "target": "s_106"
        },
        {
            "source": "r_462",
            "target": "s_219"
        },
        {
            "source": "r_463",
            "target": "s_1365"
        },
        {
            "source": "r_463",
            "target": "s_1462"
        },
        {
            "source": "r_463",
            "target": "s_1501"
        },
        {
            "source": "r_463",
            "target": "s_707"
        },
        {
            "source": "r_464",
            "target": "s_236"
        },
        {
            "source": "r_464",
            "target": "s_229"
        },
        {
            "source": "r_464",
            "target": "s_394"
        },
        {
            "source": "r_465",
            "target": "s_875"
        },
        {
            "source": "r_465",
            "target": "s_967"
        },
        {
            "source": "r_465",
            "target": "s_70"
        },
        {
            "source": "r_465",
            "target": "s_1355"
        },
        {
            "source": "r_465",
            "target": "s_132"
        },
        {
            "source": "r_466",
            "target": "s_750"
        },
        {
            "source": "r_466",
            "target": "s_1000"
        },
        {
            "source": "r_467",
            "target": "s_768"
        },
        {
            "source": "r_467",
            "target": "s_1563"
        },
        {
            "source": "r_467",
            "target": "s_259"
        },
        {
            "source": "r_467",
            "target": "s_21"
        },
        {
            "source": "r_467",
            "target": "s_1292"
        },
        {
            "source": "r_467",
            "target": "s_1295"
        },
        {
            "source": "r_467",
            "target": "s_1237"
        },
        {
            "source": "r_468",
            "target": "s_1473"
        },
        {
            "source": "r_468",
            "target": "s_553"
        },
        {
            "source": "r_468",
            "target": "s_438"
        },
        {
            "source": "r_468",
            "target": "s_405"
        },
        {
            "source": "r_469",
            "target": "s_183"
        },
        {
            "source": "r_469",
            "target": "s_795"
        },
        {
            "source": "r_469",
            "target": "s_58"
        },
        {
            "source": "r_469",
            "target": "s_280"
        },
        {
            "source": "r_469",
            "target": "s_1420"
        },
        {
            "source": "r_469",
            "target": "s_883"
        },
        {
            "source": "r_469",
            "target": "s_1199"
        },
        {
            "source": "r_470",
            "target": "s_415"
        },
        {
            "source": "r_470",
            "target": "s_1270"
        },
        {
            "source": "r_470",
            "target": "s_836"
        },
        {
            "source": "r_470",
            "target": "s_435"
        },
        {
            "source": "r_470",
            "target": "s_1581"
        },
        {
            "source": "r_470",
            "target": "s_986"
        },
        {
            "source": "r_471",
            "target": "s_143"
        },
        {
            "source": "r_471",
            "target": "s_426"
        },
        {
            "source": "r_471",
            "target": "s_198"
        },
        {
            "source": "r_472",
            "target": "s_554"
        },
        {
            "source": "r_472",
            "target": "s_182"
        },
        {
            "source": "r_472",
            "target": "s_864"
        },
        {
            "source": "r_472",
            "target": "s_1124"
        },
        {
            "source": "r_473",
            "target": "s_511"
        },
        {
            "source": "r_473",
            "target": "s_719"
        },
        {
            "source": "r_473",
            "target": "s_188"
        },
        {
            "source": "r_473",
            "target": "s_1221"
        },
        {
            "source": "r_474",
            "target": "s_1495"
        },
        {
            "source": "r_474",
            "target": "s_156"
        },
        {
            "source": "r_475",
            "target": "s_544"
        },
        {
            "source": "r_475",
            "target": "s_416"
        },
        {
            "source": "r_475",
            "target": "s_1486"
        },
        {
            "source": "r_475",
            "target": "s_826"
        },
        {
            "source": "r_476",
            "target": "s_1390"
        },
        {
            "source": "r_476",
            "target": "s_156"
        },
        {
            "source": "r_476",
            "target": "s_1115"
        },
        {
            "source": "r_477",
            "target": "s_591"
        },
        {
            "source": "r_477",
            "target": "s_266"
        },
        {
            "source": "r_477",
            "target": "s_1154"
        },
        {
            "source": "r_477",
            "target": "s_990"
        },
        {
            "source": "r_478",
            "target": "s_668"
        },
        {
            "source": "r_478",
            "target": "s_1070"
        },
        {
            "source": "r_478",
            "target": "s_991"
        },
        {
            "source": "r_478",
            "target": "s_1285"
        },
        {
            "source": "r_478",
            "target": "s_1516"
        },
        {
            "source": "r_478",
            "target": "s_735"
        },
        {
            "source": "r_478",
            "target": "s_1415"
        },
        {
            "source": "r_479",
            "target": "s_503"
        },
        {
            "source": "r_479",
            "target": "s_1323"
        },
        {
            "source": "r_479",
            "target": "s_1013"
        },
        {
            "source": "r_479",
            "target": "s_1354"
        },
        {
            "source": "r_479",
            "target": "s_66"
        },
        {
            "source": "r_480",
            "target": "s_832"
        },
        {
            "source": "r_480",
            "target": "s_321"
        },
        {
            "source": "r_480",
            "target": "s_6"
        },
        {
            "source": "r_481",
            "target": "s_1019"
        },
        {
            "source": "r_481",
            "target": "s_660"
        },
        {
            "source": "r_481",
            "target": "s_414"
        },
        {
            "source": "r_481",
            "target": "s_513"
        },
        {
            "source": "r_481",
            "target": "s_549"
        },
        {
            "source": "r_482",
            "target": "s_1161"
        },
        {
            "source": "r_482",
            "target": "s_1418"
        },
        {
            "source": "r_482",
            "target": "s_1488"
        },
        {
            "source": "r_483",
            "target": "s_691"
        },
        {
            "source": "r_483",
            "target": "s_988"
        },
        {
            "source": "r_483",
            "target": "s_49"
        },
        {
            "source": "r_483",
            "target": "s_1303"
        },
        {
            "source": "r_484",
            "target": "s_323"
        },
        {
            "source": "r_484",
            "target": "s_1353"
        },
        {
            "source": "r_484",
            "target": "s_873"
        },
        {
            "source": "r_484",
            "target": "s_836"
        },
        {
            "source": "r_484",
            "target": "s_1041"
        },
        {
            "source": "r_485",
            "target": "s_374"
        },
        {
            "source": "r_485",
            "target": "s_164"
        },
        {
            "source": "r_485",
            "target": "s_1269"
        },
        {
            "source": "r_485",
            "target": "s_1183"
        },
        {
            "source": "r_485",
            "target": "s_1123"
        },
        {
            "source": "r_485",
            "target": "s_736"
        },
        {
            "source": "r_485",
            "target": "s_1210"
        },
        {
            "source": "r_486",
            "target": "s_1034"
        },
        {
            "source": "r_486",
            "target": "s_189"
        },
        {
            "source": "r_486",
            "target": "s_397"
        },
        {
            "source": "r_486",
            "target": "s_182"
        },
        {
            "source": "r_486",
            "target": "s_1124"
        },
        {
            "source": "r_487",
            "target": "s_669"
        },
        {
            "source": "r_487",
            "target": "s_1588"
        },
        {
            "source": "r_487",
            "target": "s_1124"
        },
        {
            "source": "r_487",
            "target": "s_1323"
        },
        {
            "source": "r_488",
            "target": "s_1293"
        },
        {
            "source": "r_488",
            "target": "s_1368"
        },
        {
            "source": "r_488",
            "target": "s_1386"
        },
        {
            "source": "r_489",
            "target": "s_1018"
        },
        {
            "source": "r_489",
            "target": "s_1084"
        },
        {
            "source": "r_490",
            "target": "s_780"
        },
        {
            "source": "r_490",
            "target": "s_1417"
        },
        {
            "source": "r_490",
            "target": "s_733"
        },
        {
            "source": "r_490",
            "target": "s_108"
        },
        {
            "source": "r_490",
            "target": "s_1033"
        },
        {
            "source": "r_490",
            "target": "s_692"
        },
        {
            "source": "r_491",
            "target": "s_59"
        },
        {
            "source": "r_491",
            "target": "s_262"
        },
        {
            "source": "r_491",
            "target": "s_1195"
        },
        {
            "source": "r_491",
            "target": "s_90"
        },
        {
            "source": "r_492",
            "target": "s_436"
        },
        {
            "source": "r_492",
            "target": "s_776"
        },
        {
            "source": "r_492",
            "target": "s_330"
        },
        {
            "source": "r_492",
            "target": "s_1100"
        },
        {
            "source": "r_492",
            "target": "s_286"
        },
        {
            "source": "r_493",
            "target": "s_831"
        },
        {
            "source": "r_493",
            "target": "s_1516"
        },
        {
            "source": "r_493",
            "target": "s_1221"
        },
        {
            "source": "r_493",
            "target": "s_1547"
        },
        {
            "source": "r_494",
            "target": "s_1058"
        },
        {
            "source": "r_494",
            "target": "s_1482"
        },
        {
            "source": "r_494",
            "target": "s_594"
        },
        {
            "source": "r_494",
            "target": "s_145"
        },
        {
            "source": "r_494",
            "target": "s_607"
        },
        {
            "source": "r_494",
            "target": "s_1031"
        },
        {
            "source": "r_495",
            "target": "s_921"
        },
        {
            "source": "r_495",
            "target": "s_1564"
        },
        {
            "source": "r_495",
            "target": "s_530"
        },
        {
            "source": "r_495",
            "target": "s_1376"
        },
        {
            "source": "r_495",
            "target": "s_632"
        },
        {
            "source": "r_495",
            "target": "s_578"
        },
        {
            "source": "r_496",
            "target": "s_483"
        },
        {
            "source": "r_496",
            "target": "s_88"
        },
        {
            "source": "r_496",
            "target": "s_650"
        },
        {
            "source": "r_496",
            "target": "s_469"
        },
        {
            "source": "r_496",
            "target": "s_1354"
        },
        {
            "source": "r_497",
            "target": "s_1314"
        },
        {
            "source": "r_497",
            "target": "s_366"
        },
        {
            "source": "r_497",
            "target": "s_179"
        },
        {
            "source": "r_497",
            "target": "s_267"
        },
        {
            "source": "r_498",
            "target": "s_1531"
        },
        {
            "source": "r_498",
            "target": "s_154"
        },
        {
            "source": "r_498",
            "target": "s_394"
        },
        {
            "source": "r_499",
            "target": "s_1281"
        },
        {
            "source": "r_499",
            "target": "s_1023"
        },
        {
            "source": "r_499",
            "target": "s_49"
        },
        {
            "source": "r_499",
            "target": "s_988"
        },
        {
            "source": "r_499",
            "target": "s_1237"
        },
        {
            "source": "r_500",
            "target": "s_1402"
        },
        {
            "source": "r_500",
            "target": "s_1266"
        },
        {
            "source": "r_500",
            "target": "s_260"
        },
        {
            "source": "r_500",
            "target": "s_478"
        },
        {
            "source": "r_500",
            "target": "s_393"
        },
        {
            "source": "r_501",
            "target": "s_1185"
        },
        {
            "source": "r_501",
            "target": "s_908"
        },
        {
            "source": "r_501",
            "target": "s_307"
        },
        {
            "source": "r_502",
            "target": "s_1318"
        },
        {
            "source": "r_502",
            "target": "s_1031"
        },
        {
            "source": "r_503",
            "target": "s_130"
        },
        {
            "source": "r_503",
            "target": "s_403"
        },
        {
            "source": "r_503",
            "target": "s_494"
        },
        {
            "source": "r_503",
            "target": "s_920"
        },
        {
            "source": "r_504",
            "target": "s_400"
        },
        {
            "source": "r_504",
            "target": "s_1574"
        },
        {
            "source": "r_504",
            "target": "s_643"
        },
        {
            "source": "r_504",
            "target": "s_1147"
        },
        {
            "source": "r_504",
            "target": "s_1065"
        },
        {
            "source": "r_504",
            "target": "s_647"
        },
        {
            "source": "r_504",
            "target": "s_1415"
        },
        {
            "source": "r_505",
            "target": "s_952"
        },
        {
            "source": "r_505",
            "target": "s_1319"
        },
        {
            "source": "r_505",
            "target": "s_649"
        },
        {
            "source": "r_505",
            "target": "s_235"
        },
        {
            "source": "r_505",
            "target": "s_1430"
        },
        {
            "source": "r_505",
            "target": "s_874"
        },
        {
            "source": "r_505",
            "target": "s_1227"
        },
        {
            "source": "r_506",
            "target": "s_191"
        },
        {
            "source": "r_506",
            "target": "s_866"
        },
        {
            "source": "r_506",
            "target": "s_459"
        },
        {
            "source": "r_506",
            "target": "s_1556"
        },
        {
            "source": "r_506",
            "target": "s_41"
        },
        {
            "source": "r_507",
            "target": "s_104"
        },
        {
            "source": "r_507",
            "target": "s_1574"
        },
        {
            "source": "r_507",
            "target": "s_144"
        },
        {
            "source": "r_507",
            "target": "s_498"
        },
        {
            "source": "r_507",
            "target": "s_682"
        },
        {
            "source": "r_507",
            "target": "s_248"
        },
        {
            "source": "r_507",
            "target": "s_647"
        },
        {
            "source": "r_507",
            "target": "s_1415"
        },
        {
            "source": "r_508",
            "target": "s_160"
        },
        {
            "source": "r_508",
            "target": "s_945"
        },
        {
            "source": "r_508",
            "target": "s_612"
        },
        {
            "source": "r_508",
            "target": "s_215"
        },
        {
            "source": "r_509",
            "target": "s_1516"
        },
        {
            "source": "r_509",
            "target": "s_318"
        },
        {
            "source": "r_509",
            "target": "s_765"
        },
        {
            "source": "r_509",
            "target": "s_198"
        },
        {
            "source": "r_509",
            "target": "s_1221"
        },
        {
            "source": "r_510",
            "target": "s_317"
        },
        {
            "source": "r_510",
            "target": "s_1403"
        },
        {
            "source": "r_510",
            "target": "s_1278"
        },
        {
            "source": "r_510",
            "target": "s_1391"
        },
        {
            "source": "r_510",
            "target": "s_73"
        },
        {
            "source": "r_510",
            "target": "s_238"
        },
        {
            "source": "r_511",
            "target": "s_338"
        },
        {
            "source": "r_511",
            "target": "s_619"
        },
        {
            "source": "r_511",
            "target": "s_215"
        },
        {
            "source": "r_511",
            "target": "s_580"
        },
        {
            "source": "r_512",
            "target": "s_1066"
        },
        {
            "source": "r_512",
            "target": "s_759"
        },
        {
            "source": "r_512",
            "target": "s_1080"
        },
        {
            "source": "r_513",
            "target": "s_1424"
        },
        {
            "source": "r_513",
            "target": "s_1470"
        },
        {
            "source": "r_513",
            "target": "s_1179"
        },
        {
            "source": "r_513",
            "target": "s_456"
        },
        {
            "source": "r_513",
            "target": "s_763"
        },
        {
            "source": "r_513",
            "target": "s_626"
        },
        {
            "source": "r_513",
            "target": "s_46"
        },
        {
            "source": "r_513",
            "target": "s_1415"
        },
        {
            "source": "r_514",
            "target": "s_719"
        },
        {
            "source": "r_514",
            "target": "s_150"
        },
        {
            "source": "r_514",
            "target": "s_1517"
        },
        {
            "source": "r_515",
            "target": "s_460"
        },
        {
            "source": "r_515",
            "target": "s_545"
        },
        {
            "source": "r_515",
            "target": "s_347"
        },
        {
            "source": "r_516",
            "target": "s_220"
        },
        {
            "source": "r_516",
            "target": "s_818"
        },
        {
            "source": "r_516",
            "target": "s_1193"
        },
        {
            "source": "r_516",
            "target": "s_1438"
        },
        {
            "source": "r_517",
            "target": "s_1024"
        },
        {
            "source": "r_517",
            "target": "s_850"
        },
        {
            "source": "r_518",
            "target": "s_1341"
        },
        {
            "source": "r_518",
            "target": "s_1089"
        },
        {
            "source": "r_518",
            "target": "s_437"
        },
        {
            "source": "r_518",
            "target": "s_1569"
        },
        {
            "source": "r_518",
            "target": "s_1128"
        },
        {
            "source": "r_518",
            "target": "s_1415"
        },
        {
            "source": "r_519",
            "target": "s_568"
        },
        {
            "source": "r_519",
            "target": "s_1306"
        },
        {
            "source": "r_519",
            "target": "s_1471"
        },
        {
            "source": "r_519",
            "target": "s_527"
        },
        {
            "source": "r_519",
            "target": "s_683"
        },
        {
            "source": "r_519",
            "target": "s_1614"
        },
        {
            "source": "r_519",
            "target": "s_1046"
        },
        {
            "source": "r_519",
            "target": "s_757"
        },
        {
            "source": "r_519",
            "target": "s_1260"
        },
        {
            "source": "r_519",
            "target": "s_360"
        },
        {
            "source": "r_520",
            "target": "s_1027"
        },
        {
            "source": "r_520",
            "target": "s_430"
        },
        {
            "source": "r_520",
            "target": "s_891"
        },
        {
            "source": "r_520",
            "target": "s_1599"
        },
        {
            "source": "r_521",
            "target": "s_995"
        },
        {
            "source": "r_521",
            "target": "s_971"
        },
        {
            "source": "r_521",
            "target": "s_1110"
        },
        {
            "source": "r_521",
            "target": "s_48"
        },
        {
            "source": "r_522",
            "target": "s_62"
        },
        {
            "source": "r_522",
            "target": "s_1010"
        },
        {
            "source": "r_522",
            "target": "s_928"
        },
        {
            "source": "r_522",
            "target": "s_1574"
        },
        {
            "source": "r_522",
            "target": "s_258"
        },
        {
            "source": "r_522",
            "target": "s_737"
        },
        {
            "source": "r_522",
            "target": "s_404"
        },
        {
            "source": "r_522",
            "target": "s_1155"
        },
        {
            "source": "r_522",
            "target": "s_1415"
        },
        {
            "source": "r_523",
            "target": "s_711"
        },
        {
            "source": "r_523",
            "target": "s_1057"
        },
        {
            "source": "r_523",
            "target": "s_993"
        },
        {
            "source": "r_523",
            "target": "s_688"
        },
        {
            "source": "r_523",
            "target": "s_1124"
        },
        {
            "source": "r_524",
            "target": "s_568"
        },
        {
            "source": "r_524",
            "target": "s_1471"
        },
        {
            "source": "r_524",
            "target": "s_922"
        },
        {
            "source": "r_524",
            "target": "s_1392"
        },
        {
            "source": "r_524",
            "target": "s_96"
        },
        {
            "source": "r_524",
            "target": "s_360"
        },
        {
            "source": "r_525",
            "target": "s_59"
        },
        {
            "source": "r_525",
            "target": "s_90"
        },
        {
            "source": "r_525",
            "target": "s_973"
        },
        {
            "source": "r_525",
            "target": "s_860"
        },
        {
            "source": "r_525",
            "target": "s_1530"
        },
        {
            "source": "r_525",
            "target": "s_692"
        },
        {
            "source": "r_525",
            "target": "s_1195"
        },
        {
            "source": "r_526",
            "target": "s_1357"
        },
        {
            "source": "r_526",
            "target": "s_454"
        },
        {
            "source": "r_526",
            "target": "s_92"
        },
        {
            "source": "r_526",
            "target": "s_298"
        },
        {
            "source": "r_527",
            "target": "s_488"
        },
        {
            "source": "r_527",
            "target": "s_1386"
        },
        {
            "source": "r_528",
            "target": "s_1495"
        },
        {
            "source": "r_528",
            "target": "s_1241"
        },
        {
            "source": "r_528",
            "target": "s_1311"
        },
        {
            "source": "r_528",
            "target": "s_156"
        },
        {
            "source": "r_529",
            "target": "s_895"
        },
        {
            "source": "r_529",
            "target": "s_869"
        },
        {
            "source": "r_529",
            "target": "s_789"
        },
        {
            "source": "r_530",
            "target": "s_1588"
        },
        {
            "source": "r_530",
            "target": "s_1448"
        },
        {
            "source": "r_530",
            "target": "s_1124"
        },
        {
            "source": "r_530",
            "target": "s_1323"
        },
        {
            "source": "r_531",
            "target": "s_201"
        },
        {
            "source": "r_531",
            "target": "s_1118"
        },
        {
            "source": "r_531",
            "target": "s_1100"
        },
        {
            "source": "r_531",
            "target": "s_1580"
        },
        {
            "source": "r_532",
            "target": "s_383"
        },
        {
            "source": "r_532",
            "target": "s_338"
        },
        {
            "source": "r_532",
            "target": "s_580"
        },
        {
            "source": "r_532",
            "target": "s_680"
        },
        {
            "source": "r_533",
            "target": "s_422"
        },
        {
            "source": "r_533",
            "target": "s_1224"
        },
        {
            "source": "r_533",
            "target": "s_846"
        },
        {
            "source": "r_533",
            "target": "s_281"
        },
        {
            "source": "r_533",
            "target": "s_561"
        },
        {
            "source": "r_533",
            "target": "s_346"
        },
        {
            "source": "r_534",
            "target": "s_1561"
        },
        {
            "source": "r_534",
            "target": "s_491"
        },
        {
            "source": "r_534",
            "target": "s_1342"
        },
        {
            "source": "r_534",
            "target": "s_1428"
        },
        {
            "source": "r_534",
            "target": "s_25"
        },
        {
            "source": "r_534",
            "target": "s_170"
        },
        {
            "source": "r_534",
            "target": "s_206"
        },
        {
            "source": "r_534",
            "target": "s_476"
        },
        {
            "source": "r_535",
            "target": "s_972"
        },
        {
            "source": "r_535",
            "target": "s_969"
        },
        {
            "source": "r_535",
            "target": "s_1282"
        },
        {
            "source": "r_535",
            "target": "s_1490"
        },
        {
            "source": "r_536",
            "target": "s_692"
        },
        {
            "source": "r_536",
            "target": "s_140"
        },
        {
            "source": "r_536",
            "target": "s_1195"
        },
        {
            "source": "r_537",
            "target": "s_1500"
        },
        {
            "source": "r_537",
            "target": "s_1293"
        },
        {
            "source": "r_537",
            "target": "s_324"
        },
        {
            "source": "r_538",
            "target": "s_1606"
        },
        {
            "source": "r_538",
            "target": "s_146"
        },
        {
            "source": "r_538",
            "target": "s_510"
        },
        {
            "source": "r_539",
            "target": "s_1617"
        },
        {
            "source": "r_539",
            "target": "s_1188"
        },
        {
            "source": "r_539",
            "target": "s_1497"
        },
        {
            "source": "r_539",
            "target": "s_2"
        },
        {
            "source": "r_539",
            "target": "s_542"
        },
        {
            "source": "r_540",
            "target": "s_605"
        },
        {
            "source": "r_540",
            "target": "s_942"
        },
        {
            "source": "r_540",
            "target": "s_533"
        },
        {
            "source": "r_541",
            "target": "s_1334"
        },
        {
            "source": "r_541",
            "target": "s_114"
        },
        {
            "source": "r_541",
            "target": "s_274"
        },
        {
            "source": "r_542",
            "target": "s_1615"
        },
        {
            "source": "r_542",
            "target": "s_882"
        },
        {
            "source": "r_542",
            "target": "s_1095"
        },
        {
            "source": "r_542",
            "target": "s_141"
        },
        {
            "source": "r_542",
            "target": "s_1474"
        },
        {
            "source": "r_543",
            "target": "s_1550"
        },
        {
            "source": "r_543",
            "target": "s_936"
        },
        {
            "source": "r_543",
            "target": "s_722"
        },
        {
            "source": "r_543",
            "target": "s_1598"
        },
        {
            "source": "r_544",
            "target": "s_1165"
        },
        {
            "source": "r_544",
            "target": "s_898"
        },
        {
            "source": "r_544",
            "target": "s_1195"
        },
        {
            "source": "r_545",
            "target": "s_1116"
        },
        {
            "source": "r_545",
            "target": "s_842"
        },
        {
            "source": "r_546",
            "target": "s_35"
        },
        {
            "source": "r_546",
            "target": "s_896"
        },
        {
            "source": "r_546",
            "target": "s_279"
        },
        {
            "source": "r_546",
            "target": "s_1041"
        },
        {
            "source": "r_546",
            "target": "s_1068"
        },
        {
            "source": "r_547",
            "target": "s_400"
        },
        {
            "source": "r_547",
            "target": "s_1574"
        },
        {
            "source": "r_547",
            "target": "s_678"
        },
        {
            "source": "r_547",
            "target": "s_647"
        },
        {
            "source": "r_547",
            "target": "s_79"
        },
        {
            "source": "r_547",
            "target": "s_1415"
        },
        {
            "source": "r_548",
            "target": "s_788"
        },
        {
            "source": "r_548",
            "target": "s_1237"
        },
        {
            "source": "r_549",
            "target": "s_308"
        },
        {
            "source": "r_549",
            "target": "s_1197"
        },
        {
            "source": "r_549",
            "target": "s_848"
        },
        {
            "source": "r_549",
            "target": "s_34"
        },
        {
            "source": "r_549",
            "target": "s_518"
        },
        {
            "source": "r_549",
            "target": "s_1072"
        },
        {
            "source": "r_550",
            "target": "s_479"
        },
        {
            "source": "r_550",
            "target": "s_562"
        },
        {
            "source": "r_550",
            "target": "s_1086"
        },
        {
            "source": "r_550",
            "target": "s_17"
        },
        {
            "source": "r_550",
            "target": "s_1320"
        },
        {
            "source": "r_550",
            "target": "s_1015"
        },
        {
            "source": "r_551",
            "target": "s_151"
        },
        {
            "source": "r_551",
            "target": "s_1442"
        },
        {
            "source": "r_551",
            "target": "s_299"
        },
        {
            "source": "r_551",
            "target": "s_274"
        },
        {
            "source": "r_551",
            "target": "s_1345"
        },
        {
            "source": "r_551",
            "target": "s_1146"
        },
        {
            "source": "r_552",
            "target": "s_730"
        },
        {
            "source": "r_552",
            "target": "s_1102"
        },
        {
            "source": "r_552",
            "target": "s_651"
        },
        {
            "source": "r_552",
            "target": "s_217"
        },
        {
            "source": "r_552",
            "target": "s_1394"
        },
        {
            "source": "r_552",
            "target": "s_1556"
        },
        {
            "source": "r_552",
            "target": "s_672"
        },
        {
            "source": "r_553",
            "target": "s_853"
        },
        {
            "source": "r_553",
            "target": "s_980"
        },
        {
            "source": "r_553",
            "target": "s_946"
        },
        {
            "source": "r_553",
            "target": "s_614"
        },
        {
            "source": "r_553",
            "target": "s_540"
        },
        {
            "source": "r_554",
            "target": "s_701"
        },
        {
            "source": "r_554",
            "target": "s_622"
        },
        {
            "source": "r_554",
            "target": "s_117"
        },
        {
            "source": "r_554",
            "target": "s_810"
        },
        {
            "source": "r_554",
            "target": "s_1302"
        },
        {
            "source": "r_554",
            "target": "s_157"
        },
        {
            "source": "r_555",
            "target": "s_381"
        },
        {
            "source": "r_555",
            "target": "s_341"
        },
        {
            "source": "r_555",
            "target": "s_1526"
        },
        {
            "source": "r_555",
            "target": "s_1255"
        },
        {
            "source": "r_555",
            "target": "s_1455"
        },
        {
            "source": "r_555",
            "target": "s_1231"
        },
        {
            "source": "r_555",
            "target": "s_559"
        },
        {
            "source": "r_555",
            "target": "s_129"
        },
        {
            "source": "r_555",
            "target": "s_878"
        },
        {
            "source": "r_555",
            "target": "s_1252"
        },
        {
            "source": "r_556",
            "target": "s_265"
        },
        {
            "source": "r_556",
            "target": "s_1541"
        },
        {
            "source": "r_556",
            "target": "s_396"
        },
        {
            "source": "r_556",
            "target": "s_1208"
        },
        {
            "source": "r_556",
            "target": "s_277"
        }
    ]
}