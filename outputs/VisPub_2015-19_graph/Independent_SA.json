{
    "directed": false,
    "multigraph": false,
    "graph": {
        "name": "VisPub_2015-19_graph"
    },
    "nodes": [
        {
            "title": "Vega-Lite: A Grammar of Interactive Graphics",
            "data": "We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599030",
            "id": "r_0",
            "s_ids": [
                "s_795",
                "s_554",
                "s_1437",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.36529340380169784,
            "y": 0.1802646243736515
        },
        {
            "title": "Towards Better Analysis of Deep Convolutional Neural Networks",
            "data": "Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598831",
            "id": "r_1",
            "s_ids": [
                "s_476",
                "s_1120",
                "s_812",
                "s_1196",
                "s_1436",
                "s_301"
            ],
            "type": "rich",
            "x": -0.5090370196655368,
            "y": -0.3668221576002531
        },
        {
            "title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations",
            "data": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467191",
            "id": "r_2",
            "s_ids": [
                "s_1437",
                "s_554",
                "s_1291",
                "s_559",
                "s_1175",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.607115437924811,
            "y": 0.021466285975268583
        },
        {
            "title": "Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow",
            "data": "We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744878",
            "id": "r_3",
            "s_ids": [
                "s_1437",
                "s_990",
                "s_694",
                "s_1426",
                "s_31",
                "s_365",
                "s_432",
                "s_1212",
                "s_159"
            ],
            "type": "rich",
            "x": -0.022562521391748226,
            "y": -0.08997225172806492
        },
        {
            "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models",
            "data": "While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744718",
            "id": "r_4",
            "s_ids": [
                "s_1412",
                "s_755",
                "s_796",
                "s_569"
            ],
            "type": "rich",
            "x": -0.34943540728262823,
            "y": -0.5832570324342956
        },
        {
            "title": "LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks",
            "data": "Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744158",
            "id": "r_5",
            "s_ids": [
                "s_869",
                "s_778",
                "s_1346",
                "s_445"
            ],
            "type": "rich",
            "x": -0.1734634815415873,
            "y": -0.38087216199206014
        },
        {
            "title": "Beyond Memorability: Visualization Recognition and Recall",
            "data": "In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable \u201cat-a-glance\u201d are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467732",
            "id": "r_6",
            "s_ids": [
                "s_1069",
                "s_1383",
                "s_1263",
                "s_352",
                "s_482",
                "s_1063",
                "s_1346",
                "s_560"
            ],
            "type": "rich",
            "x": 0.6383887200858029,
            "y": -0.20241424976772732
        },
        {
            "title": "What Do We Talk About When We Talk About Dashboards?",
            "data": "Dashboards are one of the most common use cases for data visualization, and their design and contexts of use are considerably different from exploratory visualization tools. In this paper, we look at the broad scope of how dashboards are used in practice through an analysis of dashboard examples and documentation about their use. We systematically review the literature surrounding dashboard use, construct a design space for dashboards, and identify major dashboard types. We characterize dashboards by their design goals, levels of interaction, and the practices around them. Our framework and literature review suggest a number of fruitful research directions to better support dashboard design, implementation and use.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864903",
            "id": "r_7",
            "s_ids": [
                "s_39",
                "s_1215",
                "s_728",
                "s_1208",
                "s_1011"
            ],
            "type": "rich",
            "x": 0.6883502749186359,
            "y": -0.0348882347552471
        },
        {
            "title": "Visualizing the Hidden Activity of Artificial Neural Networks",
            "data": "In machine learning, pattern classification assigns high-dimensional vectors (observations) to classes based on generalization from examples. Artificial neural networks currently achieve state-of-the-art results in this task. Although such networks are typically used as black-boxes, they are also widely believed to learn (high-dimensional) higher-level representations of the original observations. In this paper, we propose using dimensionality reduction for two tasks: visualizing the relationships between learned representations of observations, and visualizing the relationships between artificial neurons. Through experiments conducted in three traditional image classification benchmark datasets, we show how visualization can provide highly valuable feedback for network designers. For instance, our discoveries in one of these datasets (SVHN) include the presence of interpretable clusters of learned representations, and the partitioning of artificial neurons into groups with apparently related discriminative roles.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598838",
            "id": "r_8",
            "s_ids": [
                "s_680",
                "s_892",
                "s_440",
                "s_133"
            ],
            "type": "rich",
            "x": -0.42501417130094127,
            "y": -0.2752076086391487
        },
        {
            "title": "RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records",
            "data": "We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865027",
            "id": "r_9",
            "s_ids": [
                "s_1181",
                "s_429",
                "s_1026",
                "s_1372",
                "s_938",
                "s_1365",
                "s_109",
                "s_320"
            ],
            "type": "rich",
            "x": -0.19912505054801746,
            "y": -0.5516678705376377
        },
        {
            "title": "Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization",
            "data": "We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467091",
            "id": "r_10",
            "s_ids": [
                "s_795",
                "s_625",
                "s_56",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.44649442740384565,
            "y": 0.38581964882686004
        },
        {
            "title": "The Role of Uncertainty, Awareness, and Trust in Visual Analytics",
            "data": "Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467591",
            "id": "r_11",
            "s_ids": [
                "s_905",
                "s_1086",
                "s_1181",
                "s_698",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.2499488488262087,
            "y": -0.28560490248980036
        },
        {
            "title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco",
            "data": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865240",
            "id": "r_12",
            "s_ids": [
                "s_554",
                "s_1209",
                "s_1234",
                "s_222",
                "s_474",
                "s_1175",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.6330709970870716,
            "y": 0.05395219481858794
        },
        {
            "title": "Embedded Data Representations",
            "data": "We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents - the real-world entities and spaces to which data corresponds - and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598608",
            "id": "r_13",
            "s_ids": [
                "s_339",
                "s_1347",
                "s_231"
            ],
            "type": "rich",
            "x": 0.4827736039531038,
            "y": 0.21988053720214443
        },
        {
            "title": "RuleMatrix: Visualizing and Understanding Classifiers with Rules",
            "data": "With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864812",
            "id": "r_14",
            "s_ids": [
                "s_1374",
                "s_137",
                "s_457"
            ],
            "type": "rich",
            "x": -0.07859066659638989,
            "y": -0.5076212825881564
        },
        {
            "title": "The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?",
            "data": "We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user's real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745941",
            "id": "r_15",
            "s_ids": [
                "s_1126",
                "s_280",
                "s_603",
                "s_1204",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.1713974366542179,
            "y": 0.06905275050124685
        },
        {
            "title": "Characterizing Guidance in Visual Analytics",
            "data": "Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598468",
            "id": "r_16",
            "s_ids": [
                "s_780",
                "s_181",
                "s_1168",
                "s_1087",
                "s_1154",
                "s_495",
                "s_372"
            ],
            "type": "rich",
            "x": 0.2775408735171294,
            "y": -0.3626502548533485
        },
        {
            "title": "Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis",
            "data": "Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a \u201chuman in the loop\u201d process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598495",
            "id": "r_17",
            "s_ids": [
                "s_905",
                "s_120",
                "s_781",
                "s_510",
                "s_505",
                "s_138",
                "s_1136",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.12347155873109397,
            "y": -0.08030900150819303
        },
        {
            "title": "MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering",
            "data": "Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468111",
            "id": "r_18",
            "s_ids": [
                "s_240",
                "s_1459",
                "s_839",
                "s_9",
                "s_1149",
                "s_512"
            ],
            "type": "rich",
            "x": 0.18443553748958977,
            "y": 0.527625127329673
        },
        {
            "title": "Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes",
            "data": "While the primary goal of visual analytics research is to improve the quality of insights and findings, a substantial amount of research in provenance has focused on the history of changes and advances throughout the analysis process. The term, provenance, has been used in a variety of ways to describe different types of records and histories related to visualization. The existing body of provenance research has grown to a point where the consolidation of design knowledge requires cross-referencing a variety of projects and studies spanning multiple domain areas. We present an organizational framework of the different types of provenance information and purposes for why they are desired in the field of visual analytics. Our organization is intended to serve as a framework to help researchers specify types of provenance and coordinate design knowledge across projects. We also discuss the relationships between these factors and the methods used to capture provenance information. In addition, our organization can be used to guide the selection of evaluation methodology and the comparison of study outcomes in provenance research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467551",
            "id": "r_19",
            "s_ids": [
                "s_356",
                "s_1469",
                "s_842",
                "s_1128"
            ],
            "type": "rich",
            "x": 0.579278482204243,
            "y": -0.3094026433175443
        },
        {
            "title": "SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations",
            "data": "The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598432",
            "id": "r_20",
            "s_ids": [
                "s_1067",
                "s_638",
                "s_1050",
                "s_1082",
                "s_98",
                "s_137",
                "s_1273"
            ],
            "type": "rich",
            "x": 0.273802468174224,
            "y": 0.25192917771909007
        },
        {
            "title": "Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers",
            "data": "Performance analysis is critical in applied machine learning because it influences the models practitioners produce. Current performance analysis tools suffer from issues including obscuring important characteristics of model behavior and dissociating performance from data. In this work, we present Squares, a performance visualization for multiclass classification problems. Squares supports estimating common performance metrics while displaying instance-level distribution information necessary for helping practitioners prioritize efforts and access data. Our controlled study shows that practitioners can assess performance significantly faster and more accurately with Squares than a confusion matrix, a common performance analysis tool in machine learning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598828",
            "id": "r_21",
            "s_ids": [
                "s_1163",
                "s_1332",
                "s_250",
                "s_935",
                "s_950"
            ],
            "type": "rich",
            "x": -0.03623105833088646,
            "y": -0.26795631058952213
        },
        {
            "title": "The What-If Tool: Interactive Probing of Machine Learning Models",
            "data": "A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934619",
            "id": "r_22",
            "s_ids": [
                "s_694",
                "s_480",
                "s_42",
                "s_159",
                "s_1212",
                "s_1426"
            ],
            "type": "rich",
            "x": 0.18692664189438557,
            "y": -0.39805987088198097
        },
        {
            "title": "Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?",
            "data": "High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs) such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving abstract data. We find significant differences between the two conditions in task completion time and the physical movements of the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599107",
            "id": "r_23",
            "s_ids": [
                "s_1204",
                "s_202",
                "s_751",
                "s_906",
                "s_1188",
                "s_184"
            ],
            "type": "rich",
            "x": 0.2356117817026649,
            "y": 0.09007760415897008
        },
        {
            "title": "TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centralities Using Taxi Trajectory Data",
            "data": "We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467771",
            "id": "r_24",
            "s_ids": [
                "s_274",
                "s_941",
                "s_1396",
                "s_745",
                "s_1253",
                "s_713"
            ],
            "type": "rich",
            "x": 0.1733602743328144,
            "y": 0.468666293500622
        },
        {
            "title": "The Topology ToolKit",
            "data": "This system paper presents the Topology ToolKit (TTK), a software platform designed for the topological analysis of scalar data in scientific visualization. While topological data analysis has gained in popularity over the last two decades, it has not yet been widely adopted as a standard data analysis tool for end users or developers. TTK aims at addressing this problem by providing a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK/C++) for fast prototyping or through direct, dependency-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code. online documentation and video tutorials are available on TTK's website [108].",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743938",
            "id": "r_25",
            "s_ids": [
                "s_611",
                "s_930",
                "s_1366",
                "s_729",
                "s_524"
            ],
            "type": "rich",
            "x": -0.497719040094224,
            "y": 0.13015830933363928
        },
        {
            "title": "Time Curves: Folding Time to Visualize Patterns of Temporal Evolution in Data",
            "data": "We introduce time curves as a general approach for visualizing patterns of evolution in temporal data. Examples of such patterns include slow and regular progressions, large sudden changes, and reversals to previous states. These patterns can be of interest in a range of domains, such as collaborative document editing, dynamic network analysis, and video analysis. Time curves employ the metaphor of folding a timeline visualization into itself so as to bring similar time points close to each other. This metaphor can be applied to any dataset where a similarity metric between temporal snapshots can be defined, thus it is largely datatype-agnostic. We illustrate how time curves can visually reveal informative patterns in a range of different datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467851",
            "id": "r_26",
            "s_ids": [
                "s_1126",
                "s_1367",
                "s_1402",
                "s_128",
                "s_97",
                "s_231"
            ],
            "type": "rich",
            "x": 0.1536761810715859,
            "y": 0.3782389402833964
        },
        {
            "title": "Considerations for Visualizing Comparison",
            "data": "Supporting comparison is a common and diverse challenge in visualization. Such support is difficult to design because solutions must address both the specifics of their scenario as well as the general issues of comparison. This paper aids designers by providing a strategy for considering those general issues. It presents four considerations that abstract comparison. These considerations identify issues and categorize solutions in a domain independent manner. The first considers how the common elements of comparison-a target set of items that are related and an action the user wants to perform on that relationship-are present in an analysis problem. The second considers why these elements lead to challenges because of their scale, in number of items, complexity of items, or complexity of relationship. The third considers what strategies address the identified scaling challenges, grouping solutions into three broad categories. The fourth considers which visual designs map to these strategies to provide solutions for a comparison analysis problem. In sequence, these considerations provide a process for developers to consider support for comparison in the design of visualization tools. Case studies show how these considerations can help in the design and evaluation of visualization solutions for comparison problems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744199",
            "id": "r_27",
            "s_ids": [
                "s_158"
            ],
            "type": "rich",
            "x": 0.24751504083550452,
            "y": -0.11912431747073053
        },
        {
            "title": "Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models",
            "data": "Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864499",
            "id": "r_28",
            "s_ids": [
                "s_65",
                "s_167",
                "s_176",
                "s_458",
                "s_921"
            ],
            "type": "rich",
            "x": -0.21563080652536698,
            "y": -0.4364958124732035
        },
        {
            "title": "DXR: A Toolkit for Building Immersive Data Visualizations",
            "data": "This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865152",
            "id": "r_29",
            "s_ids": [
                "s_280",
                "s_1427",
                "s_718",
                "s_1204",
                "s_388",
                "s_1126",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.5401798845601563,
            "y": 0.2957858489191525
        },
        {
            "title": "Understanding Hidden Memories of Recurrent Neural Networks",
            "data": "Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585721",
            "id": "r_30",
            "s_ids": [
                "s_1374",
                "s_1280",
                "s_283",
                "s_812",
                "s_411",
                "s_1474",
                "s_137"
            ],
            "type": "rich",
            "x": -0.013665746591842663,
            "y": -0.593203246997576
        },
        {
            "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations",
            "data": "Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934659",
            "id": "r_31",
            "s_ids": [
                "s_1005",
                "s_27",
                "s_511",
                "s_569"
            ],
            "type": "rich",
            "x": -0.40589591220027516,
            "y": -0.5821527497195289
        },
        {
            "title": "Bring It to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis",
            "data": "Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team. Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745181",
            "id": "r_32",
            "s_ids": [
                "s_676",
                "s_1484",
                "s_1211",
                "s_1342",
                "s_1360",
                "s_264",
                "s_493",
                "s_1149",
                "s_1321",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.3643214848509957,
            "y": 0.2587869224591985
        },
        {
            "title": "DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks",
            "data": "Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744358",
            "id": "r_33",
            "s_ids": [
                "s_966",
                "s_1157",
                "s_486",
                "s_1054",
                "s_145",
                "s_408"
            ],
            "type": "rich",
            "x": -0.4247376955097295,
            "y": -0.4549006954334899
        },
        {
            "title": "OSPRay - A CPU Ray Tracing Framework for Scientific Visualization",
            "data": "Scientific data is continually increasing in complexity, variety and size, making efficient visualization and specifically rendering an ongoing challenge. Traditional rasterization-based visualization approaches encounter performance and quality limitations, particularly in HPC environments without dedicated rendering hardware. In this paper, we present OSPRay, a turn-key CPU ray tracing framework oriented towards production-use scientific visualization which can utilize varying SIMD widths and multiple device backends found across diverse HPC resources. This framework provides a high-quality, efficient CPU-based solution for typical visualization workloads, which has already been integrated into several prevalent visualization packages. We show that this system delivers the performance, high-level API simplicity, and modular device support needed to provide a compelling new rendering framework for implementing efficient scientific visualization workflows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599041",
            "id": "r_34",
            "s_ids": [
                "s_1429",
                "s_994",
                "s_641",
                "s_1095",
                "s_1480",
                "s_1314",
                "s_514",
                "s_657"
            ],
            "type": "rich",
            "x": 0.3608080790017491,
            "y": 0.4060125904208247
        },
        {
            "title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication",
            "data": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865145",
            "id": "r_35",
            "s_ids": [
                "s_28",
                "s_1031",
                "s_1469",
                "s_735"
            ],
            "type": "rich",
            "x": 0.44914737426041795,
            "y": -0.05699828020022769
        },
        {
            "title": "iForest: Interpreting Random Forests via Visual Analytics",
            "data": "As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864475",
            "id": "r_36",
            "s_ids": [
                "s_846",
                "s_453",
                "s_1197",
                "s_896"
            ],
            "type": "rich",
            "x": -0.3052217509298612,
            "y": -0.2595965166549199
        },
        {
            "title": "Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models",
            "data": "Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865044",
            "id": "r_37",
            "s_ids": [
                "s_869",
                "s_778",
                "s_540",
                "s_1135",
                "s_1346",
                "s_445"
            ],
            "type": "rich",
            "x": -0.21571342247196432,
            "y": -0.5104895332454067
        },
        {
            "title": "Do Convolutional Neural Networks Learn Class Hierarchy?",
            "data": "Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744683",
            "id": "r_38",
            "s_ids": [
                "s_1335",
                "s_898",
                "s_922",
                "s_191",
                "s_1325"
            ],
            "type": "rich",
            "x": -0.4312241141139221,
            "y": -0.48732098617636743
        },
        {
            "title": "VLAT: Development of a Visualization Literacy Assessment Test",
            "data": "The Information Visualization community has begun to pay attention to visualization literacy; however, researchers still lack instruments for measuring the visualization literacy of users. In order to address this gap, we systematically developed a visualization literacy assessment test (VLAT), especially for non-expert users in data visualization, by following the established procedure of test development in Psychological and Educational Measurement: (1) Test Blueprint Construction, (2) Test Item Generation, (3) Content Validity Evaluation, (4) Test Tryout and Item Analysis, (5) Test Item Selection, and (6) Reliability Evaluation. The VLAT consists of 12 data visualizations and 53 multiple-choice test items that cover eight data visualization tasks. The test items in the VLAT were evaluated with respect to their essentialness by five domain experts in Information Visualization and Visual Analytics (average content validity ratio = 0.66). The VLAT was also tried out on a sample of 191 test takers and showed high reliability (reliability coefficient omega = 0.76). In addition, we demonstrated the relationship between users' visualization literacy and aptitude for learning an unfamiliar visualization and showed that they had a fairly high positive relationship (correlation coefficient = 0.64). Finally, we discuss evidence for the validity of the VLAT and potential research areas that are related to the instrument.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598920",
            "id": "r_39",
            "s_ids": [
                "s_997",
                "s_177",
                "s_1181"
            ],
            "type": "rich",
            "x": 0.4639657960803345,
            "y": -0.3046190476728029
        },
        {
            "title": "Analyzing the Training Processes of Deep Generative Models",
            "data": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744938",
            "id": "r_40",
            "s_ids": [
                "s_476",
                "s_1120",
                "s_630",
                "s_1436",
                "s_301"
            ],
            "type": "rich",
            "x": -0.36270300436063196,
            "y": -0.5115132795911792
        },
        {
            "title": "Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration",
            "data": "We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468078",
            "id": "r_41",
            "s_ids": [
                "s_1238",
                "s_886",
                "s_1329",
                "s_61"
            ],
            "type": "rich",
            "x": -0.13822613286024837,
            "y": -0.16397194790993938
        },
        {
            "title": "Scatterplots: Tasks, Data, and Designs",
            "data": "Traditional scatterplots fail to scale as the complexity and amount of data increases. In response, there exist many design options that modify or expand the traditional scatterplot design to meet these larger scales. This breadth of design options creates challenges for designers and practitioners who must select appropriate designs for particular analysis goals. In this paper, we help designers in making design choices for scatterplot visualizations. We survey the literature to catalog scatterplot-specific analysis tasks. We look at how data characteristics influence design decisions. We then survey scatterplot-like designs to understand the range of design options. Building upon these three organizations, we connect data characteristics, analysis tasks, and design choices in order to generate challenges, open questions, and example best practices for the effective design of scatterplots.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744184",
            "id": "r_42",
            "s_ids": [
                "s_39",
                "s_158"
            ],
            "type": "rich",
            "x": 0.48656297245103675,
            "y": 0.044206244253677064
        },
        {
            "title": "TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems",
            "data": "Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467196",
            "id": "r_43",
            "s_ids": [
                "s_866",
                "s_1367",
                "s_93",
                "s_305",
                "s_1177",
                "s_956"
            ],
            "type": "rich",
            "x": 0.1512697181759879,
            "y": -0.01067755682788296
        },
        {
            "title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning",
            "data": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986948",
            "id": "r_44",
            "s_ids": [
                "s_147",
                "s_1062",
                "s_1005",
                "s_1412",
                "s_1418",
                "s_295"
            ],
            "type": "rich",
            "x": -0.11236259500692661,
            "y": -0.303242151239516
        },
        {
            "title": "Colorgorical: Creating discriminable and preferable color palettes for information visualization",
            "data": "We present an evaluation of Colorgorical, a web-based tool for creating discriminable and aesthetically preferable categorical color palettes. Colorgorical uses iterative semi-random sampling to pick colors from CIELAB space based on user-defined discriminability and preference importances. Colors are selected by assigning each a weighted sum score that applies the user-defined importances to Perceptual Distance, Name Difference, Name Uniqueness, and Pair Preference scoring functions, which compare a potential sample to already-picked palette colors. After, a color is added to the palette by randomly sampling from the highest scoring palettes. Users can also specify hue ranges or build off their own starting palettes. This procedure differs from previous approaches that do not allow customization (e.g., pre-made ColorBrewer palettes) or do not consider visualization design constraints (e.g., Adobe Color and ACE). In a Palette Score Evaluation, we verified that each scoring function measured different color information. Experiment 1 demonstrated that slider manipulation generates palettes that are consistent with the expected balance of discriminability and aesthetic preference for 3-, 5-, and 8-color palettes, and also shows that the number of colors may change the effectiveness of pair-based discriminability and preference scores. For instance, if the Pair Preference slider were upweighted, users would judge the palettes as more preferable on average. Experiment 2 compared Colorgorical palettes to benchmark palettes (ColorBrewer, Microsoft, Tableau, Random). Colorgorical palettes are as discriminable and are at least as preferable or more preferable than the alternative palette sets. In sum, Colorgorical allows users to make customized color palettes that are, on average, as effective as current industry standards by balancing the importance of discriminability and aesthetic preference.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598918",
            "id": "r_45",
            "s_ids": [
                "s_813",
                "s_1148",
                "s_30"
            ],
            "type": "rich",
            "x": -0.02348804483108687,
            "y": -0.31500205455524594
        },
        {
            "title": "In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation",
            "data": "Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864889",
            "id": "r_46",
            "s_ids": [
                "s_1272",
                "s_568",
                "s_1215",
                "s_759",
                "s_920"
            ],
            "type": "rich",
            "x": 0.6776763702392657,
            "y": -0.30456457354260125
        },
        {
            "title": "Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data",
            "data": "Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467619",
            "id": "r_47",
            "s_ids": [
                "s_316",
                "s_1214",
                "s_1018",
                "s_1261",
                "s_1206",
                "s_1122",
                "s_447",
                "s_895"
            ],
            "type": "rich",
            "x": 0.32096399329819,
            "y": 0.45649871195217334
        },
        {
            "title": "explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning",
            "data": "We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934629",
            "id": "r_48",
            "s_ids": [
                "s_1088",
                "s_395",
                "s_258",
                "s_773"
            ],
            "type": "rich",
            "x": 0.09896570822573143,
            "y": -0.6032471747007025
        },
        {
            "title": "Clustervision: Visual Supervision of Unsupervised Clustering",
            "data": "Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745085",
            "id": "r_49",
            "s_ids": [
                "s_1181",
                "s_188",
                "s_20",
                "s_113",
                "s_952",
                "s_651",
                "s_1135"
            ],
            "type": "rich",
            "x": -0.07185332804912799,
            "y": 0.26606277032754266
        },
        {
            "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation",
            "data": "Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864500",
            "id": "r_50",
            "s_ids": [
                "s_1412",
                "s_762",
                "s_569",
                "s_1212",
                "s_159"
            ],
            "type": "rich",
            "x": -0.22647014565419174,
            "y": -0.6634635318071201
        },
        {
            "title": "ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories",
            "data": "Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598664",
            "id": "r_51",
            "s_ids": [
                "s_1109",
                "s_1357",
                "s_1325",
                "s_1250"
            ],
            "type": "rich",
            "x": 0.2504467861284038,
            "y": 0.05425705886964511
        },
        {
            "title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics",
            "data": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598620",
            "id": "r_52",
            "s_ids": [
                "s_1263",
                "s_217",
                "s_1080",
                "s_1042",
                "s_1328",
                "s_863",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.39336980802914123,
            "y": 0.015925837176897432
        },
        {
            "title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks",
            "data": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864504",
            "id": "r_53",
            "s_ids": [
                "s_765",
                "s_122",
                "s_353",
                "s_852"
            ],
            "type": "rich",
            "x": -0.3976055005366139,
            "y": -0.6240579669028095
        },
        {
            "title": "How do People Make Sense of Unfamiliar Visualizations?: A Grounded Model of Novice's Information Visualization Sensemaking",
            "data": "In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice's information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: 1 encountering visualization, 2 constructing a frame, 3 exploring visualization, 4 questioning the frame, and 5 floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we compare with other existing models and share further research directions that arose from our observations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467195",
            "id": "r_54",
            "s_ids": [
                "s_997",
                "s_177",
                "s_1282",
                "s_492",
                "s_1008",
                "s_1200"
            ],
            "type": "rich",
            "x": 0.3963727721030726,
            "y": -0.32984483714137064
        },
        {
            "title": "Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study",
            "data": "Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744818",
            "id": "r_55",
            "s_ids": [
                "s_867",
                "s_399",
                "s_1483",
                "s_325",
                "s_781"
            ],
            "type": "rich",
            "x": 0.023577380851238085,
            "y": -0.2736639138775895
        },
        {
            "title": "Why Authors Don't Visualize Uncertainty",
            "data": "Clear presentation of uncertainty is an exception rather than rule in media articles, data-driven reports, and consumer applications, despite proposed techniques for communicating sources of uncertainty in data. This work considers, Why do so many visualization authors choose not to visualize uncertainty? I contribute a detailed characterization of practices, associations, and attitudes related to uncertainty communication among visualization authors, derived from the results of surveying 90 authors who regularly create visualizations for others as part of their work, and interviewing thirteen influential visualization designers. My results highlight challenges that authors face and expose assumptions and inconsistencies in beliefs about the role of uncertainty in visualization. In particular, a clear contradiction arises between authors' acknowledgment of the value of depicting uncertainty and the norm of omitting direct depiction of uncertainty. To help explain this contradiction, I present a rhetorical model of uncertainty omission in visualization-based communication. I also adapt a formal statistical model of how viewers judge the strength of a signal in a visualization to visualization-based communication, to argue that uncertainty communication necessarily reduces degrees of freedom in viewers' statistical inferences. I conclude with recommendations for how visualization research on uncertainty communication could better serve practitioners' current needs and values while deepening understanding of assumptions that reinforce uncertainty omission.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934287",
            "id": "r_56",
            "s_ids": [
                "s_1272"
            ],
            "type": "rich",
            "x": 0.6002848087160475,
            "y": -0.41097118500231494
        },
        {
            "title": "Authoring Data-Driven Videos with DataClips",
            "data": "Data videos, or short data-driven motion graphics, are an increasingly popular medium for storytelling. However, creating data videos is difficult as it involves pulling together a unique combination of skills. We introduce DataClips, an authoring tool aimed at lowering the barriers to crafting data videos. DataClips allows non-experts to assemble data-driven \u201cclips\u201d together to form longer sequences. We constructed the library of data clips by analyzing the composition of over 70 data videos produced by reputable sources such as The New York Times and The Guardian. We demonstrate that DataClips can reproduce over 90% of our data videos corpus. We also report on a qualitative study comparing the authoring process and outcome achieved by (1) non-experts using DataClips, and (2) experts using Adobe Illustrator and After Effects to create data-driven clips. Results indicated that non-experts are able to learn and use DataClips with a short training period. In the span of one hour, they were able to produce more videos than experts using a professional editing tool, and their clips were rated similarly by an independent audience.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598647",
            "id": "r_57",
            "s_ids": [
                "s_483",
                "s_845",
                "s_250",
                "s_1428",
                "s_538"
            ],
            "type": "rich",
            "x": 0.3923406061680982,
            "y": 0.08337147953138123
        },
        {
            "title": "Patterns and Sequences: Interactive Exploration of Clickstreams to Understand Common Visitor Paths",
            "data": "Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difficult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598797",
            "id": "r_58",
            "s_ids": [
                "s_1080",
                "s_167",
                "s_1042",
                "s_33",
                "s_1153",
                "s_166"
            ],
            "type": "rich",
            "x": 0.25441634867527346,
            "y": 0.31084620393606766
        },
        {
            "title": "Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks",
            "data": "Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745219",
            "id": "r_59",
            "s_ids": [
                "s_28",
                "s_735"
            ],
            "type": "rich",
            "x": 0.7769078718106314,
            "y": 0.040013441076673366
        },
        {
            "title": "Visual Analysis and Dissemination of Scientific Literature Collections with SurVis",
            "data": "Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467757",
            "id": "r_60",
            "s_ids": [
                "s_383",
                "s_659",
                "s_138"
            ],
            "type": "rich",
            "x": 0.3698309070942344,
            "y": -0.27081275896868706
        },
        {
            "title": "DataShot: Automatic Generation of Fact Sheets from Tabular Data",
            "data": "Fact sheets with vivid graphical design and intriguing statistical insights are prevalent for presenting raw data. They help audiences understand data-related facts effectively and make a deep impression. However, designing a fact sheet requires both data and design expertise and is a laborious and time-consuming process. One needs to not only understand the data in depth but also produce intricate graphical representations. To assist in the design process, we present DataShot which, to the best of our knowledge, is the first automated system that creates fact sheets automatically from tabular data. First, we conduct a qualitative analysis of 245 infographic examples to explore general infographic design space at both the sheet and element levels. We identify common infographic structures, sheet layouts, fact types, and visualization styles during the study. Based on these findings, we propose a fact sheet generation pipeline, consisting of fact extraction, fact composition, and presentation synthesis, for the auto-generation workflow. To validate our system, we present use cases with three real-world datasets. We conduct an in-lab user study to understand the usage of our system. Our evaluation results show that DataShot can efficiently generate satisfactory fact sheets to support further customization and data presentation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934398",
            "id": "r_61",
            "s_ids": [
                "s_640",
                "s_1165",
                "s_351",
                "s_896",
                "s_26",
                "s_1415",
                "s_135"
            ],
            "type": "rich",
            "x": 0.4732310975198452,
            "y": -0.2324024474962357
        },
        {
            "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights",
            "data": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865191",
            "id": "r_62",
            "s_ids": [
                "s_589",
                "s_845",
                "s_1031",
                "s_1204",
                "s_508",
                "s_943"
            ],
            "type": "rich",
            "x": -0.12514607587154566,
            "y": 0.5628639749273363
        },
        {
            "title": "Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles",
            "data": "We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467204",
            "id": "r_63",
            "s_ids": [
                "s_1248",
                "s_1410",
                "s_111"
            ],
            "type": "rich",
            "x": -0.4451324641612038,
            "y": 0.5530180662240306
        },
        {
            "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics",
            "data": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934803",
            "id": "r_64",
            "s_ids": [
                "s_669",
                "s_1363",
                "s_1204",
                "s_1143",
                "s_202",
                "s_184",
                "s_1188"
            ],
            "type": "rich",
            "x": 0.5628747200557019,
            "y": -0.025675488225374108
        },
        {
            "title": "ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer",
            "data": "Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865041",
            "id": "r_65",
            "s_ids": [
                "s_1273",
                "s_1438",
                "s_768",
                "s_1217",
                "s_420",
                "s_556",
                "s_359",
                "s_1250"
            ],
            "type": "rich",
            "x": 0.02947858741265719,
            "y": 0.3751079606531161
        },
        {
            "title": "FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System",
            "data": "Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934668",
            "id": "r_66",
            "s_ids": [
                "s_967",
                "s_380"
            ],
            "type": "rich",
            "x": 0.37928998735585545,
            "y": 0.22668715674221732
        },
        {
            "title": "Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data",
            "data": "A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864503",
            "id": "r_67",
            "s_ids": [
                "s_1020",
                "s_942",
                "s_1451",
                "s_1285",
                "s_74",
                "s_4",
                "s_1250"
            ],
            "type": "rich",
            "x": -0.15872766216451742,
            "y": 0.4640867935435027
        },
        {
            "title": "SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories",
            "data": "Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as \u201cWhat were the taxi trips starting from Main Street and ending at Wall Street in the morning?\u201d or \u201cWhere are the taxis arriving at the Art Museum at noon typically coming from?\u201d, tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as \u201cMain Street\u201d, \u201cWall Street\u201d, and \u201cArt Museum\u201d. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598416",
            "id": "r_68",
            "s_ids": [
                "s_155",
                "s_730",
                "s_1387",
                "s_745",
                "s_889",
                "s_941",
                "s_1253",
                "s_1250",
                "s_1396",
                "s_667"
            ],
            "type": "rich",
            "x": 0.2796230978553969,
            "y": 0.40271340807220246
        },
        {
            "title": "Charticulator: Interactive Construction of Bespoke Chart Layouts",
            "data": "We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https://charticulator.com.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865158",
            "id": "r_69",
            "s_ids": [
                "s_1163",
                "s_250",
                "s_12"
            ],
            "type": "rich",
            "x": 0.49724573559084617,
            "y": -0.08489527359627325
        },
        {
            "title": "EventThread: Visual Summarization and Stage Analysis of Event Sequence Data",
            "data": "Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745320",
            "id": "r_70",
            "s_ids": [
                "s_400",
                "s_26",
                "s_244",
                "s_851",
                "s_1228",
                "s_866"
            ],
            "type": "rich",
            "x": 0.2949994136304821,
            "y": 0.28249013703225445
        },
        {
            "title": "Origin-Destination Flow Maps in Immersive Environments",
            "data": "Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call<i>MapsLink</i>, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that<i>careful</i>use of the third spatial dimension can resolve visual clutter in complex flow maps.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865192",
            "id": "r_71",
            "s_ids": [
                "s_36",
                "s_202",
                "s_48",
                "s_1188",
                "s_1204",
                "s_1127"
            ],
            "type": "rich",
            "x": -0.3185412564254393,
            "y": 0.3001736766392659
        },
        {
            "title": "Sequence Synopsis: Optimize Visual Summary of Temporal Event Data",
            "data": "Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745083",
            "id": "r_72",
            "s_ids": [
                "s_411",
                "s_1109",
                "s_1325"
            ],
            "type": "rich",
            "x": 0.1783303162630526,
            "y": 0.0425334603875907
        },
        {
            "title": "StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views",
            "data": "Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, smell, and touch) in their daily lives, can provide unprecedented insights on a variety of applications, such as urban planning and environment auditing. The analysis of urban forms can help planners develop high-quality urban spaces through evidence-based design. However, such analysis is complex because of the involvement of spatial, multi-scale (i.e., city, region, and street), and multivariate (e.g., greenery and sky ratios) natures of urban forms. In addition, current methods either lack quantitative measurements or are limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge in exploring human-scale urban forms based on street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer for the visual comparison of spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scales; 2) and a Street Explorer with a novel parallel coordinate plot for the exploration of the fine-grained details of the urban forms at the street-scale. We integrate visualization techniques with machine learning models to facilitate the detection of street view patterns. We illustrate the applicability of our approach with case studies on the real-world datasets of four cities, i.e., Hong Kong, Singapore, Greater London and New York City. Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744159",
            "id": "r_73",
            "s_ids": [
                "s_1423",
                "s_1298",
                "s_95",
                "s_226",
                "s_1015",
                "s_1019",
                "s_137"
            ],
            "type": "rich",
            "x": 0.0076394660293942806,
            "y": 0.47282771733700696
        },
        {
            "title": "Urban Pulse: Capturing the Rhythm of Cities",
            "data": "Cities are inherently dynamic. Interesting patterns of behavior typically manifest at several key areas of a city over multiple temporal resolutions. Studying these patterns can greatly help a variety of experts ranging from city planners and architects to human behavioral experts. Recent technological innovations have enabled the collection of enormous amounts of data that can help in these studies. However, techniques using these data sets typically focus on understanding the data in the context of the city, thus failing to capture the dynamic aspects of the city. The goal of this work is to instead understand the city in the context of multiple urban data sets. To do so, we define the concept of an \u201curban pulse\u201d which captures the spatio-temporal activity in a city across multiple temporal resolutions. The prominent pulses in a city are obtained using the topology of the data sets, and are characterized as a set of beats. The beats are then used to analyze and compare different pulses. We also design a visual exploration framework that allows users to explore the pulses within and across multiple cities under different conditions. Finally, we present three case studies carried out by experts from two different domains that demonstrate the utility of our framework.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598585",
            "id": "r_74",
            "s_ids": [
                "s_1203",
                "s_29",
                "s_904",
                "s_939",
                "s_645",
                "s_748",
                "s_1353",
                "s_380"
            ],
            "type": "rich",
            "x": 0.08202274806482386,
            "y": 0.4420699543615822
        },
        {
            "title": "egoSlider: Visual Analysis of Egocentric Network Evolution",
            "data": "Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468151",
            "id": "r_75",
            "s_ids": [
                "s_453",
                "s_354",
                "s_1283",
                "s_1388",
                "s_441",
                "s_137"
            ],
            "type": "rich",
            "x": -0.10476470969261108,
            "y": -0.45767771475121555
        },
        {
            "title": "Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability",
            "data": "Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467758",
            "id": "r_76",
            "s_ids": [
                "s_758",
                "s_817",
                "s_1330",
                "s_832",
                "s_833",
                "s_583",
                "s_114",
                "s_321"
            ],
            "type": "rich",
            "x": 0.13416083067872617,
            "y": -0.48045697569594825
        },
        {
            "title": "VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning",
            "data": "While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864838",
            "id": "r_77",
            "s_ids": [
                "s_905",
                "s_805",
                "s_1003",
                "s_266"
            ],
            "type": "rich",
            "x": 0.15383293260935996,
            "y": -0.5113062400859111
        },
        {
            "title": "Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data",
            "data": "The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \u201csmart city\u201d as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744419",
            "id": "r_78",
            "s_ids": [
                "s_866",
                "s_454",
                "s_1241",
                "s_1177",
                "s_993",
                "s_872"
            ],
            "type": "rich",
            "x": 0.19284336426532417,
            "y": 0.42538636749166314
        },
        {
            "title": "Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plots",
            "data": "Due to the uncertain nature of weather prediction, climate simulations are usually performed multiple times with different spatial resolutions. The outputs of simulations are multi-resolution spatial temporal ensembles. Each simulation run uses a unique set of values for multiple convective parameters. Distinct parameter settings from different simulation runs in different resolutions constitute a multi-resolution high-dimensional parameter space. Understanding the correlation between the different convective parameters, and establishing a connection between the parameter settings and the ensemble outputs are crucial to domain scientists. The multi-resolution high-dimensional parameter space, however, presents a unique challenge to the existing correlation visualization techniques. We present Nested Parallel Coordinates Plot (NPCP), a new type of parallel coordinates plots that enables visualization of intra-resolution and inter-resolution parameter correlations. With flexible user control, NPCP integrates superimposition, juxtaposition and explicit encodings in a single view for comparative data visualization and analysis. We develop an integrated visual analytics system to help domain scientists understand the connection between multi-resolution convective parameters and the large spatial temporal ensembles. Our system presents intricate climate ensembles with a comprehensive overview and on-demand geographic details. We demonstrate NPCP, along with the climate ensemble visualization system, based on real-world use-cases from our collaborators in computational and predictive science.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598830",
            "id": "r_79",
            "s_ids": [
                "s_765",
                "s_471",
                "s_353",
                "s_936"
            ],
            "type": "rich",
            "x": -0.2756072965086385,
            "y": 0.5334596658740358
        },
        {
            "title": "CiteRivers: Visual Analytics of Citation Patterns",
            "data": "The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467621",
            "id": "r_80",
            "s_ids": [
                "s_62",
                "s_1132",
                "s_390",
                "s_299"
            ],
            "type": "rich",
            "x": 0.40928319497586235,
            "y": -0.38850283441300176
        },
        {
            "title": "Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation",
            "data": "Models of human perception - including perceptual \u201claws\u201d - can be valuable tools for deriving visualization design recommendations. However, it is important to assess the explanatory power of such models when using them to inform design. We present a secondary analysis of data previously used to rank the effectiveness of bivariate visualizations for assessing correlation (measured with Pearson's r) according to the well-known Weber-Fechner Law. Beginning with the model of Harrison et al. [1], we present a sequence of refinements including incorporation of individual differences, log transformation, censored regression, and adoption of Bayesian statistics. Our model incorporates all observations dropped from the original analysis, including data near ceilings caused by the data collection process and entire visualizations dropped due to large numbers of observations worse than chance. This model deviates from Weber's Law, but provides improved predictive accuracy and generalization. Using Bayesian credibility intervals, we derive a partial ranking that groups visualizations with similar performance, and we give precise estimates of the difference in performance between these groups. We find that compared to other visualizations, scatterplots are unique in combining low variance between individuals and high precision on both positively- and negatively correlated data. We conclude with a discussion of the value of data sharing and replication, and share implications for modeling similar experimental data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467671",
            "id": "r_81",
            "s_ids": [
                "s_920",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.12126275023404857,
            "y": -0.15084339895258175
        },
        {
            "title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements",
            "data": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934785",
            "id": "r_82",
            "s_ids": [
                "s_896",
                "s_884",
                "s_640",
                "s_542",
                "s_500",
                "s_782",
                "s_351",
                "s_1247",
                "s_135"
            ],
            "type": "rich",
            "x": 0.6146747835347153,
            "y": -0.24488158758013603
        },
        {
            "title": "Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring",
            "data": "Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744198",
            "id": "r_83",
            "s_ids": [
                "s_306",
                "s_1272"
            ],
            "type": "rich",
            "x": 0.3372067637312468,
            "y": -0.2668549716924014
        },
        {
            "title": "InterAxis: Steering Scatterplot Axes via Observation-Level Interaction",
            "data": "Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467615",
            "id": "r_84",
            "s_ids": [
                "s_726",
                "s_320",
                "s_1068",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.28251603336887143,
            "y": 0.11175838900172406
        },
        {
            "title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics",
            "data": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585669",
            "id": "r_85",
            "s_ids": [
                "s_8",
                "s_992",
                "s_464",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.5558372674016809,
            "y": -0.4357020079938349
        },
        {
            "title": "Mapping Color to Meaning in Colormap Data Visualizations",
            "data": "To interpret data visualizations, people must determine how visual features map onto concepts. For example, to interpret colormaps, people must determine how dimensions of color (e.g., lightness, hue) map onto quantities of a given measure (e.g., brain activity, correlation magnitude). This process is easier when the encoded mappings in the visualization match people's predictions of how visual features will map onto concepts, their inferred mappings. To harness this principle in visualization design, it is necessary to understand what factors determine people's inferred mappings. In this study, we investigated how inferred color-quantity mappings for colormap data visualizations were influenced by the background color. Prior literature presents seemingly conflicting accounts of how the background color affects inferred color-quantity mappings. The present results help resolve those conflicts, demonstrating that sometimes the background has an effect and sometimes it does not, depending on whether the colormap appears to vary in opacity. When there is no apparent variation in opacity, participants infer that darker colors map to larger quantities (dark-is-more bias). As apparent variation in opacity increases, participants become biased toward inferring that more opaque colors map to larger quantities (opaque-is-more bias). These biases work together on light backgrounds and conflict on dark backgrounds. Under such conflicts, the opaque-is-more bias can negate, or even supersede the dark-is-more bias. The results suggest that if a design goal is to produce colormaps that match people's inferred mappings and are robust to changes in background color, it is beneficial to use colormaps that will not appear to vary in opacity on any background color, and to encode larger quantities in darker colors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865147",
            "id": "r_86",
            "s_ids": [
                "s_30",
                "s_813",
                "s_1230",
                "s_139",
                "s_1448"
            ],
            "type": "rich",
            "x": -0.0678684117377469,
            "y": -0.35967840342096014
        },
        {
            "title": "WeightLifter: Visual Weight Space Exploration for Multi-Criteria Decision Making",
            "data": "A common strategy in Multi-Criteria Decision Making (MCDM) is to rank alternative solutions by weighted summary scores. Weights, however, are often abstract to the decision maker and can only be set by vague intuition. While previous work supports a point-wise exploration of weight spaces, we argue that MCDM can benefit from a regional and global visual analysis of weight spaces. Our main contribution is WeightLifter, a novel interactive visualization technique for weight-based MCDM that facilitates the exploration of weight spaces with up to ten criteria. Our technique enables users to better understand the sensitivity of a decision to changes of weights, to efficiently localize weight regions where a given solution ranks high, and to filter out solutions which do not rank high enough for any plausible combination of weights. We provide a comprehensive requirement analysis for weight-based MCDM and describe an interactive workflow that meets these requirements. For evaluation, we describe a usage scenario of WeightLifter in automotive engineering and report qualitative feedback from users of a deployed version as well as preliminary feedback from decision makers in multiple domains. This feedback confirms that WeightLifter increases both the efficiency of weight-based MCDM and the awareness of uncertainty in the ultimate decisions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598589",
            "id": "r_87",
            "s_ids": [
                "s_565",
                "s_495",
                "s_1348",
                "s_1470",
                "s_1434",
                "s_270"
            ],
            "type": "rich",
            "x": 0.061550137042614,
            "y": 0.1439274492152669
        },
        {
            "title": "Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics",
            "data": "Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745258",
            "id": "r_88",
            "s_ids": [
                "s_582",
                "s_1166",
                "s_1407",
                "s_13",
                "s_55",
                "s_207"
            ],
            "type": "rich",
            "x": -0.009322449177531486,
            "y": 0.22553172181991374
        },
        {
            "title": "Time-Hierarchical Clustering and Visualization of Weather Forecast Ensembles",
            "data": "We propose a new approach for analyzing the temporal growth of the uncertainty in ensembles of weather forecasts which are started from perturbed but similar initial conditions. As an alternative to traditional approaches in meteorology, which use juxtaposition and animation of spaghetti plots of iso-contours, we make use of contour clustering and provide means to encode forecast dynamics and spread in one single visualization. Based on a given ensemble clustering in a specified time window, we merge clusters in time-reversed order to indicate when and where forecast trajectories start to diverge. We present and compare different visualizations of the resulting time-hierarchical grouping, including space-time surfaces built by connecting cluster representatives over time, and stacked contour variability plots. We demonstrate the effectiveness of our visual encodings with forecast examples of the European Centre for Medium-Range Weather Forecasts, which convey the evolution of specific features in the data as well as the temporally increasing spatial variability.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598868",
            "id": "r_89",
            "s_ids": [
                "s_1248",
                "s_103",
                "s_1390",
                "s_111"
            ],
            "type": "rich",
            "x": -0.10593347281682564,
            "y": 0.6560040426015985
        },
        {
            "title": "VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications",
            "data": "Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467871",
            "id": "r_90",
            "s_ids": [
                "s_1221",
                "s_1323",
                "s_1463",
                "s_390",
                "s_299"
            ],
            "type": "rich",
            "x": 0.38667902366231316,
            "y": -0.08326515483042785
        },
        {
            "title": "TelCoVis: Visual Exploration of Co-occurrence in Urban Human Mobility Based on Telco Data",
            "data": "Understanding co-occurrence in urban human mobility (i.e. people from two regions visit an urban place during the same time span) is of great value in a variety of applications, such as urban planning, business intelligence, social behavior analysis, as well as containing contagious diseases. In recent years, the widespread use of mobile phones brings an unprecedented opportunity to capture large-scale and fine-grained data to study co-occurrence in human mobility. However, due to the lack of systematic and efficient methods, it is challenging for analysts to carry out in-depth analyses and extract valuable information. In this paper, we present TelCoVis, an interactive visual analytics system, which helps analysts leverage their domain knowledge to gain insight into the co-occurrence in urban human mobility based on telco data. Our system integrates visualization techniques with new designs and combines them in a novel way to enhance analysts' perception for a comprehensive exploration. In addition, we propose to study the correlations in co-occurrence (i.e. people from multiple regions visit different places during the same time span) by means of biclustering techniques that allow analysts to better explore coordinated relationships among different regions and identify interesting patterns. The case studies based on a real-world dataset and interviews with domain experts have demonstrated the effectiveness of our system in gaining insights into co-occurrence and facilitating various analytical tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467194",
            "id": "r_91",
            "s_ids": [
                "s_965",
                "s_1167",
                "s_328",
                "s_976",
                "s_137",
                "s_1133",
                "s_719",
                "s_263"
            ],
            "type": "rich",
            "x": 0.24241664575515068,
            "y": 0.24704217035171996
        },
        {
            "title": "Applying Pragmatics Principles for Interaction with Visual Analytics",
            "data": "Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744684",
            "id": "r_92",
            "s_ids": [
                "s_254",
                "s_462",
                "s_1208",
                "s_658"
            ],
            "type": "rich",
            "x": 0.7680959182886044,
            "y": -0.17775668462359054
        },
        {
            "title": "The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps",
            "data": "A myriad of design rules for what constitutes a \u201cgood\u201d colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743978",
            "id": "r_93",
            "s_ids": [
                "s_738",
                "s_1033",
                "s_187",
                "s_1111",
                "s_80",
                "s_1070"
            ],
            "type": "rich",
            "x": 0.08049084882947465,
            "y": -0.3222107077602576
        },
        {
            "title": "Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning",
            "data": "Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With ccPCA, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934251",
            "id": "r_94",
            "s_ids": [
                "s_1201",
                "s_561",
                "s_687"
            ],
            "type": "rich",
            "x": -0.13540503730033673,
            "y": 0.08452270128347583
        },
        {
            "title": "A Heuristic Approach to Value-Driven Evaluation of Visualizations",
            "data": "Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization's value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865146",
            "id": "r_95",
            "s_ids": [
                "s_8",
                "s_629",
                "s_957",
                "s_107",
                "s_1277",
                "s_1469",
                "s_735"
            ],
            "type": "rich",
            "x": 0.47215149966130276,
            "y": -0.0987790130579594
        },
        {
            "title": "iTTVis: Interactive Visualization of Table Tennis Data",
            "data": "The rapid development of information technology paved the way for the recording of fine-grained data, such as stroke techniques and stroke placements, during a table tennis match. This data recording creates opportunities to analyze and evaluate matches from new perspectives. Nevertheless, the increasingly complex data poses a significant challenge to make sense of and gain insights into. Analysts usually employ tedious and cumbersome methods which are limited to watching videos and reading statistical tables. However, existing sports visualization methods cannot be applied to visualizing table tennis competitions due to different competition rules and particular data attributes. In this work, we collaborate with data analysts to understand and characterize the sophisticated domain problem of analysis of table tennis data. We propose iTTVis, a novel interactive table tennis visualization system, which to our knowledge, is the first visual analysis system for analyzing and exploring table tennis data. iTTVis provides a holistic visualization of an entire match from three main perspectives, namely, time-oriented, statistical, and tactical analyses. The proposed system with several well-coordinated views not only supports correlation identification through statistics and pattern detection of tactics with a score timeline but also allows cross analysis to gain insights. Data analysts have obtained several new insights by using iTTVis. The effectiveness and usability of the proposed system are demonstrated with four case studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744218",
            "id": "r_96",
            "s_ids": [
                "s_1273",
                "s_1053",
                "s_1377",
                "s_601",
                "s_525",
                "s_768",
                "s_556"
            ],
            "type": "rich",
            "x": 0.2943359203450763,
            "y": 0.06671698649228013
        },
        {
            "title": "A Virtual Reality Visualization Tool for Neuron Tracing",
            "data": "Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744079",
            "id": "r_97",
            "s_ids": [
                "s_612",
                "s_83",
                "s_296",
                "s_438",
                "s_1480",
                "s_566",
                "s_531",
                "s_297"
            ],
            "type": "rich",
            "x": -0.363617235288473,
            "y": -0.05395677979782261
        },
        {
            "title": "Temporal Summary Images: An Approach to Narrative Visualization via Interactive Annotation Generation and Placement",
            "data": "Visualization is a powerful technique for analysis and communication of complex, multidimensional, and time-varying data. However, it can be difficult to manually synthesize a coherent narrative in a chart or graph due to the quantity of visualized attributes, a variety of salient features, and the awareness required to interpret points of interest (POls). We present Temporal Summary Images (TSIs) as an approach for both exploring this data and creating stories from it. As a visualization, a TSI is composed of three common components: (1) a temporal layout, (2) comic strip-style data snapshots, and (3) textual annotations. To augment user analysis and exploration, we have developed a number of interactive techniques that recommend relevant data features and design choices, including an automatic annotations workflow. As the analysis and visual design processes converge, the resultant image becomes appropriate for data storytelling. For validation, we use a prototype implementation for TSIs to conduct two case studies with large-scale, scientific simulation datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598876",
            "id": "r_98",
            "s_ids": [
                "s_342",
                "s_687",
                "s_649"
            ],
            "type": "rich",
            "x": 0.44944467283516243,
            "y": -0.005441442902339608
        },
        {
            "title": "Hashedcubes: Simple, Low Memory, Real-Time Visual Exploration of Big Data",
            "data": "We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598624",
            "id": "r_99",
            "s_ids": [
                "s_490",
                "s_574",
                "s_529",
                "s_1334"
            ],
            "type": "rich",
            "x": -0.21899198459331393,
            "y": 0.42688298817838777
        },
        {
            "title": "A Linguistic Approach to Categorical Color Assignment for Data Visualization",
            "data": "When data categories have strong color associations, it is useful to use these semantically meaningful concept-color associations in data visualizations. In this paper, we explore how linguistic information about the terms defining the data can be used to generate semantically meaningful colors. To do this effectively, we need first to establish that a term has a strong semantic color association, then discover which color or colors express it. Using co-occurrence measures of color name frequencies from Google n-grams, we define a measure for colorability that describes how strongly associated a given term is to any of a set of basic color terms. We then show how this colorability score can be used with additional semantic analysis to rank and retrieve a representative color from Google Images. Alternatively, we use symbolic relationships defined by WordNet to select identity colors for categories such as countries or brands. To create visually distinct color palettes, we use k-means clustering to create visually distinct sets, iteratively reassigning terms with multiple basic color associations as needed. This can be additionally constrained to use colors only in a predefined palette.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467471",
            "id": "r_100",
            "s_ids": [
                "s_462",
                "s_1237"
            ],
            "type": "rich",
            "x": -0.14923509260123544,
            "y": -0.3729263120304874
        },
        {
            "title": "Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data",
            "data": "Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864909",
            "id": "r_101",
            "s_ids": [
                "s_759",
                "s_58",
                "s_920",
                "s_1272"
            ],
            "type": "rich",
            "x": 0.3745086063262239,
            "y": -0.21584730830268706
        },
        {
            "title": "Supporting Iterative Cohort Construction with Visual Temporal Queries",
            "data": "Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467622",
            "id": "r_102",
            "s_ids": [
                "s_448",
                "s_1135",
                "s_739"
            ],
            "type": "rich",
            "x": -0.007501729190905133,
            "y": -0.19739906479900027
        },
        {
            "title": "IDMVis: Temporal Event Sequence Visualization for Type 1 Diabetes Treatment Decision Support",
            "data": "Type 1 diabetes is a chronic, incurable autoimmune disease affecting millions of Americans in which the body stops producing insulin and blood glucose levels rise. The goal of intensive diabetes management is to lower average blood glucose through frequent adjustments to insulin protocol, diet, and behavior. Manual logs and medical device data are collected by patients, but these multiple sources are presented in disparate visualization designs to the clinician-making temporal inference difficult. We conducted a design study over 18 months with clinicians performing intensive diabetes management. We present a data abstraction and novel hierarchical task abstraction for this domain. We also contribute IDMVis: a visualization tool for temporal event sequences with multidimensional, interrelated data. IDMVis includes a novel technique for folding and aligning records by dual sentinel events and scaling the intermediate timeline. We validate our design decisions based on our domain abstractions, best practices, and through a qualitative evaluation with six clinicians. The results of this study indicate that IDMVis accurately reflects the workflow of clinicians. Using IDMVis, clinicians are able to identify issues of data quality such as missing or conflicting data, reconstruct patient records when data is missing, differentiate between days with different patterns, and promote educational interventions after identifying discrepancies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865076",
            "id": "r_103",
            "s_ids": [
                "s_761",
                "s_588",
                "s_879"
            ],
            "type": "rich",
            "x": -0.10159705387551185,
            "y": -0.22644940003599556
        },
        {
            "title": "Visual Exploration of Semantic Relationships in Neural Word Embeddings",
            "data": "Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745141",
            "id": "r_104",
            "s_ids": [
                "s_340",
                "s_438",
                "s_444",
                "s_989",
                "s_722",
                "s_532",
                "s_297"
            ],
            "type": "rich",
            "x": 0.06522084808341215,
            "y": -0.47857023165237556
        },
        {
            "title": "Visual Diagnosis of Tree Boosting Methods",
            "data": "Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744378",
            "id": "r_105",
            "s_ids": [
                "s_301",
                "s_361",
                "s_1229",
                "s_1032",
                "s_1384",
                "s_1436"
            ],
            "type": "rich",
            "x": -0.41306929360853756,
            "y": -0.308982028491684
        },
        {
            "title": "Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation",
            "data": "Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps [37] with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598885",
            "id": "r_106",
            "s_ids": [
                "s_36",
                "s_202",
                "s_91",
                "s_1188"
            ],
            "type": "rich",
            "x": -0.013636917772131654,
            "y": 0.3570018076651452
        },
        {
            "title": "Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff",
            "data": "Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934538",
            "id": "r_107",
            "s_ids": [
                "s_788",
                "s_1267",
                "s_706",
                "s_793",
                "s_635",
                "s_853",
                "s_339"
            ],
            "type": "rich",
            "x": 0.7569699080368122,
            "y": 0.1138855605740957
        },
        {
            "title": "Clustering Trajectories by Relevant Parts for Air Traffic Analysis",
            "data": "Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744322",
            "id": "r_108",
            "s_ids": [
                "s_1149",
                "s_9",
                "s_1017",
                "s_317"
            ],
            "type": "rich",
            "x": 0.1636019914676044,
            "y": 0.622638813401857
        },
        {
            "title": "The Attraction Effect in Information Visualization",
            "data": "The attraction effect is a well-studied cognitive bias in decision making research, where one's choice between two alternatives is influenced by the presence of an irrelevant (dominated) third alternative. We examine whether this cognitive bias, so far only tested with three alternatives and simple presentation formats such as numerical tables, text and pictures, also appears in visualizations. Since visualizations can be used to support decision making - e.g., when choosing a house to buy or an employee to hire - a systematic bias could have important implications. In a first crowdsource experiment, we indeed partially replicated the attraction effect with three alternatives presented as a numerical table, and observed similar effects when they were presented as a scatterplot. In a second experiment, we investigated if the effect extends to larger sets of alternatives, where the number of alternatives is too large for numerical tables to be practical. Our findings indicate that the bias persists for larger sets of alternatives presented as scatterplots. We discuss implications for future research on how to further study and possibly alleviate the attraction effect.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598594",
            "id": "r_109",
            "s_ids": [
                "s_194",
                "s_161",
                "s_231"
            ],
            "type": "rich",
            "x": 0.21940125822620737,
            "y": -0.4125133133287965
        },
        {
            "title": "A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights",
            "data": "We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467613",
            "id": "r_110",
            "s_ids": [
                "s_681",
                "s_242",
                "s_833",
                "s_1148"
            ],
            "type": "rich",
            "x": 0.6570323223504285,
            "y": -0.28871816519920196
        },
        {
            "title": "TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis",
            "data": "Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865018",
            "id": "r_111",
            "s_ids": [
                "s_1067",
                "s_1109",
                "s_1325"
            ],
            "type": "rich",
            "x": -0.030686114471511133,
            "y": 0.09556921495507537
        },
        {
            "title": "The Interactive Visualization Gap in Initial Exploratory Data Analysis",
            "data": "Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a \u201cvisualization gap\u201d during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743990",
            "id": "r_112",
            "s_ids": [
                "s_669",
                "s_1143"
            ],
            "type": "rich",
            "x": 0.8181777105174004,
            "y": -0.030653465297099596
        },
        {
            "title": "Designing Progressive and Interactive Analytics Processes for High-Dimensional Data Analysis",
            "data": "In interactive data analysis processes, the dialogue between the human and the computer is the enabling mechanism that can lead to actionable observations about the phenomena being investigated. It is of paramount importance that this dialogue is not interrupted by slow computational mechanisms that do not consider any known temporal human-computer interaction characteristics that prioritize the perceptual and cognitive capabilities of the users. In cases where the analysis involves an integrated computational method, for instance to reduce the dimensionality of the data or to perform clustering, such non-optimal processes are often likely. To remedy this, progressive computations, where results are iteratively improved, are getting increasing interest in visual analytics. In this paper, we present techniques and design considerations to incorporate progressive methods within interactive analysis processes that involve high-dimensional data. We define methodologies to facilitate processes that adhere to the perceptual characteristics of users and describe how online algorithms can be incorporated within these. A set of design recommendations and according methods to support analysts in accomplishing high-dimensional data analysis tasks are then presented. Our arguments and decisions here are informed by observations gathered over a series of analysis sessions with analysts from finance. We document observations and recommendations from this study and present evidence on how our approach contribute to the efficiency and productivity of interactive visual analysis sessions involving high-dimensional data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598470",
            "id": "r_113",
            "s_ids": [
                "s_85",
                "s_1240",
                "s_0",
                "s_367"
            ],
            "type": "rich",
            "x": 0.2329404697451817,
            "y": -0.03350691246767821
        },
        {
            "title": "The Impact of Immersion on Cluster Identification Tasks",
            "data": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934395",
            "id": "r_114",
            "s_ids": [
                "s_805",
                "s_426",
                "s_34",
                "s_281",
                "s_1003",
                "s_387"
            ],
            "type": "rich",
            "x": 0.2281347549568433,
            "y": 0.03238231965602199
        },
        {
            "title": "An Analysis of Machine- and Human-Analytics in Classification",
            "data": "In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the \u201cbag of features\u201d approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598829",
            "id": "r_115",
            "s_ids": [
                "s_1307",
                "s_172",
                "s_266"
            ],
            "type": "rich",
            "x": 0.16771156090141764,
            "y": -0.21561835221008202
        },
        {
            "title": "Visual Analytics for Mobile Eye Tracking",
            "data": "The analysis of eye tracking data often requires the annotation of areas of interest (AOIs) to derive semantic interpretations of human viewing behavior during experiments. This annotation is typically the most time-consuming step of the analysis process. Especially for data from wearable eye tracking glasses, every independently recorded video has to be annotated individually and corresponding AOIs between videos have to be identified. We provide a novel visual analytics approach to ease this annotation process by image-based, automatic clustering of eye tracking data integrated in an interactive labeling and analysis system. The annotation and analysis are tightly coupled by multiple linked views that allow for a direct interpretation of the labeled data in the context of the recorded video stimuli. The components of our analytics environment were developed with a user-centered design approach in close cooperation with an eye tracking expert. We demonstrate our approach with eye tracking data from a real experiment and compare it to an analysis of the data by manual annotation of dynamic AOIs. Furthermore, we conducted an expert user study with 6 external eye tracking researchers to collect feedback and identify analysis strategies they used while working with our application.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598695",
            "id": "r_116",
            "s_ids": [
                "s_1463",
                "s_576",
                "s_307",
                "s_138"
            ],
            "type": "rich",
            "x": 0.40623350792461377,
            "y": 0.012335706392283147
        },
        {
            "title": "What is Interaction for Data Visualization?",
            "data": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934283",
            "id": "r_117",
            "s_ids": [
                "s_194",
                "s_334"
            ],
            "type": "rich",
            "x": 0.68525386489007,
            "y": -0.08676791972414674
        },
        {
            "title": "SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance",
            "data": "Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744805",
            "id": "r_118",
            "s_ids": [
                "s_905",
                "s_805",
                "s_867",
                "s_540",
                "s_493",
                "s_1117",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.11493659414280419,
            "y": 0.17749316860120173
        },
        {
            "title": "ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding",
            "data": "Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744478",
            "id": "r_119",
            "s_ids": [
                "s_931",
                "s_971",
                "s_224",
                "s_320",
                "s_1249",
                "s_1143"
            ],
            "type": "rich",
            "x": 0.1939027200016127,
            "y": -0.6526195878993171
        },
        {
            "title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration",
            "data": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598839",
            "id": "r_120",
            "s_ids": [
                "s_1275",
                "s_726",
                "s_45",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.6938022251990242,
            "y": 0.17000786405621962
        },
        {
            "title": "Visualization, Selection, and Analysis of Traffic Flows",
            "data": "Visualization of the trajectories of moving objects leads to dense and cluttered images, which hinders exploration and understanding. It also hinders adding additional visual information, such as direction, and makes it difficult to interactively extract traffic flows, i.e., subsets of trajectories. In this paper we present our approach to visualize traffic flows and provide interaction tools to support their exploration. We show an overview of the traffic using a density map. The directions of traffic flows are visualized using a particle system on top of the density map. The user can extract traffic flows using a novel selection widget that allows for the intuitive selection of an area, and filtering on a range of directions and any additional attributes. Using simple, visual set expressions, the user can construct more complicated selections. The dynamic behaviors of selected flows may then be shown in annotation windows in which they can be interactively explored and compared. We validate our approach through use cases where we explore and analyze the temporal behavior of aircraft and vessel trajectories, e.g., landing and takeoff sequences, or the evolution of flight route density. The aircraft use cases have been developed and validated in collaboration with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467112",
            "id": "r_121",
            "s_ids": [
                "s_1013",
                "s_589",
                "s_878",
                "s_61"
            ],
            "type": "rich",
            "x": -0.007924113008103171,
            "y": 0.528618679288948
        },
        {
            "title": "Exploring Multivariate Event Sequences Using Rules, Aggregations, and Selections",
            "data": "Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are some examples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Current exploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariate data only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously by interactively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types of events or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-up glyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defined domain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication traffic and hospital treatments.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745278",
            "id": "r_122",
            "s_ids": [
                "s_991",
                "s_61"
            ],
            "type": "rich",
            "x": 0.1404182121678097,
            "y": 0.250039138760354
        },
        {
            "title": "TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections",
            "data": "Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598445",
            "id": "r_123",
            "s_ids": [
                "s_1305",
                "s_1389",
                "s_931",
                "s_320",
                "s_1143"
            ],
            "type": "rich",
            "x": 0.1757893154995322,
            "y": -0.4515965437046211
        },
        {
            "title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis",
            "data": "Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and effectively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a well-established hybrid second-order Markov chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performance in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and visually explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934630",
            "id": "r_124",
            "s_ids": [
                "s_768",
                "s_525",
                "s_1217",
                "s_346",
                "s_1438",
                "s_710",
                "s_556",
                "s_1273"
            ],
            "type": "rich",
            "x": 0.06068685576604974,
            "y": 0.0984238943429026
        },
        {
            "title": "Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations",
            "data": "Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations \u201cin the wild\u201d often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744320",
            "id": "r_125",
            "s_ids": [
                "s_1024",
                "s_1118",
                "s_1251"
            ],
            "type": "rich",
            "x": 0.09754030632003892,
            "y": -0.27303188433124415
        },
        {
            "title": "Priming and Anchoring Effects in Visualization",
            "data": "We investigate priming and anchoring effects on perceptual tasks in visualization. Priming or anchoring effects depict the phenomena that a stimulus might influence subsequent human judgments on a perceptual level, or on a cognitive level by providing a frame of reference. Using visual class separability in scatterplots as an example task, we performed a set of five studies to investigate the potential existence of priming and anchoring effects. Our findings show that - under certain circumstances - such effects indeed exist. In other words, humans judge class separability of the same scatterplot differently depending on the scatterplot(s) they have seen before. These findings inform future work on better understanding and more accurately modeling human perception of visual patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744138",
            "id": "r_126",
            "s_ids": [
                "s_958",
                "s_856",
                "s_781"
            ],
            "type": "rich",
            "x": 0.10024000316142982,
            "y": -0.3454483400177231
        },
        {
            "title": "What Would a Graph Look Like in this Layout? A Machine Learning Approach to Large Graph Visualization",
            "data": "Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a \u201cgood\u201d layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743858",
            "id": "r_127",
            "s_ids": [
                "s_561",
                "s_973",
                "s_687"
            ],
            "type": "rich",
            "x": -0.36731253665924074,
            "y": -0.21080365742754822
        },
        {
            "title": "Gaze Stripes: Image-Based Visualization of Eye Tracking Data",
            "data": "We present a new visualization approach for displaying eye tracking data from multiple participants. We aim to show the spatio-temporal data of the gaze points in the context of the underlying image or video stimulus without occlusion. Our technique, denoted as gaze stripes, does not require the explicit definition of areas of interest but directly uses the image data around the gaze points, similar to thumbnails for images. A gaze stripe consists of a sequence of such gaze point images, oriented along a horizontal timeline. By displaying multiple aligned gaze stripes, it is possible to analyze and compare the viewing behavior of the participants over time. Since the analysis is carried out directly on the image data, expensive post-processing or manual annotation are not required. Therefore, not only patterns and outliers in the participants' scanpaths can be detected, but the context of the stimulus is available as well. Furthermore, our approach is especially well suited for dynamic stimuli due to the non-aggregated temporal mapping. Complementary views, i.e., markers, notes, screenshots, histograms, and results from automatic clustering, can be added to the visualization to display analysis results. We illustrate the usefulness of our technique on static and dynamic stimuli. Furthermore, we discuss the limitations and scalability of our approach in comparison to established visualization techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468091",
            "id": "r_128",
            "s_ids": [
                "s_1463",
                "s_576",
                "s_62",
                "s_1337",
                "s_299",
                "s_138"
            ],
            "type": "rich",
            "x": -0.0017186587152473053,
            "y": 0.047548466431621375
        },
        {
            "title": "HOLA: Human-like Orthogonal Network Layout",
            "data": "Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new \u201chuman-centred\u201d methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467451",
            "id": "r_129",
            "s_ids": [
                "s_130",
                "s_202",
                "s_1188",
                "s_1326"
            ],
            "type": "rich",
            "x": -0.40364151504096885,
            "y": -0.3925143126019502
        },
        {
            "title": "Temporal MDS Plots for Analysis of Multivariate Data",
            "data": "Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or financial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identification of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the field of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467553",
            "id": "r_130",
            "s_ids": [
                "s_37",
                "s_859",
                "s_493",
                "s_1003"
            ],
            "type": "rich",
            "x": -0.01501541861979499,
            "y": 0.4473462748607602
        },
        {
            "title": "An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data",
            "data": "Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an incremental DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer's mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934433",
            "id": "r_131",
            "s_ids": [
                "s_1201",
                "s_783",
                "s_1192",
                "s_1109",
                "s_1325",
                "s_687"
            ],
            "type": "rich",
            "x": -0.24783337629619673,
            "y": 0.2307860041979988
        },
        {
            "title": "Comparing Similarity Perception in Time Series Visualizations",
            "data": "A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865077",
            "id": "r_132",
            "s_ids": [
                "s_1413",
                "s_373",
                "s_1481",
                "s_161"
            ],
            "type": "rich",
            "x": 0.07724719390360625,
            "y": 0.40237681358300914
        },
        {
            "title": "Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters",
            "data": "Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865020",
            "id": "r_133",
            "s_ids": [
                "s_1285",
                "s_1107",
                "s_479",
                "s_1344",
                "s_279",
                "s_116",
                "s_1059",
                "s_882",
                "s_1250"
            ],
            "type": "rich",
            "x": 0.15854501239055457,
            "y": 0.19142116660570788
        },
        {
            "title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics",
            "data": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745078",
            "id": "r_134",
            "s_ids": [
                "s_8",
                "s_25",
                "s_17",
                "s_1164",
                "s_45",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.18742158136393197,
            "y": -0.1523451510645608
        },
        {
            "title": "LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets",
            "data": "Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-2-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-3-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis) and the variation of LTS in structures (the combination of<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-4-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-5-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744098",
            "id": "r_135",
            "s_ids": [
                "s_279",
                "s_1399",
                "s_1250",
                "s_1180",
                "s_1380",
                "s_41",
                "s_1288"
            ],
            "type": "rich",
            "x": -0.3876766247891934,
            "y": 0.17265359990993434
        },
        {
            "title": "Bubble Treemaps for Uncertainty Visualization",
            "data": "We present a novel type of circular treemap, where we intentionally allocate extra space for additional visual variables. With this extended visual design space, we encode hierarchically structured data along with their uncertainties in a combined diagram. We introduce a hierarchical and force-based circle-packing algorithm to compute Bubble Treemaps, where each node is visualized using nested contour arcs. Bubble Treemaps do not require any color or shading, which offers additional design choices. We explore uncertainty visualization as an application of our treemaps using standard error and Monte Carlo-based statistical models. To this end, we discuss how uncertainty propagates within hierarchies. Furthermore, we show the effectiveness of our visualization using three different examples: the package structure of Flare, the S&amp;P 500 index, and the US consumer expenditure survey.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743959",
            "id": "r_136",
            "s_ids": [
                "s_232",
                "s_533",
                "s_138",
                "s_1220"
            ],
            "type": "rich",
            "x": -0.4517971592817379,
            "y": -0.11358422820972096
        },
        {
            "title": "Visual Analysis of MOOC Forums with iForum",
            "data": "Discussion forums of Massive Open Online Courses (MOOC) provide great opportunities for students to interact with instructional staff as well as other students. Exploration of MOOC forum data can offer valuable insights for these staff to enhance the course and prepare the next release. However, it is challenging due to the large, complicated, and heterogeneous nature of relevant datasets, which contain multiple dynamically interacting objects such as users, posts, and threads, each one including multiple attributes. In this paper, we present a design study for developing an interactive visual analytics system, called iForum, that allows for effectively discovering and understanding temporal patterns in MOOC forums. The design study was conducted with three domain experts in an iterative manner over one year, including a MOOC instructor and two official teaching assistants. iForum offers a set of novel visualization designs for presenting the three interleaving aspects of MOOC forums (i.e., posts, users, and threads) at three different scales. To demonstrate the effectiveness and usefulness of iForum, we describe a case study involving field experts, in which they use iForum to investigate real MOOC forum data for a course on JAVA programming.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598444",
            "id": "r_137",
            "s_ids": [
                "s_225",
                "s_1283",
                "s_896",
                "s_137"
            ],
            "type": "rich",
            "x": 0.5014162879626838,
            "y": -0.44422322576394296
        },
        {
            "title": "Visual Encodings of Temporal Uncertainty: A Comparative User Study",
            "data": "A number of studies have investigated different ways of visualizing uncertainty. However, in the temporal dimension, it is still an open question how to best represent uncertainty, since the special characteristics of time require special visual encodings and may provoke different interpretations. Thus, we have conducted a comprehensive study comparing alternative visual encodings of intervals with uncertain start and end times: gradient plots, violin plots, accumulated probability plots, error bars, centered error bars, and ambiguation. Our results reveal significant differences in error rates and completion time for these different visualization types and different tasks. We recommend using ambiguation - using a lighter color value to represent uncertain regions - or error bars for judging durations and temporal bounds, and gradient plots - using fading color or transparency - for judging probability values.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467752",
            "id": "r_138",
            "s_ids": [
                "s_181",
                "s_81",
                "s_465",
                "s_1087"
            ],
            "type": "rich",
            "x": 0.3874148160288047,
            "y": 0.04581185033611926
        },
        {
            "title": "Visualization as Seen through its Research Paper Keywords",
            "data": "We present the results of a comprehensive multi-pass analysis of visualization paper keywords supplied by authors for their papers published in the IEEE Visualization conference series (now called IEEE VIS) between 1990-2015. From this analysis we derived a set of visualization topics that we discuss in the context of the current taxonomy that is used to categorize papers and assign reviewers in the IEEE VIS reviewing process. We point out missing and overemphasized topics in the current taxonomy and start a discussion on the importance of establishing common visualization terminology. Our analysis of research topics in visualization can, thus, serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an online query tool (http://keyvis.org/) that allows visualization researchers to easily browse the 3952 keywords used for IEEE VIS papers since 1990 to find related work or make informed keyword choices.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598827",
            "id": "r_139",
            "s_ids": [
                "s_1029",
                "s_1205",
                "s_781",
                "s_1128",
                "s_1434"
            ],
            "type": "rich",
            "x": 0.6776532518181192,
            "y": -0.1671228455724575
        },
        {
            "title": "Visual Mementos: Reflecting Memories with Personal Data",
            "data": "In this paper we discuss the creation of visual mementos as a new application area for visualization. We define visual mementos as visualizations of personally relevant data for the purpose of reminiscing, and sharing of life experiences. Today more people collect digital information about their life than ever before. The shift from physical to digital archives poses new challenges and opportunities for self-reflection and self-representation. Drawing on research on autobiographical memory and on the role of artifacts in reminiscing, we identified design challenges for visual mementos: mapping data to evoke familiarity, expressing subjectivity, and obscuring sensitive details for sharing. Visual mementos can make use of the known strengths of visualization in revealing patterns to show the familiar instead of the unexpected, and extend representational mappings beyond the objective to include the more subjective. To understand whether people's subjective views on their past can be reflected in a visual representation, we developed, deployed and studied a technology probe that exemplifies our concept of visual mementos. Our results show how reminiscing has been supported and reveal promising new directions for self-reflection and sharing through visual mementos of personal experiences.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467831",
            "id": "r_140",
            "s_ids": [
                "s_427",
                "s_887",
                "s_864",
                "s_853"
            ],
            "type": "rich",
            "x": 0.38475689979198585,
            "y": -0.2913997940221968
        },
        {
            "title": "TimeSpan: Using Visualization to Explore Temporal Multi-dimensional Data of Stroke Patients",
            "data": "We present TimeSpan, an exploratory visualization tool designed to gain a better understanding of the temporal aspects of the stroke treatment process. Working with stroke experts, we seek to provide a tool to help improve outcomes for stroke victims. Time is of critical importance in the treatment of acute ischemic stroke patients. Every minute that the artery stays blocked, an estimated 1.9 million neurons and 12 km of myelinated axons are destroyed. Consequently, there is a critical need for efficiency of stroke treatment processes. Optimizing time to treatment requires a deep understanding of interval times. Stroke health care professionals must analyze the impact of procedures, events, and patient attributes on time-ultimately, to save lives and improve quality of life after stroke. First, we interviewed eight domain experts, and closely collaborated with two of them to inform the design of TimeSpan. We classify the analytical tasks which a visualization tool should support and extract design goals from the interviews and field observations. Based on these tasks and the understanding gained from the collaboration, we designed TimeSpan, a web-based tool for exploring multi-dimensional and temporal stroke data. We describe how TimeSpan incorporates factors from stacked bar graphs, line charts, histograms, and a matrix visualization to create an interactive hybrid view of temporal data. From feedback collected from domain experts in a focus group session, we reflect on the lessons we learned from abstracting the tasks and iteratively designing TimeSpan.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467325",
            "id": "r_141",
            "s_ids": [
                "s_110",
                "s_334",
                "s_871",
                "s_99",
                "s_853"
            ],
            "type": "rich",
            "x": -0.2037680991557885,
            "y": -0.20097023969361194
        },
        {
            "title": "A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations",
            "data": "Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages \u201cinstance-level explanations\u201d, measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585720",
            "id": "r_142",
            "s_ids": [
                "s_448",
                "s_1284",
                "s_487",
                "s_664",
                "s_457"
            ],
            "type": "rich",
            "x": 0.05736703293452883,
            "y": -0.40189199225453426
        },
        {
            "title": "The Visual Causality Analyst: An Interactive Interface for Causal Reasoning",
            "data": "Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst - a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467931",
            "id": "r_143",
            "s_ids": [
                "s_918",
                "s_252"
            ],
            "type": "rich",
            "x": 0.026654937955900137,
            "y": -0.2014971830522456
        },
        {
            "title": "FeatureInsight: Visual support for error-driven feature ideation in text classification",
            "data": "Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347637",
            "id": "r_144",
            "s_ids": [
                "s_893",
                "s_1332",
                "s_250",
                "s_1031",
                "s_66",
                "s_850"
            ],
            "type": "rich",
            "x": 0.1436047462283452,
            "y": -0.250782186472775
        },
        {
            "title": "Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches",
            "data": "We present the results of two perception studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during such short glances, in particular with such a limited display space of smartwatches. To shed light on this question, we conducted two perception studies that assessed the lower bounds of task time for a simple data comparison task. We tested three chart types common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies, we found that participants performed the task on average in &lt;;300 ms for the bar chart, &lt;;220 ms for the donut chart, and in &lt;; 1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.35\u00d7 higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865142",
            "id": "r_145",
            "s_ids": [
                "s_1221",
                "s_59",
                "s_161",
                "s_250",
                "s_1029"
            ],
            "type": "rich",
            "x": 0.2599039218499285,
            "y": -0.3042531770521576
        },
        {
            "title": "Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework",
            "data": "Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745080",
            "id": "r_146",
            "s_ids": [
                "s_773",
                "s_830",
                "s_1160",
                "s_1003",
                "s_1476"
            ],
            "type": "rich",
            "x": 0.24433733822790704,
            "y": -0.6564107388923384
        },
        {
            "title": "Visplause: Visual Data Quality Assessment of Many Time Series Using Plausibility Checks",
            "data": "Trends like decentralized energy production lead to an exploding number of time series from sensors and other sources that need to be assessed regarding their data quality (DQ). While the identification of DQ problems for such routinely collected data is typically based on existing automated plausibility checks, an efficient inspection and validation of check results for hundreds or thousands of time series is challenging. The main contribution of this paper is the validated design of Visplause, a system to support an efficient inspection of DQ problems for many time series. The key idea of Visplause is to utilize meta-information concerning the semantics of both the time series and the plausibility checks for structuring and summarizing results of DQ checks in a flexible way. Linked views enable users to inspect anomalies in detail and to generate hypotheses about possible causes. The design of Visplause was guided by goals derived from a comprehensive task analysis with domain experts in the energy sector. We reflect on the design process by discussing design decisions at four stages and we identify lessons learned. We also report feedback from domain experts after using Visplause for a period of one month. This feedback suggests significant efficiency gains for DQ assessment, increased confidence in the DQ, and the applicability of Visplause to summarize indicators also outside the context of DQ.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598592",
            "id": "r_147",
            "s_ids": [
                "s_597",
                "s_1470",
                "s_410",
                "s_270"
            ],
            "type": "rich",
            "x": -0.016441966656325593,
            "y": -0.11407572422284659
        },
        {
            "title": "An Uncertainty-Aware Approach for Exploratory Microblog Retrieval",
            "data": "Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467554",
            "id": "r_148",
            "s_ids": [
                "s_476",
                "s_301",
                "s_485",
                "s_750",
                "s_858",
                "s_1252"
            ],
            "type": "rich",
            "x": 0.2475917421393085,
            "y": -0.14795424448929473
        },
        {
            "title": "EventAction: Visual analytics for temporal event sequence recommendation",
            "data": "Recommender systems are being widely used to assist people in making decisions, for example, recommending films to watch or books to buy. Despite its ubiquity, the problem of presenting the recommendations of temporal event sequences has not been studied. We propose EventAction, which to our knowledge, is the first attempt at a prescriptive analytics interface designed to present and explain recommendations of temporal event sequences. EventAction provides a visual analytics approach to (1) identify similar records, (2) explore potential outcomes, (3) review recommended temporal event sequences that might help achieve the users' goals, and (4) interactively assist users as they define a personalized action plan associated with a probability of success. Following the design study framework, we designed and deployed EventAction in the context of student advising and reported on the evaluation with a student review manager and three graduate students.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883512",
            "id": "r_149",
            "s_ids": [
                "s_1131",
                "s_1336",
                "s_1000",
                "s_223"
            ],
            "type": "rich",
            "x": 0.445644366299187,
            "y": 0.10492760449982774
        },
        {
            "title": "Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics",
            "data": "Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934631",
            "id": "r_150",
            "s_ids": [
                "s_41",
                "s_193",
                "s_587",
                "s_708"
            ],
            "type": "rich",
            "x": -0.1659058914192702,
            "y": -0.5174698380265934
        },
        {
            "title": "Visual Progression Analysis of Event Sequence Data",
            "data": "Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864885",
            "id": "r_151",
            "s_ids": [
                "s_400",
                "s_624",
                "s_851",
                "s_1131",
                "s_1228",
                "s_866"
            ],
            "type": "rich",
            "x": 0.09130796078469705,
            "y": 0.02088900063413824
        },
        {
            "title": "Evaluation of Graph Sampling: A Visualization Perspective",
            "data": "Graph sampling is frequently used to address scalability issues when analyzing large graphs. Many algorithms have been proposed to sample graphs, and the performance of these algorithms has been quantified through metrics based on graph structural properties preserved by the sampling: degree distribution, clustering coefficient, and others. However, a perspective that is missing is the impact of these sampling strategies on the resultant visualizations. In this paper, we present the results of three user studies that investigate how sampling strategies influence node-link visualizations of graphs. In particular, five sampling strategies widely used in the graph mining literature are tested to determine how well they preserve visual features in node-link diagrams. Our results show that depending on the sampling strategy used different visual features are preserved. These results provide a complimentary view to metric evaluations conducted in the graph mining literature and provide an impetus to conduct future visualization studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598867",
            "id": "r_152",
            "s_ids": [
                "s_453",
                "s_866",
                "s_402",
                "s_1423",
                "s_137",
                "s_896"
            ],
            "type": "rich",
            "x": -0.22229585274341856,
            "y": -0.3850262667955821
        },
        {
            "title": "Isosurface Visualization of Data with Nonparametric Models for Uncertainty",
            "data": "The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467958",
            "id": "r_153",
            "s_ids": [
                "s_1485",
                "s_800",
                "s_160"
            ],
            "type": "rich",
            "x": -0.5798402092670063,
            "y": 0.3467633719373721
        },
        {
            "title": "Task-Driven Comparison of Topic Models",
            "data": "Topic modeling, a method of statistically extracting thematic content from a large collection of texts, is used for a wide variety of tasks within text analysis. Though there are a growing number of tools and techniques for exploring single models, comparisons between models are generally reduced to a small set of numerical metrics. These metrics may or may not reflect a model's performance on the analyst's intended task, and can therefore be insufficient to diagnose what causes differences between models. In this paper, we explore task-centric topic model comparison, considering how we can both provide detail for a more nuanced understanding of differences and address the wealth of tasks for which topic models are used. We derive comparison tasks from single-model uses of topic models, which predominantly fall into the categories of understanding topics, understanding similarity, and understanding change. Finally, we provide several visualization techniques that facilitate these tasks, including buddy plots, which combine color and position encodings to allow analysts to readily view changes in document similarity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467618",
            "id": "r_154",
            "s_ids": [
                "s_617",
                "s_158"
            ],
            "type": "rich",
            "x": 0.004302713292463413,
            "y": -0.45390752157923364
        },
        {
            "title": "In Situ Eddy Analysis in a High-Resolution Ocean Climate Model",
            "data": "An eddy is a feature associated with a rotating body of fluid, surrounded by a ring of shearing fluid. In the ocean, eddies are 10 to 150 km in diameter, are spawned by boundary currents and baroclinic instabilities, may live for hundreds of days, and travel for hundreds of kilometers. Eddies are important in climate studies because they transport heat, salt, and nutrients through the world's oceans and are vessels of biological productivity. The study of eddies in global ocean-climate models requires large-scale, high-resolution simulations. This poses a problem for feasible (timely) eddy analysis, as ocean simulations generate massive amounts of data, causing a bottleneck for traditional analysis workflows. To enable eddy studies, we have developed an in situ workflow for the quantitative and qualitative analysis of MPAS-Ocean, a high-resolution ocean climate model, in collaboration with the ocean model research and development process. Planned eddy analysis at high spatial and temporal resolutions will not be possible with a postprocessing workflow due to various constraints, such as storage size and I/O time, but the in situ workflow enables it and scales well to ten-thousand processing elements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467411",
            "id": "r_155",
            "s_ids": [
                "s_649",
                "s_248",
                "s_557",
                "s_1162",
                "s_1070",
                "s_1150"
            ],
            "type": "rich",
            "x": -0.4634272915535319,
            "y": 0.4929713162260245
        },
        {
            "title": "Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline",
            "data": "Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic. This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand for automated infographics design. As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e., the representation, scale, layout, and orientation of the timeline, and 2) the local information, i.e., the location, category, and pixels of each visual element on the timeline. At the reconstruction stage, we propose a pipeline with three techniques, i.e., Non-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934810",
            "id": "r_156",
            "s_ids": [
                "s_1457",
                "s_640",
                "s_403",
                "s_1115",
                "s_137"
            ],
            "type": "rich",
            "x": 0.2978659088001453,
            "y": -0.17355920577291745
        },
        {
            "title": "Situ: Identifying and Explaining Suspicious Behavior in Networks",
            "data": "Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865029",
            "id": "r_157",
            "s_ids": [
                "s_423",
                "s_356",
                "s_577",
                "s_1379",
                "s_1489",
                "s_401",
                "s_313",
                "s_803"
            ],
            "type": "rich",
            "x": 0.11809138088551517,
            "y": -0.03695906867982468
        },
        {
            "title": "Looks Good To Me: Visualizations As Sanity Checks",
            "data": "Famous examples such as Anscombe's Quartet highlight that one of the core benefits of visualizations is allowing people to discover visual patterns that might otherwise be hidden by summary statistics. This visual inspection is particularly important in exploratory data analysis, where analysts can use visualizations such as histograms and dot plots to identify data quality issues. Yet, these visualizations are driven by parameters such as histogram bin size or mark opacity that have a great deal of impact on the final visual appearance of the chart, but are rarely optimized to make important features visible. In this paper, we show that data flaws have varying impact on the visual features of visualizations, and that the adversarial or merely uncritical setting of design parameters of visualizations can obscure the visual signatures of these flaws. Drawing on the framework of Algebraic Visualization Design, we present the results of a crowdsourced study showing that common visualization types can appear to reasonably summarize distributional data while hiding large and important flaws such as missing data and extraneous modes. We make use of these results to propose additional best practices for visualizations of distributions for data quality tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864907",
            "id": "r_158",
            "s_ids": [
                "s_1215",
                "s_834",
                "s_1431",
                "s_529"
            ],
            "type": "rich",
            "x": 0.5766925139509925,
            "y": -0.013053167873122768
        },
        {
            "title": "PROACT: Iterative Design of a Patient-Centered Visualization for Effective Prostate Cancer Health Risk Communication",
            "data": "Prostate cancer is the most common cancer among men in the US, and yet most cases represent localized cancer for which the optimal treatment is unclear. Accumulating evidence suggests that the available treatment options, including surgery and conservative treatment, result in a similar prognosis for most men with localized prostate cancer. However, approximately 90% of patients choose surgery over conservative treatment, despite the risk of severe side effects like erectile dysfunction and incontinence. Recent medical research suggests that a key reason is the lack of patient-centered tools that can effectively communicate personalized risk information and enable them to make better health decisions. In this paper, we report the iterative design process and results of developing the PROgnosis Assessment for Conservative Treatment (PROACT) tool, a personalized health risk communication tool for localized prostate cancer patients. PROACT utilizes two published clinical prediction models to communicate the patients' personalized risk estimates and compare treatment options. In collaboration with the Maine Medical Center, we conducted two rounds of evaluations with prostate cancer survivors and urologists to identify the design elements and narrative structure that effectively facilitate patient comprehension under emotional distress. Our results indicate that visualization can be an effective means to communicate complex risk information to patients with low numeracy and visual literacy. However, the visualizations need to be carefully chosen to balance readability with ease of comprehension. In addition, due to patients' charged emotional state, an intuitive narrative structure that considers the patients' information need is critical to aid the patients' comprehension of their risk information.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598588",
            "id": "r_159",
            "s_ids": [
                "s_593",
                "s_124",
                "s_758",
                "s_143",
                "s_1002",
                "s_114",
                "s_321"
            ],
            "type": "rich",
            "x": -0.27636248026109167,
            "y": -0.3775020849524219
        },
        {
            "title": "TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text",
            "data": "We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467531",
            "id": "r_160",
            "s_ids": [
                "s_1172",
                "s_12",
                "s_802"
            ],
            "type": "rich",
            "x": 0.3651075381774799,
            "y": -0.11873446418889583
        },
        {
            "title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management",
            "data": "Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934655",
            "id": "r_161",
            "s_ids": [
                "s_1285",
                "s_1110",
                "s_1004",
                "s_1477",
                "s_1185",
                "s_116",
                "s_366",
                "s_882",
                "s_1250"
            ],
            "type": "rich",
            "x": 0.2382802578725074,
            "y": 0.1613366061124204
        },
        {
            "title": "A Framework for Creative Visualization-Opportunities Workshops",
            "data": "Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865241",
            "id": "r_162",
            "s_ids": [
                "s_87",
                "s_91",
                "s_499",
                "s_683",
                "s_908"
            ],
            "type": "rich",
            "x": 0.7489519210391619,
            "y": -0.15766015980940756
        },
        {
            "title": "Information Olfactation: Harnessing Scent to Convey Data",
            "data": "Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present viScent: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865237",
            "id": "r_163",
            "s_ids": [
                "s_169",
                "s_669",
                "s_1143"
            ],
            "type": "rich",
            "x": -0.0156084830935829,
            "y": 0.22901271866755307
        },
        {
            "title": "Face to Face: Evaluating Visual Comparison",
            "data": "Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864884",
            "id": "r_164",
            "s_ids": [
                "s_318",
                "s_75",
                "s_1143",
                "s_811"
            ],
            "type": "rich",
            "x": 0.12085066977861153,
            "y": -0.1721015299031243
        },
        {
            "title": "Investigating the Use of a Dynamic Physical Bar Chart for Data Exploration and Presentation",
            "data": "Physical data representations, or data physicalizations, are a promising new medium to represent and communicate data. Previous work mostly studied passive physicalizations which require humans to perform all interactions manually. Dynamic shape-changing displays address this limitation and facilitate data exploration tasks such as sorting, navigating in data sets which exceed the fixed size of a given physical display, or preparing \u201cviews\u201d to communicate insights about data. However, it is currently unclear how people approach and interact with such data representations. We ran an exploratory study to investigate how non-experts made use of a dynamic physical bar chart for an open-ended data exploration and presentation task. We asked 16 participants to explore a data set on European values and to prepare a short presentation of their insights using a physical display. We analyze: (1) users' body movements to understand how they approach and react to the physicalization, (2) their hand-gestures to understand how they interact with physical data, (3) system interactions to understand which subsets of the data they explored and which features they used in the process, and (4) strategies used to explore the data and present observations. We discuss the implications of our findings for the use of dynamic data physicalizations and avenues for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598498",
            "id": "r_165",
            "s_ids": [
                "s_1338",
                "s_1347",
                "s_1422",
                "s_1416",
                "s_246",
                "s_1454"
            ],
            "type": "rich",
            "x": 0.3526680721130471,
            "y": 0.1028466292584972
        },
        {
            "title": "Off the Radar: Comparative Evaluation of Radial Visualization Solutions for Composite Indicators",
            "data": "A composite indicator (CI) is a measuring and benchmark tool used to capture multi-dimensional concepts, such as Information and Communication Technology (ICT) usage. Individual indicators are selected and combined to reflect a phenomena being measured. Visualization of a composite indicator is recommended as a tool to enable interested stakeholders, as well as the public audience, to better understand the indicator components and evolution overtime. However, existing CI visualizations introduce a variety of solutions and there is a lack in CI's visualization guidelines. Radial visualizations are popular among these solutions because of CI's inherent multi-dimensionality. Although in dispute, Radar-charts are often used for CI presentation. However, no empirical evidence on Radar's effectiveness and efficiency for common CI tasks is available. In this paper, we aim to fill this gap by reporting on a controlled experiment that compares the Radar chart technique with two other radial visualization methods: Flowercharts as used in the well-known OECD Betterlife index, and Circle-charts which could be adopted for this purpose. Examples of these charts in the current context are shown in Figure 1. We evaluated these charts, showing the same data with each of the mentioned techniques applying small multiple views for different dimensions of the data. We compared users' performance and preference empirically under a formal task-taxonomy. Results indicate that the Radar chart was the least effective and least liked, while performance of the two other options were mixed and dependent on the task. Results also showed strong preference of participants toward the Flower chart. Summarizing our results, we provide specific design guidelines for composite indicator visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467322",
            "id": "r_166",
            "s_ids": [
                "s_121",
                "s_219",
                "s_627",
                "s_526"
            ],
            "type": "rich",
            "x": 0.22670459973091506,
            "y": -0.07631870395217007
        },
        {
            "title": "Interactive Correction of Mislabeled Training Data",
            "data": "In this paper, we develop a visual analysis method for interactively improving the quality of labeled data, which is essential to the success of supervised and semi-supervised learning. The quality improvement is achieved through the use of user-selected trusted items. We employ a bi-level optimization model to accurately match the labels of the trusted items and to minimize the training loss. Based on this model, a scalable data correction algorithm is developed to handle tens of thousands of labeled data efficiently. The selection of the trusted items is facilitated by an incremental tSNE with improved computational efficiency and layout stability to ensure a smooth transition between different levels. We evaluated our method on real-world datasets through quantitative evaluation and case studies, and the results were generally favorable.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986943",
            "id": "r_167",
            "s_ids": [
                "s_374",
                "s_287",
                "s_279",
                "s_1384",
                "s_1028",
                "s_301"
            ],
            "type": "rich",
            "x": -0.10559431261500506,
            "y": -0.055749200142967324
        },
        {
            "title": "The Anchoring Effect in Decision-Making with Visual Analytics",
            "data": "Anchoring effect is the tendency to focus too heavily on one piece of information when making decisions. In this paper, we present a novel, systematic study and resulting analyses that investigate the effects of anchoring effect on human decision-making using visual analytic systems. Visual analytics interfaces typically contain multiple views that present various aspects of information such as spatial, temporal, and categorical. These views are designed to present complex, heterogeneous data in accessible forms that aid decision-making. However, human decision-making is often hindered by the use of heuristics, or cognitive biases, such as anchoring effect. Anchoring effect can be triggered by the order in which information is presented or the magnitude of information presented. Through carefully designed laboratory experiments, we present evidence of anchoring effect in analysis with visual analytics interfaces when users are primed by representation of different pieces of information. We also describe detailed analyses of users' interaction logs which reveal the impact of anchoring bias on the visual representation preferred and paths of analysis. We discuss implications for future research to possibly detect and alleviate anchoring bias.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585665",
            "id": "r_168",
            "s_ids": [
                "s_1048",
                "s_1060",
                "s_330",
                "s_1471",
                "s_1235",
                "s_293"
            ],
            "type": "rich",
            "x": 0.36849438687544994,
            "y": -0.4517180504531472
        },
        {
            "title": "GPGPU Linear Complexity t-SNE Optimization",
            "data": "In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934307",
            "id": "r_169",
            "s_ids": [
                "s_966",
                "s_206",
                "s_419",
                "s_1157",
                "s_208",
                "s_1054",
                "s_145",
                "s_408"
            ],
            "type": "rich",
            "x": -0.5616522775649796,
            "y": 0.04988598930411283
        },
        {
            "title": "An Interactive Method to Improve Crowdsourced Annotations",
            "data": "In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864843",
            "id": "r_170",
            "s_ids": [
                "s_301",
                "s_703",
                "s_776",
                "s_1268",
                "s_1137"
            ],
            "type": "rich",
            "x": -0.049597290179719436,
            "y": -0.5292514283658989
        },
        {
            "title": "Taking Word Clouds Apart: An Empirical Investigation of the Design Space for Keyword Summaries",
            "data": "In this paper we present a set of four user studies aimed at exploring the visual design space of what we call keyword summaries: lists of words with associated quantitative values used to help people derive an intuition of what information a given document collection (or part of it) may contain. We seek to systematically study how different visual representations may affect people's performance in extracting information out of keyword summaries. To this purpose, we first create a design space of possible visual representations and compare the possible solutions in this design space through a variety of representative tasks and performance metrics. Other researchers have, in the past, studied some aspects of effectiveness with word clouds, however, the existing literature is somewhat scattered and do not seem to address the problem in a sufficiently systematic and holistic manner. The results of our studies showed a strong dependency on the tasks users are performing. In this paper we present details of our methodology, the results, as well as, guidelines on how to design effective keyword summaries based in our discoveries.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2746018",
            "id": "r_171",
            "s_ids": [
                "s_927",
                "s_811",
                "s_457"
            ],
            "type": "rich",
            "x": 0.37581269053589966,
            "y": -0.5104669549742868
        },
        {
            "title": "Hybrid Tactile/Tangible Interaction for 3D Data Exploration",
            "data": "We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined-focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599217",
            "id": "r_172",
            "s_ids": [
                "s_59",
                "s_650",
                "s_1286",
                "s_1205"
            ],
            "type": "rich",
            "x": 0.603797890475278,
            "y": 0.22204421616445164
        },
        {
            "title": "AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings",
            "data": "Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users' complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user's drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users' nonlinear domain knowledge; 2) the underlying model that translates users' input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users' complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598446",
            "id": "r_173",
            "s_ids": [
                "s_1181",
                "s_726",
                "s_8",
                "s_320",
                "s_1068",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.48078325913439307,
            "y": 0.10786991647762609
        },
        {
            "title": "Criteria for Rigor in Visualization Design Study",
            "data": "We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934539",
            "id": "r_174",
            "s_ids": [
                "s_1378",
                "s_499"
            ],
            "type": "rich",
            "x": 0.9062222188989494,
            "y": -0.035721628855641605
        },
        {
            "title": "A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones",
            "data": "We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934397",
            "id": "r_175",
            "s_ids": [
                "s_12",
                "s_250",
                "s_1029",
                "s_797"
            ],
            "type": "rich",
            "x": 0.3649627779703005,
            "y": -0.09356978594398388
        },
        {
            "title": "Visualizing Uncertain Tropical Cyclone Predictions using Representative Samples from Ensembles of Forecast Tracks",
            "data": "A common approach to sampling the space of a prediction is the generation of an ensemble of potential outcomes, where the ensemble's distribution reveals the statistical structure of the prediction space. For example, the US National Hurricane Center generates multiple day predictions for a storm's path, size, and wind speed, and then uses a Monte Carlo approach to sample this prediction into a large ensemble of potential storm outcomes. Various forms of summary visualizations are generated from such an ensemble, often using spatial spread to indicate its statistical characteristics. However, studies have shown that changes in the size of such summary glyphs, representing changes in the uncertainty of the prediction, are frequently confounded with other attributes of the phenomenon, such as its size or strength. In addition, simulation ensembles typically encode multivariate information, which can be difficult or confusing to include in a summary display. This problem can be overcome by directly displaying the ensemble as a set of annotated trajectories, however this solution will not be effective if ensembles are densely overdrawn or structurally disorganized. We propose to overcome these difficulties by selectively sampling the original ensemble, constructing a smaller representative and spatially well organized ensemble. This can be drawn directly as a set of paths that implicitly reveals the underlying spatial uncertainty distribution of the prediction. Since this approach does not use a visual channel to encode uncertainty, additional information can more easily be encoded in the display without leading to visual confusion. To demonstrate our argument, we describe the development of a visualization for ensembles of tropical cyclone forecast tracks, explaining how their spatial and temporal predictions, as well as other crucial storm characteristics such as size and intensity, can be clearly revealed. We verify the effectiveness of this visualization approach through a cognitive study exploring how storm damage estimates are affected by the density of tracks drawn, and by the presence or absence of annotating information on storm size and intensity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865193",
            "id": "r_176",
            "s_ids": [
                "s_1324",
                "s_218",
                "s_84",
                "s_10"
            ],
            "type": "rich",
            "x": -0.1456450394631875,
            "y": 0.26684123454615044
        },
        {
            "title": "Bridging from Goals to Tasks with Design Study Analysis Reports",
            "data": "Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744319",
            "id": "r_177",
            "s_ids": [
                "s_492",
                "s_1208",
                "s_802"
            ],
            "type": "rich",
            "x": 0.30306533894300997,
            "y": -0.1753673984988444
        },
        {
            "title": "AmbiguityVis: Visualization of Ambiguity in Graph Layouts",
            "data": "Node-link diagrams provide an intuitive way to explore networks and have inspired a large number of automated graph layout strategies that optimize aesthetic criteria. However, any particular drawing approach cannot fully satisfy all these criteria simultaneously, producing drawings with visual ambiguities that can impede the understanding of network structure. To bring attention to these potentially problematic areas present in the drawing, this paper presents a technique that highlights common types of visual ambiguities: ambiguous spatial relationships between nodes and edges, visual overlap between community structures, and ambiguity in edge bundling and metanodes. Metrics, including newly proposed metrics for abnormal edge lengths, visual overlap in community structures and node/edge aggregation, are proposed to quantify areas of ambiguity in the drawing. These metrics and others are then displayed using a heatmap-based visualization that provides visual feedback to developers of graph drawing and visualization approaches, allowing them to quickly identify misleading areas. The novel metrics and the heatmap-based visualization allow a user to explore ambiguities in graph layouts from multiple perspectives in order to make reasonable graph layout choices. The effectiveness of the technique is demonstrated through case studies and expert reviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467691",
            "id": "r_178",
            "s_ids": [
                "s_1115",
                "s_1423",
                "s_402",
                "s_1020",
                "s_1055",
                "s_1388",
                "s_137"
            ],
            "type": "rich",
            "x": -0.3388734486963101,
            "y": -0.3360207505026638
        },
        {
            "title": "Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots",
            "data": "Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864912",
            "id": "r_179",
            "s_ids": [
                "s_1059",
                "s_319",
                "s_613",
                "s_173",
                "s_781",
                "s_1339",
                "s_1220",
                "s_1161"
            ],
            "type": "rich",
            "x": -0.1873661870210891,
            "y": -0.23089777857411195
        },
        {
            "title": "BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence",
            "data": "The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864814",
            "id": "r_180",
            "s_ids": [
                "s_1009",
                "s_1377",
                "s_153",
                "s_348",
                "s_754",
                "s_1458",
                "s_338"
            ],
            "type": "rich",
            "x": 0.32292311744535235,
            "y": 0.054283330416573045
        },
        {
            "title": "In Situ Distribution Guided Analysis and Visualization of Transonic Jet Engine Simulations",
            "data": "Study of flow instability in turbine engine compressors is crucial to understand the inception and evolution of engine stall. Aerodynamics experts have been working on detecting the early signs of stall in order to devise novel stall suppression technologies. A state-of-the-art Navier-Stokes based, time-accurate computational fluid dynamics simulator, TURBO, has been developed in NASA to enhance the understanding of flow phenomena undergoing rotating stall. Despite the proven high modeling accuracy of TURBO, the excessive simulation data prohibits post-hoc analysis in both storage and I/O time. To address these issues and allow the expert to perform scalable stall analysis, we have designed an in situ distribution guided stall analysis technique. Our method summarizes statistics of important properties of the simulation data in situ using a probabilistic data modeling scheme. This data summarization enables statistical anomaly detection for flow instability in post analysis, which reveals the spatiotemporal trends of rotating stall for the expert to conceive new hypotheses. Furthermore, the verification of the hypotheses and exploratory visualization using the summarized data are realized using probabilistic visualization techniques such as uncertain isocontouring. Positive feedback from the domain scientist has indicated the efficacy of our system in exploratory stall analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598604",
            "id": "r_181",
            "s_ids": [
                "s_1301",
                "s_571",
                "s_1400",
                "s_353",
                "s_1121"
            ],
            "type": "rich",
            "x": -0.3389186592535523,
            "y": 0.37594867716231123
        },
        {
            "title": "Visualizing Social Media Content with SentenTree",
            "data": "We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598590",
            "id": "r_182",
            "s_ids": [
                "s_1040",
                "s_375",
                "s_735"
            ],
            "type": "rich",
            "x": 0.2520268329867659,
            "y": -0.46012734913770464
        },
        {
            "title": "VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model",
            "data": "Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598497",
            "id": "r_183",
            "s_ids": [
                "s_967",
                "s_380"
            ],
            "type": "rich",
            "x": 0.30023940904035856,
            "y": 0.305119560359556
        },
        {
            "title": "DropoutSeer: Visualizing learning patterns in Massive Open Online Courses for dropout reasoning and prediction",
            "data": "Aiming at massive participation and open access education, Massive Open Online Courses (MOOCs) have attracted millions of learners over the past few years. However, the high dropout rate of learners is considered to be one of the most crucial factors that may hinder the development of MOOCs. To tackle this problem, statistical models have been developed to predict dropout behavior based on learner activity logs. Although predictive models can foresee the dropout behavior, it is still difficult for users to understand the reasons behind the predicted results and further design interventions to prevent dropout. In addition, with a better understanding of dropout, researchers in the area of predictive modeling in turn can improve the models. In this paper, we introduce DropoutSeer, a visual analytics system which not only helps instructors and education experts understand the reasons for dropout, but also allows researchers to identify crucial features which can further improve the performance of the models. Both the heterogeneous data extracted from three different kinds of learner activity logs (i.e., clickstream, forum posts and assignment records) and the predicted results are visualized in the proposed system. Case studies and expert interviews have been conducted to demonstrate the usefulness and effectiveness of DropoutSeer.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883517",
            "id": "r_184",
            "s_ids": [
                "s_411",
                "s_261",
                "s_1309",
                "s_1478",
                "s_716",
                "s_137"
            ],
            "type": "rich",
            "x": 0.14440349308045888,
            "y": -0.5202303827685021
        },
        {
            "title": "Critical Reflections on Visualization Authoring Systems",
            "data": "An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed \u2014Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934281",
            "id": "r_185",
            "s_ids": [
                "s_795",
                "s_250",
                "s_1163",
                "s_1251",
                "s_735",
                "s_106",
                "s_12",
                "s_1080"
            ],
            "type": "rich",
            "x": 0.5745323459638595,
            "y": -0.12292621744928496
        },
        {
            "title": "Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization",
            "data": "In this paper, we investigate Confluent Drawings (CD), a technique for bundling edges in node-link diagrams based on network connectivity. Edge-bundling techniques are designed to reduce edge clutter in node-link diagrams by coalescing lines into common paths or bundles. Unfortunately, traditional bundling techniques introduce ambiguity since edges are only bundled by spatial proximity, rather than network connectivity; following an edge from its source to its target can lead to the perception of incorrect connectivity if edges are not clearly separated within the bundles. Contrary, CDs bundle edges based on common sources or targets. Thus, a smooth path along a confluent bundle indicates precise connectivity. While CDs have been described in theory, practical investigation and application to real-world networks (i.e., networks beyond those with certain planarity restrictions) is currently lacking. Here, we provide the first algorithm for constructing CDs from arbitrary directed and undirected networks and present a simple layout method, embedded in a sand box environment providing techniques for interactive exploration. We then investigate patterns and artifacts in CDs, which we compare to other common edge-bundling techniques. Finally, we present the first user study that compares edge-compression techniques, including CD, power graphs, metro-style, and common edge bundling. We found that users without particular expertise in visualization or network analysis are able to read small CDs without difficulty. Compared to existing bundling techniques, CDs are more likely to allow people to correctly perceive connectivity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598958",
            "id": "r_186",
            "s_ids": [
                "s_1126",
                "s_845",
                "s_589",
                "s_1188",
                "s_202"
            ],
            "type": "rich",
            "x": -0.5023604952594161,
            "y": -0.22459896757797376
        },
        {
            "title": "The Data Context Map: Fusing Data and Attributes into a Unified Display",
            "data": "Numerous methods have been described that allow the visualization of the data matrix. But all suffer from a common problem - observing the data points in the context of the attributes is either impossible or inaccurate. We describe a method that allows these types of comprehensive layouts. We achieve it by combining two similarity matrices typically used in isolation - the matrix encoding the similarity of the attributes and the matrix encoding the similarity of the data points. This combined matrix yields two of the four submatrices needed for a full multi-dimensional scaling type layout. The remaining two submatrices are obtained by creating a fused similarity matrix - one that measures the similarity of the data points with respect to the attributes, and vice versa. The resulting layout places the data objects in direct context of the attributes and hence we call it the data context map. It allows users to simultaneously appreciate (1) the similarity of data objects, (2) the similarity of attributes in the specific scope of the collection of data objects, and (3) the relationships of data objects with attributes and vice versa. The contextual layout also allows data regions to be segmented and labeled based on the locations of the attributes. This enables, for example, the map's application in selection tasks where users seek to identify one or more data objects that best fit a certain configuration of factors, using the map to visually balance the tradeoffs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467552",
            "id": "r_187",
            "s_ids": [
                "s_1408",
                "s_252"
            ],
            "type": "rich",
            "x": -0.1433821889559619,
            "y": 0.056524936148057515
        },
        {
            "title": "Analyzing the Noise Robustness of Deep Neural Networks",
            "data": "Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802509",
            "id": "r_188",
            "s_ids": [
                "s_476",
                "s_301",
                "s_899",
                "s_630",
                "s_1436"
            ],
            "type": "rich",
            "x": -0.3195003738205065,
            "y": -0.5985932420698917
        },
        {
            "title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
            "data": "Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934547",
            "id": "r_189",
            "s_ids": [
                "s_1081",
                "s_603",
                "s_230",
                "s_1263",
                "s_634",
                "s_442",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.4088421875288951,
            "y": -0.10228380351427875
        },
        {
            "title": "RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis",
            "data": "We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865043",
            "id": "r_190",
            "s_ids": [
                "s_204",
                "s_1243",
                "s_563",
                "s_163",
                "s_824",
                "s_140",
                "s_61"
            ],
            "type": "rich",
            "x": -0.10543500211647909,
            "y": -0.3791229401453079
        },
        {
            "title": "Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs",
            "data": "During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745279",
            "id": "r_191",
            "s_ids": [
                "s_1283",
                "s_284",
                "s_1029",
                "s_182",
                "s_253"
            ],
            "type": "rich",
            "x": 0.5309288430813528,
            "y": -0.26861715713979667
        },
        {
            "title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods",
            "data": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598544",
            "id": "r_192",
            "s_ids": [
                "s_1284",
                "s_195",
                "s_672",
                "s_962",
                "s_861",
                "s_670",
                "s_720"
            ],
            "type": "rich",
            "x": 0.5065864959890992,
            "y": -0.34611980051602403
        },
        {
            "title": "Poemage: Visualizing the Sonic Topology of a Poem",
            "data": "The digital humanities have experienced tremendous growth within the last decade, mostly in the context of developing computational tools that support what is called distant reading - collecting and analyzing huge amounts of textual data for synoptic evaluation. On the other end of the spectrum is a practice at the heart of the traditional humanities, close reading - the careful, in-depth analysis of a single text in order to extract, engage, and even generate as much productive meaning as possible. The true value of computation to close reading is still very much an open question. During a two-year design study, we explored this question with several poetry scholars, focusing on an investigation of sound and linguistic devices in poetry. The contributions of our design study include a problem characterization and data abstraction of the use of sound in poetry as well as Poemage, a visualization tool for interactively exploring the sonic topology of a poem. The design of Poemage is grounded in the evaluation of a series of technology probes we deployed to our poetry collaborators, and we validate the final design with several case studies that illustrate the disruptive impact technology can have on poetry scholarship. Finally, we also contribute a reflection on the challenges we faced conducting visualization research in literary studies.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467811",
            "id": "r_193",
            "s_ids": [
                "s_717",
                "s_14",
                "s_463",
                "s_908"
            ],
            "type": "rich",
            "x": 0.6377005689516743,
            "y": -0.4814226018988383
        },
        {
            "title": "CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds",
            "data": "We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467202",
            "id": "r_194",
            "s_ids": [
                "s_1174",
                "s_1098",
                "s_1029",
                "s_1205"
            ],
            "type": "rich",
            "x": -0.12409694247451412,
            "y": 0.06954733887327585
        },
        {
            "title": "Suggested Interactivity: Seeking Perceived Affordances for Information Visualization",
            "data": "In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467201",
            "id": "r_195",
            "s_ids": [
                "s_433",
                "s_384",
                "s_1290",
                "s_100"
            ],
            "type": "rich",
            "x": 0.7147826245095428,
            "y": -0.25703207338023687
        },
        {
            "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes",
            "data": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934267",
            "id": "r_196",
            "s_ids": [
                "s_1374",
                "s_1109",
                "s_777",
                "s_137",
                "s_1325"
            ],
            "type": "rich",
            "x": -0.001203704788243429,
            "y": -0.5157022510789303
        },
        {
            "title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution",
            "data": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864769",
            "id": "r_197",
            "s_ids": [
                "s_773",
                "s_1160",
                "s_1220",
                "s_1003",
                "s_1476"
            ],
            "type": "rich",
            "x": -0.05468511519923656,
            "y": -0.5792814742221286
        },
        {
            "title": "Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks",
            "data": "Complex networks require effective tools and visualizations for their analysis and comparison. Clique communities have been recognized as a powerful concept for describing cohesive structures in networks. We propose an approach that extends the computation of clique communities by considering persistent homology, a topological paradigm originally introduced to characterize and compare the global structure of shapes. Our persistence-based algorithm is able to detect clique communities and to keep track of their evolution according to different edge weight thresholds. We use this information to define comparison metrics and a new centrality measure, both reflecting the relevance of the clique communities inherent to the network. Moreover, we propose an interactive visualization tool based on nested graphs that is capable of compactly representing the evolving relationships between communities for different thresholds and clique degrees. We demonstrate the effectiveness of our approach on various network types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744321",
            "id": "r_198",
            "s_ids": [
                "s_52",
                "s_1361",
                "s_32",
                "s_35"
            ],
            "type": "rich",
            "x": -0.256237408517993,
            "y": -0.4875025306003711
        },
        {
            "title": "Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells",
            "data": "This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744278",
            "id": "r_199",
            "s_ids": [
                "s_102",
                "s_590",
                "s_603",
                "s_660",
                "s_357",
                "s_1346",
                "s_336"
            ],
            "type": "rich",
            "x": -0.6440611306420294,
            "y": -0.07084886795401969
        },
        {
            "title": "Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations",
            "data": "Evaluating the effectiveness of data visualizations is a challenging undertaking and often relies on one-off studies that test a visualization in the context of one specific task. Researchers across the fields of data science, visualization, and human-computer interaction are calling for foundational tools and principles that could be applied to assessing the effectiveness of data visualizations in a more rapid and generalizable manner. One possibility for such a tool is a model of visual saliency for data visualizations. Visual saliency models are typically based on the properties of the human visual cortex and predict which areas of a scene have visual features (e.g. color, luminance, edges) that are likely to draw a viewer's attention. While these models can accurately predict where viewers will look in a natural scene, they typically do not perform well for abstract data visualizations. In this paper, we discuss the reasons for the poor performance of existing saliency models when applied to data visualizations. We introduce the Data Visualization Saliency (DVS) model, a saliency model tailored to address some of these weaknesses, and we test the performance of the DVS model and existing saliency models by comparing the saliency maps produced by the models to eye tracking data obtained from human viewers. Finally, we describe how modified saliency models could be used as general tools for assessing the effectiveness of visualizations, including the strengths and weaknesses of this approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743939",
            "id": "r_200",
            "s_ids": [
                "s_957",
                "s_840",
                "s_1038",
                "s_1112",
                "s_555"
            ],
            "type": "rich",
            "x": 0.42324734252091134,
            "y": -0.19485516072908587
        },
        {
            "title": "Probabilistic Graph Layout for Uncertain Network Visualization",
            "data": "We present a novel uncertain network visualization technique based on node-link diagrams. Nodes expand spatially in our probabilistic graph layout, depending on the underlying probability distributions of edges. The visualization is created by computing a two-dimensional graph embedding that combines samples from the probabilistic graph. A Monte Carlo process is used to decompose a probabilistic graph into its possible instances and to continue with our graph layout technique. Splatting and edge bundling are used to visualize point clouds and network topology. The results provide insights into probability distributions for the entire network-not only for individual nodes and edges. We validate our approach using three data sets that represent a wide range of network types: synthetic data, protein-protein interactions from the STRING database, and travel times extracted from Google Maps. Our approach reveals general limitations of the force-directed layout and allows the user to recognize that some nodes of the graph are at a specific position just by chance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598919",
            "id": "r_201",
            "s_ids": [
                "s_533",
                "s_1382",
                "s_232",
                "s_1220",
                "s_259",
                "s_138"
            ],
            "type": "rich",
            "x": -0.3385059036105818,
            "y": -0.41184344627307456
        },
        {
            "title": "Map LineUps: Effects of spatial structure on graphical inference",
            "data": "Fundamental to the effective use of visualization as an analytic and descriptive tool is the assurance that presenting data visually provides the capability of making inferences from what we see. This paper explores two related approaches to quantifying the confidence we may have in making visual inferences from mapped geospatial data. We adapt Wickham et al.'s `Visual Line-up' method as a direct analogy with Null Hypothesis Significance Testing (NHST) and propose a new approach for generating more credible spatial null hypotheses. Rather than using as a spatial null hypothesis the unrealistic assumption of complete spatial randomness, we propose spatially autocorrelated simulations as alternative nulls. We conduct a set of crowdsourced experiments (n=361) to determine the just noticeable difference (JND) between pairs of choropleth maps of geographic units controlling for spatial autocorrelation (Moran's I statistic) and geometric configuration (variance in spatial unit area). Results indicate that people's abilities to perceive differences in spatial autocorrelation vary with baseline autocorrelation structure and the geometric configuration of geographic units. These results allow us, for the first time, to construct a visual equivalent of statistical power for geospatial data. Our JND results add to those provided in recent years by Klippel et al. (2011), Harrison et al. (2014) and Kay &amp;amp; Heer (2015) for correlation visualization. Importantly, they provide an empirical basis for an improved construction of visual line-ups for maps and the development of theory to inform geospatial tests of graphical inference.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598862",
            "id": "r_202",
            "s_ids": [
                "s_1184",
                "s_499",
                "s_470",
                "s_1349",
                "s_85",
                "s_1089"
            ],
            "type": "rich",
            "x": -0.046305322209892474,
            "y": 0.02782540472189775
        },
        {
            "title": "VAiRoma: A Visual Analytics System for Making Sense of Places, Times, and Events in Roman History",
            "data": "Learning and gaining knowledge of Roman history is an area of interest for students and citizens at large. This is an example of a subject with great sweep (with many interrelated sub-topics over, in this case, a 3,000 year history) that is hard to grasp by any individual and, in its full detail, is not available as a coherent story. In this paper, we propose a visual analytics approach to construct a data driven view of Roman history based on a large collection of Wikipedia articles. Extracting and enabling the discovery of useful knowledge on events, places, times, and their connections from large amounts of textual data has always been a challenging task. To this aim, we introduce VAiRoma, a visual analytics system that couples state-of-the-art text analysis methods with an intuitive visual interface to help users make sense of events, places, times, and more importantly, the relationships between them. VAiRoma goes beyond textual content exploration, as it permits users to compare, make connections, and externalize the findings all within the visual interface. As a result, VAiRoma allows users to learn and create new knowledge regarding Roman history in an informed way. We evaluated VAiRoma with 16 participants through a user study, with the task being to learn about roman piazzas through finding relevant articles and new relationships. Our study results showed that the VAiRoma system enables the participants to find more relevant articles and connections compared to Web searches and literature search conducted in a roman library. Subjective feedback on VAiRoma was also very positive. In addition, we ran two case studies that demonstrate how VAiRoma can be used for deeper analysis, permitting the rapid discovery and analysis of a small number of key documents even when the original collection contains hundreds of thousands of documents.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467971",
            "id": "r_203",
            "s_ids": [
                "s_1048",
                "s_293",
                "s_381",
                "s_200",
                "s_237"
            ],
            "type": "rich",
            "x": 0.405896724382911,
            "y": -0.4625817908594322
        },
        {
            "title": "A Deep Generative Model for Graph Layout",
            "data": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934396",
            "id": "r_204",
            "s_ids": [
                "s_561",
                "s_687"
            ],
            "type": "rich",
            "x": -0.048022627941936895,
            "y": -0.30764256445525356
        },
        {
            "title": "Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization",
            "data": "We present an improved stress majorization method that incorporates various constraints, including directional constraints without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to parallelize computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. As a result, we allow the constraint-based exploration of large graphs with 10K nodes - an approach which previous methods cannot support.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745919",
            "id": "r_205",
            "s_ids": [
                "s_1059",
                "s_131",
                "s_870",
                "s_699",
                "s_746",
                "s_1339",
                "s_781",
                "s_1220",
                "s_1161"
            ],
            "type": "rich",
            "x": -0.4467363856507499,
            "y": -0.19697907114567076
        },
        {
            "title": "TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees",
            "data": "Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745158",
            "id": "r_206",
            "s_ids": [
                "s_410",
                "s_371",
                "s_1434",
                "s_270"
            ],
            "type": "rich",
            "x": -0.5803833841339449,
            "y": -0.07286875222732044
        },
        {
            "title": "Iterating between Tools to Create and Edit Visualizations",
            "data": "A common workflow for visualization designers begins with a generative tool, like D3 or Processing, to create the initial visualization; and proceeds to a drawing tool, like Adobe Illustrator or Inkscape, for editing and cleaning. Unfortunately, this is typically a one-way process: once a visualization is exported from the generative tool into a drawing tool, it is difficult to make further, data-driven changes. In this paper, we propose a bridge model to allow designers to bring their work back from the drawing tool to re-edit in the generative tool. Our key insight is to recast this iteration challenge as a merge problem - similar to when two people are editing a document and changes between them need to reconciled. We also present a specific instantiation of this model, a tool called Hanpuku, which bridges between D3 scripts and Illustrator. We show several examples of visualizations that are iteratively created using Hanpuku in order to illustrate the flexibility of the approach. We further describe several hypothetical tools that bridge between other visualization tools to emphasize the generality of the model.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598609",
            "id": "r_207",
            "s_ids": [
                "s_925",
                "s_1031",
                "s_1011",
                "s_908"
            ],
            "type": "rich",
            "x": 0.3994605397312098,
            "y": -0.1202825071581707
        },
        {
            "title": "Visualizing Multiple Variables Across Scale and Geography",
            "data": "Comparing multiple variables to select those that effectively characterize complex entities is important in a wide variety of domains - geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend visual parameter space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467199",
            "id": "r_208",
            "s_ids": [
                "s_91",
                "s_499",
                "s_1349",
                "s_85"
            ],
            "type": "rich",
            "x": -0.127401782894667,
            "y": 0.2049619580635106
        },
        {
            "title": "Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses",
            "data": "In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of \u201cTropical Cyclone Karl\u201d, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745178",
            "id": "r_209",
            "s_ids": [
                "s_308",
                "s_883",
                "s_329",
                "s_578",
                "s_111",
                "s_1390"
            ],
            "type": "rich",
            "x": -0.17132662102811017,
            "y": 0.5400070944237542
        },
        {
            "title": "BiSet: Semantic Edge Bundling with Biclusters for Sensemaking",
            "data": "Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships, such as between lists of entities, require repetitious manual selection and significant mental aggregation in cluttered visualizations to find coordinated relationships. In this paper, we present BiSet, a visual analytics technique to support interactive exploration of coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset. Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer task-oriented semantic insights about potentially coordinated activities. We make bundles as first class objects and add a new layer, \u201cin-between\u201d, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467813",
            "id": "r_210",
            "s_ids": [
                "s_112",
                "s_688",
                "s_207",
                "s_1407"
            ],
            "type": "rich",
            "x": 0.08546230190165782,
            "y": -0.1723294327031771
        },
        {
            "title": "Guidelines for Effective Usage of Text Highlighting Techniques",
            "data": "Semi-automatic text analysis involves manual inspection of text. Often, different text annotations (like part-of-speech or named entities) are indicated by using distinctive text highlighting techniques. In typesetting there exist well-known formatting conventions, such as bold typeface, italics, or background coloring, that are useful for highlighting certain parts of a given text. Also, many advanced techniques for visualization and highlighting of text exist; yet, standard typesetting is common, and the effects of standard typesetting on the perception of text are not fully understood. As such, we surveyed and tested the effectiveness of common text highlighting techniques, both individually and in combination, to discover how to maximize pop-out effects while minimizing visual interference between techniques. To validate our findings, we conducted a series of crowd-sourced experiments to determine: i) a ranking of nine commonly-used text highlighting techniques; ii) the degree of visual interference between pairs of text highlighting techniques; iii) the effectiveness of techniques for visual conjunctive search. Our results show that increasing font size works best as a single highlighting technique, and that there are significant visual interferences between some pairs of highlighting techniques. We discuss the pros and cons of different combinations as a design guideline to choose text highlighting techniques for text viewers.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467759",
            "id": "r_211",
            "s_ids": [
                "s_869",
                "s_34",
                "s_1181",
                "s_493",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.36881564110388243,
            "y": -0.5545911084676151
        },
        {
            "title": "Illusion of Causality in Visualized Data",
            "data": "Students who eat breakfast more frequently tend to have a higher grade point average. From this data, many people might confidently state that a before-school breakfast program would lead to higher grades. This is a reasoning error, because correlation does not necessarily indicate causation \u2013 X and Y can be correlated without one directly causing the other. While this error is pervasive, its prevalence might be amplified or mitigated by the way that the data is presented to a viewer. Across three crowdsourced experiments, we examined whether how simple data relations are presented would mitigate this reasoning error. The first experiment tested examples similar to the breakfast-GPA relation, varying in the plausibility of the causal link. We asked participants to rate their level of agreement that the relation was correlated, which they rated appropriately as high. However, participants also expressed high agreement with a causal interpretation of the data. Levels of support for the causal interpretation were not equally strong across visualization types: causality ratings were highest for text descriptions and bar graphs, but weaker for scatter plots. But is this effect driven by bar graphs aggregating data into two groups or by the visual encoding type? We isolated data aggregation versus visual encoding type and examined their individual effect on perceived causality. Overall, different visualization designs afford different cognitive reasoning affordances across the same data. High levels of data aggregation by graphs tend to be associated with higher perceived causality in data. Participants perceived line and dot visual encodings as more causal than bar encodings. Our results demonstrate how some visualization designs trigger stronger causal links while choosing others can help mitigate unwarranted perceptions of causality.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934399",
            "id": "r_212",
            "s_ids": [
                "s_1119",
                "s_623",
                "s_1272",
                "s_811"
            ],
            "type": "rich",
            "x": 0.2652350675768256,
            "y": -0.39793890428348166
        },
        {
            "title": "FairSight: Visual Analytics for Fairness in Decision Making",
            "data": "Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions \u2013 understanding, measuring, diagnosing and mitigating biases \u2013 that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934262",
            "id": "r_213",
            "s_ids": [
                "s_1356",
                "s_1177"
            ],
            "type": "rich",
            "x": 0.20038473638308696,
            "y": -0.5304777849867198
        },
        {
            "title": "HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples",
            "data": "This paper presents an interactive visualization interface-HiPiler-for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like \u201csnippets\u201d. Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745978",
            "id": "r_214",
            "s_ids": [
                "s_1219",
                "s_1126",
                "s_418",
                "s_341",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.14572103857195665,
            "y": 0.15576177170093763
        },
        {
            "title": "VIGOR: Interactive Visual Exploration of Graph Query Results",
            "data": "Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744898",
            "id": "r_215",
            "s_ids": [
                "s_668",
                "s_1005",
                "s_1469",
                "s_1264",
                "s_564",
                "s_584",
                "s_1039",
                "s_295"
            ],
            "type": "rich",
            "x": 0.04391951188260182,
            "y": -0.30281870062601507
        },
        {
            "title": "EVA: Visual Analytics to Identify Fraudulent Events",
            "data": "Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744758",
            "id": "r_216",
            "s_ids": [
                "s_18",
                "s_181",
                "s_1087",
                "s_392",
                "s_1113",
                "s_49",
                "s_998"
            ],
            "type": "rich",
            "x": 0.27987858881806793,
            "y": 0.20268261947671729
        },
        {
            "title": "Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization",
            "data": "Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage - and its evaluation - of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744459",
            "id": "r_217",
            "s_ids": [
                "s_459"
            ],
            "type": "rich",
            "x": 0.7736846885707783,
            "y": -0.07748314587617032
        },
        {
            "title": "Gaussian Cubes: Real-Time Modeling for Visual Exploration of Large Multidimensional Datasets",
            "data": "Recently proposed techniques have finally made it possible for analysts to interactively explore very large datasets in real time. However powerful, the class of analyses these systems enable is somewhat limited: specifically, one can only quickly obtain plots such as histograms and heatmaps. In this paper, we contribute Gaussian Cubes, which significantly improves on state-of-the-art systems by providing interactive modeling capabilities, which include but are not limited to linear least squares and principal components analysis (PCA). The fundamental insight in Gaussian Cubes is that instead of precomputing counts of many data subsets (as state-of-the-art systems do), Gaussian Cubes precomputes the best multivariate Gaussian for the respective data subsets. As an example, Gaussian Cubes can fit hundreds of models over millions of data points in well under a second, enabling novel types of visual exploration of such large datasets. We present three case studies that highlight the visualization and analysis capabilities in Gaussian Cubes, using earthquake safety simulations, astronomical catalogs, and transportation statistics. The dataset sizes range around one hundred million elements and 5 to 10 dimensions. We present extensive performance results, a discussion of the limitations in Gaussian Cubes, and future research directions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598694",
            "id": "r_218",
            "s_ids": [
                "s_1224",
                "s_644",
                "s_1271",
                "s_68",
                "s_529"
            ],
            "type": "rich",
            "x": -0.15247530407068305,
            "y": 0.3775710447752722
        },
        {
            "title": "A Visual Analytics Approach for Understanding Reasons behind Snowballing and Comeback in MOBA Games",
            "data": "To design a successful Multiplayer Online Battle Arena (MOBA) game, the ratio of snowballing and comeback occurrences to all matches played must be maintained at a certain level to ensure its fairness and engagement. Although it is easy to identify these two types of occurrences, game developers often find it difficult to determine their causes and triggers with so many game design choices and game parameters involved. In addition, the huge amounts of MOBA game data are often heterogeneous, multi-dimensional and highly dynamic in terms of space and time, which poses special challenges for analysts. In this paper, we present a visual analytics system to help game designers find key events and game parameters resulting in snowballing or comeback occurrences in MOBA game data. We follow a user-centered design process developing the system with game analysts and testing with real data of a trial version MOBA game from NetEase Inc. We apply novel visualization techniques in conjunction with well-established ones to depict the evolution of players' positions, status and the occurrences of events. Our system can reveal players' strategies and performance throughout a single match and suggest patterns, e.g., specific player' actions and game events, that have led to the final occurrences. We further demonstrate a workflow of leveraging human analyzed patterns to improve the scalability and generality of match data analysis. Finally, we validate the usability of our system by proving the identified patterns are representative in snowballing or comeback matches in a one-month-long MOBA tournament dataset.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598415",
            "id": "r_219",
            "s_ids": [
                "s_477",
                "s_1260",
                "s_951",
                "s_640",
                "s_903",
                "s_137",
                "s_1415"
            ],
            "type": "rich",
            "x": 0.33929849470998574,
            "y": 0.2784643178600948
        },
        {
            "title": "Visually Comparing Weather Features in Forecasts",
            "data": "Meteorologists process and analyze weather forecasts using visualization in order to examine the behaviors of and relationships among weather features. In this design study conducted with meteorologists in decision support roles, we identified and attempted to address two significant common challenges in weather visualization: the employment of inconsistent and often ineffective visual encoding practices across a wide range of visualizations, and a lack of support for directly visualizing how different weather features relate across an ensemble of possible forecast outcomes. In this work, we present a characterization of the problems and data associated with meteorological forecasting, we propose a set of informed default encoding choices that integrate existing meteorological conventions with effective visualization practice, and we extend a set of techniques as an initial step toward directly visualizing the interactions of multiple features over an ensemble forecast. We discuss the integration of these contributions into a functional prototype tool, and also reflect on the many practical challenges that arise when working with weather data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467754",
            "id": "r_220",
            "s_ids": [
                "s_397",
                "s_908"
            ],
            "type": "rich",
            "x": 0.15070072413776875,
            "y": 0.44493590938121863
        },
        {
            "title": "Optimal Sets of Projections of High-Dimensional Data",
            "data": "Finding good projections of n-dimensional datasets into a 2D visualization domain is one of the most important problems in Information Visualization. Users are interested in getting maximal insight into the data by exploring a minimal number of projections. However, if the number is too small or improper projections are used, then important data patterns might be overlooked. We propose a data-driven approach to find minimal sets of projections that uniquely show certain data patterns. For this we introduce a dissimilarity measure of data projections that discards affine transformations of projections and prevents repetitions of the same data patterns. Based on this, we provide complete data tours of at most n/2 projections. Furthermore, we propose optimal paths of projection matrices for an interactive data exploration. We illustrate our technique with a set of state-of-the-art real high-dimensional benchmark datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467132",
            "id": "r_221",
            "s_ids": [
                "s_118",
                "s_697"
            ],
            "type": "rich",
            "x": -0.2779760164211147,
            "y": 0.3593518383866945
        },
        {
            "title": "D-Map: Visual Analysis of Ego-centric Information Diffusion Patterns in Social Media",
            "data": "Popular social media platforms could rapidly propagate vital information over social networks among a significant number of people. In this work we present D-Map (Diffusion Map), a novel visualization method to support exploration and analysis of social behaviors during such information diffusion and propagation on typical social media through a map metaphor. In D-Map, users who participated in reposting (i.e., resending a message initially posted by others) one central user's posts (i.e., a series of original tweets) are collected and mapped to a hexagonal grid based on their behavior similarities and in chronological order of the repostings. With additional interaction and linking, D-Map is capable of providing visual portraits of the influential users and describing their social behaviors. A comprehensive visual analysis system is developed to support interactive exploration with D-Map. We evaluate our work with real world social media data and find interesting patterns among users. Key players, important information diffusion paths, and interactions among social communities can be identified.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883510",
            "id": "r_222",
            "s_ids": [
                "s_316",
                "s_364",
                "s_1018",
                "s_1206",
                "s_1214",
                "s_866",
                "s_1022"
            ],
            "type": "rich",
            "x": 0.29731949303304595,
            "y": -0.002533509574781418
        },
        {
            "title": "Evaluating \u2018Graphical Perception\u2019 with CNNs",
            "data": "Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865138",
            "id": "r_223",
            "s_ids": [
                "s_1176",
                "s_1355",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.2751398506754556,
            "y": -0.45449320625328055
        },
        {
            "title": "A Utility-Aware Visual Approach for Anonymizing Multi-Attribute Tabular Data",
            "data": "Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745139",
            "id": "r_224",
            "s_ids": [
                "s_276",
                "s_783",
                "s_1250",
                "s_431",
                "s_1014",
                "s_1443",
                "s_687"
            ],
            "type": "rich",
            "x": 0.341858184664433,
            "y": -0.09970780102902245
        },
        {
            "title": "Conceptual and Methodological Issues in Evaluating Multidimensional Visualizations for Decision Support",
            "data": "We explore how to rigorously evaluate multidimensional visualizations for their ability to support decision making. We first define multi-attribute choice tasks, a type of decision task commonly performed with such visualizations. We then identify which of the existing multidimensional visualizations are compatible with such tasks, and set out to evaluate three elementary visualizations: parallel coordinates, scatterplot matrices and tabular visualizations. Our method consists in first giving participants low-level analytic tasks, in order to ensure that they properly understood the visualizations and their interactions. Participants are then given multi-attribute choice tasks consisting of choosing holiday packages. We assess decision support through multiple objective and subjective metrics, including a decision accuracy metric based on the consistency between the choice made and self-reported preferences for attributes. We found the three visualizations to be comparable on most metrics, with a slight advantage for tabular visualizations. In particular, tabular visualizations allow participants to reach decisions faster. Thus, although decision time is typically not central in assessing decision support, it can be used as a tie-breaker when visualizations achieve similar decision accuracy. Our results also suggest that indirect methods for assessing choice confidence may allow to better distinguish between visualizations than direct ones. We finally discuss the limitations of our methods and directions for future work, such as the need for more sensitive metrics of decision support.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745138",
            "id": "r_225",
            "s_ids": [
                "s_194",
                "s_161",
                "s_231"
            ],
            "type": "rich",
            "x": 0.38256397170572803,
            "y": -0.17895276583540992
        },
        {
            "title": "SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering",
            "data": "Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744238",
            "id": "r_226",
            "s_ids": [
                "s_336",
                "s_590",
                "s_603",
                "s_1368",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.5610464452949334,
            "y": 0.48368554310049855
        },
        {
            "title": "Visually Exploring Transportation Schedules",
            "data": "Public transportation schedules are designed by agencies to optimize service quality under multiple constraints. However, real service usually deviates from the plan. Therefore, transportation analysts need to identify, compare and explain both eventual and systemic performance issues that must be addressed so that better timetables can be created. The purely statistical tools commonly used by analysts pose many difficulties due to the large number of attributes at tripand station-level for planned and real service. Also challenging is the need for models at multiple scales to search for patterns at different times and stations, since analysts do not know exactly where or when relevant patterns might emerge and need to compute statistical summaries for multiple attributes at different granularities. To aid in this analysis, we worked in close collaboration with a transportation expert to design TR-EX, a visual exploration tool developed to identify, inspect and compare spatio-temporal patterns for planned and real transportation service. TR-EX combines two new visual encodings inspired by Marey's Train Schedule: Trips Explorer for trip-level analysis of frequency, deviation and speed; and Stops Explorer for station-level study of delay, wait time, reliability and performance deficiencies such as bunching. To tackle overplotting and to provide a robust representation for a large numbers of trips and stops at multiple scales, the system supports variable kernel bandwidths to achieve the level of detail required by users for different tasks. We justify our design decisions based on specific analysis needs of transportation analysts. We provide anecdotal evidence of the efficacy of TR-EX through a series of case studies that explore NYC subway service, which illustrate how TR-EX can be used to confirm hypotheses and derive new insights through visual exploration.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467592",
            "id": "r_227",
            "s_ids": [
                "s_1218",
                "s_544",
                "s_380",
                "s_631"
            ],
            "type": "rich",
            "x": 0.21811356756984462,
            "y": 0.4734178852562762
        },
        {
            "title": "Glyph-Based Comparative Visualization for Diffusion Tensor Fields",
            "data": "Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467435",
            "id": "r_228",
            "s_ids": [
                "s_1403",
                "s_605",
                "s_1467",
                "s_145",
                "s_408"
            ],
            "type": "rich",
            "x": -0.6928676826725801,
            "y": 0.15588184705189276
        },
        {
            "title": "Association Analysis for Visual Exploration of Multivariate Scientific Data Sets",
            "data": "The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467431",
            "id": "r_229",
            "s_ids": [
                "s_471",
                "s_353"
            ],
            "type": "rich",
            "x": -0.08626639428144099,
            "y": 0.15388974674559494
        },
        {
            "title": "InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations",
            "data": "We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934312",
            "id": "r_230",
            "s_ids": [
                "s_995",
                "s_765",
                "s_865",
                "s_1093",
                "s_353",
                "s_428",
                "s_1420",
                "s_727"
            ],
            "type": "rich",
            "x": -0.2675944139287413,
            "y": 0.4709266053027217
        },
        {
            "title": "Dynamic Composite Data Physicalization Using Wheeled Micro-Robots",
            "data": "This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865159",
            "id": "r_231",
            "s_ids": [
                "s_314",
                "s_334",
                "s_439",
                "s_100",
                "s_231"
            ],
            "type": "rich",
            "x": 0.5109291401594718,
            "y": 0.33284543422711493
        },
        {
            "title": "SRVis: Towards Better Spatial Integration in Ranking Visualization",
            "data": "Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865126",
            "id": "r_232",
            "s_ids": [
                "s_638",
                "s_1179",
                "s_1190",
                "s_1103",
                "s_368",
                "s_1273"
            ],
            "type": "rich",
            "x": 0.08091455601423832,
            "y": 0.03317947981743198
        },
        {
            "title": "Design Exposition with Literate Visualization",
            "data": "We propose a new approach to the visualization design and communication process, literate visualization, based upon and extending, Donald Knuth's idea of literate programming. It integrates the process of writing data visualization code with description of the design choices that led to the implementation (design exposition). We develop a model of design exposition characterised by four visualization designer architypes: the evaluator, the autonomist, the didacticist and the rationalist. The model is used to justify the key characteristics of literate visualization: `notebook' documents that integrate live coding input, rendered output and textual narrative; low cost of authoring textual narrative; guidelines to encourage structured visualization design and its documentation. We propose narrative schemas for structuring and validating a wide range of visualization design approaches and models, and branching narratives for capturing alternative designs and design views. We describe a new open source literate visualization environment, litvis, based on a declarative interface to Vega and Vega-Lite through the functional programming language Elm combined with markdown for formatted narrative. We informally assess the approach, its implementation and potential by considering three examples spanning a range of design abstractions: new visualization idioms; validation though visualization algebra; and feminist data visualization. We argue that the rich documentation of the design process provided by literate visualization offers the potential to improve the validity of visualization design and so benefit both academic visualization and visualization practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864836",
            "id": "r_233",
            "s_ids": [
                "s_1089",
                "s_1254",
                "s_499"
            ],
            "type": "rich",
            "x": 0.8047815037180184,
            "y": -0.14102665835072253
        },
        {
            "title": "Instant Construction and Visualization of Crowded Biological Environments",
            "data": "We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744258",
            "id": "r_234",
            "s_ids": [
                "s_449",
                "s_311",
                "s_1227",
                "s_503",
                "s_849",
                "s_519",
                "s_443"
            ],
            "type": "rich",
            "x": -0.6443935929835178,
            "y": 0.16878116984163197
        },
        {
            "title": "EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection",
            "data": "Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802454",
            "id": "r_235",
            "s_ids": [
                "s_477",
                "s_1222",
                "s_1385",
                "s_92",
                "s_1465",
                "s_1415"
            ],
            "type": "rich",
            "x": -0.27531934252935747,
            "y": -0.3339437386121548
        },
        {
            "title": "NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models",
            "data": "With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865230",
            "id": "r_236",
            "s_ids": [
                "s_340",
                "s_393",
                "s_814",
                "s_989",
                "s_297",
                "s_438"
            ],
            "type": "rich",
            "x": -0.24364046171325002,
            "y": -0.6622868149737612
        },
        {
            "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks",
            "data": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865139",
            "id": "r_237",
            "s_ids": [
                "s_1250",
                "s_527",
                "s_558",
                "s_1159",
                "s_637",
                "s_279",
                "s_255"
            ],
            "type": "rich",
            "x": -0.03172324978607264,
            "y": -0.35116863683142
        },
        {
            "title": "Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading",
            "data": "Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865119",
            "id": "r_238",
            "s_ids": [
                "s_416",
                "s_1080",
                "s_1143"
            ],
            "type": "rich",
            "x": 0.5635902027811861,
            "y": -0.49751712815704296
        },
        {
            "title": "A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications",
            "data": "Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865026",
            "id": "r_239",
            "s_ids": [
                "s_1258",
                "s_692",
                "s_252"
            ],
            "type": "rich",
            "x": -0.06446157073691032,
            "y": 0.30618843624006664
        },
        {
            "title": "VIS Author Profiles: Interactive Descriptions of Publication Records Combining Text and Visualization",
            "data": "Publication records and collaboration networks are important for assessing the expertise and experience of researchers. Existing digital libraries show the raw publication lists in author profiles, whereas visualization techniques focus on specific subproblems. Instead, we look at publication records from various perspectives mixing low-level publication data with high-level abstractions and background information. This work presents VIS Author Profiles, a novel approach to generate integrated textual and visual descriptions to highlight patterns in publication records. We leverage template-based natural language generation to summarize notable publication statistics, evolution of research topics, and collaboration relationships. Seamlessly integrated visualizations augment the textual description and are interactively connected with each other and the text. The underlying publication data and detailed explanations of the analysis are available on demand. We compare our approach to existing systems by taking into account information needs of users and demonstrate its usefulness in two realistic application examples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865022",
            "id": "r_240",
            "s_ids": [
                "s_15",
                "s_383"
            ],
            "type": "rich",
            "x": 0.43729912940780674,
            "y": -0.37234956683938386
        },
        {
            "title": "SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data",
            "data": "Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744738",
            "id": "r_241",
            "s_ids": [
                "s_846",
                "s_453",
                "s_896",
                "s_348",
                "s_127",
                "s_1115",
                "s_1197",
                "s_137"
            ],
            "type": "rich",
            "x": 0.08668722037303239,
            "y": -0.12452326552974023
        },
        {
            "title": "Visualizing Nonlinear Narratives with Story Curves",
            "data": "In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744118",
            "id": "r_242",
            "s_ids": [
                "s_1263",
                "s_1126",
                "s_1364",
                "s_1371",
                "s_1138",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.5659859301950018,
            "y": -0.07188494997602897
        },
        {
            "title": "Visualization of Time-Varying Weather Ensembles across Multiple Resolutions",
            "data": "Uncertainty quantification in climate ensembles is an important topic for the domain scientists, especially for decision making in the real-world scenarios. With powerful computers, simulations now produce time-varying and multi-resolution ensemble data sets. It is of extreme importance to understand the model sensitivity given the input parameters such that more computation power can be allocated to the parameters with higher influence on the output. Also, when ensemble data is produced at different resolutions, understanding the accuracy of different resolutions helps the total time required to produce a desired quality solution with improved storage and computation cost. In this work, we propose to tackle these non-trivial problems on the Weather Research and Forecasting (WRF) model output. We employ a moment independent sensitivity measure to quantify and analyze parameter sensitivity across spatial regions and time domain. A comparison of clustering structures across three resolutions enables the users to investigate the sensitivity variation over the spatial regions of the five input parameters. The temporal trend in the sensitivity values is explored via an MDS view linked with a line chart for interactive brushing. The spatial and temporal views are connected to provide a full exploration system for complete spatio-temporal sensitivity analysis. To analyze the accuracy across varying resolutions, we formulate a Bayesian approach to identify which regions are better predicted at which resolutions compared to the observed precipitation. This information is aggregated over the time domain and finally encoded in an output image through a custom color map that guides the domain experts towards an adaptive grid implementation given a cost model. Users can select and further analyze the spatial and temporal error patterns for multi-resolution accuracy analysis via brushing and linking on the produced image. In this work, we collaborate with a domain expert whose feedback shows the effectiveness of our proposed exploration work-flow.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598869",
            "id": "r_243",
            "s_ids": [
                "s_789",
                "s_936",
                "s_471",
                "s_353"
            ],
            "type": "rich",
            "x": -0.22184199168340943,
            "y": 0.23525685297722962
        },
        {
            "title": "NeuroBlocks - Visual Tracking of Segmentation and Proofreading for Large Connectomics Projects",
            "data": "In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467441",
            "id": "r_244",
            "s_ids": [
                "s_590",
                "s_603",
                "s_1176",
                "s_94",
                "s_1034",
                "s_1346",
                "s_336"
            ],
            "type": "rich",
            "x": -0.617298371279736,
            "y": -0.20498363541515546
        },
        {
            "title": "Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis",
            "data": "Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467436",
            "id": "r_245",
            "s_ids": [
                "s_1301",
                "s_353"
            ],
            "type": "rich",
            "x": -0.2551610245521099,
            "y": 0.19363525506926513
        },
        {
            "title": "Urbane: A 3D framework to support data driven decision making in urban development",
            "data": "Architects working with developers and city planners typically rely on experience, precedent and data analyzed in isolation when making decisions that impact the character of a city. These decisions are critical in enabling vibrant, sustainable environments but must also negotiate a range of complex political and social forces. This requires those shaping the built environment to balance maximizing the value of a new development with its impact on the character of a neighborhood. As a result architects are focused on two issues throughout the decision making process: a) what defines the character of a neighborhood? and b) how will a new development change its neighborhood? In the first, character can be influenced by a variety of factors and understanding the interplay between diverse data sets is crucial; including safety, transportation access, school quality and access to entertainment. In the second, the impact of a new development is measured, for example, by how it impacts the view from the buildings that surround it. In this paper, we work in collaboration with architects to design Urbane, a 3-dimensional multi-resolution framework that enables a data-driven approach for decision making in the design of new urban development. This is accomplished by integrating multiple data layers and impact analysis techniques facilitating architects to explore and assess the effect of these attributes on the character and value of a neighborhood. Several of these data layers, as well as impact analysis, involve working in 3-dimensions and operating in real time. Efficient computation and visualization is accomplished through the use of techniques from computer graphics. We demonstrate the effectiveness of Urbane through a case study of development in Manhattan depicting how a data-driven understanding of the value and impact of speculative buildings can benefit the design-development process between architects, planners and developers.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347636",
            "id": "r_246",
            "s_ids": [
                "s_644",
                "s_904",
                "s_29",
                "s_1439",
                "s_748",
                "s_1072",
                "s_567",
                "s_380"
            ],
            "type": "rich",
            "x": 0.23687494954781144,
            "y": 0.23426451635551487
        },
        {
            "title": "Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation",
            "data": "In the first crowdsourced visualization experiment conducted exclusively on mobile phones, we compare approaches to visualizing ranges over time on small displays. People routinely consume such data via a mobile phone, from temperatures in weather forecasting apps to sleep and blood pressure readings in personal health apps. However, we lack guidance on how to effectively visualize ranges on small displays in the context of different value retrieval and comparison tasks, or with respect to different data characteristics such as periodicity, seasonality, or the cardinality of ranges. Central to our experiment is a comparison between two ways to lay out ranges: a more conventional linear layout strikes a balance between quantitative and chronological scale resolution, while a less conventional radial layout emphasizes the cyclicality of time and may prioritize discrimination between values at its periphery. With results from 87 crowd workers, we found that while participants completed tasks more quickly with linear layouts than with radial ones, there were few differences in terms of error rate between layout conditions. We also found that participants performed similarly with both layouts in tasks that involved comparing superimposed observed and average ranges.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865234",
            "id": "r_247",
            "s_ids": [
                "s_12",
                "s_250",
                "s_1029",
                "s_797"
            ],
            "type": "rich",
            "x": 0.15109815302372914,
            "y": -0.11554256913934446
        },
        {
            "title": "Mitigating the Attraction Effect with Visualizations",
            "data": "Human decisions are prone to biases, and this is no less true for decisions made within data visualizations. Bias mitigation strategies often focus on the person, by educating people about their biases, typically with little success. We focus instead on the system, presenting the first evidence that altering the design of an interactive visualization tool can mitigate a strong bias - the attraction effect. Participants viewed 2D scatterplots where choices between superior alternatives were affected by the placement of other suboptimal points. We found that highlighting the superior alternatives weakened the bias, but did not eliminate it. We then tested an interactive approach where participants completely removed locally dominated points from the view, inspired by the elimination by aspects strategy in the decision-making literature. This approach strongly decreased the bias, leading to a counterintuitive suggestion: tools that allow removing inappropriately salient or distracting data from a view may help lead users to make more rational decisions.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865233",
            "id": "r_248",
            "s_ids": [
                "s_194",
                "s_82",
                "s_161",
                "s_811"
            ],
            "type": "rich",
            "x": 0.28570642950209846,
            "y": -0.43292614999866685
        },
        {
            "title": "Vistrates: A Component Model for Ubiquitous Analytics",
            "data": "Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components-the building blocks of this model-can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic \u201canytime\u201d and \u201canywhere\u201d motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices..",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865144",
            "id": "r_249",
            "s_ids": [
                "s_416",
                "s_185",
                "s_1449",
                "s_1296",
                "s_1143"
            ],
            "type": "rich",
            "x": 0.5602201527490492,
            "y": 0.04390067800758507
        },
        {
            "title": "iStoryline: Effective Convergence to Hand-drawn Storylines",
            "data": "Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite the improvement in the performance and extension of its application area. Existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals do not always produce optimal results when compared to hand-drawn storylines. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and check whether the visual elements in hand-drawn illustrations can be mapped back to appropriate narrative contexts. We also compared the hand-drawn storylines with storylines generated by the state-of-the-art methods and found they have significant differences. Our findings led to a design space that summarizes (1) how artists utilize narrative elements and (2) the sequence of actions artists follow to portray expressive and attractive storylines. We developed iStoryline, an authoring tool for integrating high-level user interactions into optimization algorithms and achieving a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to create novel storyline visualizations easily according to their preferences by modifying the automatically generated layouts. The effectiveness and usability of iStoryline are studied with qualitative evaluations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864899",
            "id": "r_250",
            "s_ids": [
                "s_398",
                "s_1051",
                "s_378",
                "s_896",
                "s_1174",
                "s_1273"
            ],
            "type": "rich",
            "x": 0.5226795638422316,
            "y": -0.29993686519559104
        },
        {
            "title": "EdWordle: Consistency-Preserving Word Cloud Editing",
            "data": "We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745859",
            "id": "r_251",
            "s_ids": [
                "s_1059",
                "s_741",
                "s_173",
                "s_699",
                "s_1220",
                "s_1161",
                "s_781"
            ],
            "type": "rich",
            "x": 0.10742308701454838,
            "y": -0.45430512601395273
        },
        {
            "title": "Imagining Replications: Graphical Prediction & Discrete Visualizations Improve Recall & Estimation of Effect Uncertainty",
            "data": "People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an interactive, graphical uncertainty prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one's ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples' estimates of probability distributions and to communicate uncertainty effectively.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743898",
            "id": "r_252",
            "s_ids": [
                "s_1272",
                "s_920",
                "s_150",
                "s_646"
            ],
            "type": "rich",
            "x": 0.232078066212411,
            "y": -0.25014583572621535
        },
        {
            "title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations",
            "data": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598543",
            "id": "r_253",
            "s_ids": [
                "s_1283",
                "s_284",
                "s_1134",
                "s_182",
                "s_253"
            ],
            "type": "rich",
            "x": 0.4203621581603212,
            "y": -0.2838730464664093
        },
        {
            "title": "3D Regression Heat Map Analysis of Population Study Data",
            "data": "Epidemiological studies comprise heterogeneous data about a subject group to define disease-specific risk factors. These data contain information (features) about a subject's lifestyle, medical status as well as medical image data. Statistical regression analysis is used to evaluate these features and to identify feature combinations indicating a disease (the target feature). We propose an analysis approach of epidemiological data sets by incorporating all features in an exhaustive regression-based analysis. This approach combines all independent features w.r.t. a target feature. It provides a visualization that reveals insights into the data by highlighting relationships. The 3D Regression Heat Map, a novel 3D visual encoding, acts as an overview of the whole data set. It shows all combinations of two to three independent features with a specific target disease. Slicing through the 3D Regression Heat Map allows for the detailed analysis of the underlying relationships. Expert knowledge about disease-specific hypotheses can be included into the analysis by adjusting the regression model formulas. Furthermore, the influences of features can be assessed using a difference view comparing different calculation results. We applied our 3D Regression Heat Map method to a hepatic steatosis data set to reproduce results from a data mining-driven analysis. A qualitative analysis was conducted on a breast density data set. We were able to derive new hypotheses about relations between breast density and breast lesions with breast cancer. With the 3D Regression Heat Map, we present a visual overview of epidemiological data that allows for the first time an interactive regression-based analysis of large feature sets with respect to a disease.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468291",
            "id": "r_254",
            "s_ids": [
                "s_570",
                "s_1467",
                "s_132",
                "s_1025",
                "s_1213",
                "s_1084",
                "s_619"
            ],
            "type": "rich",
            "x": -0.25613918223562143,
            "y": -0.1815591922709251
        },
        {
            "title": "Cluster Analysis of Vortical Flow in Simulations of Cerebral Aneurysm Hemodynamics",
            "data": "Computational fluid dynamic (CFD) simulations of blood flow provide new insights into the hemodynamics of vascular pathologies such as cerebral aneurysms. Understanding the relations between hemodynamics and aneurysm initiation, progression, and risk of rupture is crucial in diagnosis and treatment. Recent studies link the existence of vortices in the blood flow pattern to aneurysm rupture and report observations of embedded vortices - a larger vortex encloses a smaller one flowing in the opposite direction - whose implications are unclear. We present a clustering-based approach for the visual analysis of vortical flow in simulated cerebral aneurysm hemodynamics. We show how embedded vortices develop at saddle-node bifurcations on vortex core lines and convey the participating flow at full manifestation of the vortex by a fast and smart grouping of streamlines and the visualization of group representatives. The grouping result may be refined based on spectral clustering generating a more detailed visualization of the flow pattern, especially further off the core lines. We aim at supporting CFD engineers researching the biological implications of embedded vortices.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467203",
            "id": "r_255",
            "s_ids": [
                "s_1259",
                "s_1404",
                "s_1345",
                "s_619"
            ],
            "type": "rich",
            "x": -0.6803924856838754,
            "y": 0.44732404768715783
        },
        {
            "title": "Rotation Invariant Vortices for Flow Visualization",
            "data": "We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467200",
            "id": "r_256",
            "s_ids": [
                "s_1225",
                "s_591",
                "s_697"
            ],
            "type": "rich",
            "x": -0.7507165622115832,
            "y": 0.6340478622676864
        },
        {
            "title": "GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms",
            "data": "Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865021",
            "id": "r_257",
            "s_ids": [
                "s_276",
                "s_1250",
                "s_783",
                "s_342",
                "s_431",
                "s_1014",
                "s_198",
                "s_687"
            ],
            "type": "rich",
            "x": 0.12196628214019009,
            "y": -0.3258320625923579
        },
        {
            "title": "A Framework for Externalizing Implicit Error Using Visualization",
            "data": "This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864913",
            "id": "r_258",
            "s_ids": [
                "s_717",
                "s_227",
                "s_908"
            ],
            "type": "rich",
            "x": 0.10842019721212484,
            "y": -0.36019122185639324
        },
        {
            "title": "Evaluating the Impact of Binning 2D Scalar Fields",
            "data": "The expressiveness principle for visualization design asserts that a visualization should encode all of the available data, and only the available data, implying that continuous data types should be visualized with a continuous encoding channel. And yet, in many domains binning continuous data is not only pervasive, but it is accepted as standard practice. Prior work provides no clear guidance for when encoding continuous data continuously is preferable to employing binning techniques or how this choice affects data interpretation and decision making. In this paper, we present a study aimed at better understanding the conditions in which the expressiveness principle can or should be violated for visualizing continuous data. We provided participants with visualizations employing either continuous or binned greyscale encodings of geospatial elevation data and compared participants' ability to complete a wide variety of tasks. For various tasks, the results indicate significant differences in decision making, confidence in responses, and task completion time between continuous and binned encodings of the data. In general, participants with continuous encodings were faster to complete many of the tasks, but never outperformed those with binned encodings, while performance accuracy with binned encodings was superior to continuous encodings in some tasks. These findings suggest that strict adherence to the expressiveness principle is not always advisable. We discuss both the implications and limitations of our results and outline various avenues for potential work needed to further improve guidelines for using continuous versus binned encodings for continuous data types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599106",
            "id": "r_259",
            "s_ids": [
                "s_218",
                "s_397",
                "s_908",
                "s_84"
            ],
            "type": "rich",
            "x": -0.11729507625467699,
            "y": -0.13643482946924176
        },
        {
            "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics",
            "data": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585498",
            "id": "r_260",
            "s_ids": [
                "s_465",
                "s_636",
                "s_337",
                "s_197",
                "s_1087",
                "s_472"
            ],
            "type": "rich",
            "x": 0.3252809263272874,
            "y": -0.5580326743632988
        },
        {
            "title": "Design by Immersion: A Transdisciplinary Approach to Problem-Driven Visualizations",
            "data": "While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934790",
            "id": "r_261",
            "s_ids": [
                "s_421",
                "s_652",
                "s_1405",
                "s_864",
                "s_1089",
                "s_1476",
                "s_853"
            ],
            "type": "rich",
            "x": 0.8212510561398223,
            "y": 0.01553898172771142
        },
        {
            "title": "AirVis: Visual Analytics of Air Pollution Propagation",
            "data": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934670",
            "id": "r_262",
            "s_ids": [
                "s_1190",
                "s_638",
                "s_117",
                "s_326",
                "s_6",
                "s_1082",
                "s_98",
                "s_1273"
            ],
            "type": "rich",
            "x": -0.0663413603488881,
            "y": 0.4661370950151869
        },
        {
            "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds",
            "data": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934332",
            "id": "r_263",
            "s_ids": [
                "s_1457",
                "s_1298",
                "s_220",
                "s_880",
                "s_1339",
                "s_137"
            ],
            "type": "rich",
            "x": -0.584387678327537,
            "y": 0.11954414524723202
        },
        {
            "title": "KnowledgePearls: Provenance-Based Visualization Retrieval",
            "data": "Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865024",
            "id": "r_264",
            "s_ids": [
                "s_662",
                "s_70",
                "s_270",
                "s_1394",
                "s_495"
            ],
            "type": "rich",
            "x": 0.21096099544136432,
            "y": -0.16643650394091852
        },
        {
            "title": "Shared-Memory Parallel Computation of Morse-Smale Complexes with Improved Accuracy",
            "data": "Topological techniques have proven to be a powerful tool in the analysis and visualization of large-scale scientific data. In particular, the Morse-Smale complex and its various components provide a rich framework for robust feature definition and computation. Consequently, there now exist a number of approaches to compute Morse-Smale complexes for large-scale data in parallel. However, existing techniques are based on discrete concepts which produce the correct topological structure but are known to introduce grid artifacts in the resulting geometry. Here, we present a new approach that combines parallel streamline computation with combinatorial methods to construct a high-quality discrete Morse-Smale complex. In addition to being invariant to the orientation of the underlying grid, this algorithm allows users to selectively build a subset of features using high-quality geometry. In particular, a user may specifically select which ascending/descending manifolds are reconstructed with improved accuracy, focusing computational effort where it matters for subsequent analysis. This approach computes Morse-Smale complexes for larger data than previously feasible with significant speedups. We demonstrate and validate our approach using several examples from a variety of different scientific domains, and evaluate the performance of our method.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864848",
            "id": "r_265",
            "s_ids": [
                "s_1472",
                "s_438",
                "s_297"
            ],
            "type": "rich",
            "x": -0.6118378866289244,
            "y": 0.34633668042371046
        },
        {
            "title": "EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data",
            "data": "The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864825",
            "id": "r_266",
            "s_ids": [
                "s_26",
                "s_970",
                "s_205",
                "s_640",
                "s_866"
            ],
            "type": "rich",
            "x": -0.1794458051332794,
            "y": 0.09037841370888866
        },
        {
            "title": "Visual Causality Analysis Made Practical",
            "data": "Deriving the exact casual model that governs the relations between variables in a multidimensional dataset is difficult in practice. It is because causal inference algorithms by themselves typically cannot encode an adequate amount of domain knowledge to break all ties. Visual analytic approaches are considered a feasible alternative to fully automated methods. However, their application in real-world scenarios can be tedious. This paper focuses on these practical aspects of visual causality analysis. The most imperative of these aspects is posed by Simpson' Paradox. It implies the existence of multiple causal models differing in both structure and parameter depending on how the data is subdivided. We propose a comprehensive interface that engages human experts in identifying these subdivisions and allowing them to establish the corresponding causal models via a rich set of interactive facilities. Other features of our interface include: (1) a new causal network visualization that emphasizes the flow of causal dependencies, (2) a model scoring mechanism with visual hints for interactive model refinement, and (3) flexible approaches for handling heterogeneous data. Various real-world data examples are given.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585647",
            "id": "r_267",
            "s_ids": [
                "s_918",
                "s_252"
            ],
            "type": "rich",
            "x": 0.00836644973052701,
            "y": -0.1325284644755064
        },
        {
            "title": "Data Through Others' Eyes: The Impact of Visualizing Others' Expectations on Visualization Interpretation",
            "data": "In addition to visualizing input data, interactive visualizations have the potential to be social artifacts that reveal other people's perspectives on the data. However, how such social information embedded in a visualization impacts a viewer's interpretation of the data remains unknown. Inspired by recent interactive visualizations that display people's expectations of data against the data, we conducted a controlled experiment to evaluate the effect of showing social information in the form of other people's expectations on people's ability to recall the data, the degree to which they adjust their expectations to align with the data, and their trust in the accuracy of the data. We found that social information that exhibits a high degree of consensus lead participants to recall the data more accurately relative to participants who were exposed to the data alone. Additionally, participants trusted the accuracy of the data less and were more likely to maintain their initial expectations when other people's expectations aligned with their own initial expectations but not with the data. We conclude by characterizing the design space for visualizing others' expectations alongside data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745240",
            "id": "r_268",
            "s_ids": [
                "s_150",
                "s_1295",
                "s_1272"
            ],
            "type": "rich",
            "x": 0.5184063289503014,
            "y": -0.34201198656979787
        },
        {
            "title": "Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models",
            "data": "Distributions are often used to model uncertainty in many scientific datasets. To preserve the correlation among the spatially sampled grid locations in the dataset, various standard multivariate distribution models have been proposed in visualization literature. These models treat each grid location as a univariate random variable which models the uncertainty at that location. Standard multivariate distributions (both parametric and nonparametric) assume that all the univariate marginals are of the same type/family of distribution. But in reality, different grid locations show different statistical behavior which may not be modeled best by the same type of distribution. In this paper, we propose a new multivariate uncertainty modeling strategy to address the needs of uncertainty modeling in scientific datasets. Our proposed method is based on a statistically sound multivariate technique called Copula, which makes it possible to separate the process of estimating the univariate marginals and the process of modeling dependency, unlike the standard multivariate distributions. The modeling flexibility offered by our proposed method makes it possible to design distribution fields which can have different types of distribution (Gaussian, Histogram, KDE etc.) at the grid locations, while maintaining the correlation structure at the same time. Depending on the results of various standard statistical tests, we can choose an optimal distribution representation at each location, resulting in a more cost efficient modeling without significantly sacrificing on the analysis quality. To demonstrate the efficacy of our proposed modeling strategy, we extract and visualize uncertain features like isocontours and vortices in various real world datasets. We also study various modeling criterion to help users in the task of univariate model selection.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744099",
            "id": "r_269",
            "s_ids": [
                "s_134",
                "s_789",
                "s_353"
            ],
            "type": "rich",
            "x": -0.34215359075648805,
            "y": 0.23909456490256015
        },
        {
            "title": "TopoAngler: Interactive Topology-Based Extraction of Fishes",
            "data": "We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-Ioop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single dataset contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743980",
            "id": "r_270",
            "s_ids": [
                "s_1233",
                "s_29",
                "s_857",
                "s_380"
            ],
            "type": "rich",
            "x": -0.6316154798597899,
            "y": 0.027281269400692722
        },
        {
            "title": "Categorical Colormap Optimization with Visualization Case Studies",
            "data": "Mapping a set of categorical values to different colors is an elementary technique in data visualization. Users of visualization software routinely rely on the default colormaps provided by a system, or colormaps suggested by software such as ColorBrewer. In practice, users often have to select a set of colors in a semantically meaningful way (e.g., based on conventions, color metaphors, and logological associations), and consequently would like to ensure their perceptual differentiation is optimized. In this paper, we present an algorithmic approach for maximizing the perceptual distances among a set of given colors. We address two technical problems in optimization, i.e., (i) the phenomena of local maxima that halt the optimization too soon, and (ii) the arbitrary reassignment of colors that leads to the loss of the original semantic association. We paid particular attention to different types of constraints that users may wish to impose during the optimization process. To demonstrate the effectiveness of this work, we tested this technique in two case studies. To reach out to a wider range of users, we also developed a web application called Colourmap Hospital.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599214",
            "id": "r_271",
            "s_ids": [
                "s_1473",
                "s_394",
                "s_1071",
                "s_238",
                "s_291",
                "s_266"
            ],
            "type": "rich",
            "x": -0.07756736587438949,
            "y": -0.15661129648738942
        },
        {
            "title": "Jacobi Fiber Surfaces for Bivariate Reeb Space Computation",
            "data": "This paper presents an efficient algorithm for the computation of the Reeb space of an input bivariate piecewise linear scalar function f defined on a tetrahedral mesh. By extending and generalizing algorithmic concepts from the univariate case to the bivariate one, we report the first practical, output-sensitive algorithm for the exact computation of such a Reeb space. The algorithm starts by identifying the Jacobi set of f, the bivariate analogs of critical points in the univariate case. Next, the Reeb space is computed by segmenting the input mesh along the new notion of Jacobi Fiber Surfaces, the bivariate analog of critical contours in the univariate case. We additionally present a simplification heuristic that enables the progressive coarsening of the Reeb space. Our algorithm is simple to implement and most of its computations can be trivially parallelized. We report performance numbers demonstrating orders of magnitude speedups over previous approaches, enabling for the first time the tractable computation of bivariate Reeb spaces in practice. Moreover, unlike range-based quantization approaches (such as the Joint Contour Net), our algorithm is parameter-free. We demonstrate the utility of our approach by using the Reeb space as a semi-automatic segmentation tool for bivariate data. In particular, we introduce continuous scatterplot peeling, a technique which enables the reduction of the cluttering in the continuous scatterplot, by interactively selecting the features of the Reeb space to project. We provide a VTK-based C++ implementation of our algorithm that can be used for reproduction purposes or for the development of new Reeb space based visualization techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599017",
            "id": "r_272",
            "s_ids": [
                "s_611",
                "s_1226"
            ],
            "type": "rich",
            "x": -0.7604823126509136,
            "y": 0.3127503138030016
        },
        {
            "title": "A Psychophysical Investigation of Size as a Physical Variable",
            "data": "Physical visualizations, or data physicalizations, encode data in attributes of physical shapes. Despite a considerable body of work on visual variables, \u201cphysical variables\u201d remain poorly understood. One of them is physical size. A difficulty for solid elements is that \u201csize\u201d is ambiguous - it can refer to either length/diameter, surface, or volume. Thus, it is unclear for designers of physicalizations how to effectively encode quantities in physical size. To investigate, we ran an experiment where participants estimated ratios between quantities represented by solid bars and spheres. Our results suggest that solid bars are compared based on their length, consistent with previous findings for 2D and 3D bars on flat media. But for spheres, participants' estimates are rather proportional to their surface. Depending on the estimation method used, judgments are rather consistent across participants, thus the use of perceptually-optimized size scales seems possible. We conclude by discussing implications for the design of data physicalizations and the need for more empirical studies on physical variables.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467951",
            "id": "r_273",
            "s_ids": [
                "s_1347",
                "s_246"
            ],
            "type": "rich",
            "x": -0.2386701632117491,
            "y": 0.06957307702205233
        },
        {
            "title": "Comparative visual analysis of vector field ensembles",
            "data": "We present a new visual analysis approach to support the comparative exploration of 2D vector-valued ensemble fields. Our approach enables the user to quickly identify the most similar groups of ensemble members, as well as the locations where the variation among the members is high. We further provide means to visualize the main features of the potentially multimodal directional distributions at user-selected locations. For this purpose, directional data is modelled using mixtures of probability density functions (pdfs), which allows us to characterize and classify complex distributions with relatively few parameters. The resulting mixture models are used to determine the degree of similarity between ensemble members, and to construct glyphs showing the direction, spread, and strength of the principal modes of the directional distributions. We also propose several similarity measures, based on which we compute pairwise member similarities in the spatial domain and form clusters of similar members. The hierarchical clustering is shown using dendrograms and similarity matrices, which can be used to select particular members and visualize their variations. A user interface providing multiple linked views enables the simultaneous visualization of aggregated global and detailed local variations, as well as the selection of members for a detailed comparison.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347634",
            "id": "r_274",
            "s_ids": [
                "s_1073",
                "s_1198",
                "s_281",
                "s_111"
            ],
            "type": "rich",
            "x": -0.25847319750887643,
            "y": 0.33470157877167
        },
        {
            "title": "The Perceptual Proxies of Visual Comparison",
            "data": "Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \u201cbiggest delta\u201d, \u201cbiggest correlation\u201d) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \u201cbiggest mean\u201d and \u201cbiggest range\u201d between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \u201cMean length\u201d proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \u201cHull Area\u201d proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934786",
            "id": "r_275",
            "s_ids": [
                "s_75",
                "s_318",
                "s_1143",
                "s_811"
            ],
            "type": "rich",
            "x": 0.14081812574389335,
            "y": -0.11590561747609977
        },
        {
            "title": "Juniper: A Tree+Table Approach to Multivariate Graph Visualization",
            "data": "Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865149",
            "id": "r_276",
            "s_ids": [
                "s_1144",
                "s_495",
                "s_211"
            ],
            "type": "rich",
            "x": -0.26130344861174215,
            "y": -0.2958234066815849
        },
        {
            "title": "Doccurate: A Curation-Based Approach for Clinical Text Visualization",
            "data": "Before seeing a patient, physicians seek to obtain an overview of the patient's medical history. Text plays a major role in this activity since it represents the bulk of the clinical documentation, but reviewing it quickly becomes onerous when patient charts grow too large. Text visualization methods have been widely explored to manage this large scale through visual summaries that rely on information retrieval algorithms to structure text and make it amenable to visualization. However, the integration with such automated approaches comes with a number of limitations, including significant error rates and the need for healthcare providers to fine-tune algorithms without expert knowledge of their inner mechanics. In addition, several of these approaches obscure or substitute the original clinical text and therefore fail to leverage qualitative and rhetorical flavours of the clinical notes. These drawbacks have limited the adoption of text visualization and other summarization technologies in clinical practice. In this work we present Doccurate, a novel system embodying a curation-based approach for the visualization of large clinical text datasets. Our approach offers automation auditing and customizability to physicians while also preserving and extensively linking to the original text. We discuss findings of a formal qualitative evaluation conducted with 6 domain experts, shedding light onto physicians' information needs, perceived strengths and limitations of automated tools, and the importance of customization while balancing efficiency. We also present use case scenarios to showcase Doccurate's envisioned usage in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864905",
            "id": "r_277",
            "s_ids": [
                "s_707",
                "s_1424",
                "s_734",
                "s_182"
            ],
            "type": "rich",
            "x": 0.10122265759391437,
            "y": -0.40401634829489125
        },
        {
            "title": "Small Multiples with Gaps",
            "data": "Small multiples enable comparison by providing different views of a single data set in a dense and aligned manner. A common frame defines each view, which varies based upon values of a conditioning variable. An increasingly popular use of this technique is to project two-dimensional locations into a gridded space (e.g. grid maps), using the underlying distribution both as the conditioning variable and to determine the grid layout. Using whitespace in this layout has the potential to carry information, especially in a geographic context. Yet, the effects of doing so on the spatial properties of the original units are not understood. We explore the design space offered by such small multiples with gaps. We do so by constructing a comprehensive suite of metrics that capture properties of the layout used to arrange the small multiples for comparison (e.g. compactness and alignment) and the preservation of the original data (e.g. distance, topology and shape). We study these metrics in geographic data sets with varying properties and numbers of gaps. We use simulated annealing to optimize for each metric and measure the effects on the others. To explore these effects systematically, we take a new approach, developing a system to visualize this design space using a set of interactive matrices. We find that adding small amounts of whitespace to small multiple arrays improves some of the characteristics of 2D layouts, such as shape, distance and direction. This comes at the cost of other metrics, such as the retention of topology. Effects vary according to the input maps, with degree of variation in size of input regions found to be a factor. Optima exist for particular metrics in many cases, but at different amounts of whitespace for different maps. We suggest multiple metrics be used in optimized layouts, finding topology to be a primary factor in existing manually-crafted solutions, followed by a trade-off between shape and displacement. But the rich range of possible optimized layouts leads us to challenge single-solution thinking; we suggest to consider alternative optimized layouts for small multiples with gaps. Key to our work is the systematic, quantified and visual approach to exploring design spaces when facing a trade-off between many competing criteria-an approach likely to be of value to the analysis of other design spaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598542",
            "id": "r_278",
            "s_ids": [
                "s_470",
                "s_499",
                "s_1349",
                "s_85",
                "s_1089"
            ],
            "type": "rich",
            "x": -0.31321486401877796,
            "y": 0.07079482886389486
        },
        {
            "title": "Exploring Evolving Media Discourse Through Event Cueing",
            "data": "Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467991",
            "id": "r_279",
            "s_ids": [
                "s_776",
                "s_1461",
                "s_843",
                "s_1255",
                "s_1239",
                "s_1193",
                "s_1352",
                "s_1021",
                "s_708"
            ],
            "type": "rich",
            "x": 0.432680846678764,
            "y": -0.11898352736624807
        },
        {
            "title": "Occlusion-free Blood Flow Animation with Wall Thickness Visualization",
            "data": "We present the first visualization tool that combines pathlines from blood flow and wall thickness information. Our method uses illustrative techniques to provide occlusion-free visualization of the flow. We thus offer medical researchers an effective visual analysis tool for aneurysm treatment risk assessment. Such aneurysms bear a high risk of rupture and significant treatment-related risks. Therefore, to get a fully informed decision it is essential to both investigate the vessel morphology and the hemodynamic data. Ongoing research emphasizes the importance of analyzing the wall thickness in risk assessment. Our combination of blood flow visualization and wall thickness representation is a significant improvement for the exploration and analysis of aneurysms. As all presented information is spatially intertwined, occlusion problems occur. We solve these occlusion problems by dynamic cutaway surfaces. We combine this approach with a glyph-based blood flow representation and a visual mapping of wall thickness onto the vessel surface. We developed a GPU-based implementation of our visualizations which facilitates wall thickness analysis through real-time rendering and flexible interactive data exploration mechanisms. We designed our techniques in collaboration with domain experts, and we provide details about the evaluation of the technique and tool.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467961",
            "id": "r_280",
            "s_ids": [
                "s_1467",
                "s_132",
                "s_408",
                "s_619",
                "s_1205"
            ],
            "type": "rich",
            "x": -0.5648364619728985,
            "y": 0.11902664101031651
        },
        {
            "title": "Speculative Practices: Utilizing InfoVis to Explore Untapped Literary Collections",
            "data": "In this paper we exemplify how information visualization supports speculative thinking, hypotheses testing, and preliminary interpretation processes as part of literary research. While InfoVis has become a buzz topic in the digital humanities, skepticism remains about how effectively it integrates into and expands on traditional humanities research approaches. From an InfoVis perspective, we lack case studies that show the specific design challenges that make literary studies and humanities research at large a unique application area for information visualization. We examine these questions through our case study of the Speculative W@nderverse, a visualization tool that was designed to enable the analysis and exploration of an untapped literary collection consisting of thousands of science fiction short stories. We present the results of two empirical studies that involved general-interest readers and literary scholars who used the evolving visualization prototype as part of their research for over a year. Our findings suggest a design space for visualizing literary collections that is defined by (1) their academic and public relevance, (2) the tension between qualitative vs. quantitative methods of interpretation, (3) result-vs. process-driven approaches to InfoVis, and (4) the unique material and visual qualities of cultural collections. Through the Speculative W@nderverse we demonstrate how visualization can bridge these sometimes contradictory perspectives by cultivating curiosity and providing entry points into literary collections while, at the same time, supporting multiple aspects of humanities research processes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467452",
            "id": "r_281",
            "s_ids": [
                "s_1405",
                "s_192",
                "s_933"
            ],
            "type": "rich",
            "x": 0.7476057051273965,
            "y": -0.35806246359331795
        },
        {
            "title": "DimReader: Axis lines that explain non-linear projections",
            "data": "Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865194",
            "id": "r_282",
            "s_ids": [
                "s_518",
                "s_1340",
                "s_529"
            ],
            "type": "rich",
            "x": -0.47484109373959704,
            "y": 0.22757443418372167
        },
        {
            "title": "SIRIUS: Dual, Symmetric, Interactive Dimension Reductions",
            "data": "Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865047",
            "id": "r_283",
            "s_ids": [
                "s_873",
                "s_582",
                "s_772",
                "s_55",
                "s_13",
                "s_207"
            ],
            "type": "rich",
            "x": 0.23062003924510047,
            "y": 0.11652530076251306
        },
        {
            "title": "Stable Treemaps via Local Moves",
            "data": "Treemaps are a popular tool to visualize hierarchical data: items are represented by nested rectangles and the area of each rectangle corresponds to the data being visualized for this item. The visual quality of a treemap is commonly measured via the aspect ratio of the rectangles. If the data changes, then a second important quality criterion is the stability of the treemap: how much does the treemap change as the data changes. We present a novel stable treemapping algorithm that has very high visual quality. Whereas existing treemapping algorithms generally recompute the treemap every time the input changes, our algorithm changes the layout of the treemap using only local modifications. This approach not only gives us direct control over stability, but it also allows us to use a larger set of possible layouts, thus provably resulting in treemaps of higher visual quality compared to existing algorithms. We further prove that we can reach all possible treemap layouts using only our local modifications. Furthermore, we introduce a new measure for stability that better captures the relative positions of rectangles. We finally show via experiments on real-world data that our algorithm outperforms existing treemapping algorithms also in practice on either visual quality and/or stability. Our algorithm scores high on stability regardless of whether we use an existing stability measure or our new measure.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745140",
            "id": "r_284",
            "s_ids": [
                "s_986",
                "s_300",
                "s_1369"
            ],
            "type": "rich",
            "x": -0.5878191434177198,
            "y": -0.09998277790390987
        },
        {
            "title": "Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization",
            "data": "Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software [8], which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743958",
            "id": "r_285",
            "s_ids": [
                "s_148",
                "s_478",
                "s_854",
                "s_1414",
                "s_1289",
                "s_1233",
                "s_260"
            ],
            "type": "rich",
            "x": -0.18728242325568484,
            "y": 0.3220390577536749
        },
        {
            "title": "HindSight: Encouraging Exploration through Direct Encoding of Personal Interaction History",
            "data": "Physical and digital objects often leave markers of our use. Website links turn purple after we visit them, for example, showing us information we have yet to explore. These \u201cfootprints\u201d of interaction offer substantial benefits in information saturated environments - they enable us to easily revisit old information, systematically explore new information, and quickly resume tasks after interruption. While applying these design principles have been successful in HCI contexts, direct encodings of personal interaction history have received scarce attention in data visualization. One reason is that there is little guidance for integrating history into visualizations where many visual channels are already occupied by data. More importantly, there is not firm evidence that making users aware of their interaction history results in benefits with regards to exploration or insights. Following these observations, we propose HindSight - an umbrella term for the design space of representing interaction history directly in existing data visualizations. In this paper, we examine the value of HindSight principles by augmenting existing visualizations with visual indicators of user interaction history (e.g. How the Recession Shaped the Economy in 255 Charts, NYTimes). In controlled experiments of over 400 participants, we found that HindSight designs generally encouraged people to visit more data and recall different insights after interaction. The results of our experiments suggest that simple additions to visualizations can make users aware of their interaction history, and that these additions significantly impact users' exploration and insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599058",
            "id": "r_286",
            "s_ids": [
                "s_704",
                "s_286",
                "s_817",
                "s_124"
            ],
            "type": "rich",
            "x": 0.5355765639868556,
            "y": -0.15207377969110222
        },
        {
            "title": "The semantics of sketch: Flexibility in visual query systems for time series data",
            "data": "Sketching allows analysts to specify complex and free-form patterns of interest. Visual query systems can make use of sketches to locate these patterns of interest in large datasets. However, sketching is ambiguous: the same drawing could represent a multitude of potential queries. In this work, we investigate these ambiguities as they apply to visual query systems for time series data. We define a class of \u201cinvariants\u201d - the properties of a time series that the analyst wishes to ignore when performing a sketch-based query. We present the results of a crowd-sourced study, showing that these invariants are key components of how people rate the strength of match between sketch and target. We adapt a number of algorithms for time series matching to support invariants in sketches. Lastly, we present a web-deployed prototype sketch-based visual query system that relies on these invariants. We apply the prototype to data from finance, the digital humanities, and political science.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883519",
            "id": "r_287",
            "s_ids": [
                "s_1215",
                "s_158"
            ],
            "type": "rich",
            "x": 0.17814500429261207,
            "y": 0.23672406249499556
        },
        {
            "title": "Supporting visual exploration for multiple users in large display environments",
            "data": "We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883506",
            "id": "r_288",
            "s_ids": [
                "s_416",
                "s_483",
                "s_1143",
                "s_538"
            ],
            "type": "rich",
            "x": 0.5668772317542057,
            "y": 0.022567427380035712
        },
        {
            "title": "Uncertainty-Aware Principal Component Analysis",
            "data": "We present a technique to perform dimensionality reduction on data that is subject to uncertainty. Our method is a generalization of traditional principal component analysis (PCA) to multivariate probability distributions. In comparison to non-linear methods, linear dimensionality reduction techniques have the advantage that the characteristics of such probability distributions remain intact after projection. We derive a representation of the PCA sample covariance matrix that respects potential uncertainty in each of the inputs, building the mathematical foundation of our new method: uncertainty-aware PCA. In addition to the accuracy and performance gained by our approach over sampling-based strategies, our formulation allows us to perform sensitivity analysis with regard to the uncertainty in the data. For this, we propose factor traces as a novel visualization that enables to better understand the influence of uncertainty on the chosen principal components. We provide multiple examples of our technique using real-world datasets. As a special case, we show how to propagate multivariate normal distributions through PCA in closed form. Furthermore, we discuss extensions and limitations of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934812",
            "id": "r_289",
            "s_ids": [
                "s_232",
                "s_1088",
                "s_386",
                "s_138",
                "s_1220"
            ],
            "type": "rich",
            "x": -0.3749018605956317,
            "y": 0.27310778508193867
        },
        {
            "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures",
            "data": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934261",
            "id": "r_290",
            "s_ids": [
                "s_604",
                "s_1135",
                "s_321",
                "s_869"
            ],
            "type": "rich",
            "x": -0.3160172703877627,
            "y": -0.49593573092361387
        },
        {
            "title": "A Structural Average of Labeled Merge Trees for Uncertainty Visualization",
            "data": "Physical phenomena in science and engineering are frequently modeled using scalar fields. In scalar field topology, graph-based topological descriptors such as merge trees, contour trees, and Reeb graphs are commonly used to characterize topological changes in the (sub)level sets of scalar fields. One of the biggest challenges and opportunities to advance topology-based visualization is to understand and incorporate uncertainty into such topological descriptors to effectively reason about their underlying data. In this paper, we study a structural average of a set of labeled merge trees and use it to encode uncertainty in data. Specifically, we compute a 1-center tree that minimizes its maximum distance to any other tree in the set under a well-defined metric called the interleaving distance. We provide heuristic strategies that compute structural averages of merge trees whose labels do not fully agree. We further provide an interactive visualization system that resembles a numerical calculator that takes as input a set of merge trees and outputs a tree as their structural average. We also highlight structural similarities between the input and the average and incorporate uncertainty information for visual exploration. We develop a novel measure of uncertainty, referred to as consistency, via a metric-space view of the input trees. Finally, we demonstrate an application of our framework through merge trees that arise from ensembles of scalar fields. Our work is the first to employ interleaving distances and consistency to study a global, mathematically rigorous, structural average of merge trees in the context of uncertainty visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934242",
            "id": "r_291",
            "s_ids": [
                "s_1452",
                "s_747",
                "s_1170",
                "s_837",
                "s_722"
            ],
            "type": "rich",
            "x": -0.5527550883687253,
            "y": 0.02182968878547897
        },
        {
            "title": "Time-Dependent Flow seen through Approximate Observer Killing Fields",
            "data": "Flow fields are usually visualized relative to a global observer, i.e., a single frame of reference. However, often no global frame can depict all flow features equally well. Likewise, objective criteria for detecting features such as vortices often use either a global reference frame, or compute a separate frame for each point in space and time. We propose the first general framework that enables choosing a smooth trade-off between these two extremes. Using global optimization to minimize specific differential geometric properties, we compute a time-dependent observer velocity field that describes the motion of a continuous field of observers adapted to the input flow. This requires developing the novel notion of an observed time derivative. While individual observers are restricted to rigid motions, overall we compute an approximate Killing field, corresponding to almost-rigid motion. This enables continuous transitions between different observers. Instead of focusing only on flow features, we furthermore develop a novel general notion of visualizing how all observers jointly perceive the input field. This in fact requires introducing the concept of an observation time, with respect to which a visualization is computed. We develop the corresponding notions of observed stream, path, streak, and time lines. For efficiency, these characteristic curves can be computed using standard approaches, by first transforming the input field accordingly. Finally, we prove that the input flow perceived by the observer field is objective. This makes derived flow features, such as vortices, objective as well.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864839",
            "id": "r_292",
            "s_ids": [
                "s_336",
                "s_1152",
                "s_1359",
                "s_876"
            ],
            "type": "rich",
            "x": -0.4022224598980911,
            "y": 0.49880482848533636
        },
        {
            "title": "MyBrush: Brushing and Linking with Personal Agency",
            "data": "We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743859",
            "id": "r_293",
            "s_ids": [
                "s_674",
                "s_334",
                "s_129",
                "s_309",
                "s_853"
            ],
            "type": "rich",
            "x": 0.6453479050009423,
            "y": -0.3689538326734731
        },
        {
            "title": "Combined Visualization of Vessel Deformation and Hemodynamics in Cerebral Aneurysms",
            "data": "We present the first visualization tool that combines patient-specific hemodynamics with information about the vessel wall deformation and wall thickness in cerebral aneurysms. Such aneurysms bear the risk of rupture, whereas their treatment also carries considerable risks for the patient. For the patient-specific rupture risk evaluation and treatment analysis, both morphological and hemodynamic data have to be investigated. Medical researchers emphasize the importance of analyzing correlations between wall properties such as the wall deformation and thickness, and hemodynamic attributes like the Wall Shear Stress and near-wall flow. Our method uses a linked 2.5D and 3D depiction of the aneurysm together with blood flow information that enables the simultaneous exploration of wall characteristics and hemodynamic attributes during the cardiac cycle. We thus offer medical researchers an effective visual exploration tool for aneurysm treatment risk assessment. The 2.5D view serves as an overview that comprises a projection of the vessel surface to a 2D map, providing an occlusion-free surface visualization combined with a glyph-based depiction of the local wall thickness. The 3D view represents the focus upon which the data exploration takes place. To support the time-dependent parameter exploration and expert collaboration, a camera path is calculated automatically, where the user can place landmarks for further exploration of the properties. We developed a GPU-based implementation of our visualizations with a flexible interactive data exploration mechanism. We designed our techniques in collaboration with domain experts, and provide details about the evaluation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598795",
            "id": "r_294",
            "s_ids": [
                "s_808",
                "s_677",
                "s_968",
                "s_619",
                "s_1467"
            ],
            "type": "rich",
            "x": -0.42921508010428444,
            "y": 0.03705190313385901
        },
        {
            "title": "Visualizing Dimension Coverage to Support Exploratory Analysis",
            "data": "Data analysis involves constantly formulating and testing new hypotheses and questions about data. When dealing with a new dataset, especially one with many dimensions, it can be cumbersome for the analyst to clearly remember which aspects of the data have been investigated (i.e., visually examined for patterns, trends, outliers etc.) and which combinations have not. Yet this information is critical to help the analyst formulate new questions that they have not already answered. We observe that for tabular data, questions are typically comprised of varying combinations of data dimensions (e.g., what are the trends of Sales and Profit for different Regions?). We propose representing analysis history from the angle of dimension coverage (i.e., which data dimensions have been investigated and in which combinations). We use scented widgets to incorporate dimension coverage of the analysts' past work into interaction widgets of a visualization tool. We demonstrate how this approach can assist analysts with the question formation process. Our approach extends the concept of scented widgets to reveal aspects of one's own analysis history, and offers a different perspective on one's past work than typical visualization history tools. Results of our empirical study showed that participants with access to embedded dimension coverage information relied on this information when formulating questions, asked more questions about the data, generated more top-level findings, and showed greater breadth of their analysis without sacrificing depth.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598466",
            "id": "r_295",
            "s_ids": [
                "s_1313",
                "s_1208",
                "s_1445"
            ],
            "type": "rich",
            "x": 0.26803579948072725,
            "y": -0.07964037679753273
        },
        {
            "title": "MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data",
            "data": "Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468292",
            "id": "r_296",
            "s_ids": [
                "s_215",
                "s_1143",
                "s_1139"
            ],
            "type": "rich",
            "x": 0.11431593475452272,
            "y": 0.43715440006529627
        },
        {
            "title": "High-Quality Ultra-Compact Grid Layout of Grouped Networks",
            "data": "Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467251",
            "id": "r_297",
            "s_ids": [
                "s_504",
                "s_202",
                "s_534",
                "s_130",
                "s_751",
                "s_1188"
            ],
            "type": "rich",
            "x": -0.39717386665020715,
            "y": -0.34530190534546074
        },
        {
            "title": "Matches, Mismatches, and Methods: Multiple-View Workflows for Energy Portfolio Analysis",
            "data": "The energy performance of large building portfolios is challenging to analyze and monitor, as current analysis tools are not scalable or they present derived and aggregated data at too coarse of a level. We conducted a visualization design study, beginning with a thorough work domain analysis and a characterization of data and task abstractions. We describe generalizable visual encoding design choices for time-oriented data framed in terms of matches and mismatches, as well as considerations for workflow design. Our designs address several research questions pertaining to scalability, view coordination, and the inappropriateness of line charts for derived and aggregated data due to a combination of data semantics and domain convention. We also present guidelines relating to familiarity and trust, as well as methodological considerations for visualization design studies. Our designs were adopted by our collaborators and incorporated into the design of an energy analysis software application that will be deployed to tens of thousands of energy workers in their client base.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2466971",
            "id": "r_298",
            "s_ids": [
                "s_12",
                "s_1276",
                "s_437",
                "s_802"
            ],
            "type": "rich",
            "x": 0.5318341345262506,
            "y": 0.06424968244803969
        },
        {
            "title": "DocuCompass: Effective exploration of document landscapes",
            "data": "The creation of interactive visualization to analyze text documents has gained an impressive momentum in recent years. This is not surprising in the light of massive and still increasing amounts of available digitized texts. Websites, social media, news wire, and digital libraries are just few examples of the diverse text sources whose visual analysis and exploration offers new opportunities to effectively mine and manage the information and knowledge hidden within them. A popular visualization method for large text collections is to represent each document by a glyph in 2D space. These landscapes can be the result of optimizing pairwise distances in 2D to represent document similarities, or they are provided directly as meta data, such as geo-locations. For well-defined information needs, suitable interaction methods are available for these spatializations. However, free exploration and navigation on a level of abstraction between a labeled document spatialization and reading single documents is largely unsupported. As a result, vital foraging steps for task-tailored actions, such as selecting subgroups of documents for detailed inspection, or subsequent sense-making steps are hampered. To fill in this gap, we propose DocuCompass, a focus+context approach based on the lens metaphor. It comprises multiple methods to characterize local groups of documents, and to efficiently guide exploration based on users' requirements. DocuCompass thus allows for effective interactive exploration of document landscapes without disrupting the mental map of users by changing the layout itself. We discuss the suitability of multiple navigation and characterization methods for different spatializations and texts. Finally, we provide insights generated through user feedback and discuss the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883507",
            "id": "r_299",
            "s_ids": [
                "s_62",
                "s_1323",
                "s_1132",
                "s_390",
                "s_299"
            ],
            "type": "rich",
            "x": 0.2619619744744972,
            "y": -0.22179498127016065
        },
        {
            "title": "TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization",
            "data": "We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934255",
            "id": "r_300",
            "s_ids": [
                "s_894",
                "s_530"
            ],
            "type": "rich",
            "x": -0.44226958799672006,
            "y": -0.10734367914584723
        },
        {
            "title": "Magnostics: Image-Based Search of Interesting Matrix Views for Guided Network Exploration",
            "data": "In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or Magnostics), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. Magnostics can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors-27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS-with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for Magnostics and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598467",
            "id": "r_301",
            "s_ids": [
                "s_540",
                "s_1126",
                "s_545",
                "s_596",
                "s_794",
                "s_100",
                "s_493"
            ],
            "type": "rich",
            "x": -0.2920924322587154,
            "y": -0.05950569038395717
        },
        {
            "title": "NameClarifier: A Visual Analytics System for Author Name Disambiguation",
            "data": "In this paper, we present a novel visual analytics system called NameClarifier to interactively disambiguate author names in publications by keeping humans in the loop. Specifically, NameClarifier quantifies and visualizes the similarities between ambiguous names and those that have been confirmed in digital libraries. The similarities are calculated using three key factors, namely, co-authorships, publication venues, and temporal information. Our system estimates all possible allocations, and then provides visual cues to users to help them validate every ambiguous case. By looping users in the disambiguation process, our system can achieve more reliable results than general data mining models for highly ambiguous cases. In addition, once an ambiguous case is resolved, the result is instantly added back to our system and serves as additional cues for all the remaining unidentified names. In this way, we open up the black box in traditional disambiguation processes, and help intuitively and comprehensively explain why the corresponding classifications should hold. We conducted two use cases and an expert review to demonstrate the effectiveness of NameClarifier.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598465",
            "id": "r_302",
            "s_ids": [
                "s_1423",
                "s_1016",
                "s_1066",
                "s_453",
                "s_137",
                "s_896"
            ],
            "type": "rich",
            "x": 0.13579254288715062,
            "y": -0.39327268323956144
        },
        {
            "title": "Interactive Visual Profiling of Musicians",
            "data": "Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project, priority (1) the definition of various measures to determine the similarity of musicians' attributes, and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467620",
            "id": "r_303",
            "s_ids": [
                "s_302",
                "s_327",
                "s_841"
            ],
            "type": "rich",
            "x": 0.2678578740619031,
            "y": 0.034838864586916325
        },
        {
            "title": "VisOHC: Designing Visual Analytics for Online Health Communities",
            "data": "Through online health communities (OHCs), patients and caregivers exchange their illness experiences and strategies for overcoming the illness, and provide emotional support. To facilitate healthy and lively conversations in these communities, their members should be continuously monitored and nurtured by OHC administrators. The main challenge of OHC administrators' tasks lies in understanding the diverse dimensions of conversation threads that lead to productive discussions in their communities. In this paper, we present a design study in which three domain expert groups participated, an OHC researcher and two OHC administrators of online health communities, which was conducted to find with a visual analytic solution. Through our design study, we characterized the domain goals of OHC administrators and derived tasks to achieve these goals. As a result of this study, we propose a system called VisOHC, which visualizes individual OHC conversation threads as collapsed boxes-a visual metaphor of conversation threads. In addition, we augmented the posters' reply authorship network with marks and/or beams to show conversation dynamics within threads. We also developed unique measures tailored to the characteristics of OHCs, which can be encoded for thread visualizations at the users' requests. Our observation of the two administrators while using VisOHC showed that it supports their tasks and reveals interesting insights into online health communities. Finally, we share our methodological lessons on probing visual designs together with domain experts by allowing them to freely encode measurements into visual variables.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467555",
            "id": "r_304",
            "s_ids": [
                "s_1181",
                "s_177",
                "s_997",
                "s_320",
                "s_825",
                "s_1200"
            ],
            "type": "rich",
            "x": 0.5655186464068269,
            "y": -0.3817256530493889
        },
        {
            "title": "Mixed-initiative visual analytics using task-driven recommendations",
            "data": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347625",
            "id": "r_305",
            "s_ids": [
                "s_670",
                "s_861",
                "s_196",
                "s_1433",
                "s_988",
                "s_764",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.4267761156384201,
            "y": -0.2414318114040451
        },
        {
            "title": "DeepDrawing: A Deep Learning Approach to Graph Drawing",
            "data": "Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934798",
            "id": "r_306",
            "s_ids": [
                "s_1115",
                "s_1171",
                "s_403",
                "s_896",
                "s_488",
                "s_137"
            ],
            "type": "rich",
            "x": -0.0938596072980248,
            "y": -0.25230932563601316
        },
        {
            "title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics",
            "data": "User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934609",
            "id": "r_307",
            "s_ids": [
                "s_5",
                "s_551",
                "s_316",
                "s_9",
                "s_1149",
                "s_983",
                "s_85"
            ],
            "type": "rich",
            "x": 0.4966869503881333,
            "y": -0.06247401733742072
        },
        {
            "title": "Vector Field Topology of Time-Dependent Flows in a Steady Reference Frame",
            "data": "The topological analysis of unsteady vector fields remains to this day one of the largest challenges in flow visualization. We build up on recent work on vortex extraction to define a time-dependent vector field topology for 2D and 3D flows. In our work, we split the vector field into two components: a vector field in which the flow becomes steady, and the remaining ambient flow that describes the motion of topological elements (such as sinks, sources and saddles) and feature curves (vortex corelines and bifurcation lines). To this end, we expand on recent local optimization approaches by modeling spatially-varying deformations through displacement transformations from continuum mechanics. We compare and discuss the relationships with existing local and integration-based topology extraction methods, showing for instance that separatrices seeded from saddles in the optimal frame align with the integration-based streakline vector field topology. In contrast to the streakline-based approach, our method gives a complete picture of the topology for every time slice, including the steps near the temporal domain boundaries. With our work it now becomes possible to extract topological information even when only few time slices are available. We demonstrate the method in several analytical and numerically-simulated flows and discuss practical aspects, limitations and opportunities for future work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934375",
            "id": "r_308",
            "s_ids": [
                "s_96",
                "s_1225"
            ],
            "type": "rich",
            "x": -0.7133072297612753,
            "y": 0.5770536081010682
        },
        {
            "title": "Diderot: a Domain-Specific Language for Portable Parallel Scientific Visualization and Image Analysis",
            "data": "Many algorithms for scientific visualization and image analysis are rooted in the world of continuous scalar, vector, and tensor fields, but are programmed in low-level languages and libraries that obscure their mathematical foundations. Diderot is a parallel domain-specific language that is designed to bridge this semantic gap by providing the programmer with a high-level, mathematical programming notation that allows direct expression of mathematical concepts in code. Furthermore, Diderot provides parallel performance that takes advantage of modern multicore processors and GPUs. The high-level notation allows a concise and natural expression of the algorithms and the parallelism allows efficient execution on real-world datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467449",
            "id": "r_309",
            "s_ids": [
                "s_1431",
                "s_1189",
                "s_1061",
                "s_682",
                "s_1440"
            ],
            "type": "rich",
            "x": -0.46535038769390746,
            "y": 0.3370408875151875
        },
        {
            "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media",
            "data": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585638",
            "id": "r_310",
            "s_ids": [
                "s_316",
                "s_364",
                "s_312",
                "s_1214",
                "s_1206",
                "s_255"
            ],
            "type": "rich",
            "x": 0.4750674995809212,
            "y": 0.27802933504352195
        },
        {
            "title": "How ideas flow across multiple social groups",
            "data": "Tracking how correlated ideas flow within and across multiple social groups facilitates the understanding of the transfer of information, opinions, and thoughts on social media. In this paper, we present IdeaFlow, a visual analytics system for analyzing the lead-lag changes within and across pre-defined social groups regarding a specific set of correlated ideas, each of which is described by a set of words. To model idea flows accurately, we develop a random-walk-based correlation model and integrate it with Bayesian conditional cointegration and a tensor-based technique. To convey complex lead-lag relationships over time, IdeaFlow combines the strengths of a bubble tree, a flow map, and a timeline. In particular, we develop a Voronoi-treemap-based bubble tree to help users get an overview of a set of ideas quickly. A correlated-clustering-based layout algorithm is used to simultaneously generate multiple flow maps with less ambiguity. We also introduce a focus+context timeline to explore huge amounts of temporal data at different levels of time granularity. Quantitative evaluation and case studies demonstrate the accuracy and effectiveness of IdeaFlow.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883511",
            "id": "r_311",
            "s_ids": [
                "s_1032",
                "s_301",
                "s_1028",
                "s_40",
                "s_1466",
                "s_745",
                "s_1151"
            ],
            "type": "rich",
            "x": 0.16541829994768384,
            "y": 0.12808563214709529
        },
        {
            "title": "Persistence Atlas for Critical Point Variability in Ensembles",
            "data": "This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864432",
            "id": "r_312",
            "s_ids": [
                "s_930",
                "s_1256",
                "s_389",
                "s_611"
            ],
            "type": "rich",
            "x": -0.42226783250473715,
            "y": 0.24466267444620937
        },
        {
            "title": "Active Reading of Visualizations",
            "data": "We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745958",
            "id": "r_313",
            "s_ids": [
                "s_788",
                "s_864",
                "s_334",
                "s_162",
                "s_666",
                "s_853"
            ],
            "type": "rich",
            "x": 0.7058597781870161,
            "y": -0.07885274353222023
        },
        {
            "title": "TACO: Visualizing Changes in Tables Over Time",
            "data": "Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions/deletions, reorder, merge/split, and content changes. For textual data, a variety of commonplace \u201cdiff\u201d tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selected tables. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745298",
            "id": "r_314",
            "s_ids": [
                "s_275",
                "s_662",
                "s_168",
                "s_1358",
                "s_472",
                "s_495"
            ],
            "type": "rich",
            "x": 0.26951886240177947,
            "y": 0.13171525127460548
        },
        {
            "title": "Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for Parallel Particle Tracing",
            "data": "We propose a dynamically load-balanced algorithm for parallel particle tracing, which periodically attempts to evenly redistribute particles across processes based on k-d tree decomposition. Each process is assigned with (1) a statically partitioned, axis-aligned data block that partially overlaps with neighboring blocks in other processes and (2) a dynamically determined k-d tree leaf node that bounds the active particles for computation; the bounds of the k-d tree nodes are constrained by the geometries of data blocks. Given a certain degree of overlap between blocks, our method can balance the number of particles as much as possible. Compared with other load-balancing algorithms for parallel particle tracing, the proposed method does not require any preanalysis, does not use any heuristics based on flow features, does not make any assumptions about seed distribution, does not move any data blocks during the run, and does not need any master process for work redistribution. Based on a comprehensive performance study up to 8K processes on a Blue Gene/Q system, the proposed algorithm outperforms baseline approaches in both load balance and scalability on various flow visualization and analysis problems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744059",
            "id": "r_315",
            "s_ids": [
                "s_822",
                "s_865",
                "s_673",
                "s_1214",
                "s_727"
            ],
            "type": "rich",
            "x": -0.5868887349965765,
            "y": 0.43228961987152853
        },
        {
            "title": "Robust Detection and Visualization of Jet-Stream Core Lines in Atmospheric Flow",
            "data": "Jet-streams, their core lines and their role in atmospheric dynamics have been subject to considerable meteorological research since the first half of the twentieth century. Yet, until today no consistent automated feature detection approach has been proposed to identify jet-stream core lines from 3D wind fields. Such 3D core lines can facilitate meteorological analyses previously not possible. Although jet-stream cores can be manually analyzed by meteorologists in 2D as height ridges in the wind speed field, to the best of our knowledge no automated ridge detection approach has been applied to jet-stream core detection. In this work, we -a team of visualization scientists and meteorologists-propose a method that exploits directional information in the wind field to extract core lines in a robust and numerically less involved manner than traditional 3D ridge detection. For the first time, we apply the extracted 3D core lines to meteorological analysis, considering real-world case studies and demonstrating our method's benefits for weather forecasting and meteorological research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743989",
            "id": "r_316",
            "s_ids": [
                "s_705",
                "s_38",
                "s_679",
                "s_111",
                "s_1390"
            ],
            "type": "rich",
            "x": -0.4428200548890203,
            "y": 0.6631176564093967
        },
        {
            "title": "PowerSet: A Comprehensive Visualization of Set Intersections",
            "data": "When analyzing a large amount of data, analysts often define groups over data elements that share certain properties. Using these groups as the unit of analysis not only reduces the data volume, but also allows detecting various patterns in the data. This involves analyzing intersection relations between these groups, and how the element attributes vary between these intersections. This kind of set-based analysis has various applications in a variety of domains, due to the generic and powerful notion of sets. However, visualizing intersections relations is challenging because their number grows exponentially with the number of sets. We present a novel technique based on Treemaps to provide a comprehensive overview of non-empty intersections in a set system in a scalable way. It enables gaining insight about how elements are distributed across these intersections as well as performing fine-grained analysis to explore and compare their attributes both in overview and in detail. Interaction allows querying and filtering these elements based on their set memberships. We demonstrate how our technique supports various use cases in data exploration and analysis by providing insights into set-based data, beyond the limits of state-of-the-art techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598496",
            "id": "r_317",
            "s_ids": [
                "s_1335",
                "s_1325"
            ],
            "type": "rich",
            "x": -0.07737640933616904,
            "y": 0.19759108430170366
        },
        {
            "title": "TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data and Unstructured Text",
            "data": "We describe TextTile, a data visualization tool for investigation of datasets and questions that require seamless and flexible analysis of structured data and unstructured text. TextTile is based on real-world data analysis problems gathered through our interaction with a number of domain experts and provides a general purpose solution to such problems. The system integrates a set of operations that can interchangeably be applied to the structured as well as to unstructured text part of the data to generate useful data summaries. Such summaries are then organized in visual tiles in a grid layout to allow their analysis and comparison. We validate TextTile with task analysis, use cases and a user study showing the system can be easily learned and proficiently used to carry out nontrivial tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598447",
            "id": "r_318",
            "s_ids": [
                "s_927",
                "s_599",
                "s_457"
            ],
            "type": "rich",
            "x": 0.3609561881390945,
            "y": -0.307321310226821
        },
        {
            "title": "Orientation-Enhanced Parallel Coordinate Plots",
            "data": "Parallel Coordinate Plots (PCPs) is one of the most powerful techniques for the visualization of multivariate data. However, for large datasets, the representation suffers from clutter due to overplotting. In this case, discerning the underlying data information and selecting specific interesting patterns can become difficult. We propose a new and simple technique to improve the display of PCPs by emphasizing the underlying data structure. Our Orientation-enhanced Parallel Coordinate Plots (OPCPs) improve pattern and outlier discernibility by visually enhancing parts of each PCP polyline with respect to its slope. This enhancement also allows us to introduce a novel and efficient selection method, the Orientation-enhanced Brushing (O-Brushing). Our solution is particularly useful when multiple patterns are present or when the view on certain patterns is obstructed by noise. We present the results of our approach with several synthetic and real-world datasets. Finally, we conducted a user evaluation, which verifies the advantages of the OPCPs in terms of discernibility of information in complex data. It also confirms that O-Brushing eases the selection of data patterns in PCPs and reduces the amount of necessary user interactions compared to state-of-the-art brushing techniques.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467872",
            "id": "r_319",
            "s_ids": [
                "s_826",
                "s_947",
                "s_77",
                "s_145",
                "s_408"
            ],
            "type": "rich",
            "x": -0.13841364594204805,
            "y": 0.10808556462394611
        },
        {
            "title": "Do What I Mean, Not What I Say! Design Considerations for Supporting Intent and Context in Analytical Conversation",
            "data": "Natural language can be a useful modality for creating and interacting with visualizations but users often have unrealistic expectations about the intelligence of natural language systems. The gulf between user expectations and system capabilities may lead to a disappointing user experience. So - if we want to engineer a natural language system, what are the requirements around system intelligence? This work takes a retrospective look at how we answered this question in the design of Ask Data, a natural language interaction feature for Tableau. We examine two factors contributing to perceived system intelligence: the system's ability to understand the analytic intent behind an input utterance and the ability to interpret an utterance contextually (i.e. taking into account the current visualization state and recent actions). Our aim was to understand the ways in which a system would need to support these two aspects of intelligence to enable a positive user experience. We first describe a pre-design Wizard of Oz study that offered insight into this question and narrowed the space of designs under consideration. We then reflect on the impact of this study on system development, examining how design implications from the study played out in practice. Our work contributes insights for the design of natural language interaction in visual analytics as well as a reflection on the value of pre-design empirical studies in the development of visual analytic systems.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986918",
            "id": "r_320",
            "s_ids": [
                "s_1208",
                "s_462"
            ],
            "type": "rich",
            "x": 0.5232769157568571,
            "y": -0.38580492006669925
        },
        {
            "title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems",
            "data": "Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers and ensuring the efficient operation of the systems. To this end, a variety of automated techniques have been developed to identify anomalies in cloud computing. These techniques are usually adopted to track the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of cloud computing data, the effectiveness of these automated methods is affected. Thus, substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing systems. A novel unsupervised anomaly detection algorithm is developed to identify anomalies based on the specific temporal patterns of the given metrics data (e.g., the periodic pattern). Rich visualization and interaction designs are used to help understand the anomalies in the spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934613",
            "id": "r_321",
            "s_ids": [
                "s_26",
                "s_640",
                "s_1194",
                "s_507",
                "s_709",
                "s_1376",
                "s_580",
                "s_351",
                "s_137"
            ],
            "type": "rich",
            "x": 0.11712371561627402,
            "y": 0.3304110860863839
        },
        {
            "title": "Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual Pull",
            "data": "In visual depictions of data, position (i.e., the vertical height of a line or a bar) is believed to be the most precise way to encode information compared to other encodings (e.g., hue). Not only are other encodings less precise than position, but they can also be prone to systematic biases (e.g., color category boundaries can distort perceived differences between hues). By comparison, position's high level of precision may seem to protect it from such biases. In contrast, across three empirical studies, we show that while position may be a precise form of data encoding, it can also produce systematic biases in how values are visually encoded, at least for reports of average position across a short delay. In displays with a single line or a single set of bars, reports of average positions were significantly biased, such that line positions were underestimated and bar positions were overestimated. In displays with multiple data series (i.e., multiple lines and/or sets of bars), this systematic bias still persisted. We also observed an effect of \u201cperceptual pull\u201d, where the average position estimate for each series was \u2018pulled\u2019 toward the other. These findings suggest that, although position may still be the most precise form of visual data encoding, it can also be systematically biased.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934400",
            "id": "r_322",
            "s_ids": [
                "s_1119",
                "s_1207",
                "s_123",
                "s_811"
            ],
            "type": "rich",
            "x": 0.11748113288900006,
            "y": -0.42878606208834
        },
        {
            "title": "Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification",
            "data": "Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864526",
            "id": "r_323",
            "s_ids": [
                "s_1244",
                "s_108",
                "s_453"
            ],
            "type": "rich",
            "x": 0.26439699780276166,
            "y": -0.3470834821788176
        },
        {
            "title": "PelVis: Atlas-based Surgical Planning for Oncological Pelvic Surgery",
            "data": "Due to the intricate relationship between the pelvic organs and vital structures, such as vessels and nerves, pelvic anatomy is often considered to be complex to comprehend. In oncological pelvic surgery, a trade-off has to be made between complete tumor resection and preserving function by preventing damage to the nerves. Damage to the autonomic nerves causes undesirable post-operative side-effects such as fecal and urinal incontinence, as well as sexual dysfunction in up to 80 percent of the cases. Since these autonomic nerves are not visible in pre-operative MRI scans or during surgery, avoiding nerve damage during such a surgical procedure becomes challenging. In this work, we present visualization methods to represent context, target, and risk structures for surgical planning. We employ distance-based and occlusion management techniques in an atlas-based surgical planning tool for oncological pelvic surgery. Patient-specific pre-operative MRI scans are registered to an atlas model that includes nerve information. Through several interactive linked views, the spatial relationships and distances between the organs, tumor and risk zones are visualized to improve understanding, while avoiding occlusion. In this way, the surgeon can examine surgically relevant structures and plan the procedure before going into the operating theater, thus raising awareness of the autonomic nerve zone regions and potentially reducing post-operative complications. Furthermore, we present the results of a domain expert evaluation with surgical oncologists that demonstrates the advantages of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598826",
            "id": "r_324",
            "s_ids": [
                "s_1430",
                "s_1467",
                "s_1079",
                "s_731",
                "s_385",
                "s_900",
                "s_145",
                "s_408"
            ],
            "type": "rich",
            "x": -0.5685300078560792,
            "y": -0.25980847152543174
        },
        {
            "title": "Visual Verification of Space Weather Ensemble Simulations",
            "data": "We propose a system to analyze and contextualize simulations of coronal mass ejections. As current simulation techniques require manual input, uncertainty is introduced into the simulation pipeline leading to inaccurate predictions that can be mitigated through ensemble simulations. We provide the space weather analyst with a multi-view system providing visualizations to: 1. compare ensemble members against ground truth measurements, 2. inspect time-dependent information derived from optical flow analysis of satellite images, and 3. combine satellite images with a volumetric rendering of the simulations. This three-tier workflow provides experts with tools to discover correlations between errors in predictions and simulation parameters, thus increasing knowledge about the evolution and propagation of coronal mass ejections that pose a danger to Earth and interplanetary travel.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429487",
            "id": "r_325",
            "s_ids": [
                "s_1233",
                "s_446",
                "s_186",
                "s_451",
                "s_209",
                "s_260"
            ],
            "type": "rich",
            "x": -0.31868039907234,
            "y": 0.4201904483239726
        },
        {
            "title": "Details-First, Show Context, Overview Last: Supporting Exploration of Viscous Fingers in Large-Scale Ensemble Simulations",
            "data": "Visualization research often seeks designs that first establish an overview of the data, in accordance to the information seeking mantra: \u201cOverview first, zoom and filter, then details on demand\u201d. However, in computational fluid dynamics (CFD), as well as in other domains, there are many situations where such a spatial overview is not relevant or practical for users, for example when the experts already have a good mental overview of the data, or when an analysis of a large overall structure may not be related to the specific, information-driven tasks of users. Using scientific workflow theory and, as a vehicle, the problem of viscous finger evolution, we advocate an alternative model that allows domain experts to explore features of interest first, then explore the context around those features, and finally move to a potentially unfamiliar summarization overview. In a model instantiation, we show how a computational back-end can identify and track over time low-level, small features, then be used to filter the context of those features while controlling the complexity of the visualization, and finally to summarize and compare simulations. We demonstrate the effectiveness of this approach with an online web-based exploration of a total volume of data approaching half a billion seven-dimensional data points, and report supportive feedback provided by domain experts with respect to both the instantiation and the theoretical model.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864849",
            "id": "r_326",
            "s_ids": [
                "s_347",
                "s_1036",
                "s_1409",
                "s_144",
                "s_459"
            ],
            "type": "rich",
            "x": 0.24920827613198018,
            "y": -0.012700167170850147
        },
        {
            "title": "PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models",
            "data": "PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes-each with its own temporally evolving prevalence and co-occurrence of phenotypes-without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745118",
            "id": "r_327",
            "s_ids": [
                "s_284",
                "s_810",
                "s_265",
                "s_182",
                "s_253",
                "s_912",
                "s_734"
            ],
            "type": "rich",
            "x": -0.12139081784780444,
            "y": -0.5002849649243132
        },
        {
            "title": "Skeleton-Based Scagnostics",
            "data": "Scatterplot matrices (SPLOMs) are widely used for exploring multidimensional data. Scatterplot diagnostics (scagnostics) approaches measure characteristics of scatterplots to automatically find potentially interesting plots, thereby making SPLOMs more scalable with the dimension count. While statistical measures such as regression lines can capture orientation, and graph-theoretic scagnostics measures can capture shape, there is no scatterplot characterization measure that uses both descriptors. Based on well-known results in shape analysis, we propose a scagnostics approach that captures both scatterplot shape and orientation using skeletons (or medial axes). Our representation can handle complex spatial distributions, helps discovery of principal trends in a multiscale way, scales visually well with the number of samples, is robust to noise, and is automatic and fast to compute. We define skeleton-based similarity metrics for the visual exploration and analysis of SPLOMs. We perform a user study to measure the human perception of scatterplot similarity and compare the outcome to our results as well as to graph-based scagnostics and other visual quality metrics. Our skeleton-based metrics outperform previously defined measures both in terms of closeness to perceptually-based similarity and computation time efficiency.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744339",
            "id": "r_328",
            "s_ids": [
                "s_409",
                "s_133",
                "s_890"
            ],
            "type": "rich",
            "x": -0.29227103286507305,
            "y": 0.17421071739965924
        },
        {
            "title": "PhenoBlocks: Phenotype Comparison Visualizations",
            "data": "The differential diagnosis of hereditary disorders is a challenging task for clinicians due to the heterogeneity of phenotypes that can be observed in patients. Existing clinical tools are often text-based and do not emphasize consistency, completeness, or granularity of phenotype reporting. This can impede clinical diagnosis and limit their utility to genetics researchers. Herein, we present PhenoBlocks, a novel visual analytics tool that supports the comparison of phenotypes between patients, or between a patient and the hallmark features of a disorder. An informal evaluation of PhenoBlocks with expert clinicians suggested that the visualization effectively guides the process of differential diagnosis and could reinforce the importance of complete, granular phenotypic reporting.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467733",
            "id": "r_329",
            "s_ids": [
                "s_284",
                "s_675",
                "s_182",
                "s_1134",
                "s_253",
                "s_684",
                "s_734"
            ],
            "type": "rich",
            "x": -0.3765434575495463,
            "y": -0.36677340906033545
        },
        {
            "title": "CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees",
            "data": "We present a novel approach to rendering large particle data sets from molecular dynamics, astrophysics and other sources. We employ a new data structure adapted from the original balanced k-d tree, which allows for representation of data with trivial or no overhead. In the OSPRay visualization framework, we have developed an efficient CPU algorithm for traversing, classifying and ray tracing these data. Our approach is able to render up to billions of particles on a typical workstation, purely on the CPU, without any approximations or level-of-detail techniques, and optionally with attribute-based color mapping, dynamic range query, and advanced lighting models such as ambient occlusion and path tracing.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429492",
            "id": "r_330",
            "s_ids": [
                "s_1429",
                "s_1480",
                "s_994",
                "s_612",
                "s_297",
                "s_656"
            ],
            "type": "rich",
            "x": -0.33388100404371085,
            "y": 0.5224777136343881
        },
        {
            "title": "Improving the Robustness of Scagnostics",
            "data": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934796",
            "id": "r_331",
            "s_ids": [
                "s_1059",
                "s_434",
                "s_44",
                "s_1215",
                "s_46",
                "s_1220",
                "s_781"
            ],
            "type": "rich",
            "x": -0.19840978388254396,
            "y": -0.15596489895033147
        },
        {
            "title": "A Declarative Rendering Model for Multiclass Density Maps",
            "data": "Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model-a simple yet expressive JSON grammar associated with visual semantics-that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865141",
            "id": "r_332",
            "s_ids": [
                "s_1308",
                "s_350",
                "s_231",
                "s_100"
            ],
            "type": "rich",
            "x": -0.21850286524030615,
            "y": 0.0003654312306877226
        },
        {
            "title": "Tensor Field Visualization using Fiber Surfaces of Invariant Space",
            "data": "Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as pre-images of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three-dimensional for symmetric second-order tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864846",
            "id": "r_333",
            "s_ids": [
                "s_1302",
                "s_860",
                "s_1386",
                "s_807",
                "s_1130",
                "s_233",
                "s_1064",
                "s_841"
            ],
            "type": "rich",
            "x": -0.7472301098486734,
            "y": 0.4028070430793463
        },
        {
            "title": "Dynamic Influence Networks for Rule-Based Models",
            "data": "We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule-based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types, we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745280",
            "id": "r_334",
            "s_ids": [
                "s_661",
                "s_1036",
                "s_979",
                "s_136",
                "s_69",
                "s_322",
                "s_1057"
            ],
            "type": "rich",
            "x": -0.5357616984509417,
            "y": -0.08723447430978541
        },
        {
            "title": "Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures",
            "data": "We present an approach to represent DNA nanostructures in varying forms of semantic abstraction, describe ways to smoothly transition between them, and thus create a continuous multiscale visualization and interaction space for applications in DNA nanotechnology. This new way of observing, interacting with, and creating DNA nanostructures enables domain experts to approach their work in any of the semantic abstraction levels, supporting both low-level manipulations and high-level visualization and modifications. Our approach allows them to deal with the increasingly complex DNA objects that they are designing, to improve their features, and to add novel functions in a way that no existing single-scale approach offers today. For this purpose we collaborated with DNA nanotechnology experts to design a set of ten semantic scales. These scales take the DNA's chemical and structural behavior into account and depict it from atoms to the targeted architecture with increasing levels of abstraction. To create coherence between the discrete scales, we seamlessly transition between them in a well-defined manner. We use special encodings to allow experts to estimate the nanoscale object's stability. We also add scale-adaptive interactions that facilitate the intuitive modification of complex structures at multiple scales. We demonstrate the applicability of our approach on an experimental use case. Moreover, feedback from our collaborating domain experts confirmed an increased time efficiency and certainty for analysis and modification tasks on complex DNA structures. Our method thus offers exciting new opportunities with promising applications in medicine and biotechnology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743981",
            "id": "r_335",
            "s_ids": [
                "s_1043",
                "s_236",
                "s_521",
                "s_1316",
                "s_175",
                "s_1205",
                "s_519",
                "s_396",
                "s_443"
            ],
            "type": "rich",
            "x": -0.5123531087099976,
            "y": 0.12587065243599382
        },
        {
            "title": "What do Constraint Programming Users Want to See? Exploring the Role of Visualisation in Profiling of Models and Search",
            "data": "Constraint programming allows difficult combinatorial problems to be modelled declaratively and solved automatically. Advances in solver technologies over recent years have allowed the successful use of constraint programming in many application areas. However, when a particular solver's search for a solution takes too long, the complexity of the constraint program execution hinders the programmer's ability to profile that search and understand how it relates to their model. Therefore, effective tools to support such profiling and allow users of constraint programming technologies to refine their model or experiment with different search parameters are essential. This paper details the first user-centred design process for visual profiling tools in this domain. We report on: our insights and opportunities identified through an on-line questionnaire and a creativity workshop with domain experts carried out to elicit requirements for analytical and visual profiling techniques; our designs and functional prototypes realising such techniques; and case studies demonstrating how these techniques shed light on the behaviour of the solvers in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598545",
            "id": "r_336",
            "s_ids": [
                "s_91",
                "s_1236",
                "s_202",
                "s_1265",
                "s_949",
                "s_271"
            ],
            "type": "rich",
            "x": 0.18346292984760348,
            "y": -0.26143945453132
        },
        {
            "title": "booc.io: An Education System with Hierarchical Concept Maps and Dynamic Non-linear Learning Plans",
            "data": "Information hierarchies are difficult to express when real-world space or time constraints force traversing the hierarchy in linear presentations, such as in educational books and classroom courses. We present booc.io, which allows linear and non-linear presentation and navigation of educational concepts and material. To support a breadth of material for each concept, booc.io is Web based, which allows adding material such as lecture slides, book chapters, videos, and LTIs. A visual interface assists the creation of the needed hierarchical structures. The goals of our system were formed in expert interviews, and we explain how our design meets these goals. We adapt a real-world course into booc.io, and perform introductory qualitative evaluation with students.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598518",
            "id": "r_337",
            "s_ids": [
                "s_963",
                "s_869",
                "s_1355",
                "s_156",
                "s_1094",
                "s_376",
                "s_844",
                "s_430",
                "s_714",
                "s_1346"
            ],
            "type": "rich",
            "x": 0.3205323594117832,
            "y": -0.4602218753923927
        },
        {
            "title": "LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design",
            "data": "State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468011",
            "id": "r_338",
            "s_ids": [
                "s_521",
                "s_101",
                "s_1435",
                "s_57",
                "s_519",
                "s_270"
            ],
            "type": "rich",
            "x": -0.02023689701156562,
            "y": 0.3062305679421562
        },
        {
            "title": "Interstitial and Interlayer Ion Diffusion Geometry Extraction in Graphitic Nanosphere Battery Materials",
            "data": "Large-scale molecular dynamics (MD) simulations are commonly used for simulating the synthesis and ion diffusion of battery materials. A good battery anode material is determined by its capacity to store ion or other diffusers. However, modeling of ion diffusion dynamics and transport properties at large length and long time scales would be impossible with current MD codes. To analyze the fundamental properties of these materials, therefore, we turn to geometric and topological analysis of their structure. In this paper, we apply a novel technique inspired by discrete Morse theory to the Delaunay triangulation of the simulated geometry of a thermally annealed carbon nanosphere. We utilize our computed structures to drive further geometric analysis to extract the interstitial diffusion structure as a single mesh. Our results provide a new approach to analyze the geometry of the simulated carbon nanosphere, and new insights into the role of carbon defect size and distribution in determining the charge capacity and charge dynamics of these carbon based battery materials.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467432",
            "id": "r_339",
            "s_ids": [
                "s_1472",
                "s_1480",
                "s_1333",
                "s_722",
                "s_438",
                "s_656",
                "s_618",
                "s_297"
            ],
            "type": "rich",
            "x": -0.698383964821159,
            "y": 0.3928008178480896
        },
        {
            "title": "Construct-A-Vis: Exploring the Free-Form Visualization Processes of Children",
            "data": "Building data analysis skills is part of modern elementary school curricula. Recent research has explored how to facilitate children's understanding of visual data representations through completion exercises which highlight links between concrete and abstract mappings. This approach scaffolds visualization activities by presenting a target visualization to children. But how can we engage children in more free-form visual data mapping exercises that are driven by their own mapping ideas? How can we scaffold a creative exploration of visualization techniques and mapping possibilities? We present Construct-A-Vis, a tablet-based tool designed to explore the feasibility of free-form and constructive visualization activities with elementary school children. Construct-A-Vis provides adjustable levels of scaffolding visual mapping processes. It can be used by children individually or as part of collaborative activities. Findings from a study with elementary school children using Construct-A-Vis individually and in pairs highlight the potential of this free-form constructive approach, as visible in children's diverse visualization outcomes and their critical engagement with the data and mapping processes. Based on our study findings we contribute insights into the design of free-form visualization tools for children, including the role of tool-based scaffolding mechanisms and shared interactions to guide visualization activities with children.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934804",
            "id": "r_340",
            "s_ids": [
                "s_820",
                "s_575",
                "s_1096",
                "s_786",
                "s_595",
                "s_1405"
            ],
            "type": "rich",
            "x": 0.7539482547719539,
            "y": -0.030252856277856476
        },
        {
            "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos",
            "data": "Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934656",
            "id": "r_341",
            "s_ids": [
                "s_328",
                "s_1142",
                "s_760",
                "s_1115",
                "s_477",
                "s_1469",
                "s_137"
            ],
            "type": "rich",
            "x": 0.4775053586968263,
            "y": -0.188278807145433
        },
        {
            "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference",
            "data": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934595",
            "id": "r_342",
            "s_ids": [
                "s_778",
                "s_869",
                "s_1081",
                "s_1346",
                "s_445"
            ],
            "type": "rich",
            "x": 0.021004286736282272,
            "y": -0.5604081461511099
        },
        {
            "title": "Searching the Visual Style and Structure of D3 Visualizations",
            "data": "We present a search engine for D3 visualizations that allows queries based on their visual style and underlying structure. To build the engine we crawl a collection of 7860 D3 visualizations from the Web and deconstruct each one to recover its data, its data-encoding marks and the encodings describing how the data is mapped to visual attributes of the marks. We also extract axes and other non-data-encoding attributes of marks (e.g., typeface, background color). Our search engine indexes this style and structure information as well as metadata about the webpage containing the chart. We show how visualization developers can search the collection to find visualizations that exhibit specific design characteristics and thereby explore the space of possible designs. We also demonstrate how researchers can use the search engine to identify commonly used visual design patterns and we perform such a demographic design analysis across our collection of D3 charts. A user study reveals that visualization developers found our style and structure based search engine to be significantly more useful and satisfying for finding different designs of D3 charts, than a baseline search engine that only allows keyword search over the webpage containing a chart.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934431",
            "id": "r_343",
            "s_ids": [
                "s_254",
                "s_653"
            ],
            "type": "rich",
            "x": 0.5200865196236301,
            "y": 0.12919946201358545
        },
        {
            "title": "At a Glance: Pixel Approximate Entropy as a Measure of Line Chart Complexity",
            "data": "When inspecting information visualizations under time critical settings, such as emergency response or monitoring the heart rate in a surgery room, the user only has a small amount of time to view the visualization \u201cat a glance\u201d. In these settings, it is important to provide a quantitative measure of the visualization to understand whether or not the visualization is too \u201ccomplex\u201d to accurately judge at a glance. This paper proposes Pixel Approximate Entropy (PAE), which adapts the approximate entropy statistical measure commonly used to quantify regularity and unpredictability in time-series data, as a measure of visual complexity for line charts. We show that PAE is correlated with user-perceived chart complexity, and that increased chart PAE correlates with reduced judgement accuracy. `We also find that the correlation between PAE values and participants' judgment increases when the user has less time to examine the line charts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865264",
            "id": "r_344",
            "s_ids": [
                "s_249",
                "s_909",
                "s_321",
                "s_1065"
            ],
            "type": "rich",
            "x": 0.466493752220708,
            "y": -0.1766740484211474
        },
        {
            "title": "Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs",
            "data": "Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865232",
            "id": "r_345",
            "s_ids": [
                "s_403",
                "s_812",
                "s_225",
                "s_896",
                "s_137"
            ],
            "type": "rich",
            "x": 0.5837126277891537,
            "y": -0.26091305132271314
        },
        {
            "title": "Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web",
            "data": "The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865117",
            "id": "r_346",
            "s_ids": [
                "s_704",
                "s_817",
                "s_124"
            ],
            "type": "rich",
            "x": 0.7134910717804884,
            "y": -0.11616850849413779
        },
        {
            "title": "A Visual Analytics Framework for Spatiotemporal Trade Network Analysis",
            "data": "Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864844",
            "id": "r_347",
            "s_ids": [
                "s_1255",
                "s_776",
                "s_643",
                "s_1461",
                "s_835",
                "s_67",
                "s_708"
            ],
            "type": "rich",
            "x": 0.2121691625100331,
            "y": 0.1398028771144596
        },
        {
            "title": "Interactive 3D Visual Analysis of Atmospheric Fronts",
            "data": "Atmospheric fronts play a central role in meteorology, as the boundaries between different air masses and as fundamental features of extra-tropical cyclones. They appear in numerous conceptual model depictions of extra-tropical weather systems. Conceptually, fronts are three-dimensional surfaces in space possessing an innate structural complexity, yet in meteorology, both manual and objective identification and depiction have historically focused on the structure in two dimensions. In this work, we -a team of visualization scientists and meteorologists-propose a novel visualization approach to analyze the three-dimensional structure of atmospheric fronts and related physical and dynamical processes. We build upon existing approaches to objectively identify fronts as lines in two dimensions and extend these to obtain frontal surfaces in three dimensions, using the magnitude of temperature change along the gradient of a moist potential temperature field as the primary identifying factor. We introduce the use of normal curves in the temperature gradient field to visualize a frontal zone (i.e., the transitional zone between the air masses) and the distribution of atmospheric variables in such zones. To enable for the first time a statistical analysis of frontal zones, we present a new approach to obtain the volume enclosed by a zone, by classifying grid boxes that intersect with normal curves emanating from a selected front. We introduce our method by means of an idealized numerical simulation and demonstrate its use with two real-world cases using numerical weather prediction data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864806",
            "id": "r_348",
            "s_ids": [
                "s_705",
                "s_38",
                "s_766",
                "s_111",
                "s_1390"
            ],
            "type": "rich",
            "x": -0.2718549282875406,
            "y": 0.601872259486944
        },
        {
            "title": "Dynamic Volume Lines: Visual Comparison of 3D Volumes through Space-filling Curves",
            "data": "The comparison of many members of an ensemble is difficult, tedious, and error-prone, which is aggravated by often just subtle differences. In this paper, we introduce Dynamic Volume Lines for the interactive visual analysis and comparison of sets of 3D volumes. Each volume is linearized along a Hilbert space-filling curve into a 1D Hilbert line plot, which depicts the intensities over the Hilbert indices. We present a nonlinear scaling of these 1D Hilbert line plots based on the intensity variations in the ensemble of 3D volumes, which enables a more effective use of the available screen space. The nonlinear scaling builds the basis for our interactive visualization techniques. An interactive histogram heatmap of the intensity frequencies serves as overview visualization. When zooming in, the frequencies are replaced by detailed 1D Hilbert line plots and optional functional boxplots. To focus on important regions of the volume ensemble, nonlinear scaling is incorporated into the plots. An interactive scaling widget depicts the local ensemble variations. Our brushing and linking interface reveals, for example, regions with a high ensemble variation by showing the affected voxels in a 3D spatial view. We show the applicability of our concepts using two case studies on ensembles of 3D volumes resulting from tomographic reconstruction. In the first case study, we evaluate an artificial specimen from simulated industrial 3D X-ray computed tomography (XCT). In the second case study, a real-world XCT foam specimen is investigated. Our results show that Dynamic Volume Lines can identify regions with high local intensity variations, allowing the user to draw conclusions, for example, about the choice of reconstruction parameters. Furthermore, it is possible to detect ring artifacts in reconstructions volumes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864510",
            "id": "r_349",
            "s_ids": [
                "s_598",
                "s_1186",
                "s_519",
                "s_838",
                "s_19"
            ],
            "type": "rich",
            "x": -0.4281978695738361,
            "y": 0.3097385452836336
        },
        {
            "title": "BiDots: Visual Exploration of Weighted Biclusters",
            "data": "Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity. This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744458",
            "id": "r_350",
            "s_ids": [
                "s_1283",
                "s_112",
                "s_466",
                "s_742"
            ],
            "type": "rich",
            "x": 0.23420258429777602,
            "y": 0.020582902677863623
        },
        {
            "title": "CyteGuide: Visual Guidance for Hierarchical Single-Cell Analysis",
            "data": "Single-cell analysis through mass cytometry has become an increasingly important tool for immunologists to study the immune system in health and disease. Mass cytometry creates a high-dimensional description vector for single cells by time-of-flight measurement. Recently, t-Distributed Stochastic Neighborhood Embedding (t-SNE) has emerged as one of the state-of-the-art techniques for the visualization and exploration of single-cell data. Ever increasing amounts of data lead to the adoption of Hierarchical Stochastic Neighborhood Embedding (HSNE), enabling the hierarchical representation of the data. Here, the hierarchy is explored selectively by the analyst, who can request more and more detail in areas of interest. Such hierarchies are usually explored by visualizing disconnected plots of selections in different levels of the hierarchy. This poses problems for navigation, by imposing a high cognitive load on the analyst. In this work, we present an interactive summary-visualization to tackle this problem. CyteGuide guides the analyst through the exploration of hierarchically represented single-cell data, and provides a complete overview of the current state of the analysis. We conducted a two-phase user study with domain experts that use HSNE for data exploration. We first studied their problems with their current workflow using HSNE and the requirements to ease this workflow in a field study. These requirements have been the basis for our visual design. In the second phase, we verified our proposed solution in a user evaluation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744318",
            "id": "r_351",
            "s_ids": [
                "s_1157",
                "s_966",
                "s_1350",
                "s_289",
                "s_1054",
                "s_408"
            ],
            "type": "rich",
            "x": -0.1284518750579961,
            "y": -0.03301620304802908
        },
        {
            "title": "Surprise! Bayesian Weighting for De-Biasing Thematic Maps",
            "data": "Thematic maps are commonly used for visualizing the density of events in spatial data. However, these maps can mislead by giving visual prominence to known base rates (such as population densities) or to artifacts of sample size and normalization (such as outliers arising from smaller, and thus more variable, samples). In this work, we adapt Bayesian surprise to generate maps that counter these biases. Bayesian surprise, which has shown promise for modeling human visual attention, weights information with respect to how it updates beliefs over a space of models. We introduce Surprise Maps, a visualization technique that weights event data relative to a set of spatia-temporal models. Unexpected events (those that induce large changes in belief over the model space) are visualized more prominently than those that follow expected patterns. Using both synthetic and real-world datasets, we demonstrate how Surprise Maps overcome some limitations of traditional event maps.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598618",
            "id": "r_352",
            "s_ids": [
                "s_1215",
                "s_1251"
            ],
            "type": "rich",
            "x": -0.04191485788246081,
            "y": 0.05485779347381342
        },
        {
            "title": "PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations",
            "data": "Cross-sectional phenotype studies are used by genetics researchers to better understand how phenotypes vary across patients with genetic diseases, both within and between cohorts. Analyses within cohorts identify patterns between phenotypes and patients (e.g., co-occurrence) and isolate special cases (e.g., potential outliers). Comparing the variation of phenotypes between two cohorts can help distinguish how different factors affect disease manifestation (e.g., causal genes, age of onset, etc.). PhenoStacks is a novel visual analytics tool that supports the exploration of phenotype variation within and between cross-sectional patient cohorts. By leveraging the semantic hierarchy of the Human Phenotype Ontology, phenotypes are presented in context, can be grouped and clustered, and are summarized via overviews to support the exploration of phenotype distributions. The design of PhenoStacks was motivated by formative interviews with genetics researchers: we distil high-level tasks, present an algorithm for simplifying ontology topologies for visualization, and report the results of a deployment evaluation with four expert genetics researchers. The results suggest that PhenoStacks can help identify phenotype patterns, investigate data quality issues, and inform data collection design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598469",
            "id": "r_353",
            "s_ids": [
                "s_284",
                "s_1401",
                "s_182",
                "s_253",
                "s_734",
                "s_912"
            ],
            "type": "rich",
            "x": -0.3727231384550784,
            "y": -0.3019714753089971
        },
        {
            "title": "Vials: Visualizing Alternative Splicing of Genes",
            "data": "Alternative splicing is a process by which the same DNA sequence is used to assemble different proteins, called protein isoforms. Alternative splicing works by selectively omitting some of the coding regions (exons) typically associated with a gene. Detection of alternative splicing is difficult and uses a combination of advanced data acquisition methods and statistical inference. Knowledge about the abundance of isoforms is important for understanding both normal processes and diseases and to eventually improve treatment through targeted therapies. The data, however, is complex and current visualizations for isoforms are neither perceptually efficient nor scalable. To remedy this, we developed Vials, a novel visual analysis tool that enables analysts to explore the various datasets that scientists use to make judgments about isoforms: the abundance of reads associated with the coding regions of the gene, evidence for junctions, i.e., edges connecting the coding regions, and predictions of isoform frequencies. Vials is scalable as it allows for the simultaneous analysis of many samples in multiple groups. Our tool thus enables experts to (a) identify patterns of isoform abundance in groups of samples and (b) evaluate the quality of the data. We demonstrate the value of our tool in case studies using publicly available datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467911",
            "id": "r_354",
            "s_ids": [
                "s_869",
                "s_1335",
                "s_213",
                "s_756",
                "s_647",
                "s_1346",
                "s_211"
            ],
            "type": "rich",
            "x": -0.38824588890242595,
            "y": -0.04452601014948056
        },
        {
            "title": "AnimoAminoMiner: Exploration of Protein Tunnels and their Properties in Molecular Dynamics",
            "data": "In this paper we propose a novel method for the interactive exploration of protein tunnels. The basic principle of our approach is that we entirely abstract from the 3D/4D space the simulated phenomenon is embedded in. A complex 3D structure and its curvature information is represented only by a straightened tunnel centerline and its width profile. This representation focuses on a key aspect of the studied geometry and frees up graphical estate to key chemical and physical properties represented by surrounding amino acids. The method shows the detailed tunnel profile and its temporal aggregation. The profile is interactively linked with a visual overview of all amino acids which are lining the tunnel over time. In this overview, each amino acid is represented by a set of colored lines depicting the spatial and temporal impact of the amino acid on the corresponding tunnel. This representation clearly shows the importance of amino acids with respect to selected criteria. It helps the biochemists to select the candidate amino acids for mutation which changes the protein function in a desired way. The AnimoAminoMiner was designed in close cooperation with domain experts. Its usefulness is documented by their feedback and a case study, which are included.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467434",
            "id": "r_355",
            "s_ids": [
                "s_874",
                "s_1486",
                "s_519",
                "s_443",
                "s_1227"
            ],
            "type": "rich",
            "x": -0.5492740375060834,
            "y": 0.08800386017612317
        },
        {
            "title": "Visualization-by-Sketching: An Artist's Interface for Creating Multivariate Time-Varying Data Visualizations",
            "data": "We present Visualization-by-Sketching, a direct-manipulation user interface for designing new data visualizations. The goals are twofold: First, make the process of creating real, animated, data-driven visualizations of complex information more accessible to artists, graphic designers, and other visual experts with traditional, non-technical training. Second, support and enhance the role of human creativity in visualization design, enabling visual experimentation and workflows similar to what is possible with traditional artistic media. The approach is to conceive of visualization design as a combination of processes that are already closely linked with visual creativity: sketching, digital painting, image editing, and reacting to exemplars. Rather than studying and tweaking low-level algorithms and their parameters, designers create new visualizations by painting directly on top of a digital data canvas, sketching data glyphs, and arranging and blending together multiple layers of animated 2D graphics. This requires new algorithms and techniques to interpret painterly user input relative to data \u201cunder\u201d the canvas, balance artistic freedom with the need to produce accurate data visualizations, and interactively explore large (e.g., terabyte-sized) multivariate datasets. Results demonstrate a variety of multivariate data visualization techniques can be rapidly recreated using the interface. More importantly, results and feedback from artists support the potential for interfaces in this style to attract new, creative users to the challenging task of designing more effective data visualizations and to help these users stay \u201cin the creative zone\u201d as they work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467153",
            "id": "r_356",
            "s_ids": [
                "s_700",
                "s_1199"
            ],
            "type": "rich",
            "x": 0.6419515496625008,
            "y": 0.1474951806534578
        },
        {
            "title": "BarcodeTree: Scalable Comparison of Multiple Hierarchies",
            "data": "We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT's utility.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934535",
            "id": "r_357",
            "s_ids": [
                "s_331",
                "s_724",
                "s_1058",
                "s_1206",
                "s_543",
                "s_366",
                "s_285",
                "s_1214"
            ],
            "type": "rich",
            "x": -0.5496555994850069,
            "y": -0.19190764305540436
        },
        {
            "title": "MotionRugs: Visualizing Collective Trends in Space and Time",
            "data": "Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865049",
            "id": "r_358",
            "s_ids": [
                "s_1468",
                "s_37",
                "s_1310",
                "s_259",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.03347391173986697,
            "y": 0.4361777277943584
        },
        {
            "title": "Interactive Visualization of 3D Histopathology in Native Resolution",
            "data": "We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864816",
            "id": "r_359",
            "s_ids": [
                "s_592",
                "s_260",
                "s_358",
                "s_944"
            ],
            "type": "rich",
            "x": -0.4738704441414579,
            "y": -0.0546486748468245
        },
        {
            "title": "Measures of the Benefit of Direct Encoding of Data Deltas for Data Pair Relation Perception",
            "data": "The power of data visualization is not to convey absolute values of individual data points, but to allow the exploration of relations (increases or decreases in a data value) among them. One approach to highlighting these relations is to explicitly encode the numeric differences (deltas) between data values. Because this approach removes the context of the individual data values, it is important to measure how much of a performance improvement it actually offers, especially across differences in encodings and tasks, to ensure that it is worth adding to a visualization design. Across 3 different tasks, we measured the increase in visual processing efficiency for judging the relations between pairs of data values, from when only the values were shown, to when the deltas between the values were explicitly encoded, across position and length visual feature encodings (and slope encodings in Experiments 1 & 2). In Experiment 1, the participant's task was to locate a pair of data values with a given relation (e.g., Find the \u2018small bar to the left of a tall bar\u2019 pair) among pairs of the opposite relation, and we measured processing efficiency from the increase in response times as the number of pairs increased. In Experiment 2, the task was to judge which of two relation types was more prevalent in a briefly presented display of 10 data pairs (e.g., Are there more \u2018small bar to the left of a tall bar\u2019 pairs or more \u2018tall bar to the left of a small bar\u2019 pairs?). In the final experiment, the task was to estimate the average delta within a briefly presented display of 6 data pairs (e.g., What is the average bar height difference across all \u2018small bar to the left of a tall bar\u2019 pairs?). Across all three experiments, visual processing of relations between data value pairs was significantly better when directly encoded as deltas rather than implicitly between individual data points, and varied substantially depending on the task (improvement ranged from 25% to 95%). Considering the ubiquity of bar charts and dot plots, relation perception for individual data values is highly inefficient, and confirms the need for alternative designs that provide not only absolute values, but also direct encoding of critical relationships between those values.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934801",
            "id": "r_360",
            "s_ids": [
                "s_475",
                "s_811"
            ],
            "type": "rich",
            "x": 0.05738607322297312,
            "y": -0.1924727615171813
        },
        {
            "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections",
            "data": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934654",
            "id": "r_361",
            "s_ids": [
                "s_773",
                "s_1381",
                "s_1476",
                "s_1003",
                "s_1220"
            ],
            "type": "rich",
            "x": 0.28072686742763814,
            "y": -0.619329115463283
        },
        {
            "title": "Dynamic Nested Tracking Graphs",
            "data": "This work describes an approach for the interactive visual analysis of large-scale simulations, where numerous superlevel set components and their evolution are of primary interest. The approach first derives, at simulation runtime, a specialized Cinema database that consists of images of component groups, and topological abstractions. This database is processed by a novel graph operation-based nested tracking graph algorithm (GO-NTG) that dynamically computes NTGs for component groups based on size, overlap, persistence, and level thresholds. The resulting NTGs are in turn used in a feature-centered visual analytics framework to query specific database elements and update feature parameters, facilitating flexible post hoc analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934368",
            "id": "r_362",
            "s_ids": [
                "s_32",
                "s_961",
                "s_929",
                "s_806",
                "s_708",
                "s_35"
            ],
            "type": "rich",
            "x": 0.03462583568699639,
            "y": 0.269697493872806
        },
        {
            "title": "Progressive Wasserstein Barycenters of Persistence Diagrams",
            "data": "This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12], [51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the $k$-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934256",
            "id": "r_363",
            "s_ids": [
                "s_541",
                "s_916",
                "s_611"
            ],
            "type": "rich",
            "x": -0.6269197936319084,
            "y": 0.2780980961419321
        },
        {
            "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots",
            "data": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934208",
            "id": "r_364",
            "s_ids": [
                "s_404",
                "s_1357",
                "s_1285",
                "s_685",
                "s_547",
                "s_654",
                "s_1250"
            ],
            "type": "rich",
            "x": -0.08297733974582101,
            "y": -0.06960270133096631
        },
        {
            "title": "Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities",
            "data": "Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864901",
            "id": "r_365",
            "s_ids": [
                "s_308",
                "s_1390",
                "s_578",
                "s_111"
            ],
            "type": "rich",
            "x": -0.2026860841258762,
            "y": 0.5295323970742637
        },
        {
            "title": "MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration",
            "data": "Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864886",
            "id": "r_366",
            "s_ids": [
                "s_1244",
                "s_1080",
                "s_701",
                "s_108"
            ],
            "type": "rich",
            "x": 0.20015710365804756,
            "y": 0.139404556980955
        },
        {
            "title": "Labels on Levels: Labeling of Multi-Scale Multi-Instance and Crowded 3D Biological Environments",
            "data": "Labeling is intrinsically important for exploring and understanding complex environments and models in a variety of domains. We present a method for interactive labeling of crowded 3D scenes containing very many instances of objects spanning multiple scales in size. In contrast to previous labeling methods, we target cases where many instances of dozens of types are present and where the hierarchical structure of the objects in the scene presents an opportunity to choose the most suitable level for each placed label. Our solution builds on and goes beyond labeling techniques in medical 3D visualization, cartography, and biological illustrations from books and prints. In contrast to these techniques, the main characteristics of our new technique are: 1) a novel way of labeling objects as part of a bigger structure when appropriate, 2) visual clutter reduction by labeling only representative instances for each type of an object, and a strategy of selecting those. The appropriate level of label is chosen by analyzing the scene's depth buffer and the scene objects' hierarchy tree. We address the topic of communicating the parent-children relationship between labels by employing visual hierarchy concepts adapted from graphic design. Selecting representative instances considers several criteria tailored to the character of the data and is combined with a greedy optimization approach. We demonstrate the usage of our method with models from mesoscale biology where these two characteristics-multi-scale and multi-instance-are abundant, along with the fact that these scenes are extraordinarily dense.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864491",
            "id": "r_367",
            "s_ids": [
                "s_978",
                "s_89",
                "s_1227",
                "s_496",
                "s_1362",
                "s_503",
                "s_849",
                "s_519",
                "s_443"
            ],
            "type": "rich",
            "x": -0.31655507076193484,
            "y": -0.10600634870202261
        },
        {
            "title": "Understanding the Relationship Between Interactive Optimisation and Visual Analytics in the Context of Prostate Brachytherapy",
            "data": "The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose \u201cblack-box\u201d solver. In practice, however, many problems cannot be solved completely automatically, but require a \u201chuman-in-the-loop\u201d to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient's tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744418",
            "id": "r_368",
            "s_ids": [
                "s_767",
                "s_202",
                "s_1188",
                "s_170",
                "s_11"
            ],
            "type": "rich",
            "x": 0.014548011879522168,
            "y": -0.2884036898536056
        },
        {
            "title": "Blinded with Science or Informed by Charts? A Replication Study",
            "data": "We provide a reappraisal of Tal and Wansink's study \u201cBlinded with Science\u201d, where seemingly trivial charts were shown to increase belief in drug efficacy, presumably because charts are associated with science. Through a series of four replications conducted on two crowdsourcing platforms, we investigate an alternative explanation, namely, that the charts allowed participants to better assess the drug's efficacy. Considered together, our experiments suggest that the chart seems to have indeed promoted understanding, although the effect is likely very small. Meanwhile, we were unable to replicate the original study's findings, as text with chart appeared to be no more persuasive - and sometimes less persuasive - than text alone. This suggests that the effect may not be as robust as claimed and may need specific conditions to be reproduced. Regardless, within our experimental settings and considering our study as a whole (<inline-formula><tex-math notation=\"LaTeX\">$\\mathrm{N}=623$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-dragicevic-2744298-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>), the chart's contribution to understanding was clearly larger than its contribution to persuasion.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744298",
            "id": "r_369",
            "s_ids": [
                "s_231",
                "s_1347"
            ],
            "type": "rich",
            "x": 0.2360398776063962,
            "y": -0.5331583335096611
        },
        {
            "title": "GlyphLens: View-Dependent Occlusion Management in the Interactive Glyph Visualization",
            "data": "Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599049",
            "id": "r_370",
            "s_ids": [
                "s_494",
                "s_1075",
                "s_353"
            ],
            "type": "rich",
            "x": -0.3570615964583475,
            "y": 0.1581922172577251
        },
        {
            "title": "Visualization and Extraction of Carvings for Heritage Conservation",
            "data": "We present novel techniques for visualizing, illustrating, analyzing, and generating carvings in surfaces. In particular, we consider the carvings in the plaster of the cloister of the Magdeburg cathedral, which dates to the 13th century. Due to aging and weathering, the carvings have flattened. Historians and restorers are highly interested in using digitalization techniques to analyze carvings in historic artifacts and monuments and to get impressions and illustrations of their original shape and appearance. Moreover, museums and churches are interested in such illustrations for presenting them to visitors. The techniques that we propose allow for detecting, selecting, and visualizing carving structures. In addition, we introduce an example-based method for generating carvings. The resulting tool, which integrates all techniques, was evaluated by three experienced restorers to assess the usefulness and applicability. Furthermore, we compared our approach with exaggerated shading and other state-of-the-art methods.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598603",
            "id": "r_371",
            "s_ids": [
                "s_1467",
                "s_152",
                "s_619",
                "s_628"
            ],
            "type": "rich",
            "x": -0.1806838560021803,
            "y": -0.06080978134550829
        },
        {
            "title": "Exploring the Possibilities of Embedding Heterogeneous Data Attributes in Familiar Visualizations",
            "data": "Heterogeneous multi-dimensional data are now sufficiently common that they can be referred to as ubiquitous. The most frequent approach to visualizing these data has been to propose new visualizations for representing these data. These new solutions are often inventive but tend to be unfamiliar. We take a different approach. We explore the possibility of extending well-known and familiar visualizations through including Heterogeneous Embedded Data Attributes (HEDA) in order to make familiar visualizations more powerful. We demonstrate how HEDA is a generic, interactive visualization component that can extend common visualization techniques while respecting the structure of the familiar layout. HEDA is a tabular visualization building block that enables individuals to visually observe, explore, and query their familiar visualizations through manipulation of embedded multivariate data. We describe the design space of HEDA by exploring its application to familiar visualizations in the D3 gallery. We characterize these familiar visualizations by the extent to which HEDA can facilitate data queries based on attribute reordering.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598586",
            "id": "r_372",
            "s_ids": [
                "s_110",
                "s_334",
                "s_1476",
                "s_853"
            ],
            "type": "rich",
            "x": 0.2650277039815414,
            "y": 0.19796101955130052
        },
        {
            "title": "Accurate Interactive Visualization of Large Deformations and Variability in Biomedical Image Ensembles",
            "data": "Large image deformations pose a challenging problem for the visualization and statistical analysis of 3D image ensembles which have a multitude of applications in biology and medicine. Simple linear interpolation in the tangent space of the ensemble introduces artifactual anatomical structures that hamper the application of targeted visual shape analysis techniques. In this work we make use of the theory of stationary velocity fields to facilitate interactive non-linear image interpolation and plausible extrapolation for high quality rendering of large deformations and devise an efficient image warping method on the GPU. This does not only improve quality of existing visualization techniques, but opens up a field of novel interactive methods for shape ensemble analysis. Taking advantage of the efficient non-linear 3D image warping, we showcase four visualizations: 1) browsing on-the-fly computed group mean shapes to learn about shape differences between specific classes, 2) interactive reformation to investigate complex morphologies in a single view, 3) likelihood volumes to gain a concise overview of variability and 4) streamline visualization to show variation in detail, specifically uncovering its component tangential to a reference surface. Evaluation on a real world dataset shows that the presented method outperforms the state-of-the-art in terms of visual quality while retaining interactive frame rates. A case study with a domain expert was performed in which the novel analysis and visualization methods are applied on standard model structures, namely skull and mandible of different rodents, to investigate and compare influence of phylogeny, diet and geography on shape. The visualizations enable for instance to distinguish (population-)normal and pathological morphology, assist in uncovering correlation to extrinsic factors and potentially support assessment of model quality.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467198",
            "id": "r_373",
            "s_ids": [
                "s_620",
                "s_262",
                "s_605",
                "s_572"
            ],
            "type": "rich",
            "x": -0.6325488591060807,
            "y": 0.14666135105710418
        },
        {
            "title": "Visualizing Tensor Normal Distributions at Multiple Levels of Detail",
            "data": "Despite the widely recognized importance of symmetric second order tensor fields in medicine and engineering, the visualization of data uncertainty in tensor fields is still in its infancy. A recently proposed tensorial normal distribution, involving a fourth order covariance tensor, provides a mathematical description of how different aspects of the tensor field, such as trace, anisotropy, or orientation, vary and covary at each point. However, this wealth of information is far too rich for a human analyst to take in at a single glance, and no suitable visualization tools are available. We propose a novel approach that facilitates visual analysis of tensor covariance at multiple levels of detail. We start with a visual abstraction that uses slice views and direct volume rendering to indicate large-scale changes in the covariance structure, and locations with high overall variance. We then provide tools for interactive exploration, making it possible to drill down into different types of variability, such as in shape or orientation. Finally, we allow the analyst to focus on specific locations of the field, and provide tensor glyph animations and overlays that intuitively depict confidence intervals at those points. Our system is demonstrated by investigating the effects of measurement noise on diffusion tensor MRI, and by analyzing two ensembles of stress tensor fields from solid mechanics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467031",
            "id": "r_374",
            "s_ids": [
                "s_517",
                "s_1195",
                "s_620",
                "s_605"
            ],
            "type": "rich",
            "x": -0.5730766666915698,
            "y": 0.2971248538280602
        },
        {
            "title": "Visual analysis and coding of data-rich user behavior",
            "data": "Investigating user behavior involves abstracting low-level events to higher-level concepts. This requires an analyst to study individual user activities, assign codes which categorize behavior, and develop a consistent classification scheme. To better support this reasoning process of an analyst, we suggest a novel visual analytics approach which integrates rich user data including transcripts, videos, eye movement data, and interaction logs. Word-sized visualizations embedded into a tabular representation provide a space-efficient and detailed overview of user activities. An analyst assigns codes, grouped into code categories, as part of an interactive process. Filtering and searching helps to select specific activities and focus an analysis. A comparison visualization summarizes results of coding and reveals relationships between codes. Editing features support efficient assignment, refinement, and aggregation of codes. We demonstrate the practical applicability and usefulness of our approach in a case study and describe expert feedback.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883520",
            "id": "r_375",
            "s_ids": [
                "s_1221",
                "s_383",
                "s_435",
                "s_299",
                "s_138"
            ],
            "type": "rich",
            "x": 0.29531926346884496,
            "y": -0.194327245100404
        },
        {
            "title": "Wavelet-based visualization of time-varying data on graphs",
            "data": "Visualizing time-varying data defined on the nodes of a graph is a challenging problem that has been faced with different approaches. Although techniques based on aggregation, topology, and topic modeling have proven their usefulness, the visual analysis of smooth and/or abrupt data variations as well as the evolution of such variations over time are aspects not properly tackled by existing methods. In this work we propose a novel visualization methodology that relies on graph wavelet theory and stacked graph metaphor to enable the visual analysis of time-varying data defined on the nodes of a graph. The proposed method is able to identify regions where data presents abrupt and mild spacial and/or temporal variation while still been able to show how such changes evolve over time, making the identification of events an easier task. The usefulness of our approach is shown through a set of results using synthetic as well as a real data set involving taxi trips in downtown Manhattan. The methodology was able to reveal interesting phenomena and events such as the identification of specific locations with abrupt variation in the number of taxi pickups.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347624",
            "id": "r_376",
            "s_ids": [
                "s_1140",
                "s_1444",
                "s_1294",
                "s_380",
                "s_349"
            ],
            "type": "rich",
            "x": 0.07116913547154122,
            "y": 0.5003372341113357
        },
        {
            "title": "A Comparison of Visualizations for Identifying Correlation over Space and Time",
            "data": "Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization's effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934807",
            "id": "r_377",
            "s_ids": [
                "s_278",
                "s_509",
                "s_161"
            ],
            "type": "rich",
            "x": 0.212875867797582,
            "y": 0.19774890263578063
        },
        {
            "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots",
            "data": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934541",
            "id": "r_378",
            "s_ids": [
                "s_319",
                "s_613",
                "s_1306",
                "s_1161",
                "s_1339",
                "s_1220",
                "s_1059"
            ],
            "type": "rich",
            "x": -0.5237375807496081,
            "y": 0.06794507834506208
        },
        {
            "title": "Common Fate for Animated Transitions in Visualization",
            "data": "The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion > (dynamic luminance, size, luminance); dynamic size > (dynamic luminance, position); and dynamic luminance > size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934288",
            "id": "r_379",
            "s_ids": [
                "s_243",
                "s_1",
                "s_931",
                "s_723",
                "s_50",
                "s_1143",
                "s_182"
            ],
            "type": "rich",
            "x": 0.14354902975308195,
            "y": 0.0007439985682786086
        },
        {
            "title": "Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous Parameter Space",
            "data": "We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865051",
            "id": "r_380",
            "s_ids": [
                "s_413",
                "s_1199",
                "s_789",
                "s_1070",
                "s_80"
            ],
            "type": "rich",
            "x": -0.15085787883046717,
            "y": 0.2739652462419643
        },
        {
            "title": "Structure-aware Fisheye Views for Efficient Large Graph Exploration",
            "data": "Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864911",
            "id": "r_381",
            "s_ids": [
                "s_1059",
                "s_131",
                "s_1450",
                "s_870",
                "s_1339",
                "s_781",
                "s_1161",
                "s_1220"
            ],
            "type": "rich",
            "x": -0.3855660155670182,
            "y": -0.14099588330294627
        },
        {
            "title": "Physics-Based Visual Characterization of Molecular Interaction Forces",
            "data": "Molecular simulations are used in many areas of biotechnology, such as drug design and enzyme engineering. Despite the development of automatic computational protocols, analysis of molecular interactions is still a major aspect where human comprehension and intuition are key to accelerate, analyze, and propose modifications to the molecule of interest. Most visualization algorithms help the users by providing an accurate depiction of the spatial arrangement: the atoms involved in inter-molecular contacts. There are few tools that provide visual information on the forces governing molecular docking. However, these tools, commonly restricted to close interaction between atoms, do not consider whole simulation paths, long-range distances and, importantly, do not provide visual cues for a quick and intuitive comprehension of the energy functions (modeling intermolecular interactions) involved. In this paper, we propose visualizations designed to enable the characterization of interaction forces by taking into account several relevant variables such as molecule-ligand distance and the energy function, which is essential to understand binding affinities. We put emphasis on mapping molecular docking paths obtained from Molecular Dynamics or Monte Carlo simulations, and provide time-dependent visualizations for different energy components and particle resolutions: atoms, groups or residues. The presented visualizations have the potential to support domain experts in a more efficient drug or enzyme design process.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598825",
            "id": "r_382",
            "s_ids": [
                "s_594",
                "s_1046",
                "s_332",
                "s_209",
                "s_2",
                "s_1354"
            ],
            "type": "rich",
            "x": -0.42757121452025726,
            "y": 0.3454492409371981
        },
        {
            "title": "Correlated Photon Mapping for Interactive Global Illumination of Time-Varying Volumetric Data",
            "data": "We present a method for interactive global illumination of both static and time-varying volumetric data based on reduction of the overhead associated with re-computation of photon maps. Our method uses the identification of photon traces invariant to changes of visual parameters such as the transfer function (TF), or data changes between time-steps in a 4D volume. This lets us operate on a variant subset of the entire photon distribution. The amount of computation required in the two stages of the photon mapping process, namely tracing and gathering, can thus be reduced to the subset that are affected by a data or visual parameter change. We rely on two different types of information from the original data to identify the regions that have changed. A low resolution uniform grid containing the minimum and maximum data values of the original data is derived for each time step. Similarly, for two consecutive time-steps, a low resolution grid containing the difference between the overlapping data is used. We show that this compact metadata can be combined with the transfer function to identify the regions that have changed. Each photon traverses the low-resolution grid to identify if it can be directly transferred to the next photon distribution state or if it needs to be recomputed. An efficient representation of the photon distribution is presented leading to an order of magnitude improved performance of the raycasting step. The utility of the method is demonstrated in several examples that show visual fidelity, as well as performance. The examples show that visual quality can be retained when the fraction of retraced photons is as low as 40%-50%.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598430",
            "id": "r_383",
            "s_ids": [
                "s_1488",
                "s_260"
            ],
            "type": "rich",
            "x": -0.3435413442053134,
            "y": 0.46379937971424845
        },
        {
            "title": "Intuitive Exploration of Volumetric Data Using Dynamic Galleries",
            "data": "In this work we present a volume exploration method designed to be used by novice users and visitors to science centers and museums. The volumetric digitalization of artifacts in museums is of rapidly increasing interest as enhanced user experience through interactive data visualization can be achieved. This is, however, a challenging task since the vast majority of visitors are not familiar with the concepts commonly used in data exploration, such as mapping of visual properties from values in the data domain using transfer functions. Interacting in the data domain is an effective way to filter away undesired information but it is difficult to predict where the values lie in the spatial domain. In this work we make extensive use of dynamic previews instantly generated as the user explores the data domain. The previews allow the user to predict what effect changes in the data domain will have on the rendered image without being aware that visual parameters are set in the data domain. Each preview represents a subrange of the data domain where overview and details are given on demand through zooming and panning. The method has been designed with touch interfaces as the target platform for interaction. We provide a qualitative evaluation performed with visitors to a science center to show the utility of the approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467294",
            "id": "r_384",
            "s_ids": [
                "s_1488",
                "s_592",
                "s_260"
            ],
            "type": "rich",
            "x": 0.1389318138203421,
            "y": 0.162019855385056
        },
        {
            "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization",
            "data": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934799",
            "id": "r_385",
            "s_ids": [
                "s_154",
                "s_740",
                "s_535",
                "s_1220",
                "s_972"
            ],
            "type": "rich",
            "x": -0.5367076631928567,
            "y": 0.23455419881488163
        },
        {
            "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling",
            "data": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934266",
            "id": "r_386",
            "s_ids": [
                "s_379",
                "s_1327",
                "s_999",
                "s_921"
            ],
            "type": "rich",
            "x": 0.19518881721539644,
            "y": -0.2869450687212656
        },
        {
            "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media",
            "data": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934263",
            "id": "r_387",
            "s_ids": [
                "s_364",
                "s_303",
                "s_316",
                "s_1214"
            ],
            "type": "rich",
            "x": 0.18541447839064013,
            "y": -0.06184092774664778
        },
        {
            "title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics",
            "data": "Tennis players and coaches of all proficiency levels seek to understand and improve their play. Summary statistics alone are inadequate to provide the insights players need to improve their games. Spatio-temporal data capturing player and ball movements is likely to provide the actionable insights needed to identify player strengths, weaknesses, and strategies. To fully utilize this spatio-temporal data, we need to integrate it with domain-relevant context meta-data. In this paper, we propose CourtTime, a novel approach to perform data-driven visual analysis of individual tennis matches. Our visual approach introduces a novel visual metaphor, namely 1\u2013D Space-Time Charts that enable the analysis of single points at a glance based on small multiples. We also employ user-driven sorting and clustering techniques and a layout technique that aligns the last few shots in a point to facilitate shot pattern discovery. We discuss the usefulness of CourtTime via an extensive case study and report on feedback from an amateur tennis player and three tennis coaches.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934243",
            "id": "r_388",
            "s_ids": [
                "s_125",
                "s_37",
                "s_1341",
                "s_745"
            ],
            "type": "rich",
            "x": 0.2132114089942231,
            "y": 0.35356577830723607
        },
        {
            "title": "A Study of the Trade-off Between Reducing Precision and Reducing Resolution for Data Analysis and Visualization",
            "data": "There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through quantization, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and/or resolution in a principled manner. We represent data reduction schemes as progressive streams of bits and study how various bit orderings such as by resolution, by precision, etc., impact the resulting approximation error across a variety of data sets as well as analysis tasks. Furthermore, we compute streams that are optimized for different tasks to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864853",
            "id": "r_389",
            "s_ids": [
                "s_579",
                "s_83",
                "s_212",
                "s_438",
                "s_1010",
                "s_297"
            ],
            "type": "rich",
            "x": -0.36206919231919815,
            "y": 0.2804772409499253
        },
        {
            "title": "Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization",
            "data": "Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744843",
            "id": "r_390",
            "s_ids": [
                "s_28",
                "s_1292",
                "s_1469",
                "s_108"
            ],
            "type": "rich",
            "x": -0.22633296105113812,
            "y": -0.35701616598392705
        },
        {
            "title": "Visualization Multi-Pipeline for Communicating Biology",
            "data": "We propose a system to facilitate biology communication by developing a pipeline to support the instructional visualization of heterogeneous biological data on heterogeneous user-devices. Discoveries and concepts in biology are typically summarized with illustrations assembled manually from the interpretation and application of heterogenous data. The creation of such illustrations is time consuming, which makes it incompatible with frequent updates to the measured data as new discoveries are made. Illustrations are typically non-interactive, and when an illustration is updated, it still has to reach the user. Our system is designed to overcome these three obstacles. It supports the integration of heterogeneous datasets, reflecting the knowledge that is gained from different data sources in biology. After pre-processing the datasets, the system transforms them into visual representations as inspired by scientific illustrations. As opposed to traditional scientific illustration these representations are generated in real-time - they are interactive. The code generating the visualizations can be embedded in various software environments. To demonstrate this, we implemented both a desktop application and a remote-rendering server in which the pipeline is embedded. The remote-rendering server supports multi-threaded rendering and it is able to handle multiple users simultaneously. This scalability to different hardware environments, including multi-GPU setups, makes our system useful for efficient public dissemination of biological discoveries.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744518",
            "id": "r_391",
            "s_ids": [
                "s_1421",
                "s_978",
                "s_521",
                "s_298",
                "s_1391",
                "s_1362",
                "s_519",
                "s_443"
            ],
            "type": "rich",
            "x": -0.012730084367704316,
            "y": 0.11326807456722161
        },
        {
            "title": "Glyphs for General Second-Order 2D and 3D Tensors",
            "data": "Glyphs are a powerful tool for visualizing second-order tensors in a variety of scientic data as they allow to encode physical behavior in geometric properties. Most existing techniques focus on symmetric tensors and exclude non-symmetric tensors where the eigenvectors can be non-orthogonal or complex. We present a new construction of 2d and 3d tensor glyphs based on piecewise rational curves and surfaces with the following properties: invariance to (a) isometries and (b) scaling, (c) direct encoding of all real eigenvalues and eigenvectors, (d) one-to-one relation between the tensors and glyphs, (e) glyph continuity under changing the tensor. We apply the glyphs to visualize the Jacobian matrix fields of a number of 2d and 3d vector fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598998",
            "id": "r_392",
            "s_ids": [
                "s_901",
                "s_815",
                "s_697"
            ],
            "type": "rich",
            "x": -0.7840317195038415,
            "y": 0.3652421204138218
        },
        {
            "title": "Molecular Surface Maps",
            "data": "We present Molecular Surface Maps, a novel, view-independent, and concise representation for molecular surfaces. It transfers the well-known world map metaphor to molecular visualization. Our application maps the complex molecular surface to a simple 2D representation through a spherical intermediate, the Molecular Surface Globe. The Molecular Surface Map concisely shows arbitrary attributes of the original molecular surface, such as biochemical properties or geometrical features. This results in an intuitive overview, which allows researchers to assess all molecular surface attributes at a glance. Our representation can be used as a visual summarization of a molecule's interface with its environment. In particular, Molecular Surface Maps simplify the analysis and comparison of different data sets or points in time. Furthermore, the map representation can be used in a Space-time Cube to analyze time-dependent data from molecular simulations without the need for animation. We show the feasibility of Molecular Surface Maps for different typical analysis tasks of biomolecular data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598824",
            "id": "r_393",
            "s_ids": [
                "s_926",
                "s_790",
                "s_1097",
                "s_733",
                "s_199",
                "s_1393",
                "s_1270",
                "s_299"
            ],
            "type": "rich",
            "x": -0.5743193898582024,
            "y": 0.32077445851095737
        },
        {
            "title": "A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections",
            "data": "Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598479",
            "id": "r_394",
            "s_ids": [
                "s_1258",
                "s_1320",
                "s_252"
            ],
            "type": "rich",
            "x": -0.2766090587483207,
            "y": 0.08766761003257192
        },
        {
            "title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems",
            "data": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598460",
            "id": "r_395",
            "s_ids": [
                "s_848",
                "s_464",
                "s_1469",
                "s_670"
            ],
            "type": "rich",
            "x": 0.38249097042960756,
            "y": -0.37907438065789956
        },
        {
            "title": "A Simple Approach for Boundary Improvement of Euler Diagrams",
            "data": "General methods for drawing Euler diagrams tend to generate irregular polygons. Yet, empirical evidence indicates that smoother contours make these diagrams easier to read. In this paper, we present a simple method to smooth the boundaries of any Euler diagram drawing. When refining the diagram, the method must ensure that set elements remain inside their appropriate boundaries and that no region is removed or created in the diagram. Our approach uses a force system that improves the diagram while at the same time ensuring its topological structure does not change. We demonstrate the effectiveness of the approach through case studies and quantitative evaluations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467992",
            "id": "r_396",
            "s_ids": [
                "s_913",
                "s_402",
                "s_529"
            ],
            "type": "rich",
            "x": -0.6111480610297126,
            "y": -0.03898564743751993
        },
        {
            "title": "AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations",
            "data": "Datasets commonly include multi-value (set-typed) attributes that describe set memberships over elements, such as genres per movie or courses taken per student. Set-typed attributes describe rich relations across elements, sets, and the set intersections. Increasing the number of sets results in a combinatorial growth of relations and creates scalability challenges. Exploratory tasks (e.g. selection, comparison) have commonly been designed in separation for set-typed attributes, which reduces interface consistency. To improve on scalability and to support rich, contextual exploration of set-typed data, we present AggreSet. AggreSet creates aggregations for each data dimension: sets, set-degrees, set-pair intersections, and other attributes. It visualizes the element count per aggregate using a matrix plot for set-pair intersections, and histograms for set lists, set-degrees and other attributes. Its non-overlapping visual design is scalable to numerous and large sets. AggreSet supports selection, filtering, and comparison as core exploratory tasks. It allows analysis of set relations inluding subsets, disjoint sets and set intersection strength, and also features perceptual set ordering for detecting patterns in set matrices. Its interaction is designed for rich and rapid data exploration. We demonstrate results on a wide range of datasets from different domains with varying characteristics, and report on expert reviews and a case study using student enrollment and degree data with assistant deans at a major public university.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467051",
            "id": "r_397",
            "s_ids": [
                "s_424",
                "s_1143",
                "s_1223"
            ],
            "type": "rich",
            "x": 0.0049662200816198935,
            "y": -0.012200289548480694
        },
        {
            "title": "Origraph: Interactive Network Wrangling",
            "data": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986909",
            "id": "r_398",
            "s_ids": [
                "s_925",
                "s_1144",
                "s_1378",
                "s_211"
            ],
            "type": "rich",
            "x": -0.14015626445472285,
            "y": -0.27478134924550845
        },
        {
            "title": "A Comparison of Radial and Linear Charts for Visualizing Daily Patterns",
            "data": "Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts \u2013 even for visualizing periodical daily patterns.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934784",
            "id": "r_399",
            "s_ids": [
                "s_1373",
                "s_1047",
                "s_953",
                "s_695",
                "s_468",
                "s_818"
            ],
            "type": "rich",
            "x": 0.054702776686770586,
            "y": 0.054794670191307834
        },
        {
            "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation",
            "data": "Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934661",
            "id": "r_400",
            "s_ids": [
                "s_851",
                "s_119",
                "s_622",
                "s_537",
                "s_436"
            ],
            "type": "rich",
            "x": 0.1441679201500257,
            "y": 0.26924363281279684
        },
        {
            "title": "NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation",
            "data": "Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters, which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We performed two case studies, and discovered multiple new parameter configurations, which can trigger high cell polarization results in the original simulation model. We evaluated our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934591",
            "id": "r_401",
            "s_ids": [
                "s_134",
                "s_665",
                "s_1093",
                "s_353",
                "s_1279"
            ],
            "type": "rich",
            "x": -0.35409557864624847,
            "y": 0.04189746073854843
        },
        {
            "title": "Visualization of Neuronal Structures in Wide-Field Microscopy Brain Images",
            "data": "Wide-field microscopes are commonly used in neurobiology for experimental studies of brain samples. Available visualization tools are limited to electron, two-photon, and confocal microscopy datasets, and current volume rendering techniques do not yield effective results when used with wide-field data. We present a workflow for the visualization of neuronal structures in wide-field microscopy images of brain samples. We introduce a novel gradient-based distance transform that overcomes the out-of-focus blur caused by the inherent design of wide-field microscopes. This is followed by the extraction of the 3D structure of neurites using a multi-scale curvilinear filter and cell-bodies using a Hessian-based enhancement filter. The response from these filters is then applied as an opacity map to the raw data. Based on the visualization challenges faced by domain experts, our workflow provides multiple rendering modes to enable qualitative analysis of neuronal structures, which includes separation of cell-bodies from neurites and an intensity-based classification of the structures. Additionally, we evaluate our visualization results against both a standard image processing deconvolution technique and a confocal microscopy image of the same specimen. We show that our method is significantly faster and requires less computational resources, while producing high quality visualizations. We deploy our workflow in an immersive gigapixel facility as a paradigm for the processing and visualization of large, high-resolution, wide-field microscopy brain datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864852",
            "id": "r_402",
            "s_ids": [
                "s_804",
                "s_914",
                "s_178",
                "s_1318",
                "s_415",
                "s_732"
            ],
            "type": "rich",
            "x": -0.6028776079095413,
            "y": 0.04001608443586696
        },
        {
            "title": "Visualization of Large Molecular Trajectories",
            "data": "The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to derive conclusions about the protein-ligand interplay. Typically, several charts are inspected, and 3D animations can be played side-by-side to obtain a deeper understanding of the data. With the advances in simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Unfortunately, such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers specialized widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where multiple ligands are involved. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers, and we describe the expert feedback.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864851",
            "id": "r_403",
            "s_ids": [
                "s_663",
                "s_594",
                "s_209",
                "s_1227",
                "s_2",
                "s_1354"
            ],
            "type": "rich",
            "x": -0.22907118409318974,
            "y": 0.2277851467091719
        },
        {
            "title": "VIBR: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle",
            "data": "Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864826",
            "id": "r_404",
            "s_ids": [
                "s_1169",
                "s_1109",
                "s_969",
                "s_1325"
            ],
            "type": "rich",
            "x": -0.03110453156194994,
            "y": -0.09480158457632466
        },
        {
            "title": "Exploring Time-Varying Multivariate Volume Data Using Matrix of Isosurface Similarity Maps",
            "data": "We present a novel visual representation and interface named the matrix of isosurface similarity maps (MISM) for effective exploration of large time-varying multivariate volumetric data sets. MISM synthesizes three types of similarity maps (i.e., self, temporal, and variable similarity maps) to capture the essential relationships among isosurfaces of different variables and time steps. Additionally, it serves as the main visual mapping and navigation tool for examining the vast number of isosurfaces and exploring the underlying time-varying multivariate data set. We present temporal clustering, variable grouping, and interactive filtering to reduce the huge exploration space of MISM. In conjunction with the isovalue and isosurface views, MISM allows users to identify important isosurfaces or isosurface pairs and compare them over space, time, and value range. More importantly, we introduce path recommendation that suggests, animates, and compares traversal paths for effectively exploring MISM under varied criteria and at different levels-of-detail. A silhouette-based method is applied to render multiple surfaces of interest in a visually succinct manner. We demonstrate the effectiveness of our approach with case studies of several time-varying multivariate data sets and an ensemble data set, and evaluate our work with two domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864808",
            "id": "r_405",
            "s_ids": [
                "s_686",
                "s_1083",
                "s_530",
                "s_1287",
                "s_865",
                "s_1370",
                "s_539"
            ],
            "type": "rich",
            "x": -0.2874920555030606,
            "y": 0.4043216507619629
        },
        {
            "title": "Interactive Dynamic Volume Illumination with Refraction and Caustics",
            "data": "In recent years, significant progress has been made in developing high-quality interactive methods for realistic volume illumination. However, refraction - despite being an important aspect of light propagation in participating media - has so far only received little attention. In this paper, we present a novel approach for refractive volume illumination including caustics capable of interactive frame rates. By interleaving light and viewing ray propagation, our technique avoids memory-intensive storage of illumination information and does not require any precomputation. It is fully dynamic and all parameters such as light position and transfer function can be modified interactively without a performance penalty.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744438",
            "id": "r_406",
            "s_ids": [
                "s_53",
                "s_900"
            ],
            "type": "rich",
            "x": -0.2560757956299778,
            "y": 0.5770037522535957
        },
        {
            "title": "Functional Decomposition for Bundled Simplification of Trail Sets",
            "data": "Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744338",
            "id": "r_407",
            "s_ids": [
                "s_589",
                "s_16",
                "s_1006",
                "s_133"
            ],
            "type": "rich",
            "x": -0.614152213433703,
            "y": 0.19397238970848527
        },
        {
            "title": "Assessing the Graphical Perception of Time and Speed on 2D+Time Trajectories",
            "data": "We empirically evaluate the extent to which people perceive non-constant time and speed encoded on 2D paths. In our graphical perception study, we evaluate nine encodings from the literature for both straight and curved paths. Visualizing time and speed information is a challenge when the x and y axes already encode other data dimensions, for example when plotting a trip on a map. This is particularly true in disciplines such as time-geography and movement analytics that often require visualizing spatio-temporal trajectories. A common approach is to use 2D+time trajectories, which are 2D paths for which time is an additional dimension. However, there are currently no guidelines regarding how to represent time and speed on such paths. Our study results provide InfoVis designers with clear guidance regarding which encodings to use and which ones to avoid; in particular, we suggest using color value to encode speed and segment length to encode time whenever possible.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743918",
            "id": "r_408",
            "s_ids": [
                "s_334",
                "s_162",
                "s_666",
                "s_853"
            ],
            "type": "rich",
            "x": 0.11819047140792954,
            "y": 0.6240897644038166
        },
        {
            "title": "An Evaluation of Visual Search Support in Maps",
            "data": "Visual search can be time-consuming, especially if the scene contains a large number of possibly relevant objects. An instance of this problem is present when using geographic or schematic maps with many different elements representing cities, streets, sights, and the like. Unless the map is well-known to the reader, the full map or at least large parts of it must be scanned to find the elements of interest. In this paper, we present a controlled eye-tracking study (30 participants) to compare four variants of map annotation with labels: within-image annotations, grid reference annotation, directional annotation, and miniature annotation. Within-image annotation places labels directly within the map without any further search support. Grid reference annotation corresponds to the traditional approach known from atlases. Directional annotation utilizes a label in combination with an arrow pointing in the direction of the label within the map. Miniature annotation shows a miniature grid to guide the reader to the area of the map in which the label is located. The study results show that within-image annotation is outperformed by all other annotation approaches. Best task completion times are achieved with miniature annotation. The analysis of eye-movement data reveals that participants applied significantly different visual task solution strategies for the different visual annotations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598898",
            "id": "r_409",
            "s_ids": [
                "s_126",
                "s_576",
                "s_1337",
                "s_455",
                "s_1242",
                "s_138"
            ],
            "type": "rich",
            "x": 0.038408158603017405,
            "y": -0.10611628948810009
        },
        {
            "title": "Corresponding Supine and Prone Colon Visualization Using Eigenfunction Analysis and Fold Modeling",
            "data": "We present a method for registration and visualization of corresponding supine and prone virtual colonoscopy scans based on eigenfunction analysis and fold modeling. In virtual colonoscopy, CT scans are acquired with the patient in two positions, and their registration is desirable so that physicians can corroborate findings between scans. Our algorithm performs this registration efficiently through the use of Fiedler vector representation (the second eigenfunction of the Laplace-Beltrami operator). This representation is employed to first perform global registration of the two colon positions. The registration is then locally refined using the haustral folds, which are automatically segmented using the 3D level sets of the Fiedler vector. The use of Fiedler vectors and the segmented folds presents a precise way of visualizing corresponding regions across datasets and visual modalities. We present multiple methods of visualizing the results, including 2D flattened rendering and the corresponding 3D endoluminal views. The precise fold modeling is used to automatically find a suitable cut for the 2D flattening, which provides a less distorted visualization. Our approach is robust, and we demonstrate its efficiency and efficacy by showing matched views on both the 2D flattened colons and in the 3D endoluminal view. We analytically evaluate the results by measuring the distance between features on the registered colons, and we also assess our fold segmentation against 20 manually labeled datasets. We have compared our results analytically to previous methods, and have found our method to achieve superior results. We also prove the hot spots conjecture for modeling cylindrical topology using Fiedler vector representation, which allows our approach to be used for general cylindrical geometry modeling and feature extraction.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598791",
            "id": "r_410",
            "s_ids": [
                "s_757",
                "s_21",
                "s_269",
                "s_732"
            ],
            "type": "rich",
            "x": -0.6029121775112557,
            "y": 0.22782022945341182
        },
        {
            "title": "Planar Visualization of Treelike Structures",
            "data": "We present a novel method to create planar visualizations of treelike structures (e.g., blood vessels and airway trees) where the shape of the object is well preserved, allowing for easy recognition by users familiar with the structures. Based on the extracted skeleton within the treelike object, a radial planar embedding is first obtained such that there are no self-intersections of the skeleton which would have resulted in occlusions in the final view. An optimization procedure which adjusts the angular positions of the skeleton nodes is then used to reconstruct the shape as closely as possible to the original, according to a specified view plane, which thus preserves the global geometric context of the object. Using this shape recovered embedded skeleton, the object surface is then flattened to the plane without occlusions using harmonic mapping. The boundary of the mesh is adjusted during the flattening step to account for regions where the mesh is stretched over concavities. This parameterized surface can then be used either as a map for guidance during endoluminal navigation or directly for interrogation and decision making. Depth cues are provided with a grayscale border to aid in shape understanding. Examples are presented using bronchial trees, cranial and lower limb blood vessels, and upper aorta datasets, and the results are evaluated quantitatively and with a user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467413",
            "id": "r_411",
            "s_ids": [
                "s_21",
                "s_732"
            ],
            "type": "rich",
            "x": -0.6854293846836793,
            "y": 0.13501304273690876
        },
        {
            "title": "Interactive Visual Alignment of Medieval Text Versions",
            "data": "Textual criticism consists of the identification and analysis of variant readings among different versions of a text. Being a relatively simple task for modern languages, the collation of medieval text traditions ranges from the complex to the virtually impossible depending on the degree of instability of textual transmission. We present a visual analytics environment that supports computationally aligning such complex textual differences typical of orally inflected medieval poetry. For the purpose of analyzing alignment, we provide interactive visualizations for different text hierarchy levels, specifically, a meso reading view to support investigating repetition and variance at the line level across text segments. In addition to outlining important aspects of our interdisciplinary collaboration, we emphasize the utility of the proposed system by various usage scenarios in medieval French literature.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585505",
            "id": "r_412",
            "s_ids": [
                "s_302",
                "s_1023"
            ],
            "type": "rich",
            "x": 0.43620837745121116,
            "y": -0.5945639929450619
        },
        {
            "title": "DemographicVis: Analyzing demographic information based on user generated content",
            "data": "The wide-spread of social media provides unprecedented sources of written language that can be used to model and infer online demographics. In this paper, we introduce a novel visual text analytics system, DemographicVis, to aid interactive analysis of such demographic information based on user-generated content. Our approach connects categorical data (demographic information) with textual data, allowing users to understand the characteristics of different demographic groups in a transparent and exploratory manner. The modeling and visualization are based on ground truth demographic information collected via a survey conducted on Reddit.com. Detailed user information is taken into our modeling process that connects the demographic groups with features that best describe the distinguishing characteristics of each group. Features including topical and linguistic are generated from the user-generated contents. Such features are then analyzed and ranked based on their ability to predict the users' demographic information. To enable interactive demographic analysis, we introduce a web-based visual interface that presents the relationship of the demographic groups, their topic interests, as well as the predictive power of various features. We present multiple case studies to showcase the utility of our visual analytics approach in exploring and understanding the interests of different demographic groups. We also report results from a comparative evaluation, showing that the DemographicVis is quantitatively superior or competitive and subjectively preferred when compared to a commercial text analysis tool.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347631",
            "id": "r_413",
            "s_ids": [
                "s_293",
                "s_1048",
                "s_290",
                "s_320",
                "s_1304",
                "s_237"
            ],
            "type": "rich",
            "x": 0.33252711995916084,
            "y": -0.21692887770882727
        },
        {
            "title": "Integrating predictive analytics into a spatiotemporal epidemic simulation",
            "data": "The Epidemic Simulation System (EpiSimS) is a scalable, complex modeling tool for analyzing disease within the United States. Due to its high input dimensionality, time requirements, and resource constraints, simulating over the entire parameter space is unfeasible. One solution is to take a granular sampling of the input space and use simpler predictive models (emulators) in between. The quality of the implemented emulator depends on many factors: robustness, sophistication, configuration, and suitability to the input data. Visual analytics can be leveraged to provide guidance and understanding of these things to the user. In this paper, we have implemented a novel interface and workflow for emulator building and use. We introduce a workflow to build emulators, make predictions, and then analyze the results. Our prediction process first predicts temporal time series, and uses these to derive predicted spatial densities. Integrated into the EpiSimS framework, we target users who are non-experts at statistical modeling. This approach allows for a high level of analysis into the state of the built emulators and their resultant predictions. We present our workflow, models, the associated system, and evaluate the overall utility with feedback from EpiSimS scientists.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347626",
            "id": "r_414",
            "s_ids": [
                "s_342",
                "s_516",
                "s_216",
                "s_687"
            ],
            "type": "rich",
            "x": -0.2379824171901405,
            "y": -0.0448047782023685
        },
        {
            "title": "RSATree: Distribution-Aware Data Representation of Large-Scale Tabular Datasets for Flexible Visual Query",
            "data": "Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934800",
            "id": "r_415",
            "s_ids": [
                "s_1357",
                "s_1250",
                "s_404",
                "s_562",
                "s_685",
                "s_547",
                "s_1285",
                "s_279"
            ],
            "type": "rich",
            "x": -0.15641137731589178,
            "y": 0.22908728637875125
        },
        {
            "title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness",
            "data": "Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934614",
            "id": "r_416",
            "s_ids": [
                "s_417",
                "s_721",
                "s_1327",
                "s_915",
                "s_921"
            ],
            "type": "rich",
            "x": 0.028665911441727695,
            "y": -0.34967625632240573
        },
        {
            "title": "Scale Trotter: Illustrative Visual Travels Across Negative Scales",
            "data": "We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels\u2014the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out\u2014instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934334",
            "id": "r_417",
            "s_ids": [
                "s_241",
                "s_1043",
                "s_978",
                "s_519",
                "s_443",
                "s_1205"
            ],
            "type": "rich",
            "x": -0.24468581406009404,
            "y": 0.13658355594729804
        },
        {
            "title": "Toward Objective Evaluation of Working Memory in Visualizations: A Case Study Using Pupillometry and a Dual-Task Paradigm",
            "data": "Cognitive science has established widely used and validated procedures for evaluating working memory in numerous applied domains, but surprisingly few studies have employed these methodologies to assess claims about the impacts of visualizations on working memory. The lack of information visualization research that uses validated procedures for measuring working memory may be due, in part, to the absence of cross-domain methodological guidance tailored explicitly to the unique needs of visualization research. This paper presents a set of clear, practical, and empirically validated methods for evaluating working memory during visualization tasks and provides readers with guidance in selecting an appropriate working memory evaluation paradigm. As a case study, we illustrate multiple methods for evaluating working memory in a visual-spatial aggregation task with geospatial data. The results show that the use of dual-task experimental designs (simultaneous performance of several tasks compared to single-task performance) and pupil dilation can reveal working memory demands associated with task difficulty and dual-tasking. In a dual-task experimental design, measures of task completion times and pupillometry revealed the working memory demands associated with both task difficulty and dual-tasking. Pupillometry demonstrated that participants' pupils were significantly larger when they were completing a more difficult task and when multitasking. We propose that researchers interested in the relative differences in working memory between visualizations should consider a converging methods approach, where physiological measures and behavioral measures of working memory are employed to generate a rich evaluation of visualization effort.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934286",
            "id": "r_418",
            "s_ids": [
                "s_218",
                "s_798",
                "s_397",
                "s_324",
                "s_84"
            ],
            "type": "rich",
            "x": 0.4550090823511282,
            "y": -0.23690540819455908
        },
        {
            "title": "CPU Isosurface Ray Tracing of Adaptive Mesh Refinement Data",
            "data": "Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of block-structured AMR data. We contribute a novel reconstruction strategy-the octant method-which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our octant method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining volume and isosurface representations of BS-AMR data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale block-structured AMR datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864850",
            "id": "r_419",
            "s_ids": [
                "s_165",
                "s_1429",
                "s_749",
                "s_612",
                "s_343"
            ],
            "type": "rich",
            "x": -0.5013589490797445,
            "y": 0.5771765462471653
        },
        {
            "title": "Vol\u00b2velle: Printable Interactive Volume Visualization",
            "data": "Interaction is an indispensable aspect of data visualization. The presentation of volumetric data, in particular, often significantly benefits from interactive manipulation of parameters such as transfer functions, rendering styles, or clipping planes. However, when we want to create hardcopies of such visualizations, this essential aspect is lost. In this paper, we present a novel approach for creating hardcopies of volume visualizations which preserves a certain degree of interactivity. We present a method for automatically generating Volvelles, printable tangible wheel charts that can be manipulated to explore different parameter settings. Our interactive system allows the flexible mapping of arbitrary visualization parameters and supports advanced features such as linked views. The resulting designs can be easily reproduced using a standard printer and assembled within a few minutes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599211",
            "id": "r_420",
            "s_ids": [
                "s_1281",
                "s_900"
            ],
            "type": "rich",
            "x": 0.36530010166203003,
            "y": 0.14716228619194796
        },
        {
            "title": "Mining Graphs for Understanding Time-Varying Volumetric Data",
            "data": "A notable recent trend in time-varying volumetric data analysis and visualization is to extract data relationships and represent them in a low-dimensional abstract graph view for visual understanding and making connections to the underlying data. Nevertheless, the ever-growing size and complexity of data demands novel techniques that go beyond standard brushing and linking to allow significant reduction of cognition overhead and interaction cost. In this paper, we present a mining approach that automatically extracts meaningful features from a graph-based representation for exploring time-varying volumetric data. This is achieved through the utilization of a series of graph analysis techniques including graph simplification, community detection, and visual recommendation. We investigate the most important transition relationships for time-varying data and evaluate our solution with several time-varying data sets of different sizes and characteristics. For gaining insights from the data, we show that our solution is more efficient and effective than simply asking users to extract relationships via standard interaction techniques, especially when the data set is large and the relationships are complex. We also collect expert feedback to confirm the usefulness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2468031",
            "id": "r_421",
            "s_ids": [
                "s_784",
                "s_530",
                "s_727",
                "s_1232",
                "s_539"
            ],
            "type": "rich",
            "x": -0.011432428787331734,
            "y": 0.09986922103270826
        },
        {
            "title": "JiTTree: A Just-in-Time Compiled Sparse GPU Volume Data Structure",
            "data": "Sparse volume data structures enable the efficient representation of large but sparse volumes in GPU memory for computation and visualization. However, the choice of a specific data structure for a given data set depends on several factors, such as the memory budget, the sparsity of the data, and data access patterns. In general, there is no single optimal sparse data structure, but a set of several candidates with individual strengths and drawbacks. One solution to this problem are hybrid data structures which locally adapt themselves to the sparsity. However, they typically suffer from increased traversal overhead which limits their utility in many applications. This paper presents JiTTree, a novel sparse hybrid volume data structure that uses just-in-time compilation to overcome these problems. By combining multiple sparse data structures and reducing traversal overhead we leverage their individual advantages. We demonstrate that hybrid data structures adapt well to a large range of data sets. They are especially superior to other sparse data structures for data sets that locally vary in sparsity. Possible optimization criteria are memory, performance and a combination thereof. Through just-in-time (JIT) compilation, JiTTree reduces the traversal overhead of the resulting optimal data structure. As a result, our hybrid volume data structure enables efficient computations on the GPU, while being superior in terms of memory usage when compared to non-hybrid data structures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467331",
            "id": "r_422",
            "s_ids": [
                "s_72",
                "s_900",
                "s_519",
                "s_336",
                "s_876"
            ],
            "type": "rich",
            "x": -0.40583384896219715,
            "y": 0.3947053058200423
        },
        {
            "title": "FDive: Learning Relevance Models Using Pattern-based Similarity Measures",
            "data": "The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models, assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986940",
            "id": "r_423",
            "s_ids": [
                "s_473",
                "s_125",
                "s_774",
                "s_493",
                "s_1346",
                "s_540"
            ],
            "type": "rich",
            "x": -0.05087704738256373,
            "y": -0.19690087092554245
        },
        {
            "title": "Visually and statistically guided imputation of missing values in univariate seasonal time series",
            "data": "Missing values are a problem in many real world applications, for example failing sensor measurements. For further analysis these missing values need to be imputed. Thus, imputation of such missing values is important in a wide range of applications. We propose a visually and statistically guided imputation approach, that allows applying different imputation techniques to estimate the missing values as well as evaluating and fine tuning the imputation by visual guidance. In our approach we include additional visual information about uncertainty and employ the cyclic structure of time inherent in the data. Including this cyclic structure enables visually judging the adequateness of the estimated values with respect to the uncertainty/error boundaries and according to the patterns of the neighbouring time points in linear and cyclic (e.g., the months of the year) time.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347672",
            "id": "r_424",
            "s_ids": [
                "s_81",
                "s_1392",
                "s_181",
                "s_1087",
                "s_472",
                "s_337",
                "s_405"
            ],
            "type": "rich",
            "x": -0.18184195116336577,
            "y": -0.11052491298424486
        },
        {
            "title": "Persistent Homology Guided Force-Directed Graph Layouts",
            "data": "Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934802",
            "id": "r_425",
            "s_ids": [
                "s_452",
                "s_1331",
                "s_722",
                "s_529",
                "s_469"
            ],
            "type": "rich",
            "x": -0.2807114171792763,
            "y": -0.1692778821343361
        },
        {
            "title": "Cohort-based T-SSIM Visual Computing for Radiation Therapy Prediction and Exploration",
            "data": "We describe a visual computing approach to radiation therapy (RT) planning, based on spatial similarity within a patient cohort. In radiotherapy for head and neck cancer treatment, dosage to organs at risk surrounding a tumor is a large cause of treatment toxicity. Along with the availability of patient repositories, this situation has lead to clinician interest in understanding and predicting RT outcomes based on previously treated similar patients. To enable this type of analysis, we introduce a novel topology-based spatial similarity measure, T-SSIM, and a predictive algorithm based on this similarity measure. We couple the algorithm with a visual steering interface that intertwines visual encodings for the spatial data and statistical results, including a novel parallel-marker encoding that is spatially aware. We report quantitative results on a cohort of 165 patients, as well as a qualitative evaluation with domain experts in radiation oncology, data management, biostatistics, and medical imaging, who are collaborating remotely.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934546",
            "id": "r_426",
            "s_ids": [
                "s_257",
                "s_1106",
                "s_347",
                "s_1085",
                "s_552",
                "s_836",
                "s_1293",
                "s_422",
                "s_459"
            ],
            "type": "rich",
            "x": -0.40201123423011537,
            "y": -0.19076338923865532
        },
        {
            "title": "Artifact-Based Rendering: Harnessing Natural and Traditional Visual Media for More Expressive and Engaging 3D Visualizations",
            "data": "We introduce Artifact-Based Rendering (ABR), a framework of tools, algorithms, and processes that makes it possible to produce real, data-driven 3D scientific visualizations with a visual language derived entirely from colors, lines, textures, and forms created using traditional physical media or found in nature. A theory and process for ABR is presented to address three current needs: (i) designing better visualizations by making it possible for non-programmers to rapidly design and critique many alternative data-to-visual mappings; (ii) expanding the visual vocabulary used in scientific visualizations to depict increasingly complex multivariate data; (iii) bringing a more engaging, natural, and human-relatable handcrafted aesthetic to data visualization. New tools and algorithms to support ABR include front-end applets for constructing artifact-based colormaps, optimizing 3D scanned meshes for use in data visualization, and synthesizing textures from artifacts. These are complemented by an interactive rendering engine with custom algorithms and interfaces that demonstrate multiple new visual styles for depicting point, line, surface, and volume data. A within-the-research-team design study provides early evidence of the shift in visualization design processes that ABR is believed to enable when compared to traditional scientific visualization systems. Qualitative user feedback on applications to climate science and brain imaging support the utility of ABR for scientific discovery and public communication.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934260",
            "id": "r_427",
            "s_ids": [
                "s_954",
                "s_187",
                "s_1475",
                "s_911",
                "s_1202",
                "s_247",
                "s_456",
                "s_1052",
                "s_1199"
            ],
            "type": "rich",
            "x": 0.27171513941820336,
            "y": 0.39129080144070066
        },
        {
            "title": "Recirculation Surfaces for Flow Visualization",
            "data": "We present a formal approach to the visual analysis of recirculation in flows by introducing recirculation surfaces for 3D unsteady flow fields. Recirculation surfaces are the loci where massless particle integration returns to its starting point after some variable, finite integration. We give a rigorous definition of recirculation surfaces as 2-manifolds embedded in 5D space and study their properties. Based on this we construct an algorithm for their extraction, which searches for intersections of a recirculation surface with lines defined in 3D. This reduces the problem to a repeated search for critical points in 3D vector fields. We provide a uniform sampling of the search space paired with a surface reconstruction and visualize results. This way, we present the first algorithm for a comprehensive feature extraction in the 5D flow map of a 3D flow. The problem of finding isolated closed orbits in steady vector fields occurs as a special case of recirculation surfaces. This includes isolated closed orbits with saddle behavior. We show recirculation surfaces for a number of artificial and real flow data sets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864813",
            "id": "r_428",
            "s_ids": [
                "s_294",
                "s_815",
                "s_697"
            ],
            "type": "rich",
            "x": -0.7314757160623375,
            "y": 0.4975520045132028
        },
        {
            "title": "Analysis of Flight Variability: a Systematic Approach",
            "data": "In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864811",
            "id": "r_429",
            "s_ids": [
                "s_9",
                "s_1149",
                "s_317",
                "s_174"
            ],
            "type": "rich",
            "x": 0.22940235203220952,
            "y": 0.5299113123180559
        },
        {
            "title": "CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data",
            "data": "CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864801",
            "id": "r_430",
            "s_ids": [
                "s_134",
                "s_1301",
                "s_353",
                "s_1121"
            ],
            "type": "rich",
            "x": -0.24957897913776955,
            "y": 0.287027478279783
        },
        {
            "title": "Gaia Sky: Navigating the Gaia Catalog",
            "data": "In this paper, we present Gaia Sky, a free and open-source multiplatform 3D Universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data release 2 represents the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, currently no actively maintained cross-platform, modern, and open alternative exists.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864508",
            "id": "r_431",
            "s_ids": [
                "s_585",
                "s_369",
                "s_228",
                "s_679"
            ],
            "type": "rich",
            "x": -0.1036838424815057,
            "y": 0.5388086095443155
        },
        {
            "title": "Quantifying the Visual Impact of Classification Boundaries in Choropleth Maps",
            "data": "One critical visual task when using choropleth maps is to identify spatial clusters in the data. If spatial units have the same color and are in the same neighborhood, this region can be visually identified as a spatial cluster. However, the choice of classification method used to create the choropleth map determines the visual output. The critical map elements in the classification scheme are those that lie near the classification boundary as those elements could potentially belong to different classes with a slight adjustment of the classification boundary. Thus, these elements have the most potential to impact the visual features (i.e., spatial clusters) that occur in the choropleth map. We present a methodology to enable analysts and designers to identify spatial regions where the visual appearance may be the result of spurious data artifacts. The proposed methodology automatically detects the critical boundary cases that can impact the overall visual presentation of the choropleth map using a classification metric of cluster stability. The map elements that belong to a critical boundary case are then automatically assessed to quantify the visual impact of classification edge effects. Our results demonstrate the impact of boundary elements on the resulting visualization and suggest that special attention should be given to these elements during map design.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598541",
            "id": "r_432",
            "s_ids": [
                "s_743",
                "s_708"
            ],
            "type": "rich",
            "x": -0.31055316673177163,
            "y": 0.0673804932814366
        },
        {
            "title": "CrystalBall: A Visual Analytic System for Future Event Discovery and Analysis from Social Media Data",
            "data": "Social media data bear valuable insights regarding events that occur around the world. Events are inherently temporal and spatial. Existing visual text analysis systems have focused on detecting and analyzing past and ongoing events. Few have leveraged social media information to look for events that may occur in the future. In this paper, we present an interactive visual analytic system, CrystalBall, that automatically identifies and ranks future events from Twitter streams. CrystalBall integrates new methods to discover events with interactive visualizations that permit sensemaking of the identified future events. Our computational methods integrate seven different measures to identify and characterize future events, leveraging information regarding time, location, social networks, and the informativeness of the messages. A visual interface is tightly coupled with the computational methods to present a concise summary of the possible future events. A novel connection graph and glyphs are designed to visualize the characteristics of the future events. To demonstrate the efficacy of CrystalBall in identifying future events and supporting interactive analysis, we present multiple case studies and validation studies on analyzing events derived from Twitter data.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585658",
            "id": "r_433",
            "s_ids": [
                "s_1048",
                "s_1060",
                "s_1257",
                "s_237",
                "s_293"
            ],
            "type": "rich",
            "x": 0.420925156533243,
            "y": 0.17654242353154972
        },
        {
            "title": "A Visual Analytics System for Optimizing Communications in Massively Parallel Applications",
            "data": "Current and future supercomputers have tens of thousands of compute nodes interconnected with high-dimensional networks and complex network topologies for improved performance. Application developers are required to write scalable parallel programs in order to achieve high throughput on these machines. Application performance is largely determined by efficient inter-process communication. A common way to analyze and optimize performance is through profiling parallel codes to identify communication bottlenecks. However, understanding gigabytes of profiled at a is not a trivial task. In this paper, we present a visual analytics system for identifying the scalability bottlenecks and improving the communication efficiency of massively parallel applications. Visualization methods used in this system are designed to comprehend large-scale and varied communication patterns on thousands of nodes in complex networks such as the 5D torus and the dragonfly. We also present efficient rerouting and remapping algorithms that can be coupled with our interactive visual analytics design for performance optimization. We demonstrate the utility of our system with several case studies using three benchmark applications on two leading supercomputers. The mapping suggestion from our system led to 38% improvement in hop-bytes for Mini AMR application on 4,096 MPI processes.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585646",
            "id": "r_434",
            "s_ids": [
                "s_1201",
                "s_1074",
                "s_985",
                "s_1216",
                "s_656",
                "s_687"
            ],
            "type": "rich",
            "x": -0.3278331062387666,
            "y": 0.12470179478552457
        },
        {
            "title": "Interactive visual steering of hierarchical simulation ensembles",
            "data": "Multi-level simulation models, i.e., models where different components are simulated using sub-models of varying levels of complexity, belong to the current state-of-the-art in simulation. The existing analysis practice for multi-level simulation results is to manually compare results from different levels of complexity, amounting to a very tedious and error-prone, trial-and-error exploration process. In this paper, we introduce hierarchical visual steering, a new approach to the exploration and design of complex systems. Hierarchical visual steering makes it possible to explore and analyze hierarchical simulation ensembles at different levels of complexity. At each level, we deal with a dynamic simulation ensemble - the ensemble grows during the exploration process. There is at least one such ensemble per simulation level, resulting in a collection of dynamic ensembles, analyzed simultaneously. The key challenge is to map the multi-dimensional parameter space of one ensemble to the multi-dimensional parameter space of another ensemble (from another level). In order to support the interactive visual analysis of such complex data we propose a novel approach to interactive and semi-automatic parameter space segmentation and comparison. The approach combines a novel interaction technique and automatic, computational methods - clustering, concave hull computation, and concave polygon overlapping - to support the analysts in the cross-ensemble parameter space mapping. In addition to the novel parameter space segmentation we also deploy coordinated multiple views with standard plots. We describe the abstract analysis tasks, identified during a case study, i.e., the design of a variable valve actuation system of a car engine. The study is conducted in cooperation with experts from the automotive industry. Very positive feedback indicates the usefulness and efficiency of the newly proposed approach.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347635",
            "id": "r_435",
            "s_ids": [
                "s_695",
                "s_818",
                "s_953",
                "s_1245",
                "s_367"
            ],
            "type": "rich",
            "x": -0.27895514637467445,
            "y": 0.16713625096505255
        },
        {
            "title": "Four considerations for supporting visual analysis in display ecologies",
            "data": "The current proliferation of large displays and mobile devices presents a number of exciting opportunities for visual analytics and information visualization. The display ecology enables multiple displays to function in concert within a broader technological environment to accomplish visual analysis tasks. Based on a comprehensive survey of multi-display systems from a variety of fields, we propose four key considerations for visual analysis in display ecologies: 1) Display Composition, 2) Information Coordination/Transfer, 3) Information Connection, and 4) Display Membership. Different aspects of display ecologies stemming from these design considerations will enable users to transform and empower multiple displays as a display ecology for visual analysis.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347628",
            "id": "r_436",
            "s_ids": [
                "s_1375",
                "s_207",
                "s_831",
                "s_1128"
            ],
            "type": "rich",
            "x": 0.5445717363979693,
            "y": 0.17901501429417735
        },
        {
            "title": "Discriminability Tests for Visualization Effectiveness and Scalability",
            "data": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934432",
            "id": "r_437",
            "s_ids": [
                "s_1317",
                "s_1476"
            ],
            "type": "rich",
            "x": 0.08923263011114646,
            "y": -0.055348503349344845
        },
        {
            "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns",
            "data": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934285",
            "id": "r_438",
            "s_ids": [
                "s_940",
                "s_925",
                "s_891"
            ],
            "type": "rich",
            "x": 0.47374159331016313,
            "y": -0.015771376668713016
        },
        {
            "title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories",
            "data": "Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934275",
            "id": "r_439",
            "s_ids": [
                "s_1108",
                "s_1482",
                "s_411",
                "s_1115",
                "s_981",
                "s_719",
                "s_655",
                "s_137"
            ],
            "type": "rich",
            "x": 0.18984920463682944,
            "y": 0.07586167243080891
        },
        {
            "title": "Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach",
            "data": "Many real-world datasets are large, multivariate, and relational in nature and relevant associated decisions frequently require a simultaneous consideration of both attributes and connections. Existing visualization systems and approaches, however, often make an explicit trade-off between either affording rich exploration of individual data units and their attributes or exploration of the underlying network structure. In doing so, important analysis opportunities and insights are potentially missed. In this study, we aim to address this gap by (1) considering visualizations and interaction techniques that blend the spectrum between unit and network visualizations, (2) discussing the nature of different forms of contexts and the challenges in implementing them, and (3) demonstrating the value of our approach for visual exploration of multivariate, relational data for a real-world use case. Specifically, we demonstrate through a system called Graphicle how network structure can be layered on top of unit visualization techniques to create new opportunities for visual exploration of physician characteristics and referral data. We report on the design, implementation, and evaluation of the system and effectiveness of our blended approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865151",
            "id": "r_440",
            "s_ids": [
                "s_203",
                "s_108"
            ],
            "type": "rich",
            "x": 0.2595059393101906,
            "y": -0.11035141004376502
        },
        {
            "title": "Enhancing Web-based Analytics Applications through Provenance",
            "data": "Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865039",
            "id": "r_441",
            "s_ids": [
                "s_1278",
                "s_546",
                "s_112",
                "s_691"
            ],
            "type": "rich",
            "x": 0.3414558269094782,
            "y": -0.3019994280728834
        },
        {
            "title": "An Interactive Framework for Visualization of Weather Forecast Ensembles",
            "data": "Numerical Weather Prediction (NWP) ensembles are commonly used to assess the uncertainty and confidence in weather forecasts. Spaghetti plots are conventional tools for meteorologists to directly examine the uncertainty exhibited by ensembles, where they simultaneously visualize isocontours of all ensemble members. To avoid visual clutter in practical usages, one needs to select a small number of informative isovalues for visual analysis. Moreover, due to the complex topology and variation of ensemble isocontours, it is often a challenging task to interpret the spaghetti plot for even a single isovalue in large ensembles. In this paper, we propose an interactive framework for uncertainty visualization of weather forecast ensembles that significantly improves and expands the utility of spaghetti plots in ensemble analysis. Complementary to state-of-the-art methods, our approach provides a complete framework for visual exploration of ensemble isocontours, including isovalue selection, interactive isocontour variability exploration, and interactive sub-region selection and re-analysis. Our framework is built upon the high-density clustering paradigm, where the mode structure of the density function is represented as a hierarchy of nested subsets of the data. We generalize the high-density clustering for isocontours and propose a bandwidth selection method for estimating the density function of ensemble isocontours. We present novel visualizations based on high-density clustering results, called the mode plot and the simplified spaghetti plot. The proposed mode plot visually encodes the structure provided by the high-density clustering result and summarizes the distribution of ensemble isocontours. It also enables the selection of subsets of interesting isocontours, which are interactively highlighted in a linked spaghetti plot for providing spatial context. To provide an interpretable overview of the positional variability of isocontours, our system allows for selection of informative isovalues from the simplified spaghetti plot. Due to the spatial variability of ensemble isocontours, the system allows for interactive selection and focus on sub-regions for local uncertainty and clustering re-analysis. We examine a number of ensemble datasets to establish the utility of our approach and discuss its advantages over state-of-the-art visual analysis tools for ensemble data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864815",
            "id": "r_442",
            "s_ids": [
                "s_907",
                "s_160"
            ],
            "type": "rich",
            "x": -0.45077765311367574,
            "y": 0.4073015234222491
        },
        {
            "title": "Visual Analysis of Aneurysm Data using Statistical Graphics",
            "data": "This paper presents a framework to explore multi-field data of aneurysms occurring at intracranial and cardiac arteries by using statistical graphics. The rupture of an aneurysm is often a fatal scenario, whereas during treatment serious complications for the patient can occur. Whether an aneurysm ruptures or whether a treatment is successful depends on the interaction of different morphological such as wall deformation and thickness, and hemodynamic attributes like wall shear stress and pressure. Therefore, medical researchers are very interested in better understanding these relationships. However, the required analysis is a time-consuming process, where suspicious wall regions are difficult to detect due to the time-dependent behavior of the data. Our proposed visualization framework enables medical researchers to efficiently assess aneurysm risk and treatment options. This comprises a powerful set of views including 2D and 3D depictions of the aneurysm morphology as well as statistical plots of different scalar fields. Brushing and linking aids the user to identify interesting wall regions and to understand the influence of different attributes on the aneurysm's state. Moreover, a visual comparison of pre- and post-treatment as well as different treatment options is provided. Our analysis techniques are designed in collaboration with domain experts, e.g., physicians, and we provide details about the evaluation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864509",
            "id": "r_443",
            "s_ids": [
                "s_808",
                "s_1225",
                "s_22",
                "s_600",
                "s_619",
                "s_1467"
            ],
            "type": "rich",
            "x": -0.4638345106587279,
            "y": -0.1578580032604696
        },
        {
            "title": "Probabilistic Asymptotic Decider for Topological Ambiguity Resolution in Level-Set Extraction for Uncertain 2D Data",
            "data": "We present a framework for the analysis of uncertainty in isocontour extraction. The marching squares (MS) algorithm for isocontour reconstruction generates a linear topology that is consistent with hyperbolic curves of a piecewise bilinear interpolation. The saddle points of the bilinear interpolant cause topological ambiguity in isocontour extraction. The midpoint decider and the asymptotic decider are well-known mathematical techniques for resolving topological ambiguities. The latter technique investigates the data values at the cell saddle points for ambiguity resolution. The uncertainty in data, however, leads to uncertainty in underlying bilinear interpolation functions for the MS algorithm, and hence, their saddle points. In our work, we study the behavior of the asymptotic decider when data at grid vertices is uncertain. First, we derive closed-form distributions characterizing variations in the saddle point values for uncertain bilinear interpolants. The derivation assumes uniform and nonparametric noise models, and it exploits the concept of ratio distribution for analytic formulations. Next, the probabilistic asymptotic decider is devised for ambiguity resolution in uncertain data using distributions of the saddle point values derived in the first step. Finally, the confidence in probabilistic topological decisions is visualized using a colormapping technique. We demonstrate the higher accuracy and stability of the probabilistic asymptotic decider in uncertain data with regard to existing decision frameworks, such as deciders in the mean field and the probabilistic midpoint decider, through the isocontour visualization of synthetic and real datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864505",
            "id": "r_444",
            "s_ids": [
                "s_1485",
                "s_343"
            ],
            "type": "rich",
            "x": -0.6663085598703861,
            "y": 0.29912348994968213
        },
        {
            "title": "How Do Ancestral Traits Shape Family Trees Over Generations?",
            "data": "Whether and how does the structure of family trees differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, previous work mostly focuses on parent-child scenarios due to the lack of proper tools to handle the complexity of extending the research to multi-generational processes. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744080",
            "id": "r_445",
            "s_ids": [
                "s_225",
                "s_917",
                "s_896",
                "s_1283",
                "s_137"
            ],
            "type": "rich",
            "x": -0.5518194404923221,
            "y": -0.15291476627531078
        },
        {
            "title": "An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding",
            "data": "In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744078",
            "id": "r_446",
            "s_ids": [
                "s_1303",
                "s_718",
                "s_549",
                "s_388"
            ],
            "type": "rich",
            "x": -0.3884060703928701,
            "y": 0.0699661036852944
        },
        {
            "title": "Progressive Direct Volume-to-Volume Transformation",
            "data": "We present a novel technique to generate transformations between arbitrary volumes, providing both expressive distances and smooth interpolates. In contrast to conventional morphing or warping approaches, our technique requires no user guidance, intermediate representations (like extracted features), or blending, and imposes no restrictions regarding shape or structure. Our technique operates directly on the volumetric data representation, and while linear programming approaches could solve the underlying problem optimally, their polynomial complexity makes them infeasible for high-resolution volumes. We therefore propose a progressive refinement approach designed for parallel execution that is able to quickly deliver approximate results that are iteratively improved toward the optimum. On this basis, we further present a new approach for the streaming selection of time steps in temporal data that allows for the reconstruction of the full sequence with a user-specified error bound. We finally demonstrate the utility of our technique for different applications, compare our approach against alternatives, and evaluate its characteristics with a variety of different data sets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599042",
            "id": "r_447",
            "s_ids": [
                "s_251",
                "s_299"
            ],
            "type": "rich",
            "x": -0.48047059133379083,
            "y": 0.30202682335117237
        },
        {
            "title": "Direct Multifield Volume Ray Casting of Fiber Surfaces",
            "data": "Multifield data are common in visualization. However, reducing these data to comprehensible geometry is a challenging problem. Fiber surfaces, an analogy of isosurfaces to bivariate volume data, are a promising new mechanism for understanding multifield volumes. In this work, we explore direct ray casting of fiber surfaces from volume data without any explicit geometry extraction. We sample directly along rays in domain space, and perform geometric tests in range space where fibers are defined, using a signed distance field derived from the control polygons. Our method requires little preprocess, and enables real-time exploration of data, dynamic modification and pixel-exact rendering of fiber surfaces, and support for higher-order interpolation in domain space. We demonstrate this approach on several bivariate datasets, including analysis of multi-field combustion data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599040",
            "id": "r_448",
            "s_ids": [
                "s_752",
                "s_1480",
                "s_977",
                "s_1226",
                "s_297"
            ],
            "type": "rich",
            "x": -0.6279184230014352,
            "y": 0.43222455880080185
        },
        {
            "title": "Topological Analysis of Inertial Dynamics",
            "data": "Traditional vector field visualization has a close focus on velocity, and is typically constrained to the dynamics of massless particles. In this paper, we present a novel approach to the analysis of the force-induced dynamics of inertial particles. These forces can arise from acceleration fields such as gravitation, but also be dependent on the particle dynamics itself, as in the case of magnetism. Compared to massless particles, the velocity of an inertial particle is not determined solely by its position and time in a vector field. In contrast, its initial velocity can be arbitrary and impacts the dynamics over its entire lifetime. This leads to a four-dimensional problem for 2D setups, and a six-dimensional problem for the 3D case. Our approach avoids this increase in dimensionality and tackles the visualization by an integrated topological analysis approach. We demonstrate the utility of our approach using a synthetic time-dependent acceleration field, a system of magnetic dipoles, and N-body systems both in 2D and 3D.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599018",
            "id": "r_449",
            "s_ids": [
                "s_585",
                "s_369",
                "s_151",
                "s_1444",
                "s_349",
                "s_679"
            ],
            "type": "rich",
            "x": -0.6570568161965478,
            "y": 0.5597585443933232
        },
        {
            "title": "Optimizing Hierarchical Visualizations with the Minimum Description Length Principle",
            "data": "In this paper we examine how the Minimum Description Length (MDL) principle can be used to efficiently select aggregated views of hierarchical datasets that feature a good balance between clutter and information. We present MDL formulae for generating uneven tree cuts tailored to treemap and sunburst diagrams, taking into account the available display space and information content of the data. We present the results of a proof-of-concept implementation. In addition, we demonstrate how such tree cuts can be used to enhance drill-down interaction in hierarchical visualizations by implementing our approach in an existing visualization tool. Validation is done with the feature congestion measure of clutter in views of a subset of the current DMOZ web directory, which contains nearly half million categories. The results show that MDL views achieve near constant clutter level across display resolutions. We also present the results of a crowdsourced user study where participants were asked to find targets in views of DMOZ generated by our approach and a set of baseline aggregation methods. The results suggest that, in some conditions, participants are able to locate targets (in particular, outliers) faster using the proposed approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598591",
            "id": "r_450",
            "s_ids": [
                "s_1317",
                "s_1476"
            ],
            "type": "rich",
            "x": -0.23652403912531253,
            "y": -0.04385412765897119
        },
        {
            "title": "Automatic Selection of Partitioning Variables for Small Multiple Displays",
            "data": "Effective small multiple displays are created by partitioning a visualization on variables that reveal interesting conditional structure in the data. We propose a method that automatically ranks partitioning variables, allowing analysts to focus on the most promising small multiple displays. Our approach is based on a randomized, non-parametric permutation test, which allows us to handle a wide range of quality measures for visual patterns defined on many different visualization types, while discounting spurious patterns. We demonstrate the effectiveness of our approach on scatterplots of real-world, multidimensional datasets.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467323",
            "id": "r_451",
            "s_ids": [
                "s_1291",
                "s_1101"
            ],
            "type": "rich",
            "x": 0.10065877472532371,
            "y": 0.1514594767375662
        },
        {
            "title": "Real-Time Molecular Visualization Supporting Diffuse Interreflections and Ambient Occlusion",
            "data": "Today molecular simulations produce complex data sets capturing the interactions of molecules in detail. Due to the complexity of this time-varying data, advanced visualization techniques are required to support its visual analysis. Current molecular visualization techniques utilize ambient occlusion as a global illumination approximation to improve spatial comprehension. Besides these shadow-like effects, interreflections are also known to improve the spatial comprehension of complex geometric structures. Unfortunately, the inherent computational complexity of interreflections would forbid interactive exploration, which is mandatory in many scenarios dealing with static and time-varying data. In this paper, we introduce a novel analytic approach for capturing interreflections of molecular structures in real-time. By exploiting the knowledge of the underlying space filling representations, we are able to reduce the required parameters and can thus apply symbolic regression to obtain an analytic expression for interreflections. We show how to obtain the data required for the symbolic regression analysis, and how to exploit our analytic solution to enhance interactive molecular visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467293",
            "id": "r_452",
            "s_ids": [
                "s_491",
                "s_1354",
                "s_332",
                "s_209"
            ],
            "type": "rich",
            "x": -0.48434276483083044,
            "y": 0.35681893987357594
        },
        {
            "title": "Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D Complex-Valued Superconductor Simulation Data",
            "data": "We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 1D curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2466838",
            "id": "r_453",
            "s_ids": [
                "s_865",
                "s_360",
                "s_727",
                "s_1231",
                "s_63"
            ],
            "type": "rich",
            "x": -0.6862528974145213,
            "y": 0.7150535136051447
        },
        {
            "title": "VIANA: Visual Interactive Annotation of Argumentation",
            "data": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986917",
            "id": "r_454",
            "s_ids": [
                "s_1160",
                "s_830",
                "s_1381",
                "s_773"
            ],
            "type": "rich",
            "x": 0.2684811801853158,
            "y": -0.510039372213081
        },
        {
            "title": "PorosityAnalyzer: Visual Analysis and Evaluation of Segmentation Pipelines to Determine the Porosity in Fiber-Reinforced Polymers",
            "data": "In this paper we present PorosityAnalyzer, a novel tool for detailed evaluation and visual analysis of pore segmentation pipelines to determine the porosity in fiber-reinforced polymers (FRPs). The presented tool consists of two modules: the computation module and the analysis module. The computation module enables a convenient setup and execution of distributed off-line-computations on industrial 3D X-ray computed tomography datasets. It allows the user to assemble individual segmentation pipelines in the form of single pipeline steps, and to specify the parameter ranges as well as the sampling of the parameter-space of each pipeline segment. The result of a single segmentation run consists of the input parameters, the calculated 3D binary-segmentation mask, the resulting porosity value, and other derived results (e.g., segmentation pipeline run-time). The analysis module presents the data at different levels of detail by drill-down filtering in order to determine accurate and robust segmentation pipelines. Overview visualizations allow to initially compare and evaluate the segmentation pipelines. With a scatter plot matrix (SPLOM), the segmentation pipelines are examined in more detail based on their input and output parameters. Individual segmentation-pipeline runs are selected in the SPLOM and visually examined and compared in 2D slice views and 3D renderings by using aggregated segmentation masks and statistical contour renderings. PorosityAnalyzer has been thoroughly evaluated with the help of twelve domain experts. Two case studies demonstrate the applicability of our proposed concepts and visualization techniques, and show that our tool helps domain experts to gain new insights and improve their workflow efficiency.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883516",
            "id": "r_455",
            "s_ids": [
                "s_598",
                "s_382",
                "s_519",
                "s_838",
                "s_19"
            ],
            "type": "rich",
            "x": -0.41907038741366054,
            "y": 0.10777999500191644
        },
        {
            "title": "SocialBrands: Visual analysis of public perceptions of brands on social media",
            "data": "Public perceptions of a brand is critical to its performance. While social media has demonstrated a huge potential to shape public perceptions of brands, existing tools are not intuitive and explanatory for domain users to use as they fail to provide a comprehensive analysis framework for perceptions of brands. In this paper, we present SocialBrands, a novel visual analysis tool for brand managers to understand public perceptions of brands on social media. Social-Brands leverages brand personality framework in marketing literature and social computing approaches to compute the personality of brands from three driving factors (user imagery, employee imagery, and official announcement) on social media, and construct an evidence network explaining the association between brand personality and driving factors. These computational results are then integrated with new interactive visualizations to help brand managers understand personality traits and their driving factors. We demonstrate the usefulness and effectiveness of SocialBrands through a series of user studies with brand managers in an enterprise context. Design lessons are also derived from our studies.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883513",
            "id": "r_456",
            "s_ids": [
                "s_471",
                "s_1432",
                "s_122",
                "s_141",
                "s_847",
                "s_353"
            ],
            "type": "rich",
            "x": 0.4816431779770406,
            "y": -0.15069969203795464
        },
        {
            "title": "TimeStitch: Interactive multi-focus cohort discovery and comparison",
            "data": "Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347682",
            "id": "r_457",
            "s_ids": [
                "s_288",
                "s_763",
                "s_1412",
                "s_1447",
                "s_295"
            ],
            "type": "rich",
            "x": -0.012926949640827183,
            "y": -0.00834355759033188
        },
        {
            "title": "VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions",
            "data": "Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934797",
            "id": "r_458",
            "s_ids": [
                "s_256",
                "s_881",
                "s_1283",
                "s_877",
                "s_315",
                "s_581"
            ],
            "type": "rich",
            "x": 0.4295075462938848,
            "y": -0.48338880201135154
        },
        {
            "title": "Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets",
            "data": "We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934555",
            "id": "r_459",
            "s_ids": [
                "s_1219",
                "s_540",
                "s_1126",
                "s_418",
                "s_341",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.11465444481871542,
            "y": -0.09896071277509576
        },
        {
            "title": "Separating the Wheat from the Chaff: Comparative Visual Cues for Transparent Diagnostics of Competing Models",
            "data": "Experts in data and physical sciences have to regularly grapple with the problem of competing models. Be it analytical or physics-based models, a cross-cutting challenge for experts is to reliably diagnose which model outcomes appropriately predict or simulate real-world phenomena. Expert judgment involves reconciling information across many, and often, conflicting criteria that describe the quality of model outcomes. In this paper, through a design study with climate scientists, we develop a deeper understanding of the problem and solution space of model diagnostics, resulting in the following contributions: i) a problem and task characterization using which we map experts' model diagnostics goals to multi-way visual comparison tasks, ii) a design space of comparative visual cues for letting experts quickly understand the degree of disagreement among competing models and gauge the degree of stability of model outputs with respect to alternative criteria, and iii) design and evaluation of MyriadCues, an interactive visualization interface for exploring alternative hypotheses and insights about good and bad models by leveraging comparative visual cues. We present case studies and subjective feedback by experts, which validate how MyriadCues enables more transparent model diagnostic mechanisms, as compared to the state of the art.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934540",
            "id": "r_460",
            "s_ids": [
                "s_1284",
                "s_1255",
                "s_827",
                "s_497"
            ],
            "type": "rich",
            "x": -0.22742283526394022,
            "y": -0.2664646862905453
        },
        {
            "title": "CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral Artery Network Visualization",
            "data": "Blood circulation in the human brain is supplied through a network of cerebral arteries. If a clinician suspects a patient has a stroke or other cerebrovascular condition, they order imaging tests. Neuroradiologists visually search the resulting scans for abnormalities. Their visual search tasks correspond to the abstract network analysis tasks of browsing and path following. To assist neuroradiologists in identifying cerebral artery abnormalities, we designed CerebroVis, a novel abstract\u2014yet spatially contextualized\u2014cerebral artery network visualization. In this design study, we contribute a novel framing and definition of the cerebral artery system in terms of network theory and characterize neuroradiologist domain goals as abstract visualization and network analysis tasks. Through an iterative, user-centered design process we developed an abstract network layout technique which incorporates cerebral artery spatial context. The abstract visualization enables increased domain task performance over 3D geometry representations, while including spatial context helps preserve the user's mental map of the underlying geometry. We provide open source implementations of our network layout technique and prototype cerebral artery visualization tool. We demonstrate the robustness of our technique by successfully laying out 61 open source brain scans. We evaluate the effectiveness of our layout through a mixed methods study with three neuroradiologists. In a formative controlled experiment our study participants used CerebroVis and a conventional 3D visualization to examine real cerebral artery imaging data to identify a simulated intracranial artery stenosis. Participants were more accurate at identifying stenoses using CerebroVis (absolute risk difference 13%). A free copy of this paper, the evaluation stimuli and data, and source code are available at osf.io/e5sxt.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934402",
            "id": "r_461",
            "s_ids": [
                "s_450",
                "s_1125",
                "s_146",
                "s_1397",
                "s_1462",
                "s_1104",
                "s_632",
                "s_879",
                "s_1069"
            ],
            "type": "rich",
            "x": -0.4797825482296675,
            "y": -0.02054234799454709
        },
        {
            "title": "On the Treatment of Field Quantities and Elemental Continuity in FEM Solutions",
            "data": "As the finite element method (FEM) and the finite volume method (FVM), both traditional and high-order variants, continue their proliferation into various applied engineering disciplines, it is important that the visualization techniques and corresponding data analysis tools that act on the results produced by these methods faithfully represent the underlying data. To state this in another way: the interpretation of data generated by simulation needs to be consistent with the numerical schemes that underpin the specific solver technology. As the verifiable visualization literature has demonstrated: visual artifacts produced by the introduction of either explicit or implicit data transformations, such as data resampling, can sometimes distort or even obfuscate key scientific features in the data. In this paper, we focus on the handling of elemental continuity, which is often only<inline-formula><tex-math notation=\"LaTeX\">$C^{0}$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-jallepalli-2744058-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>continuous or piecewise discontinuous, when visualizing primary or derived fields from FEM or FVM simulations. We demonstrate that traditional data handling and visualization of these fields introduce visual errors. In addition, we show how the use of the recently proposed line-SIAC filter provides a way of handling elemental continuity issues in an accuracy-conserving manner with the added benefit of casting the data in a smooth context even if the representation is element discontinuous.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744058",
            "id": "r_462",
            "s_ids": [
                "s_536",
                "s_586",
                "s_79",
                "s_1453",
                "s_502"
            ],
            "type": "rich",
            "x": -0.3954965117645191,
            "y": 0.3134580993906041
        },
        {
            "title": "Inviwo ??? An extensible, multi-purpose visualization framework",
            "data": "To enable visualization research impacting other scientific domains, the availability of easy-to-use visualization frameworks is essential. Nevertheless, an easy-to-use system also has to be adapted to the capabilities of modern hardware architectures, as only this allows for realizing interactive visualizations. With this trade-off in mind, we have designed and realized the cross-platform Inviwo (Interactive Visualization Workshop) visualization framework, that supports both interactive visualization research as well as efficient visualization application development and deployment. In this poster we give an overview of the architecture behind Inviwo, and show how its design enables us and other researchers to realize their visualization ideas efficiently. Inviwo consists of a modern and lightweight, graphics independent core, which is extended by optional modules that encapsulate visualization algorithms, well-known utility libraries and commonly used parallel-processing APIs (such as OpenGL and OpenCL). The core enables a simplistic structure for creating bridges between the different modules regarding data transfer across architecture and devices with an easy-to-use screen graph and minimalistic programming. Making the base structures in a modern way while providing intuitive methods of extending the functionality and creating modules based on other modules, we hope that Inviwo can help the visualization community to perform research through a rapid-prototyping design and GUI, while at the same time allowing users to take advantage of the results implemented in the system in any way they desire later on. Inviwo is publicly available at www.inviwo.org, and can be used freely by anyone under a permissive free software license (Simplified BSD).",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429514",
            "id": "r_463",
            "s_ids": [
                "s_267",
                "s_937",
                "s_149",
                "s_1488",
                "s_78",
                "s_592",
                "s_209"
            ],
            "type": "rich",
            "x": 0.601152127614062,
            "y": 0.2716911876216828
        },
        {
            "title": "A Classification of User Tasks in Visual Analysis of Volume Data",
            "data": "Empirical findings from studies in one scientific domain have very limited applicability to other domains, unless we formally establish deeper insights on the generalizability of task types. We present a domain-independent classification of visual analysis tasks with volume visualizations. This taxonomy will help researchers design experiments, ensure coverage, and generate hypotheses in empirical studies with volume datasets. To develop our taxonomy, we first interviewed scientists working with spatial data in disparate domains. We then ran a survey to evaluate the design participants in which were scientists and professionals from around the world, working with volume data in various scientific domains. Respondents agreed substantially with our taxonomy design, but also suggested important refinements. We report the results in the form of a goal-based generic categorization of visual analysis tasks with volume visualizations. Our taxonomy covers tasks performed with a wide variety of volume datasets.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429485",
            "id": "r_464",
            "s_ids": [
                "s_906",
                "s_975",
                "s_1148",
                "s_7"
            ],
            "type": "rich",
            "x": 0.3637932181247964,
            "y": -0.04011870664496376
        },
        {
            "title": "Pattern Trails: Visual Analysis of Pattern Transitions in Subspaces",
            "data": "Subspace analysis methods have gained interest for identifying patterns in subspaces of high-dimensional data. Existing techniques allow to visualize and compare patterns in subspaces. However, many subspace analysis methods produce an abundant amount of patterns, which often remain redundant and are difficult to relate. Creating effective layouts for comparison of subspace patterns remains challenging. We introduce Pattern Trails, a novel approach for visually ordering and comparing subspace patterns. Central to our approach is the notion of pattern transitions as an interpretable structure imposed to order and compare patterns between subspaces. The basic idea is to visualize projections of subspaces side-by-side, and indicate changes between adjacent patterns in the subspaces by a linked representation, hence introducing pattern transitions. Our contributions comprise a systematization for how pairs of subspace patterns can be compared, and how changes can be interpreted in terms of pattern transitions. We also contribute a technique for visual subspace analysis based on a data-driven similarity measure between subspace representations. This measure is useful to order the patterns, and interactively group subspaces to reduce redundancy. We demonstrate the usefulness of our approach by application to several use cases, indicating that data can be meaningfully ordered and interpreted in terms of pattern transitions.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585613",
            "id": "r_465",
            "s_ids": [
                "s_37",
                "s_545",
                "s_540",
                "s_1003",
                "s_493"
            ],
            "type": "rich",
            "x": -0.32357002616734004,
            "y": 0.2149760964926597
        },
        {
            "title": "ShapeWordle: Tailoring Wordles using Shape-aware Archimedean Spirals",
            "data": "We present a new technique to enable the creation of shape-bounded Wordles, we call ShapeWordle, in which we fit words to form a given shape. To guide word placement within a shape, we extend the traditional Archimedean spirals to be shape-aware by formulating the spirals in a differential form using the distance field of the shape. To handle non-convex shapes, we introduce a multi-centric Wordle layout method that segments the shape into parts for our shape-aware spirals to adaptively fill the space and generate word placements. In addition, we offer a set of editing interactions to facilitate the creation of semantically-meaningful Wordles. Lastly, we present three evaluations: a comprehensive comparison of our results against the state-of-the-art technique (WordArt), case studies with 14 users, and a gallery to showcase the coverage of our technique.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934783",
            "id": "r_466",
            "s_ids": [
                "s_1059",
                "s_741",
                "s_690",
                "s_173",
                "s_1147",
                "s_1306",
                "s_1339",
                "s_589",
                "s_250",
                "s_1220"
            ],
            "type": "rich",
            "x": 0.16357748216013887,
            "y": -0.5909154415591128
        },
        {
            "title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios",
            "data": "Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present s Portfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system's effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934660",
            "id": "r_467",
            "s_ids": [
                "s_1009",
                "s_60",
                "s_1191",
                "s_711",
                "s_105",
                "s_924",
                "s_137"
            ],
            "type": "rich",
            "x": 0.27240491100241243,
            "y": -0.041610935236783025
        },
        {
            "title": "P5: Portable Progressive Parallel Processing Pipelines for Interactive Data Analysis and Visualization",
            "data": "We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934537",
            "id": "r_468",
            "s_ids": [
                "s_955",
                "s_687"
            ],
            "type": "rich",
            "x": 0.4502476328574851,
            "y": 0.17971449507511794
        },
        {
            "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries",
            "data": "The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Nowadays, the coordination between the muscles to generate simple movements is still not well understood, hindering the knowledge of how to best treat patients with this type of peripheral nerve injury. To acquire enough information for medical data analysis, physicians conduct motion analysis assessments with patients to produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Without the ability to integrate, compare, and compute multiple data sources in one platform, physicians can only compute simple statistical values to describe patient's behavior vaguely, which limits the possibility to answer clinical questions and generate hypotheses for research. To address this challenge, we have developed Motion Browser, an interactive visual analytics system which provides an efficient framework to extract and compare muscle activity patterns from the patient's limbs and coordinated views to help users analyze muscle signals, motion data, and video information to address different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies showing physicians can utilize the information displayed to understand how individuals coordinate their muscles to initiate appropriate treatment and generate new hypotheses for future research.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934280",
            "id": "r_469",
            "s_ids": [
                "s_1169",
                "s_349",
                "s_1100",
                "s_210",
                "s_1487",
                "s_380"
            ],
            "type": "rich",
            "x": -0.2862743412826684,
            "y": -0.14312711275239415
        },
        {
            "title": "OpenSpace: A System for Astrographics",
            "data": "Human knowledge about the cosmos is rapidly increasing as instruments and simulations are generating new data supporting the formation of theory and understanding of the vastness and complexity of the universe. OpenSpace is a software system that takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecrafts to billions of light years when visualizing the early universe. The ambition is to support research in astronomy and space exploration, science communication at museums and in planetariums as well as bringing exploratory astrographics to the class room. There is a multitude of challenges that need to be met in reaching this goal such as the data variety, multiple spatio-temporal scales, collaboration capabilities, etc. Furthermore, the system has to be flexible and modular to enable rapid prototyping and inclusion of new research results or space mission data and thereby shorten the time from discovery to dissemination. To support the different use cases the system has to be hardware agnostic and support a range of platforms and interaction paradigms. In this paper we describe how OpenSpace meets these challenges in an open source effort that is paving the path for the next generation of interactive astrographics.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934259",
            "id": "r_470",
            "s_ids": [
                "s_1233",
                "s_260",
                "s_478",
                "s_1455",
                "s_548",
                "s_90",
                "s_520",
                "s_1414",
                "s_380",
                "s_642"
            ],
            "type": "rich",
            "x": 0.06936798495158854,
            "y": 0.1550453707204079
        },
        {
            "title": "Nonlinear Dot Plots",
            "data": "Conventional dot plots use a constant dot size and are typically applied to show the frequency distribution of small data sets. Unfortunately, they are not designed for a high dynamic range of frequencies. We address this problem by introducing nonlinear dot plots. Adopting the idea of nonlinear scaling from logarithmic bar charts, our plots allow for dots of varying size so that columns with a large number of samples are reduced in height. For the construction of these diagrams, we introduce an efficient two-way sweep algorithm that leads to a dense and symmetrical layout. We compensate aliasing artifacts at high dot densities by a specifically designed low-pass filtering method. Examples of nonlinear dot plots are compared to conventional dot plots as well as linear and logarithmic histograms. Finally, we include feedback from an expert review.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744018",
            "id": "r_471",
            "s_ids": [
                "s_609",
                "s_138"
            ],
            "type": "rich",
            "x": -0.40693065059909284,
            "y": 0.2710599412928763
        },
        {
            "title": "VEEVVIE: Visual Explorer for Empirical Visualization, VR and Interaction Experiments",
            "data": "Empirical, hypothesis-driven, experimentation is at the heart of the scientific discovery process and has become commonplace in human-factors related fields. To enable the integration of visual analytics in such experiments, we introduce VEEVVIE, the Visual Explorer for Empirical Visualization, VR and Interaction Experiments. VEEVVIE is comprised of a back-end ontology which can model several experimental designs encountered in these fields. This formalization allows VEEVVIE to capture experimental data in a query-able form and makes it accessible through a front-end interface. This front-end offers several multi-dimensional visualization widgets with built-in filtering and highlighting functionality. VEEVVIE is also expandable to support custom experimental measurements and data types through a plug-in visualization widget architecture. We demonstrate VEEVVIE through several case studies of visual analysis, performed on the design and data collected during an experiment on the scalability of high-resolution, immersive, tiled-display walls.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467954",
            "id": "r_472",
            "s_ids": [
                "s_791",
                "s_157",
                "s_732"
            ],
            "type": "rich",
            "x": 0.41303521209944283,
            "y": 0.21935209492718644
        },
        {
            "title": "The Effect of Semantic Interaction on Foraging in Text Analysis",
            "data": "Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1) a synthesis model that aids in document layout, and (2) a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword search foraging (KSF), \u201csemantic interaction foraging\u201d (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword search.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802424",
            "id": "r_473",
            "s_ids": [
                "s_582",
                "s_104",
                "s_873",
                "s_13",
                "s_207"
            ],
            "type": "rich",
            "x": 0.4217761088854458,
            "y": -0.5050633481212962
        },
        {
            "title": "Supporting activity recognition by visual analytics",
            "data": "Recognizing activities has become increasingly relevant in many application domains, such as security or ambient assisted living. To handle different scenarios, the underlying automated algorithms are configured using multiple input parameters. However, the influence and interplay of these parameters is often not clear, making exhaustive evaluations necessary. On this account, we propose a visual analytics approach to supporting users in understanding the complex relationships among parameters, recognized activities, and associated accuracies. First, representative parameter settings are determined. Then, the respective output is computed and statistically analyzed to assess parameters' influence in general. Finally, visualizing the parameter settings along with the activities provides overview and allows to investigate the computed results in detail. Coordinated interaction helps to explore dependencies, compare different settings, and examine individual activities. By integrating automated, visual, and interactive means users can select parameter values that meet desired quality criteria. We demonstrate the application of our solution in a use case with realistic complexity, involving a study of human protagonists in daily living with respect to hundreds of parameter settings.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347629",
            "id": "r_474",
            "s_ids": [
                "s_1145",
                "s_528",
                "s_614",
                "s_932",
                "s_737",
                "s_81",
                "s_1335",
                "s_1087"
            ],
            "type": "rich",
            "x": 0.07348338125381575,
            "y": -0.06862022244724049
        },
        {
            "title": "Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots",
            "data": "This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934811",
            "id": "r_475",
            "s_ids": [
                "s_1417",
                "s_180",
                "s_219",
                "s_377",
                "s_1045",
                "s_344",
                "s_972"
            ],
            "type": "rich",
            "x": -0.21735551906465375,
            "y": 0.09691368213112893
        },
        {
            "title": "Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations",
            "data": "Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934805",
            "id": "r_476",
            "s_ids": [
                "s_1059",
                "s_179",
                "s_131",
                "s_1297",
                "s_1161",
                "s_1339",
                "s_589"
            ],
            "type": "rich",
            "x": -0.13531776255975836,
            "y": 0.003208653609179653
        },
        {
            "title": "A Natural-language-based Visual Query Approach of Uncertain Human Trajectories",
            "data": "Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934671",
            "id": "r_477",
            "s_ids": [
                "s_506",
                "s_941",
                "s_1250",
                "s_693",
                "s_43",
                "s_1464",
                "s_770",
                "s_964",
                "s_64"
            ],
            "type": "rich",
            "x": 0.23033756778859135,
            "y": 0.5911246243289173
        },
        {
            "title": "Galex: Exploring the Evolution and Intersection of Disciplines",
            "data": "Revealing the evolution of science and the intersections among its sub-fields is extremely important to understand the characteristics of disciplines, discover new topics, and predict the future. The current work focuses on either building the skeleton of science, lacking interaction, detailed exploration and interpretation or on the lower topic level, missing high-level macro-perspective. To fill this gap, we design and implement Galaxy Evolution Explorer (Galex), a hierarchical visual analysis system, in combination with advanced text mining technologies, that could help analysts to comprehend the evolution and intersection of one discipline rapidly. We divide Galex into three progressively fine-grained levels: discipline, area, and institution levels. The combination of interactions enables analysts to explore an arbitrary piece of history and an arbitrary part of the knowledge space of one discipline. Using a flexible spotlight component, analysts could freely select and quickly understand an exploration region. A tree metaphor allows analysts to perceive the expansion, decline, and intersection of topics intuitively. A synchronous spotlight interaction aids in comparing research contents among institutions easily. Three cases demonstrate the effectiveness of our system.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934667",
            "id": "r_478",
            "s_ids": [
                "s_689",
                "s_1178",
                "s_1030",
                "s_895"
            ],
            "type": "rich",
            "x": 0.3103187975927437,
            "y": -0.24337049087269352
        },
        {
            "title": "Temporal Views of Flattened Mitral Valve Geometries",
            "data": "The mitral valve, one of the four valves in the human heart, controls the bloodflow between the left atrium and ventricle and may suffer from various pathologies. Malfunctioning valves can be treated by reconstructive surgeries, which have to be carefully planned and evaluated. While current research focuses on the modeling and segmentation of the valve, we base our work on existing segmentations of patient-specific mitral valves, that are also time-resolved ($3\\mathrm{D}+\\mathrm{t}$) over the cardiac cycle. The interpretation of the data can be ambiguous, due to the complex surface of the valve and multiple time steps. We therefore propose a software prototype to analyze such $3\\mathrm{D}+\\mathrm{t}$ data, by extracting pathophysiological parameters and presenting them via dimensionally reduced visualizations. For this, we rely on an existing algorithm to unroll the convoluted valve surface towards a flattened 2D representation. In this paper, we show that the $3\\mathrm{D}+\\mathrm{t}$ data can be transferred to 3D or 2D representations in a way that allows the domain expert to faithfully grasp important aspects of the cardiac cycle. In this course, we not only consider common pathophysiological parameters, but also introduce new observations that are derived from landmarks within the segmentation model. Our analysis techniques were developed in collaboration with domain experts and a survey showed that the insights have the potential to support mitral valve diagnosis and the comparison of the pre- and post-operative condition of a patient.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934337",
            "id": "r_479",
            "s_ids": [
                "s_501",
                "s_1266",
                "s_1442",
                "s_363",
                "s_1467"
            ],
            "type": "rich",
            "x": -0.6978847041513984,
            "y": 0.06554548885855493
        },
        {
            "title": "Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data",
            "data": "The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias\u2013when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort \u201cdrift\u201d, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934209",
            "id": "r_480",
            "s_ids": [
                "s_436",
                "s_622",
                "s_119",
                "s_537",
                "s_851"
            ],
            "type": "rich",
            "x": 0.030966231481124085,
            "y": -0.04003424743640002
        },
        {
            "title": "Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach",
            "data": "With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as<i>culling</i>. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel<i>query-adaptive</i>culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864847",
            "id": "r_481",
            "s_ids": [
                "s_603",
                "s_102",
                "s_1368",
                "s_590",
                "s_1346",
                "s_336"
            ],
            "type": "rich",
            "x": -0.4104481345810417,
            "y": 0.1530918776372748
        },
        {
            "title": "Interactive obstruction-free lensing for volumetric data visualization",
            "data": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864690",
            "id": "r_482",
            "s_ids": [
                "s_829",
                "s_589",
                "s_133"
            ],
            "type": "rich",
            "x": -0.5119884534269126,
            "y": 0.29863754397808173
        },
        {
            "title": "Synteny Explorer: An Interactive Visualization Application for Teaching Genome Evolution",
            "data": "Rapid advances in biology demand new tools for more active research dissemination and engaged teaching. This paper presents Synteny Explorer, an interactive visualization application designed to let college students explore genome evolution of mammalian species. The tool visualizes synteny blocks: segments of homologous DNA shared between various extant species that can be traced back or reconstructed in extinct, ancestral species. We take a karyogram-based approach to create an interactive synteny visualization, leading to a more appealing and engaging design for undergraduate-level genome evolution education. For validation, we conduct three user studies: two focused studies on color and animation design choices and a larger study that performs overall system usability testing while comparing our karyogram-based designs with two more common genome mapping representations in an educational context. While existing views communicate the same information, study participants found the interactive, karyogram-based views much easier and likable to use. We additionally discuss feedback from biology and genomics faculty, who judge Synteny Explorer's fitness for use in classrooms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598789",
            "id": "r_483",
            "s_ids": [
                "s_342",
                "s_1274",
                "s_687",
                "s_489",
                "s_406",
                "s_919",
                "s_610",
                "s_86"
            ],
            "type": "rich",
            "x": 0.08391964017702483,
            "y": -0.11944249284401037
        },
        {
            "title": "Multi-Granular Trend Detection for Time-Series Analysis",
            "data": "Time series (such as stock prices) and ensembles (such as model runs for weather forecasts) are two important types of one-dimensional time-varying data. Such data is readily available in large quantities but visual analysis of the raw data quickly becomes infeasible, even for moderately sized data sets. Trend detection is an effective way to simplify time-varying data and to summarize salient information for visual display and interactive analysis. We propose a geometric model for trend-detection in one-dimensional time-varying data, inspired by topological grouping structures for moving objects in two- or higher-dimensional space. Our model gives provable guarantees on the trends detected and uses three natural parameters: granularity, support-size, and duration. These parameters can be changed on-demand. Our system also supports a variety of selection brushes and a time-sweep to facilitate refined searches and interactive visualization of (sub-)trends. We explore different visual styles and interactions through which trends, their persistence, and evolution can be explored.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598619",
            "id": "r_484",
            "s_ids": [
                "s_333",
                "s_1299",
                "s_823",
                "s_499",
                "s_300"
            ],
            "type": "rich",
            "x": 0.02248911020882803,
            "y": 0.39937490280163
        },
        {
            "title": "SchemeLens: A Content-Aware Vector-Based Fisheye Technique for Navigating Large Systems Diagrams",
            "data": "System schematics, such as those used for electrical or hydraulic systems, can be large and complex. Fisheye techniques can help navigate such large documents by maintaining the context around a focus region, but the distortion introduced by traditional fisheye techniques can impair the readability of the diagram. We present SchemeLens, a vector-based, topology-aware fisheye technique which aims to maintain the readability of the diagram. Vector-based scaling reduces distortion to components, but distorts layout. We present several strategies to reduce this distortion by using the structure of the topology, including orthogonality and alignment, and a model of user intention to foster smooth and predictable navigation. We evaluate this approach through two user studies: Results show that (1) SchemeLens is 16-27% faster than both round and rectangular flat-top fisheye lenses at finding and identifying a target along one or several paths in a network diagram; (2) augmenting SchemeLens with a model of user intentions aids in learning the network topology.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467035",
            "id": "r_485",
            "s_ids": [
                "s_1173",
                "s_229",
                "s_82",
                "s_1035",
                "s_809"
            ],
            "type": "rich",
            "x": -0.2931346787429054,
            "y": -0.033912723806866836
        },
        {
            "title": "TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling",
            "data": "Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or \u201ctargets\u201d rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986922",
            "id": "r_486",
            "s_ids": [
                "s_726",
                "s_696",
                "s_1116",
                "s_1469",
                "s_1068"
            ],
            "type": "rich",
            "x": 0.22084235149017808,
            "y": -0.5862751747239797
        },
        {
            "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach",
            "data": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802486",
            "id": "r_487",
            "s_ids": [
                "s_1076",
                "s_540",
                "s_948",
                "s_273",
                "s_523",
                "s_164",
                "s_1102",
                "s_595",
                "s_1003"
            ],
            "type": "rich",
            "x": 0.025116335796312235,
            "y": 0.2038368255577099
        },
        {
            "title": "C2A: Crowd consensus analytics for virtual colonoscopy",
            "data": "We present a medical crowdsourcing visual analytics platform called C<sup>2</sup>A to visualize, classify and filter crowdsourced clinical data. More specifically, C<sup>2</sup>A is used to build consensus on a clinical diagnosis by visualizing crowd responses and filtering out anomalous activity. Crowdsourcing medical applications have recently shown promise where the non-expert users (the crowd) were able to achieve accuracy similar to the medical experts. This has the potential to reduce interpretation/reading time and possibly improve accuracy by building a consensus on the findings beforehand and letting the medical experts make the final diagnosis. In this paper, we focus on a virtual colonoscopy (VC) application with the clinical technicians as our target users, and the radiologists acting as consultants and classifying segments as benign or malignant. In particular, C<sup>2</sup>A is used to analyze and explore crowd responses on video segments, created from fly-throughs in the virtual colon. C<sup>2</sup>A provides several interactive visualization components to build crowd consensus on video segments, to detect anomalies in the crowd data and in the VC video segments, and finally, to improve the non-expert user's work quality and performance by A/B testing for the optimal crowdsourcing platform and application-specific parameters. Case studies and domain experts feedback demonstrate the effectiveness of our framework in improving workers' output quality, the potential to reduce the radiologists' interpretation time, and hence, the potential to improve the traditional clinical workflow by marking the majority of the video segments as benign based on the crowd consensus.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883508",
            "id": "r_488",
            "s_ids": [
                "s_245",
                "s_757",
                "s_602",
                "s_732"
            ],
            "type": "rich",
            "x": -0.16605744401303643,
            "y": -0.3734984929538646
        },
        {
            "title": "Visual Analytics for fraud detection and monitoring",
            "data": "One of the primary concerns of financial institutions is to guarantee security and legitimacy in their services. Being able to detect and avoid fraudulent schemes also enhances the credibility of these institutions. Currently, fraud detection approaches still lack Visual Analytics techniques. We propose a Visual Analytics process that tackles the main challenges in the area of fraud detection.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347678",
            "id": "r_489",
            "s_ids": [
                "s_18",
                "s_181",
                "s_1087",
                "s_49",
                "s_998"
            ],
            "type": "rich",
            "x": 0.2993912924757964,
            "y": -0.08595559485073466
        },
        {
            "title": "High-throughput feature extraction for measuring attributes of deforming open-cell foams",
            "data": "Metallic open-cell foams are promising structural materials with applications in multifunctional systems such as biomedical implants, energy absorbers in impact, noise mitigation, and batteries. There is a high demand for means to understand and correlate the design space of material performance metrics to the material structure in terms of attributes such as density, ligament and node properties, void sizes, and alignments. Currently, X-ray Computed Tomography (CT) scans of these materials are segmented either manually or with skeletonization approaches that may not accurately model the variety of shapes present in nodes and ligaments, especially irregularities that arise from manufacturing, image artifacts, or deterioration due to compression. In this paper, we present a new workflow for analysis of open-cell foams that combines a new density measurement to identify nodal structures, and topological approaches to identify ligament structures between them. Additionally, we provide automated measurement of foam properties. We demonstrate stable extraction of features and time-tracking in an image sequence of a foam being compressed. Our approach allows researchers to study larger and more complex foams than could previously be segmented only manually, and enables the high-throughput analysis needed to predict future foam performance.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934620",
            "id": "r_490",
            "s_ids": [
                "s_513",
                "s_1472",
                "s_235",
                "s_190",
                "s_54",
                "s_171",
                "s_297"
            ],
            "type": "rich",
            "x": -0.77205805181501,
            "y": 0.20590439832650073
        },
        {
            "title": "Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction",
            "data": "We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934534",
            "id": "r_491",
            "s_ids": [
                "s_1275",
                "s_864",
                "s_334",
                "s_1469"
            ],
            "type": "rich",
            "x": 0.6658857593945129,
            "y": 0.09691703106129503
        },
        {
            "title": "Extraction and Visual Analysis of Potential Vorticity Banners around the Alps",
            "data": "Potential vorticity is among the most important scalar quantities in atmospheric dynamics. For instance, potential vorticity plays a key role in particularly strong wind peaks in extratropical cyclones and it is able to explain the occurrence of frontal rain bands. Potential vorticity combines the key quantities of atmospheric dynamics, namely rotation and stratification. Under suitable wind conditions elongated banners of potential vorticity appear in the lee of mountains. Their role in atmospheric dynamics has recently raised considerable interest in the meteorological community for instance due to their influence in aviation wind hazards and maritime transport. In order to support meteorologists and climatologists in the analysis of these structures, we developed an extraction algorithm and a visual exploration framework consisting of multiple linked views. For the extraction we apply a predictor-corrector algorithm that follows streamlines and realigns them with extremal lines of potential vorticity. Using the agglomerative hierarchical clustering algorithm, we group banners from different sources based on their proximity. To visually analyze the time-dependent banner geometry, we provide interactive overviews and enable the query for detail on demand, including the analysis of different time steps, potentially correlated scalar quantities, and the wind vector field. In particular, we study the relationship between relative humidity and the banners for their potential in indicating the development of precipitation. Working with our method, the collaborating meteorologists gained a deeper understanding of the three-dimensional processes, which may spur follow-up research in the future.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934310",
            "id": "r_492",
            "s_ids": [
                "s_1419",
                "s_1425",
                "s_828",
                "s_1300",
                "s_923",
                "s_1225"
            ],
            "type": "rich",
            "x": -0.3709390769315982,
            "y": 0.5914769101763889
        },
        {
            "title": "InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming",
            "data": "Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864887",
            "id": "r_493",
            "s_ids": [
                "s_183",
                "s_256",
                "s_640",
                "s_1283",
                "s_47",
                "s_912"
            ],
            "type": "rich",
            "x": 0.49733896548546985,
            "y": -0.5319598486302225
        },
        {
            "title": "A Declarative Grammar of Flexible Volume Visualization Pipelines",
            "data": "This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864841",
            "id": "r_494",
            "s_ids": [
                "s_639",
                "s_355",
                "s_687"
            ],
            "type": "rich",
            "x": 0.1871810660607537,
            "y": 0.22844758670202547
        },
        {
            "title": "Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores",
            "data": "This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744686",
            "id": "r_495",
            "s_ids": [
                "s_1099",
                "s_775",
                "s_335",
                "s_118"
            ],
            "type": "rich",
            "x": -0.07033342014437347,
            "y": 0.06972189510630794
        },
        {
            "title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment",
            "data": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599378",
            "id": "r_496",
            "s_ids": [
                "s_1244",
                "s_965",
                "s_976",
                "s_137"
            ],
            "type": "rich",
            "x": -0.005810695005788646,
            "y": 0.15343218565790231
        },
        {
            "title": "A Fractional Cartesian Composition Model for Semi-Spatial Comparative Visualization Design",
            "data": "The study of spatial data ensembles leads to substantial visualization challenges in a variety of applications. In this paper, we present a model for comparative visualization that supports the design of according ensemble visualization solutions by partial automation. We focus on applications, where the user is interested in preserving selected spatial data characteristics of the data as much as possible-even when many ensemble members should be jointly studied using comparative visualization. In our model, we separate the design challenge into a minimal set of user-specified parameters and an optimization component for the automatic configuration of the remaining design variables. We provide an illustrated formal description of our model and exemplify our approach in the context of several application examples from different domains in order to demonstrate its generality within the class of comparative visualization problems for spatial data ensembles.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598870",
            "id": "r_497",
            "s_ids": [
                "s_391",
                "s_900",
                "s_443",
                "s_367"
            ],
            "type": "rich",
            "x": -0.10442052609554137,
            "y": 0.4208553730069494
        },
        {
            "title": "Screenit: Visual Analysis of Cellular Screens",
            "data": "High-throughput and high-content screening enables large scale, cost-effective experiments in which cell cultures are exposed to a wide spectrum of drugs. The resulting multivariate data sets have a large but shallow hierarchical structure. The deepest level of this structure describes cells in terms of numeric features that are derived from image data. The subsequent level describes enveloping cell cultures in terms of imposed experiment conditions (exposure to drugs). We present Screenit, a visual analysis approach designed in close collaboration with screening experts. Screenit enables the navigation and analysis of multivariate data at multiple hierarchy levels and at multiple levels of detail. Screenit integrates the interactive modeling of cell physical states (phenotypes) and the effects of drugs on cell cultures (hits). In addition, quality control is enabled via the detection of anomalies that indicate low-quality data, while providing an interface that is designed to match workflows of screening experts. We demonstrate analyses for a real-world data set, CellMorph, with 6 million cells across 20,000 cell cultures.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598587",
            "id": "r_498",
            "s_ids": [
                "s_626",
                "s_869",
                "s_1077",
                "s_885",
                "s_647",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.505605554575354,
            "y": -0.05785335794826009
        },
        {
            "title": "Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation",
            "data": "Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467952",
            "id": "r_499",
            "s_ids": [
                "s_571",
                "s_1301",
                "s_471",
                "s_1400",
                "s_353",
                "s_1121"
            ],
            "type": "rich",
            "x": -0.2011651161766941,
            "y": 0.3258956209149404
        },
        {
            "title": "DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection",
            "data": "Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with a data structuring and exploration scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assistance widgets to interactively detect the relations of multi-dimensional data.",
            "url": "http://dx.doi.org/10.1109/VAST.2016.7883514",
            "id": "r_500",
            "s_ids": [
                "s_1123",
                "s_1250",
                "s_1090",
                "s_888",
                "s_304",
                "s_921"
            ],
            "type": "rich",
            "x": -0.06691863893814311,
            "y": 0.23622453175022143
        },
        {
            "title": "Collaborative visual analysis with RCloud",
            "data": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347627",
            "id": "r_501",
            "s_ids": [
                "s_1136",
                "s_1037",
                "s_779",
                "s_370"
            ],
            "type": "rich",
            "x": 0.3394109809530645,
            "y": -0.1897773858760047
        },
        {
            "title": "Exploranative Code Quality Documents",
            "data": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934669",
            "id": "r_502",
            "s_ids": [
                "s_1141",
                "s_15",
                "s_383",
                "s_138"
            ],
            "type": "rich",
            "x": 0.6851856439416867,
            "y": -0.22428546450242565
        },
        {
            "title": "The Role of Latency and Task Complexity in Predicting Visual Search Behavior",
            "data": "Latency in a visualization system is widely believed to affect user behavior in measurable ways, such as requiring the user to wait for the visualization system to respond, leading to interruption of the analytic flow. While this effect is frequently observed and widely accepted, precisely how latency affects different analysis scenarios is less well understood. In this paper, we examine the role of latency in the context of visual search, an essential task in data foraging and exploration using visualization. We conduct a series of studies on Amazon Mechanical Turk and find that under certain conditions, latency is a statistically significant predictor of visual search behavior, which is consistent with previous studies. However, our results also suggest that task type, task complexity, and other factors can modulate the effect of latency, in some cases rendering latency statistically insignificant in predicting user behavior. This suggests a more nuanced view of the role of latency than previously reported. Building on these results and the findings of prior studies, we propose design guidelines for measuring and interpreting the effects of latency when evaluating performance on visual search tasks.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934556",
            "id": "r_503",
            "s_ids": [
                "s_910",
                "s_848",
                "s_1041",
                "s_115",
                "s_321",
                "s_1460"
            ],
            "type": "rich",
            "x": 0.4313808135942891,
            "y": -0.34893957270837006
        },
        {
            "title": "SmartCube: An Adaptive Data Management Architecture for the Real-Time Visualization of Spatiotemporal Datasets",
            "data": "Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data pre-processing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934434",
            "id": "r_504",
            "s_ids": [
                "s_1322",
                "s_573",
                "s_76",
                "s_1214"
            ],
            "type": "rich",
            "x": -0.22683293741968436,
            "y": 0.39115130769628254
        },
        {
            "title": "Toward Localized Topological Data Structures: Querying the Forest for the Tree",
            "data": "Topological approaches to data analysis can answer complex questions about the number, connectivity, and scale of intrinsic features in scalar data. However, the global nature of many topological structures makes their computation challenging at scale, and thus often limits the size of data that can be processed. One key quality to achieving scalability and performance on modern architectures is data locality, i.e., a process operates on data that resides in a nearby memory system, avoiding frequent jumps in data access patterns. From this perspective, topological computations are particularly challenging because the implied data structures represent features that can span the entire data set, often requiring a global traversal phase that limits their scalability. Traditionally, expensive preprocessing is considered an acceptable trade-off as it accelerates all subsequent queries. Most published use cases, however, explore only a fraction of all possible queries, most often those returning small, local features. In these cases, much of the global information is not utilized, yet computing it dominates the overall response time. We address this challenge for merge trees, one of the most commonly used topological structures. In particular, we propose an alternative representation, the merge forest, a collection of local trees corresponding to regions in a domain decomposition. Local trees are connected by a bridge set that allows us to recover any necessary global information at query time. The resulting system couples (i) a preprocessing that scales linearly in practice with (ii) fast runtime queries that provide the same functionality as traditional queries of a global merge tree. We test the scalability of our approach on a shared-memory parallel computer and demonstrate how data structure locality enables the analysis of large data with an order of magnitude performance improvement over the status quo. Furthermore, a merge forest reduces the memory overhead compared to a global merge tree and enables the processing of data sets that are an order of magnitude larger than possible with previous algorithms.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934257",
            "id": "r_505",
            "s_ids": [
                "s_83",
                "s_1472",
                "s_438",
                "s_297"
            ],
            "type": "rich",
            "x": -0.4786136095566942,
            "y": 0.2056057088654373
        },
        {
            "title": "An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments",
            "data": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865025",
            "id": "r_506",
            "s_ids": [
                "s_266",
                "s_1044",
                "s_1395",
                "s_1343"
            ],
            "type": "rich",
            "x": 0.6199703063673456,
            "y": -0.01691712940178638
        },
        {
            "title": "Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities",
            "data": "The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865023",
            "id": "r_507",
            "s_ids": [
                "s_974",
                "s_927",
                "s_407",
                "s_1311",
                "s_457"
            ],
            "type": "rich",
            "x": 0.03470482925247656,
            "y": 0.11181957449600854
        },
        {
            "title": "Objective Vortex Corelines of Finite-sized Objects in Fluid Flows",
            "data": "Vortices are one of the most-frequently studied phenomena in fluid flows. The center of the rotating motion is called the vortex coreline and its successful detection strongly depends on the choice of the reference frame. The optimal frame moves with the center of the vortex, which incidentally makes the observed fluid flow steady and thus standard vortex coreline extractors such as Sujudi-Haimes become applicable. Recently, an objective optimization framework was proposed that determines a near-steady reference frame for tracer particles. In this paper, we extend this technique to the detection of vortex corelines of inertial particles. An inertial particle is a finite-sized object that is carried by a fluid flow. In contrast to the usual tracer particles, they do not move tangentially with the flow, since they are subject to gravity and exhibit mass-dependent inertia. Their particle state is determined by their position and own velocity, which makes the search for the optimal frame a high-dimensional problem. We demonstrate in this paper that the objective detection of an inertial vortex coreline can be reduced in 2D to a critical point search in 2D. For 3D flows, however, the vortex coreline criterion remains a parallel vectors condition in 6D. To detect the vortex corelines we propose a recursive subdivision approach that is tailored to the underlying structure of the 6D vectors. The resulting algorithm is objective, and we demonstrate the vortex coreline extraction in a number of 2D and 3D vector fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864828",
            "id": "r_508",
            "s_ids": [
                "s_1225",
                "s_697"
            ],
            "type": "rich",
            "x": -0.6640586729638034,
            "y": 0.7476804657482716
        },
        {
            "title": "Backward Finite-Time Lyapunov Exponents in Inertial Flows",
            "data": "Inertial particles are finite-sized objects that are carried by fluid flows and in contrast to massless tracer particles they are subject to inertia effects. In unsteady flows, the dynamics of tracer particles have been extensively studied by the extraction of Lagrangian coherent structures (LCS), such as hyperbolic LCS as ridges of the Finite-Time Lyapunov Exponent (FTLE). The extension of the rich LCS framework to inertial particles is currently a hot topic in the CFD literature and is actively under research. Recently, backward FTLE on tracer particles has been shown to correlate with the preferential particle settling of small inertial particles. For larger particles, inertial trajectories may deviate strongly from (massless) tracer trajectories, and thus for a better agreement, backward FTLE should be computed on inertial trajectories directly. Inertial backward integration, however, has not been possible until the recent introduction of the influence curve concept, which - given an observation and an initial velocity - allows to recover all sources of inertial particles as tangent curves of a derived vector field. In this paper, we show that FTLE on the influence curve vector field is in agreement with preferential particle settling and more importantly it is not only valid for small (near-tracer) particles. We further generalize the influence curve concept to general equations of motion in unsteady spatio-velocity phase spaces, which enables backward integration with more general equations of motion. Applying the influence curve concept to tracer particles in the spatio-velocity domain emits streaklines in massless flows as tangent curves of the influence curve vector field. We demonstrate the correlation between inertial backward FTLE and the preferential particle settling in a number of unsteady vector fields",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599016",
            "id": "r_509",
            "s_ids": [
                "s_1225",
                "s_697"
            ],
            "type": "rich",
            "x": -0.5809226699206906,
            "y": 0.6592235528212694
        },
        {
            "title": "A Visual Voting Framework for Weather Forecast Calibration",
            "data": "Numerical weather predictions have been widely used for weather forecasting. Many large meteorological centers are producing highly accurate ensemble forecasts routinely to provide effective weather forecast services. However, biases frequently exist in forecast products because of various reasons, such as the imperfection of the weather forecast models. Failure to identify and neutralize the biases would result in unreliable forecast products that might mislead analysts; consequently, unreliable weather predictions are produced. The analog method has been commonly used to overcome the biases. Nevertheless, this method has some serious limitations including the difficulties in finding effective similar past forecasts, the large search space for proper parameters and the lack of support for interactive, real-time analysis. In this study, we develop a visual analytics system based on a novel voting framework to circumvent the problems. The framework adopts the idea of majority voting to combine judiciously the different variants of analog methods towards effective retrieval of the proper analogs for calibration. The system seamlessly integrates the analog methods into an interactive visualization pipeline with a set of coordinated views that characterizes the different methods. Instant visual hints are provided in the views to guide users in finding and refining analogs. We have worked closely with the domain experts in the meteorological research to develop the system. The effectiveness of the system is demonstrated using two case studies. An informal evaluation with the experts proves the usability and usefulness of the system.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429488",
            "id": "r_510",
            "s_ids": [
                "s_787",
                "s_1273",
                "s_934",
                "s_799",
                "s_1059",
                "s_1182",
                "s_556",
                "s_1250"
            ],
            "type": "rich",
            "x": -0.08448305520862166,
            "y": 0.038765772161357986
        },
        {
            "title": "Visual analysis of route choice behaviour based on GPS trajectories",
            "data": "There are often multiple routes between regions. Many factors potentially affect driver's route choice, such as expected time cost, length etc. In this work, we present a visual analysis system to explore driver's route choice behaviour based on taxi GPS trajectory data. With interactive trajectory filtering, the system constructs feasible routes between regions of interest. Using a rank-based visualization, the attributes of multiple routes are explored and compared. Based on a statistical model, the system supports to verify trajectory-related factors' impact on route choice behaviour. The effectiveness of the system is demonstrated by applying to real trajectory dataset.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347679",
            "id": "r_511",
            "s_ids": [
                "s_1417",
                "s_425",
                "s_88",
                "s_1206",
                "s_1214"
            ],
            "type": "rich",
            "x": 0.16886737392873993,
            "y": 0.6986541806868232
        },
        {
            "title": "Estimating Color-Concept Associations from Image Statistics",
            "data": "To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934536",
            "id": "r_512",
            "s_ids": [
                "s_461",
                "s_412",
                "s_221",
                "s_30"
            ],
            "type": "rich",
            "x": -0.09284116117439017,
            "y": -0.33701900176717187
        },
        {
            "title": "GUIRO: User-Guided Matrix Reordering",
            "data": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934300",
            "id": "r_513",
            "s_ids": [
                "s_540",
                "s_493",
                "s_1346"
            ],
            "type": "rich",
            "x": -0.1949290706136687,
            "y": -0.054838574716555046
        },
        {
            "title": "Scale-Space Splatting: Reforming Spacetime for Cross-Scale Exploration of Integral Measures in Molecular Dynamics",
            "data": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space/time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934258",
            "id": "r_514",
            "s_ids": [
                "s_498",
                "s_874",
                "s_900",
                "s_367"
            ],
            "type": "rich",
            "x": -0.342212224041896,
            "y": 0.32699253058184136
        },
        {
            "title": "Structuring Visualization Mock-Ups at the Graphical Level by Dividing the Display Space",
            "data": "Mock-ups are rapid, low fidelity prototypes, that are used in many design-related fields to generate and share ideas. While their creation is supported by many mature methods and tools, surprisingly few are suited for the needs of information visualization. In this article, we introduce a novel approach to creating visualizations mock-ups, based on a dialogue between graphic design and parametric toolkit explorations. Our approach consists in iteratively subdividing the display space, while progressively informing each division with realistic data. We show that a wealth of mock-ups can easily be created using only temporary data attributes, as we wait for more realistic data to become available. We describe the implementation of this approach in a D3-based toolkit, which we use to highlight its generative power, and we discuss the potential for transitioning towards higher fidelity prototypes.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743998",
            "id": "r_515",
            "s_ids": [
                "s_943",
                "s_433"
            ],
            "type": "rich",
            "x": 0.4056006681532282,
            "y": 0.30366507137492094
        },
        {
            "title": "A Versatile and Efficient GPU Data Structure for Spatial Indexing",
            "data": "In this paper we present a novel GPU-based data structure for spatial indexing. Based on Fenwick trees-a special type of binary indexed trees-our data structure allows construction in linear time. Updates and prefixes can be computed in logarithmic time, whereas point queries require only constant time on average. Unlike competing data structures such as summed-area tables and spatial hashing, our data structure requires a constant amount of bits for each data element, and it offers unconstrained point queries. This property makes our data structure ideally suited for applications requiring unconstrained indexing of large data, such as block-storage of large and block-sparse volumes. Finally, we provide asymptotic bounds on both run-time and memory requirements, and we show applications for which our new data structure is useful.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2599043",
            "id": "r_516",
            "s_ids": [
                "s_712",
                "s_876"
            ],
            "type": "rich",
            "x": -0.47962314995676214,
            "y": 0.46947333165564686
        },
        {
            "title": "Interactive Visualization for Singular Fibers of Functions f : R3 \u2192 R2",
            "data": "Scalar topology in the form of Morse theory has provided computational tools that analyze and visualize data from scientific and engineering tasks. Contracting isocontours to single points encapsulates variations in isocontour connectivity in the Reeb graph. For multivariate data, isocontours generalize to fibers-inverse images of points in the range, and this area is therefore known as fiber topology. However, fiber topology is less fully developed than Morse theory, and current efforts rely on manual visualizations. This paper presents how to accelerate and semi-automate this task through an interface for visualizing fiber singularities of multivariate functions R<sup>3</sup>\u2192R<sup>2</sup>. This interface exploits existing conventions of fiber topology, but also introduces a 3D view based on the extension of Reeb graphs to Reeb spaces. Using the Joint Contour Net, a quantized approximation of the Reeb space, this accelerates topological visualization and permits online perturbation to reduce or remove degeneracies in functions under study. Validation of the interface is performed by assessing whether the interface supports the mathematical workflow both of experts and of less experienced mathematicians.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467433",
            "id": "r_517",
            "s_ids": [
                "s_1124",
                "s_467",
                "s_1226",
                "s_496",
                "s_744",
                "s_875",
                "s_984"
            ],
            "type": "rich",
            "x": -0.7153823781341918,
            "y": 0.21388940916501567
        },
        {
            "title": "Reconstruction and Visualization of Coordinated 3D Cell Migration Based on Optical Flow",
            "data": "Animal development is marked by the repeated reorganization of cells and cell populations, which ultimately determine form and shape of the growing organism. One of the central questions in developmental biology is to understand precisely how cells reorganize, as well as how and to what extent this reorganization is coordinated. While modern microscopes can record video data for every cell during animal development in 3D+t, analyzing these videos remains a major challenge: reconstruction of comprehensive cell tracks turned out to be very demanding especially with decreasing data quality and increasing cell densities. In this paper, we present an analysis pipeline for coordinated cellular motions in developing embryos based on the optical flow of a series of 3D images. We use numerical integration to reconstruct cellular long-term motions in the optical flow of the video, we take care of data validation, and we derive a LIC-based, dense flow visualization for the resulting pathlines. This approach allows us to handle low video quality such as noisy data or poorly separated cells, and it allows the biologists to get a comprehensive understanding of their data by capturing dynamic growth processes in stills. We validate our methods using three videos of growing fruit fly embryos.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467291",
            "id": "r_518",
            "s_ids": [
                "s_633",
                "s_725",
                "s_945",
                "s_1262",
                "s_1092",
                "s_35"
            ],
            "type": "rich",
            "x": -0.6856259452784277,
            "y": -0.005216806321980054
        },
        {
            "title": "Feature-Based Tensor Field Visualization for Fiber Reinforced Polymers",
            "data": "Virtual testing is an integral part of modern product development in mechanical engineering. Numerical structure simulations allow the computation of local stresses which are given as tensor fields. For homogeneous materials, the tensor information is usually reduced to a scalar field like the von Mises stress. A material-dependent threshold defines the material failure answering the key question of engineers. This leads to a rather simple feature-based visualisation. For composite materials like short fiber reinforced polymers, the situation is much more complex. The material property is determined by the fiber distribution at every position, often described as fiber orientation tensor field. Essentially, the material's ability to cope with stress becomes anisotropic and inhomogeneous. We show how to combine the stress field and the fiber orientation field in such cases, leading to a feature-based visualization of tensor fields for composite materials. The resulting features inform the engineer about potential improvements in the product development.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429491",
            "id": "r_519",
            "s_ids": [
                "s_1129",
                "s_1064",
                "s_841"
            ],
            "type": "rich",
            "x": -0.6317189726049227,
            "y": 0.2671660267572584
        },
        {
            "title": "ICE: An Interactive Configuration Explorer for High Dimensional Categorical Parameter Spaces",
            "data": "There are many applications where users seek to explore the impact of the settings of several categorical variables with respect to one dependent numerical variable. For example, a computer systems analyst might want to study how the type of file system or storage device affects system performance. A usual choice is the method of Parallel Sets designed to visualize multivariate categorical variables, However, we found that the magnitude of the parameter impacts on the numerical variable cannot be easily observed here. We also attempted a dimension reduction approach based on Multiple Correspondence Analysis but found that the SVD-generated 2D layout resulted in a loss of information. We hence propose a novel approach, the Interactive Configuration Explorer (ICE), which directly addresses the need of analysts to learn how the dependent numerical variable is affected by the parameter settings given multiple optimization objectives. No information is lost as ICE shows the complete distribution and statistics of the dependent variable in context with each categorical variable. Analysts can interactively filter the variables to optimize for certain goals such as achieving a system with maximum performance, low variance, etc. Our system was developed in tight collaboration with a group of systems performance researchers and its final effectiveness was evaluated with expert interviews, a comparative user study, and two case studies.",
            "url": "http://dx.doi.org/10.1109/VAST47406.2019.8986923",
            "id": "r_520",
            "s_ids": [
                "s_239",
                "s_1312",
                "s_1398",
                "s_1114",
                "s_252"
            ],
            "type": "rich",
            "x": -0.06286266589073797,
            "y": 0.0407446556716878
        },
        {
            "title": "Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts",
            "data": "Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of ego-networks one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.",
            "url": "http://dx.doi.org/10.1109/VAST.2018.8802415",
            "id": "r_521",
            "s_ids": [
                "s_1244",
                "s_453",
                "s_108"
            ],
            "type": "rich",
            "x": -0.05050280106056556,
            "y": -0.400662664808566
        },
        {
            "title": "CRICTO: Supporting Sensemaking through Crowdsourced Information Schematization",
            "data": "We present CRICTO, a new crowdsourcing visual analytics environment for making sense of and analyzing text data, whereby multiple crowdworkers are able to parallelize the simple information schematization tasks of relating and connecting entities across documents. The diverse links from these schematization tasks are then automatically combined and the system visualizes them based on the semantic types of the linkages. CRICTO also includes several tools that allow analysts to interactively explore and refine crowdworkers' results to better support their own sensemaking processes. We evaluated CRICTO's techniques and analysis workflow with deployments of CRICTO using Amazon Mechanical Turk and a user study that assess the effect of crowdsourced schematization in sensemaking tasks. The results of our evaluation show that CRICTO's crowdsourcing approaches and workflow help analysts explore diverse aspects of datasets, and uncover more accurate hidden stories embedded in the text datasets.",
            "url": "http://dx.doi.org/10.1109/VAST.2017.8585484",
            "id": "r_522",
            "s_ids": [
                "s_1375",
                "s_987",
                "s_801",
                "s_1441"
            ],
            "type": "rich",
            "x": 0.47059964863433906,
            "y": -0.49661513162730864
        },
        {
            "title": "Scalable Topological Data Analysis and Visualization for Evaluating Data-Driven Models in Scientific Applications",
            "data": "With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous datasets that require techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, namely topology aware datacubes, we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934594",
            "id": "r_523",
            "s_ids": [
                "s_340",
                "s_982",
                "s_345",
                "s_460",
                "s_212",
                "s_297",
                "s_1001",
                "s_438",
                "s_362",
                "s_959",
                "s_902",
                "s_444",
                "s_606",
                "s_621",
                "s_1351",
                "s_736"
            ],
            "type": "rich",
            "x": -0.30855806859282514,
            "y": -0.010902545349744342
        },
        {
            "title": "Decoding a Complex Visualization in a Science Museum \u2013 An Empirical Study",
            "data": "This study describes a detailed analysis of museum visitors' decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934401",
            "id": "r_524",
            "s_ids": [
                "s_608",
                "s_687",
                "s_1183"
            ],
            "type": "rich",
            "x": 0.6538359305519041,
            "y": -0.13016668549087446
        },
        {
            "title": "Shape-preserving Star Coordinates",
            "data": "Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape-preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865118",
            "id": "r_525",
            "s_ids": [
                "s_1210",
                "s_890"
            ],
            "type": "rich",
            "x": -0.4369111106784675,
            "y": 0.2544160284424249
        },
        {
            "title": "Embedded Merge & Split: Visual Adjustment of Data Grouping",
            "data": "Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865075",
            "id": "r_526",
            "s_ids": [
                "s_1313",
                "s_1275",
                "s_1469",
                "s_268"
            ],
            "type": "rich",
            "x": 0.1135118944321465,
            "y": 0.04410715035532502
        },
        {
            "title": "Beyond Tasks: An Activity Typology for Visual Analytics",
            "data": "As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2745180",
            "id": "r_527",
            "s_ids": [
                "s_1246",
                "s_845",
                "s_1315",
                "s_1056"
            ],
            "type": "rich",
            "x": 0.7017708424842262,
            "y": -0.3200675004839657
        },
        {
            "title": "Decision Graph Embedding for High-Resolution Manometry Diagnosis",
            "data": "High-resolution manometry is an imaging modality which enables the categorization of esophageal motility disorders. Spatio-temporal pressure data along the esophagus is acquired using a tubular device and multiple test swallows are performed by the patient. Current approaches visualize these swallows as individual instances, despite the fact that aggregated metrics are relevant in the diagnostic process. Based on the current Chicago Classification, which serves as the gold standard in this area, we introduce a visualization supporting an efficient and correct diagnosis. To reach this goal, we propose a novel decision graph representing the Chicago Classification with workflow optimization in mind. Based on this graph, we are further able to prioritize the different metrics used during diagnosis and can exploit this prioritization in the actual data visualization. Thus, different disorders and their related parameters are directly represented and intuitively influence the appearance of our visualization. Within this paper, we introduce our novel visualization, justify the design decisions, and provide the results of a user study we performed with medical students as well as a domain expert. On top of the presented visualization, we further discuss how to derive a visual signature for individual patients that allows us for the first time to perform an intuitive comparison between subjects, in the form of small multiples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2744299",
            "id": "r_528",
            "s_ids": [
                "s_51",
                "s_3",
                "s_515",
                "s_209"
            ],
            "type": "rich",
            "x": -0.34259616268093734,
            "y": -0.1324934627537705
        },
        {
            "title": "Adaptive Multilinear Tensor Product Wavelets",
            "data": "Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467412",
            "id": "r_529",
            "s_ids": [
                "s_1155",
                "s_1010"
            ],
            "type": "rich",
            "x": -0.7914181630901707,
            "y": 0.4540192612842055
        },
        {
            "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization",
            "data": "LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934658",
            "id": "r_530",
            "s_ids": [
                "s_678",
                "s_57",
                "s_1435",
                "s_145",
                "s_181"
            ],
            "type": "rich",
            "x": 0.20172637452337522,
            "y": 0.26758248242791244
        },
        {
            "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics",
            "data": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934264",
            "id": "r_531",
            "s_ids": [
                "s_379",
                "s_1327",
                "s_921",
                "s_71"
            ],
            "type": "rich",
            "x": 0.19386939274495985,
            "y": -0.3188130290542504
        },
        {
            "title": "Visualization of Bubble Formation in Porous Media",
            "data": "We present a visualization approach for the analysis of CO<sub>2</sub>bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO<sub>2</sub>bubbles and their surrounding porous structure from X-ray computed tomography data (XCT) measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact with the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO<sub>2</sub>.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864506",
            "id": "r_532",
            "s_ids": [
                "s_522",
                "s_251",
                "s_481",
                "s_1406",
                "s_299",
                "s_214"
            ],
            "type": "rich",
            "x": -0.8231904358459099,
            "y": 0.29120354018189387
        },
        {
            "title": "Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data",
            "data": "Molecular dynamics (MD) simulations are crucial to investigating important processes in physics and thermodynamics. The simulated atoms are usually visualized as hard spheres with Phong shading, where individual particles and their local density can be perceived well in close-up views. However, for large-scale simulations with 10 million particles or more, the visualization of large fields-of-view usually suffers from strong aliasing artifacts, because the mismatch between data size and output resolution leads to severe under-sampling of the geometry. Excessive super-sampling can alleviate this problem, but is prohibitively expensive. This paper presents a novel visualization method for large-scale particle data that addresses aliasing while enabling interactive high-quality rendering. We introduce the novel concept of screen-space normal distribution functions (S-NDFs) for particle data. S-NDFs represent the distribution of surface normals that map to a given pixel in screen space, which enables high-quality re-lighting without re-rendering particles. In order to facilitate interactive zooming, we cache S-NDFs in a screen-space mipmap (S-MIP). Together, these two concepts enable interactive, scale-consistent re-lighting and shading changes, as well as zooming, without having to re-sample the particle data. We show how our method facilitates the interactive exploration of real-world large-scale MD simulation data in different scenarios.",
            "url": "http://dx.doi.org/10.1109/TVCG.2017.2743979",
            "id": "r_533",
            "s_ids": [
                "s_23",
                "s_819",
                "s_876",
                "s_733",
                "s_336"
            ],
            "type": "rich",
            "x": -0.4262632412303886,
            "y": 0.48668942542947546
        },
        {
            "title": "AnaFe: Visual Analytics of Image-derived Temporal Features Focusing on the Spleen",
            "data": "We present a novel visualization framework, AnaFe, targeted at observing changes in the spleen over time through multiple image-derived features. Accurate monitoring of progressive changes is crucial for diseases that result in enlargement of the organ. Our system is comprised of multiple linked views combining visualization of temporal 3D organ data, related measurements, and features. Thus it enables the observation of progression and allows for simultaneous comparison within and between the subjects. AnaFe offers insights into the overall distribution of robustly extracted and reproducible quantitative imaging features and their changes within the population, and also enables detailed analysis of individual cases. It performs similarity comparison of temporal series of one subject to all other series in both sick and healthy groups. We demonstrate our system through two use case scenarios on a population of 189 spleen datasets from 68 subjects with various conditions observed over time.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598463",
            "id": "r_534",
            "s_ids": [
                "s_157",
                "s_553",
                "s_732",
                "s_616"
            ],
            "type": "rich",
            "x": -0.40770498266999566,
            "y": -0.18668333900380607
        },
        {
            "title": "OpenSpace: Public dissemination of space mission profiles",
            "data": "This work presents a visualization system and its application to space missions. The system allows the public to disseminate the scientific findings of space craft and gain a greater understanding thereof. Instruments' field-of-views and their measurements are embedded in an accurate 3 dimensional rendering of the solar system to provide context to past measurements or the planning of future events. We tested our system with NASA's New Horizons at the Pluto Pallooza event in New York and will expose it to the greater public on the upcoming July 14th Pluto flyby.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429503",
            "id": "r_535",
            "s_ids": [
                "s_1233",
                "s_868",
                "s_1411",
                "s_1414",
                "s_260"
            ],
            "type": "rich",
            "x": -0.014709671761037789,
            "y": 0.3579098538751796
        },
        {
            "title": "MetricsVis: A Visual Analytics System for Evaluating Employee Performance in Public Safety Agencies",
            "data": "Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934603",
            "id": "r_536",
            "s_ids": [
                "s_999",
                "s_1327",
                "s_417",
                "s_771",
                "s_946",
                "s_921"
            ],
            "type": "rich",
            "x": 0.502144896333819,
            "y": -0.05287503146698603
        },
        {
            "title": "Analysis of the Near-Wall Flow in a Turbine Cascade by Splat Visualization",
            "data": "Turbines are essential components of jet planes and power plants. Therefore, their efficiency and service life are of central engineering interest. In the case of jet planes or thermal power plants, the heating of the turbines due to the hot gas flow is critical. Besides effective cooling, it is a major goal of engineers to minimize heat transfer between gas flow and turbine by design. Since it is known that splat events have a substantial impact on the heat transfer between flow and immersed surfaces, we adapt a splat detection and visualization method to a turbine cascade simulation in this case study. Because splat events are small phenomena, we use a direct numerical simulation resolving the turbulence in the flow as the base of our analysis. The outcome shows promising insights into splat formation and its relation to vortex structures. This may lead to better turbine design in the future.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934367",
            "id": "r_537",
            "s_ids": [
                "s_73",
                "s_841",
                "s_928",
                "s_1027",
                "s_855",
                "s_1105"
            ],
            "type": "rich",
            "x": -0.5159897227803547,
            "y": 0.5241483436522427
        },
        {
            "title": "Multiscale Visual Drilldown for the Analysis of Large Ensembles of Multi-Body Protein Complexes",
            "data": "When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data \u2013 from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels. At each level, we offer a set of selection and filtering operations that enable the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934333",
            "id": "r_538",
            "s_ids": [
                "s_292",
                "s_1007",
                "s_1227",
                "s_367",
                "s_874"
            ],
            "type": "rich",
            "x": -0.36147049002950565,
            "y": 0.08987289405069251
        },
        {
            "title": "Visual data quality analysis for taxi GPS data",
            "data": "We present a novel visual analysis method to systematically discover data quality problems in raw taxi GPS data. It combines semi-supervised active learning and interactive visual exploration. It helps analysts interactively discover unknown data quality problems, and automatically extract known problems. We report analysis results on Beijing taxi GPS data.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347689",
            "id": "r_539",
            "s_ids": [
                "s_1122",
                "s_1214",
                "s_88",
                "s_671",
                "s_316",
                "s_1206",
                "s_1479",
                "s_272",
                "s_1022"
            ],
            "type": "rich",
            "x": 0.12597623440531283,
            "y": 0.34151632756964606
        },
        {
            "title": "Sequencing of categorical time series",
            "data": "Exploring and comparing categorical time series and finding temporal patterns are complex tasks in the field of time series data mining. Although different analysis approaches exist, these tasks remain challenging, especially when numerous time series are considered at once. We propose a visual analysis approach that supports exploring such data by ordering time series in meaningful ways. We provide interaction techniques to steer the automated arrangement and to allow users to investigate patterns in detail.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347684",
            "id": "r_540",
            "s_ids": [
                "s_816",
                "s_528",
                "s_1145",
                "s_737"
            ],
            "type": "rich",
            "x": 0.15805289520489813,
            "y": 0.5053892573118898
        },
        {
            "title": "Interactive semi-automatic categorization for spinel group minerals",
            "data": "Spinel group minerals are excellent indicators of geological environments (tectonic settings). In 2001, Barnes and Roeder defined a set of contours corresponding to compositional fields for spinel group minerals. Geologists typically use this contours to estimate the tectonic environment where a particular spinel composition could have been formed. This task is prone to errors and requires tedious manual comparison of overlapping diagrams. We introduce a semi-automatic, interactive detection of tectonic settings for an arbitrary dataset based on the Barnes and Roeder contours. The new approach integrates the mentioned contours and includes a novel interaction called contour brush. The new methodology is integrated in the Spinel Explorer system and it improves the scientist's workflow significantly.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347676",
            "id": "r_541",
            "s_ids": [
                "s_615",
                "s_277",
                "s_715",
                "s_189",
                "s_960",
                "s_519",
                "s_818"
            ],
            "type": "rich",
            "x": -0.5418548068131281,
            "y": 0.17055287543324935
        },
        {
            "title": "A software developer's guide to informal evaluation of Visual Analytics environments using VAST Challenge information",
            "data": "The VAST Challenge has been a popular venue for academic and industry participants for over ten years. Many participants comment that the majority of their time in preparing VAST Challenge entries is discovering elements in their software environments that need to be redesigned in order to solve the given task. Fortunately, there is no need to wait until the VAST Challenge is announced to test out software systems. The Visual Analytics Benchmark Repository contains all past VAST Challenge tasks, data, solutions and submissions. In this poster we describe how developers can perform informal evaluations of various aspects of their visual analytics environments using VAST Challenge information.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347674",
            "id": "r_542",
            "s_ids": [
                "s_670",
                "s_1187",
                "s_234"
            ],
            "type": "rich",
            "x": 0.5475778873570587,
            "y": -0.3116383242784225
        },
        {
            "title": "FPSSeer: Visual analysis of game frame rate data",
            "data": "The rate at which frames are rendered in a computer game directly influences both game playability and enjoyability. Players frequently have to deal with the trade-off between high frame rates and good resolution. Analyzing patterns in frame rate data and their correlation with the overall game performance is important in designing games (e.g., graphic card/display setting suggestion and game performance measurement). However, this task is challenging because game frame rates vary both temporally and spatially. In addition, players may adjust their display settings based on their gaming experience and hardware conditions, which further contributes to the unpredictability of frame rates. In this paper, we present a comprehensive visual analytics system FPSSeer, to help game designers gain insight into frame rate data. Our system consists of four major views: 1) a frame rate view to show the overall distribution in a geographic scale, 2) a grid view to show the frame rate distribution and grid element clusters based on their similarity, 3) a FootRiver view to reveal the temporal patterns in game condition changes and potential spatiotemporal correlations, and 4) a comparison view to evaluate game performance discrepancy under different game tests. The real-world case studies demonstrate the effectiveness of our system. The system has been applied to an online commercial game to monitor its performance and to provide feedbacks to designers and developers.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347633",
            "id": "r_543",
            "s_ids": [
                "s_477",
                "s_1260",
                "s_137"
            ],
            "type": "rich",
            "x": 0.07245735277418008,
            "y": 0.29010398377986163
        },
        {
            "title": "Multi-Scale Procedural Animations of Microtubule Dynamics Based on Measured Data",
            "data": "Biologists often use computer graphics to visualize structures, which due to physical limitations are not possible to image with a microscope. One example for such structures are microtubules, which are present in every eukaryotic cell. They are part of the cytoskeleton maintaining the shape of the cell and playing a key role in the cell division. In this paper, we propose a scientifically-accurate multi-scale procedural model of microtubule dynamics as a novel application scenario for procedural animation, which can generate visualizations of their overall shape, molecular structure, as well as animations of the dynamic behaviour of their growth and disassembly. The model is spanning from tens of micrometers down to atomic resolution. All the aspects of the model are driven by scientific data. The advantage over a traditional, manual animation approach is that when the underlying data change, for instance due to new evidence, the model can be recreated immediately. The procedural animation concept is presented in its generic form, with several novel extensions, facilitating an easy translation to other domains with emergent multi-scale behavior.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934612",
            "id": "r_544",
            "s_ids": [
                "s_449",
                "s_443",
                "s_519",
                "s_1421"
            ],
            "type": "rich",
            "x": -0.5902802289847768,
            "y": 0.2195321269311151
        },
        {
            "title": "The Effect of Data Transformations on Scalar Field Topological Analysis of High-Order FEM Solutions",
            "data": "High-order finite element methods (HO-FEM) are gaining popularity in the simulation community due to their success in solving complex flow dynamics. There is an increasing need to analyze the data produced as output by these simulations. Simultaneously, topological analysis tools are emerging as powerful methods for investigating simulation data. However, most of the current approaches to topological analysis have had limited application to HO-FEM simulation data for two reasons. First, the current topological tools are designed for linear data (polynomial degree one), but the polynomial degree of the data output by these simulations is typically higher (routinely up to polynomial degree six). Second, the simulation data and derived quantities of the simulation data have discontinuities at element boundaries, and these discontinuities do not match the input requirements for the topological tools. One solution to both issues is to transform the high-order data to achieve low-order, continuous inputs for topological analysis. Nevertheless, there has been little work evaluating the possible transformation choices and their downstream effect on the topological analysis. We perform an empirical study to evaluate two commonly used data transformation methodologies along with the recently introduced L-SIAC filter for processing high-order simulation data. Our results show diverse behaviors are possible. We offer some guidance about how best to consider a pipeline of topological analysis of HO-FEM simulations with the currently available implementations of topological analysis.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934338",
            "id": "r_545",
            "s_ids": [
                "s_536",
                "s_1366",
                "s_502"
            ],
            "type": "rich",
            "x": -0.5526841126552053,
            "y": 0.30829745201966285
        },
        {
            "title": "Accelerated Monte Carlo Rendering of Finite-Time Lyapunov Exponents",
            "data": "Time-dependent fluid flows often contain numerous hyperbolic Lagrangian coherent structures, which act as transport barriers that guide the advection. The finite-time Lyapunov exponent is a commonly-used approximation to locate these repelling or attracting structures. Especially on large numerical simulations, the FTLE ridges can become arbitrarily sharp and very complex. Thus, the discrete sampling onto a grid for a subsequent direct volume rendering is likely to miss sharp ridges in the visualization. For this reason, an unbiased Monte Carlo-based rendering approach was recently proposed that treats the FTLE field as participating medium with single scattering. This method constructs a ground truth rendering without discretization, but it is prohibitively slow with render times in the order of days or weeks for a single image. In this paper, we accelerate the rendering process significantly, which allows us to compute video sequence of high-resolution FTLE animations in a much more reasonable time frame. For this, we follow two orthogonal approaches to improve on the rendering process: the volumetric light path integration in gradient domain and an acceleration of the transmittance estimation. We analyze the convergence and performance of the proposed method and demonstrate the approach by rendering complex FTLE fields in several 3D vector fields.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934313",
            "id": "r_546",
            "s_ids": [
                "s_96",
                "s_607",
                "s_1225"
            ],
            "type": "rich",
            "x": -0.6220401323049026,
            "y": 0.5617681185529876
        },
        {
            "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning",
            "data": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934289",
            "id": "r_547",
            "s_ids": [
                "s_785",
                "s_769",
                "s_1156",
                "s_61",
                "s_878"
            ],
            "type": "rich",
            "x": 0.12085526105383923,
            "y": 0.3155875598487142
        },
        {
            "title": "Image-Based Aspect Ratio Selection",
            "data": "Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer's co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2865266",
            "id": "r_548",
            "s_ids": [
                "s_1059",
                "s_434",
                "s_1339",
                "s_1242",
                "s_1220",
                "s_138"
            ],
            "type": "rich",
            "x": -0.5115257068989628,
            "y": 0.19984908696243883
        },
        {
            "title": "DT-MRI Streamsurfaces Revisited",
            "data": "DT-MRI streamsurfaces, defined as surfaces that are everywhere tangential to the major and medium eigenvector fields, have been proposed as a tool for visualizing regions of predominantly planar behavior in diffusion tensor MRI. Even though it has long been known that their construction assumes that the involved eigenvector fields satisfy an integrability condition, it has never been tested systematically whether this condition is met in real-world data. We introduce a suitable and efficiently computable test to the visualization literature, demonstrate that it can be used to distinguish integrable from nonintegrable configurations in simulations, and apply it to whole-brain datasets of 15 healthy subjects. We conclude that streamsurface integrability is approximately satisfied in a substantial part of the brain, but not everywhere, including some regions of planarity. As a consequence, algorithms for streamsurface extraction should explicitly test local integrability. Finally, we propose a novel patch-based approach to streamsurface visualization that reduces visual artifacts, and is shown to more fully sample the extent of streamsurfaces.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864845",
            "id": "r_549",
            "s_ids": [
                "s_201",
                "s_605"
            ],
            "type": "rich",
            "x": -0.6751944599310804,
            "y": 0.35305406717935783
        },
        {
            "title": "Firefly: Virtual Illumination Drones for Interactive Visualization",
            "data": "Light specification in three dimensional scenes is a complex problem and several approaches have been presented that aim to automate this process. However, there are many scenarios where a static light setup is insufficient, as the scene content and camera position may change. Simultaneous manual control over the camera and light position imposes a high cognitive load on the user. To address this challenge, we introduce a novel approach for automatic scene illumination with Fireflies. Fireflies are intelligent virtual light drones that illuminate the scene by traveling on a closed path. The Firefly path automatically adapts to changes in the scene based on an outcome-oriented energy function. To achieve interactive performance, we employ a parallel rendering pipeline for the light path evaluations. We provide a catalog of energy functions for various application scenarios and discuss the applicability of our method on several examples.",
            "url": "http://dx.doi.org/10.1109/TVCG.2018.2864656",
            "id": "r_550",
            "s_ids": [
                "s_1281",
                "s_862",
                "s_900"
            ],
            "type": "rich",
            "x": -0.20101479242423884,
            "y": 0.46498889810324234
        },
        {
            "title": "Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces",
            "data": "Previous perceptual research and human factors studies have identified several effective methods for texturing 3D surfaces to ensure that their curvature is accurately perceived by viewers. However, most of these studies examined the application of these techniques to static surfaces. This paper explores the effectiveness of applying these techniques to dynamically changing surfaces. When these surfaces change shape, common texturing methods, such as grids and contours, induce a range of different motion cues, which can draw attention and provide information about the size, shape, and rate of change. A human factors study was conducted to evaluate the relative effectiveness of these methods when applied to dynamically changing pseudo-terrain surfaces. The results indicate that, while no technique is most effective for all cases, contour lines generally perform best, and that the pseudo-contour lines induced by banded color scales convey the same benefits.",
            "url": "http://dx.doi.org/10.1109/TVCG.2015.2467962",
            "id": "r_551",
            "s_ids": [
                "s_648",
                "s_1078"
            ],
            "type": "rich",
            "x": -0.5287818257897764,
            "y": 0.15120891464388964
        },
        {
            "title": "3D superquadric glyphs for visualizing myocardial motion",
            "data": "Various cardiac diseases can be diagnosed by the analysis of myocardial motion. Relevant biomarkers are radial, longitudinal, and rotational velocities of the cardiac muscle computed locally from MR images. We designed a visual encoding that maps these three attributes to glyph shapes according to a barycentric space formed by 3D superquadric glyphs. The glyphs show aggregated myocardial motion information following the AHA model and are displayed in a respective 3D layout.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429504",
            "id": "r_552",
            "s_ids": [
                "s_996",
                "s_550",
                "s_821",
                "s_282",
                "s_890"
            ],
            "type": "rich",
            "x": -0.7230831350211092,
            "y": -0.043821593747670175
        },
        {
            "title": "Using Maximum Topology Matching to Explore Differences in Species Distribution Models",
            "data": "Species distribution models (SDM) are used to help understand what drives the distribution of various plant and animal species. These models are typically high dimensional scalar functions, where the dimensions of the domain correspond to predictor variables of the model algorithm. Understanding and exploring the differences between models help ecologists understand areas where their data or understanding of the system is incomplete and will help guide further investigation in these regions. These differences can also indicate an important source of model to model uncertainty. However, it is cumbersome and often impractical to perform this analysis using existing tools, which allows for manual exploration of the models usually as 1-dimensional curves. In this paper, we propose a topology-based framework to help ecologists explore the differences in various SDMs directly in the high dimensional domain. In order to accomplish this, we introduce the concept of maximum topology matching that computes a locality-aware correspondence between similar extrema of two scalar functions. The matching is then used to compute the similarity between two functions. We also design a visualization interface that allows ecologists to explore SDMs using their topological features and to study the differences between pairs of models found using maximum topological matching. We demonstrate the utility of the proposed framework through several use cases using different data sets and report the feedback obtained from ecologists.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429486",
            "id": "r_553",
            "s_ids": [
                "s_1024",
                "s_29",
                "s_1091",
                "s_980",
                "s_380"
            ],
            "type": "rich",
            "x": -0.3981557243614022,
            "y": 0.022010949630718566
        },
        {
            "title": "Tell me what do you see: Detecting perceptually-separable visual patterns via clustering of image-space features in visualizations",
            "data": "Visualization helps users infer structures and relationships in the data by encoding information as visual features that can be processed by the human visual-perceptual system. However, users would typically need to expend significant effort to scan and analyze a large number of views before they can begin to recognize relationships in a visualization. We propose a technique to partially automate the process of analyzing visualizations. By deriving and analyzing image-space features from visualizations, we can detect perceptually-separable patterns in the information space. We summarize these patterns with a tree-based meta-visualization and present it to the user to aid exploration. We illustrate this technique with an example scenario involving the analysis of census data.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347683",
            "id": "r_554",
            "s_ids": [
                "s_985",
                "s_1446",
                "s_1456",
                "s_656"
            ],
            "type": "rich",
            "x": 0.18566256929794414,
            "y": 0.0019195499119069805
        },
        {
            "title": "Comparing Cross-Sections and 3D Renderings for Surface Matching Tasks Using Physical Ground Truths",
            "data": "Within the visualization community there are some well-known techniques for visualizing 3D spatial data and some general assumptions about how perception affects the performance of these techniques in practice. However, there is a lack of empirical research backing up the possible performance differences among the basic techniques for general tasks. One such assumption is that 3D renderings are better for obtaining an overview, whereas cross sectional visualizations such as the commonly used Multi-Planar Reformation (MPR) are better for supporting detailed analysis tasks. In the present study we investigated this common assumption by examining the difference in performance between MPR and 3D rendering for correctly identifying a known surface. We also examined whether prior experience working with image data affects the participant's performance, and whether there was any difference between interactive or static versions of the visualizations. Answering this question is important because it can be used as part of a scientific and empirical basis for determining when to use which of the two techniques. An advantage of the present study compared to other studies is that several factors were taken into account to compare the two techniques. The problem was examined through an experiment with 45 participants, where physical objects were used as the known surface (ground truth). Our findings showed that: 1. The 3D renderings largely outperformed the cross sections; 2. Interactive visualizations were partially more effective than static visualizations; and 3. The high experience group did not generally outperform the low experience group.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598602",
            "id": "r_555",
            "s_ids": [
                "s_702",
                "s_900"
            ],
            "type": "rich",
            "x": 0.024338217611315376,
            "y": -0.10165115279540257
        },
        {
            "title": "Hairy Slices: Evaluating the Perceptual Effectiveness of Cutting Plane Glyphs for 3D Vector Fields",
            "data": "Three-dimensional vector fields are common datasets throughout the sciences. Visualizing these fields is inherently difficult due to issues such as visual clutter and self-occlusion. Cutting planes are often used to overcome these issues by presenting more manageable slices of data. The existing literature provides many techniques for visualizing the flow through these cutting planes; however, there is a lack of empirical studies focused on the underlying perceptual cues that make popular techniques successful. This paper presents a quantitative human factors study that evaluates static monoscopic depth and orientation cues in the context of cutting plane glyph designs for exploring and analyzing 3D flow fields. The goal of the study was to ascertain the relative effectiveness of various techniques for portraying the direction of flow through a cutting plane at a given point, and to identify the visual cues and combinations of cues involved, and how they contribute to accurate performance. It was found that increasing the dimensionality of line-based glyphs into tubular structures enhances their ability to convey orientation through shading, and that increasing their diameter intensifies this effect. These tube-based glyphs were also less sensitive to visual clutter issues at higher densities. Adding shadows to lines was also found to increase perception of flow direction. Implications of the experimental results are discussed and extrapolated into a number of guidelines for designing more perceptually effective glyphs for 3D vector field visualizations.",
            "url": "http://dx.doi.org/10.1109/TVCG.2016.2598448",
            "id": "r_556",
            "s_ids": [
                "s_1078",
                "s_648",
                "s_1111"
            ],
            "type": "rich",
            "x": -0.45192455366555606,
            "y": 0.18816745138559055
        },
        {
            "title": "An evaluation of three methods for visualizing uncertainty in architecture and archaeology",
            "data": "This project explores the representation of uncertainty in visualizations for archaeological research and provides insights obtained from user feedback. Our 3D models brought together information from standing architecture and excavated remains, surveyed plans, ground penetrating radar (GPR) data from the Carthusian monastery of Bourgfontaine in northern France. We also included information from comparative Carthusian sites and a bird's eye representation of the site in an early modern painting. Each source was assigned a certainty value which was then mapped to a color or texture for the model. Certainty values between one and zero were assigned by one subject matter expert and should be considered qualitative. Students and faculty from the fields of architectural history and archaeology at two institutions interacted with the models and answered a short survey with four questions about each. We discovered equal preference for color and transparency and a strong dislike for the texture model. Discoveries during model building also led to changes of the excavation plans for summer 2015.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429507",
            "id": "r_557",
            "s_ids": [
                "s_1146",
                "s_1269",
                "s_1148"
            ],
            "type": "rich",
            "x": -0.10818927436620165,
            "y": -0.06572516684491081
        },
        {
            "title": "StreamVisND: Visualizing relationships in streaming multivariate data",
            "data": "In streaming acquisitions the data changes over time. ThemeRiver and line charts are common methods to display data over time. However, these methods can only show the values of the variables (or attributes) but not the relationships among them over time. We propose a framework we call StreamVis&lt;sup&gt;ND&lt;/sup&gt; that can display these types of streaming data relations. It first slices the data stream into different time slices, then it visualizes each slice with a sequence of multivariate 2D data layouts, and finally it flattens this series of displays into a parallel coordinate type display. Our framework is fully interactive and lends itself well to real-time displays.",
            "url": "http://dx.doi.org/10.1109/VAST.2015.7347673",
            "id": "r_558",
            "s_ids": [
                "s_1408",
                "s_897",
                "s_1049",
                "s_484",
                "s_252"
            ],
            "type": "rich",
            "x": 0.1782123610009526,
            "y": 0.32260753864936864
        },
        {
            "title": "OntoPlot: A Novel Visualisation for Non-hierarchical Associations in Large Ontologies",
            "data": "Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology's large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Prot\u00e9g\u00e9. The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts.",
            "url": "http://dx.doi.org/10.1109/TVCG.2019.2934557",
            "id": "r_559",
            "s_ids": [
                "s_1319",
                "s_1326",
                "s_1158",
                "s_310",
                "s_1012"
            ],
            "type": "rich",
            "x": 0.347711439407224,
            "y": -0.1514092196093935
        },
        {
            "title": "High performance flow field visualization with high-order access dependencies",
            "data": "We present a novel model based on high-order access dependencies for high performance pathline computation in flow field. The high-order access dependencies are defined as transition probabilities from one data block to other blocks based on a few historical data accesses. Compared with existing methods which employed first-order access dependencies, our approach takes the advantages of high order access dependencies with higher accuracy and reliability in data access prediction. In our work, high-order access dependencies are calculated by tracing densely-seeded pathlines. The efficiency of our proposed approach is demonstrated through a parallel particle tracing framework with high-order data prefetching. Results show that our method can achieve higher data locality than the first-order access dependencies based method, thereby reducing the I/O requests and improving the efficiency of pathline computation in various applications.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429515",
            "id": "r_560",
            "s_ids": [
                "s_822",
                "s_865",
                "s_1214"
            ],
            "type": "rich",
            "x": -0.5424375206620278,
            "y": 0.4042784657673212
        },
        {
            "title": "Visualizing 3D flow through cutting planes",
            "data": "Studies have found conflicting results regarding the effectiveness of tube-like structures for representing 3D flow data. This paper presents the findings of a small-scale pilot study contrasting static monoscopic depth cues to ascertain their importance in perceiving the orientation of a three-dimensional glyph with respect to a cutting plane. A simple striped texture and shading were found to reduce judgement errors when used with a 3D tube glyph as compared to plain or shaded line glyphs. A discussion of considerations for a full-scale study and possible future work follows.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429513",
            "id": "r_561",
            "s_ids": [
                "s_1111",
                "s_1078"
            ],
            "type": "rich",
            "x": -0.4020168787978953,
            "y": 0.006538353801327157
        },
        {
            "title": "PathlinesExplorer ??? Image-based exploration of large-scale pathline fields",
            "data": "PathlinesExplorer is a novel image-based tool, which has been designed to visualize large scale pathline fields on a single computer [7]. PathlinesExplorer integrates explorable images (EI) technique [4] with order-independent transparency (OIT) method [2]. What makes this method different is that it allows users to handle large data on a single workstation. Although it is a view-dependent method, PathlinesExplorer combines both exploration and modification of visual aspects without re-accessing the original huge data. Our approach is based on constructing a per-pixel linked list data structure in which each pixel contains a list of pathline segments. With this view-dependent method, it is possible to filter, color-code, and explore large-scale flow data in real-time. In addition, optimization techniques such as early-ray termination and deferred shading are applied, which further improves the performance and scalability of our approach.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429512",
            "id": "r_562",
            "s_ids": [
                "s_414",
                "s_336",
                "s_24"
            ],
            "type": "rich",
            "x": -0.3755763189265813,
            "y": 0.3518772963309961
        },
        {
            "title": "A bottom-up scheme for user-defined feature exploration in vector field ensembles",
            "data": "Most of the existing approaches to visualize vector field ensembles are achieved by visualizing the uncertainty of individual variables from different simulation runs. However, the comparison of the derived feature or user-defined feature, such as the vortex in ensemble flow is also of vital significance since they often make more sense according to the domain knowledge. In this work, we present a framework to extract user-defined feature from different simulation runs. Specially, we use a bottom-up searching scheme to help to extract vortex with a user-defined shape, and further compute the geometry information including the size, and the geo-spatial location of the extracted vortex. Finally we design some linked views to compare the feature between different runs.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429510",
            "id": "r_563",
            "s_ids": [
                "s_792",
                "s_865",
                "s_1214"
            ],
            "type": "rich",
            "x": -0.41276813296110154,
            "y": 0.4622516338274883
        },
        {
            "title": "Real-time interactive time correction on the GPU",
            "data": "The study of physical phenomena and their dynamic evolution is supported by the analysis and visualization of time-enabled data. In many applications, available data are sparsely distributed in the space-time domain, which leads to incomprehensible visualizations. We present an interactive approach for the dynamic tracking and visualization of measured data particles through advection in a simulated flow. We introduce a fully GPU-based technique for efficient spatio-temporal interpolation, using a kd-tree forest for acceleration. As the user interacts with the system using a time slider, particle positions are reconstructed for the time selected by the user. Our results show that the proposed technique achieves highly accurate parallel tracking for thousands of particles. The rendering performance is mainly affected by the size of the query set.",
            "url": "http://dx.doi.org/10.1109/SciVis.2015.7429505",
            "id": "r_564",
            "s_ids": [
                "s_323",
                "s_953",
                "s_142",
                "s_765",
                "s_753"
            ],
            "type": "rich",
            "x": -0.20115832379010723,
            "y": 0.5141730150025183
        },
        {
            "id": "s_0",
            "name": "Selim Balcisoy",
            "type": "sparse",
            "x": -0.05959061143975931,
            "y": 0.08617626808206279
        },
        {
            "id": "s_1",
            "name": "Jacob Ritchie",
            "type": "sparse",
            "x": 0.1667913332618201,
            "y": -0.19840445249478633
        },
        {
            "id": "s_2",
            "name": "\u00c0lvar Vinacua",
            "type": "sparse",
            "x": -0.28511764507049714,
            "y": 0.09061293756964654
        },
        {
            "id": "s_3",
            "name": "Alexander Hann",
            "type": "sparse",
            "x": -0.4542750888941352,
            "y": 0.18171019346246733
        },
        {
            "id": "s_4",
            "name": "Miaoxin Hu",
            "type": "sparse",
            "x": 0.17857237137275608,
            "y": 0.2525418440489317
        },
        {
            "id": "s_5",
            "name": "Phong H. Nguyen",
            "type": "sparse",
            "x": 0.02712950173306426,
            "y": 0.2788497398696647
        },
        {
            "id": "s_6",
            "name": "Zhibin Wang",
            "type": "sparse",
            "x": 0.098700411212704,
            "y": 0.2927527988621309
        },
        {
            "id": "s_7",
            "name": "John J. Socha",
            "type": "sparse",
            "x": -0.22367951681122622,
            "y": 0.20736757819107784
        },
        {
            "id": "s_8",
            "name": "Emily Wall",
            "type": "sparse",
            "x": 0.04662444674104909,
            "y": -0.26375820258152
        },
        {
            "id": "s_9",
            "name": "Natalia V. Andrienko",
            "type": "sparse",
            "x": 0.016185211466372477,
            "y": 0.32233718198969125
        },
        {
            "id": "s_10",
            "name": "Donald H. House",
            "type": "sparse",
            "x": -0.4498362988959796,
            "y": 0.40847034095708945
        },
        {
            "id": "s_11",
            "name": "Annette Haworth",
            "type": "sparse",
            "x": -0.37973822441208926,
            "y": 0.048975657097658075
        },
        {
            "id": "s_12",
            "name": "Matthew Brehmer",
            "type": "sparse",
            "x": 0.015916878755897146,
            "y": -0.17616344596016376
        },
        {
            "id": "s_13",
            "name": "Leanna House",
            "type": "sparse",
            "x": 0.44440467188556587,
            "y": 0.2684968617629081
        },
        {
            "id": "s_14",
            "name": "Julie Lein",
            "type": "sparse",
            "x": -0.4017527224371161,
            "y": 0.25802203422960235
        },
        {
            "id": "s_15",
            "name": "Shahid Latif",
            "type": "sparse",
            "x": 0.13988793614049777,
            "y": -0.24610400924274875
        },
        {
            "id": "s_16",
            "name": "St\u00e9phane Puechmorel",
            "type": "sparse",
            "x": -0.18356035388596567,
            "y": 0.33435628331731826
        },
        {
            "id": "s_17",
            "name": "Ravish Chawla",
            "type": "sparse",
            "x": 0.061512278638203854,
            "y": -0.30112990884626295
        },
        {
            "id": "s_18",
            "name": "Roger A. Leite",
            "type": "sparse",
            "x": -0.16375133036023973,
            "y": -0.5368255680546377
        },
        {
            "id": "s_19",
            "name": "Christoph Heinzl",
            "type": "sparse",
            "x": -0.1431119854783375,
            "y": -0.24776087507835287
        },
        {
            "id": "s_20",
            "name": "Janu Verma",
            "type": "sparse",
            "x": -0.019698814455041198,
            "y": -0.1383795072360775
        },
        {
            "id": "s_21",
            "name": "Joseph Marino",
            "type": "sparse",
            "x": 0.4460787542482151,
            "y": 0.8421709678599122
        },
        {
            "id": "s_22",
            "name": "Philipp Berg",
            "type": "sparse",
            "x": -0.4061346589003143,
            "y": -0.24032682814676481
        },
        {
            "id": "s_23",
            "name": "Mohamed Ibrahim",
            "type": "sparse",
            "x": -0.09061208335158609,
            "y": -0.42080457599078724
        },
        {
            "id": "s_24",
            "name": "Madhusudhanan Srinivasan",
            "type": "sparse",
            "x": -0.20574139380830425,
            "y": -0.41676597586707015
        },
        {
            "id": "s_25",
            "name": "Subhajit Das 0002",
            "type": "sparse",
            "x": 0.06151677433470035,
            "y": -0.30765116140515475
        },
        {
            "id": "s_26",
            "name": "Ke Xu",
            "type": "sparse",
            "x": 0.2792732037112986,
            "y": 0.045505276137735466
        },
        {
            "id": "s_27",
            "name": "Haekyu Park",
            "type": "sparse",
            "x": 0.12248485757124283,
            "y": -0.6442819478183737
        },
        {
            "id": "s_28",
            "name": "Arjun Srinivasan",
            "type": "sparse",
            "x": -0.02710969965724226,
            "y": -0.14538054928656402
        },
        {
            "id": "s_29",
            "name": "Harish Doraiswamy",
            "type": "sparse",
            "x": -0.16127112048057587,
            "y": 0.30926772323086116
        },
        {
            "id": "s_30",
            "name": "Karen B. Schloss",
            "type": "sparse",
            "x": 0.09144928804211848,
            "y": 0.4032174027959253
        },
        {
            "id": "s_31",
            "name": "Dandelion Man\u00e9",
            "type": "sparse",
            "x": -0.0022843730796031805,
            "y": -0.5837809226148041
        },
        {
            "id": "s_32",
            "name": "Jonas Lukasczyk",
            "type": "sparse",
            "x": 0.19421170933263246,
            "y": 0.44041670219096585
        },
        {
            "id": "s_33",
            "name": "Matthew Hoffman 0001",
            "type": "sparse",
            "x": -0.07944177676701614,
            "y": 0.042716817011775786
        },
        {
            "id": "s_34",
            "name": "Daniela Oelke",
            "type": "sparse",
            "x": 0.015071326396463126,
            "y": -0.007538477825512006
        },
        {
            "id": "s_35",
            "name": "Heike Leitte",
            "type": "sparse",
            "x": 0.22888562550032568,
            "y": 0.4469652517513101
        },
        {
            "id": "s_36",
            "name": "Yalong Yang 0001",
            "type": "sparse",
            "x": -0.3164786558109859,
            "y": 0.07304232092982524
        },
        {
            "id": "s_37",
            "name": "Dominik J\u00e4ckle",
            "type": "sparse",
            "x": -0.06643672004926346,
            "y": 0.15969464828680008
        },
        {
            "id": "s_38",
            "name": "Tim Hewson",
            "type": "sparse",
            "x": 0.46261641003601073,
            "y": 0.23136151877034475
        },
        {
            "id": "s_39",
            "name": "Alper Sarikaya 0001",
            "type": "sparse",
            "x": -0.07643274233498674,
            "y": -0.0337561458916314
        },
        {
            "id": "s_40",
            "name": "Tai-Quan Peng",
            "type": "sparse",
            "x": 0.03829602064610998,
            "y": 0.5001924160128131
        },
        {
            "id": "s_41",
            "name": "Yuxin Ma",
            "type": "sparse",
            "x": 0.09920994409415611,
            "y": 0.4333109106610138
        },
        {
            "id": "s_42",
            "name": "Tolga Bolukbasi",
            "type": "sparse",
            "x": 0.010211541120674864,
            "y": -0.6417081381212432
        },
        {
            "id": "s_43",
            "name": "Kejie Yu",
            "type": "sparse",
            "x": 0.10715954657186019,
            "y": 0.5044440905831118
        },
        {
            "id": "s_44",
            "name": "Tingting Liu",
            "type": "sparse",
            "x": -0.01964927356163495,
            "y": 0.0037228080725761514
        },
        {
            "id": "s_45",
            "name": "Eli T. Brown",
            "type": "sparse",
            "x": 0.04468852451562285,
            "y": -0.2697514973976692
        },
        {
            "id": "s_46",
            "name": "Zhanglin Cheng",
            "type": "sparse",
            "x": -0.02311941534849877,
            "y": 0.006064825685004177
        },
        {
            "id": "s_47",
            "name": "Michelle Annett",
            "type": "sparse",
            "x": 0.35312971067308996,
            "y": -0.009655001770802014
        },
        {
            "id": "s_48",
            "name": "Bernhard Jenny",
            "type": "sparse",
            "x": -0.330578143539138,
            "y": 0.04306352065513109
        },
        {
            "id": "s_49",
            "name": "Erich Gstrein",
            "type": "sparse",
            "x": -0.15970008832538235,
            "y": -0.5399053554086888
        },
        {
            "id": "s_50",
            "name": "Nicolas Roussel 0001",
            "type": "sparse",
            "x": 0.16751837423157803,
            "y": -0.20087540846869967
        },
        {
            "id": "s_51",
            "name": "Julian Kreiser",
            "type": "sparse",
            "x": -0.45601075354206777,
            "y": 0.19627998057294563
        },
        {
            "id": "s_52",
            "name": "Bastian Rieck",
            "type": "sparse",
            "x": 0.24164065668449766,
            "y": 0.4678164515555883
        },
        {
            "id": "s_53",
            "name": "Jens G. Magnus",
            "type": "sparse",
            "x": 0.00559621534441357,
            "y": -0.3717003405621607
        },
        {
            "id": "s_54",
            "name": "Michael Czabaj",
            "type": "sparse",
            "x": 0.3749989760734857,
            "y": -0.29641664605780343
        },
        {
            "id": "s_55",
            "name": "Scotland Leman",
            "type": "sparse",
            "x": 0.4505735334222822,
            "y": 0.2770087562082255
        },
        {
            "id": "s_56",
            "name": "Jane Hoffswell",
            "type": "sparse",
            "x": -0.05824468494665557,
            "y": -0.3390067040840237
        },
        {
            "id": "s_57",
            "name": "Michael Schw\u00e4rzler",
            "type": "sparse",
            "x": -0.1258882435504593,
            "y": -0.3619865918905184
        },
        {
            "id": "s_58",
            "name": "Francis Nguyen",
            "type": "sparse",
            "x": -0.19480627647733356,
            "y": -0.29813552815465666
        },
        {
            "id": "s_59",
            "name": "Lonni Besan\u00e7on",
            "type": "sparse",
            "x": 0.01789270051026867,
            "y": -0.22854297573400584
        },
        {
            "id": "s_60",
            "name": "Jiaxin Bai",
            "type": "sparse",
            "x": -0.03099486785798306,
            "y": 0.19210376496532958
        },
        {
            "id": "s_61",
            "name": "Jarke J. van Wijk",
            "type": "sparse",
            "x": 0.09636503784023774,
            "y": 0.5547209164225688
        },
        {
            "id": "s_62",
            "name": "Florian Heimerl",
            "type": "sparse",
            "x": 0.10266263567343291,
            "y": -0.2985077582332235
        },
        {
            "id": "s_63",
            "name": "Andreas Glatz",
            "type": "sparse",
            "x": 0.17032618986578532,
            "y": 0.4009686487461545
        },
        {
            "id": "s_64",
            "name": "Mingliang Xu 0001",
            "type": "sparse",
            "x": 0.1174915559419779,
            "y": 0.4986328585848213
        },
        {
            "id": "s_65",
            "name": "Jiawei Zhang 0003",
            "type": "sparse",
            "x": 0.02563779660321242,
            "y": 0.23856571113794778
        },
        {
            "id": "s_66",
            "name": "Ashish Kapoor",
            "type": "sparse",
            "x": -0.12146085594656976,
            "y": -0.08560627310419466
        },
        {
            "id": "s_67",
            "name": "Steven Landis",
            "type": "sparse",
            "x": 0.005862699124108547,
            "y": 0.43325003630750064
        },
        {
            "id": "s_68",
            "name": "Aarthy Sankari Bhaskar",
            "type": "sparse",
            "x": -0.09774578490690555,
            "y": 0.23540548430852842
        },
        {
            "id": "s_69",
            "name": "Pierre Boutillier",
            "type": "sparse",
            "x": -0.8082379583495665,
            "y": 0.08708382021015502
        },
        {
            "id": "s_70",
            "name": "Samuel Gratzl",
            "type": "sparse",
            "x": -0.15162934726821956,
            "y": -0.41387587773066464
        },
        {
            "id": "s_71",
            "name": "Arif Ghafoor",
            "type": "sparse",
            "x": 0.18077225531179666,
            "y": 0.5029785849395717
        },
        {
            "id": "s_72",
            "name": "Matthias Labsch\u00fctz",
            "type": "sparse",
            "x": -0.14137051427169617,
            "y": -0.3327764732161554
        },
        {
            "id": "s_73",
            "name": "Baldwin Nsonga",
            "type": "sparse",
            "x": -0.7808659723682858,
            "y": -0.46593206808929455
        },
        {
            "id": "s_74",
            "name": "Zhiyong Guo",
            "type": "sparse",
            "x": 0.17461521782605846,
            "y": 0.2551328555431274
        },
        {
            "id": "s_75",
            "name": "Nicole Jardine",
            "type": "sparse",
            "x": 0.0029966938287992986,
            "y": -0.29701913439830235
        },
        {
            "id": "s_76",
            "name": "Hanning Shao",
            "type": "sparse",
            "x": 0.18646632829080143,
            "y": 0.2785978299480484
        },
        {
            "id": "s_77",
            "name": "Marcel Breeuwer",
            "type": "sparse",
            "x": -0.15605573299044326,
            "y": -0.5284363477274476
        },
        {
            "id": "s_78",
            "name": "Rickard Englund",
            "type": "sparse",
            "x": -0.43175057712870024,
            "y": 0.27001978891728706
        },
        {
            "id": "s_79",
            "name": "Jennifer K. Ryan",
            "type": "sparse",
            "x": 0.7243645708198383,
            "y": -0.49617458487474636
        },
        {
            "id": "s_80",
            "name": "David H. Rogers 0001",
            "type": "sparse",
            "x": 0.2560064725546044,
            "y": -0.2625080851793042
        },
        {
            "id": "s_81",
            "name": "Markus B\u00f6gl",
            "type": "sparse",
            "x": -0.11695507452151406,
            "y": -0.4289317753645713
        },
        {
            "id": "s_82",
            "name": "Gilles Bailly",
            "type": "sparse",
            "x": 0.08834892149465215,
            "y": -0.45350489762539026
        },
        {
            "id": "s_83",
            "name": "Pavol Klacansky",
            "type": "sparse",
            "x": 0.2680390194467376,
            "y": -0.28477023992332834
        },
        {
            "id": "s_84",
            "name": "Sarah H. Creem-Regehr",
            "type": "sparse",
            "x": -0.41116419793581044,
            "y": 0.3630346431896987
        },
        {
            "id": "s_85",
            "name": "Cagatay Turkay",
            "type": "sparse",
            "x": -0.0937696662383787,
            "y": 0.20415833532576078
        },
        {
            "id": "s_86",
            "name": "Marta Farre",
            "type": "sparse",
            "x": 0.49043578214840755,
            "y": 0.11497891512844483
        },
        {
            "id": "s_87",
            "name": "Ethan Kerzner",
            "type": "sparse",
            "x": -0.3193015363073632,
            "y": 0.23567749915289
        },
        {
            "id": "s_88",
            "name": "Tangzhi Ye",
            "type": "sparse",
            "x": 0.306761877677549,
            "y": 0.26451279610116296
        },
        {
            "id": "s_89",
            "name": "Ladislav Cmol\u00edk",
            "type": "sparse",
            "x": -0.03509578437463386,
            "y": -0.1398315456382777
        },
        {
            "id": "s_90",
            "name": "Micah Acinapura",
            "type": "sparse",
            "x": -0.26258944229468784,
            "y": 0.4015501574922353
        },
        {
            "id": "s_91",
            "name": "Sarah Goodwin",
            "type": "sparse",
            "x": -0.2877403373791306,
            "y": 0.16820247154074178
        },
        {
            "id": "s_92",
            "name": "Qiaoan Chen",
            "type": "sparse",
            "x": 0.16625930591075386,
            "y": -0.0752254874518351
        },
        {
            "id": "s_93",
            "name": "W. Sabrina Lin",
            "type": "sparse",
            "x": 0.08194606562350028,
            "y": 0.04526858465417997
        },
        {
            "id": "s_94",
            "name": "Narayanan Kasthuri",
            "type": "sparse",
            "x": -0.2809461614073016,
            "y": -0.26125283227856083
        },
        {
            "id": "s_95",
            "name": "Yu Ye 0002",
            "type": "sparse",
            "x": 0.13279742952576667,
            "y": 0.18203071105616767
        },
        {
            "id": "s_96",
            "name": "Irene Baeza Rojo",
            "type": "sparse",
            "x": -0.5308237926964924,
            "y": -0.0005959917878872112
        },
        {
            "id": "s_97",
            "name": "Thomas J. Grabowski",
            "type": "sparse",
            "x": -0.0829477259841006,
            "y": -0.04767868149977809
        },
        {
            "id": "s_98",
            "name": "Yu Zheng 0004",
            "type": "sparse",
            "x": 0.10469216765532267,
            "y": 0.26285244151195136
        },
        {
            "id": "s_99",
            "name": "Michael D. Hill",
            "type": "sparse",
            "x": -0.2836388202026988,
            "y": -0.07601063677994682
        },
        {
            "id": "s_100",
            "name": "Jean-Daniel Fekete",
            "type": "sparse",
            "x": -0.20893588057066048,
            "y": -0.05855476952477456
        },
        {
            "id": "s_101",
            "name": "Thomas Ortner",
            "type": "sparse",
            "x": -0.11549141586396942,
            "y": -0.3227059553773269
        },
        {
            "id": "s_102",
            "name": "Haneen Mohammed",
            "type": "sparse",
            "x": -0.257535599572794,
            "y": -0.25719493714655534
        },
        {
            "id": "s_103",
            "name": "Mathias Kanzler",
            "type": "sparse",
            "x": 0.5586382861482395,
            "y": 0.1964587130086544
        },
        {
            "id": "s_104",
            "name": "Lauren Bradel",
            "type": "sparse",
            "x": 0.47051848760291487,
            "y": 0.28536010949860013
        },
        {
            "id": "s_105",
            "name": "Abishek Puri",
            "type": "sparse",
            "x": -0.03917640296394055,
            "y": 0.18991092547247151
        },
        {
            "id": "s_106",
            "name": "John Thompson 0002",
            "type": "sparse",
            "x": -0.04573615043311479,
            "y": -0.19564662277054892
        },
        {
            "id": "s_107",
            "name": "Kristin Divis",
            "type": "sparse",
            "x": 0.03526353298705031,
            "y": -0.31106080857319357
        },
        {
            "id": "s_108",
            "name": "Rahul C. Basole",
            "type": "sparse",
            "x": 0.03443402250313296,
            "y": -0.05544459371620587
        },
        {
            "id": "s_109",
            "name": "Jimeng Sun 0001",
            "type": "sparse",
            "x": 0.19675697341440673,
            "y": -0.24029892264752
        },
        {
            "id": "s_110",
            "name": "Mona Hosseinkhani Loorak",
            "type": "sparse",
            "x": -0.2522079415991212,
            "y": -0.05758129436593596
        },
        {
            "id": "s_111",
            "name": "R\u00fcdiger Westermann",
            "type": "sparse",
            "x": 0.446104587516671,
            "y": 0.16537227630118517
        },
        {
            "id": "s_112",
            "name": "Maoyuan Sun",
            "type": "sparse",
            "x": 0.3880004042303062,
            "y": 0.27204292410115705
        },
        {
            "id": "s_113",
            "name": "Kenney Ng",
            "type": "sparse",
            "x": -0.02162152860535233,
            "y": -0.13435686369915148
        },
        {
            "id": "s_114",
            "name": "Paul K. J. Han",
            "type": "sparse",
            "x": -0.38236492497805796,
            "y": 0.10326930864863372
        },
        {
            "id": "s_115",
            "name": "Ananda Montoly",
            "type": "sparse",
            "x": -0.24458480715940073,
            "y": -0.01430387688538682
        },
        {
            "id": "s_116",
            "name": "Fangfang Zhou",
            "type": "sparse",
            "x": 0.15147373289352595,
            "y": 0.3082449720738094
        },
        {
            "id": "s_117",
            "name": "Jiahui Chen",
            "type": "sparse",
            "x": 0.10236864338775895,
            "y": 0.28875659518197366
        },
        {
            "id": "s_118",
            "name": "Dirk J. Lehmann",
            "type": "sparse",
            "x": -0.6311230247610352,
            "y": 0.14948780969120862
        },
        {
            "id": "s_119",
            "name": "Jonathan Zhang",
            "type": "sparse",
            "x": 0.4334561919052791,
            "y": 0.0369249918236953
        },
        {
            "id": "s_120",
            "name": "Leishi Zhang",
            "type": "sparse",
            "x": 0.0070864331946858435,
            "y": -0.018997415342892784
        },
        {
            "id": "s_121",
            "name": "Yael Albo",
            "type": "sparse",
            "x": 0.5085056616911152,
            "y": 0.07308434929630173
        },
        {
            "id": "s_122",
            "name": "Liang Gou",
            "type": "sparse",
            "x": 0.40020127883736384,
            "y": 0.19116545655104186
        },
        {
            "id": "s_123",
            "name": "Casimir J. H. Ludwig",
            "type": "sparse",
            "x": -0.006421294507648232,
            "y": -0.42780008704789574
        },
        {
            "id": "s_124",
            "name": "Lane Harrison",
            "type": "sparse",
            "x": -0.4290158439006994,
            "y": 0.10352598403521693
        },
        {
            "id": "s_125",
            "name": "Tom Polk",
            "type": "sparse",
            "x": -0.10094875163668617,
            "y": 0.18875478513120675
        },
        {
            "id": "s_126",
            "name": "Rudolf Netzel",
            "type": "sparse",
            "x": 0.07593041877749028,
            "y": -0.16879419925326555
        },
        {
            "id": "s_127",
            "name": "Yuan Chen",
            "type": "sparse",
            "x": 0.08334596053616355,
            "y": 0.12373097938343927
        },
        {
            "id": "s_128",
            "name": "Tara M. Madhyastha",
            "type": "sparse",
            "x": -0.08078626687364789,
            "y": -0.05244191312139151
        },
        {
            "id": "s_129",
            "name": "Jo Vermeulen",
            "type": "sparse",
            "x": -0.30687315785626473,
            "y": -0.09768930546880335
        },
        {
            "id": "s_130",
            "name": "Steve Kieffer",
            "type": "sparse",
            "x": -0.3654551320486805,
            "y": 0.048886450947727325
        },
        {
            "id": "s_131",
            "name": "Yanyan Wang",
            "type": "sparse",
            "x": -0.011025297415888018,
            "y": 0.10949594425738395
        },
        {
            "id": "s_132",
            "name": "Sylvia Gla\u00dfer",
            "type": "sparse",
            "x": -0.27437091778022243,
            "y": -0.3464088042910329
        },
        {
            "id": "s_133",
            "name": "Alexandru C. Telea",
            "type": "sparse",
            "x": -0.21816581927136977,
            "y": 0.4204320808947067
        },
        {
            "id": "s_134",
            "name": "Subhashis Hazarika",
            "type": "sparse",
            "x": 0.3341371668652988,
            "y": 0.13304487298129916
        },
        {
            "id": "s_135",
            "name": "Dongmei Zhang 0001",
            "type": "sparse",
            "x": 0.24223116173511058,
            "y": 0.021164965118140443
        },
        {
            "id": "s_136",
            "name": "Xing Li",
            "type": "sparse",
            "x": -0.800704354265134,
            "y": 0.09171928263276569
        },
        {
            "id": "s_137",
            "name": "Huamin Qu",
            "type": "sparse",
            "x": 0.15821938531940896,
            "y": 0.10368828728766351
        },
        {
            "id": "s_138",
            "name": "Daniel Weiskopf",
            "type": "sparse",
            "x": 0.06950258365555491,
            "y": -0.12034067884203292
        },
        {
            "id": "s_139",
            "name": "Madeline L. Parker",
            "type": "sparse",
            "x": 0.1134366700971029,
            "y": 0.42148724332442966
        },
        {
            "id": "s_140",
            "name": "R. Arthur Bouwman",
            "type": "sparse",
            "x": 0.1398846457647091,
            "y": 0.6577256902549588
        },
        {
            "id": "s_141",
            "name": "Haibin Liu",
            "type": "sparse",
            "x": 0.4401786774255448,
            "y": 0.18192176192477744
        },
        {
            "id": "s_142",
            "name": "Mohamed A. Gad",
            "type": "sparse",
            "x": 0.24473288545199895,
            "y": 0.10141503816174673
        },
        {
            "id": "s_143",
            "name": "Nathan Winters",
            "type": "sparse",
            "x": -0.40738682588978925,
            "y": 0.09209569538292704
        },
        {
            "id": "s_144",
            "name": "Jonathan Komperda",
            "type": "sparse",
            "x": -0.7831866474166009,
            "y": 0.13362886924092826
        },
        {
            "id": "s_145",
            "name": "Elmar Eisemann",
            "type": "sparse",
            "x": -0.17202424785664433,
            "y": -0.45570082256194017
        },
        {
            "id": "s_146",
            "name": "Geoffrey S. Young",
            "type": "sparse",
            "x": -0.49038707989087993,
            "y": -0.24896002041799925
        },
        {
            "id": "s_147",
            "name": "\u00c1ngel Alexander Cabrera",
            "type": "sparse",
            "x": 0.13394427668880837,
            "y": -0.6136215759645572
        },
        {
            "id": "s_148",
            "name": "Karl Bladin",
            "type": "sparse",
            "x": -0.3209680918895023,
            "y": 0.4062028678224434
        },
        {
            "id": "s_149",
            "name": "Sathish Kottravel",
            "type": "sparse",
            "x": -0.4272848295731851,
            "y": 0.27572424794558575
        },
        {
            "id": "s_150",
            "name": "Yea-Seul Kim",
            "type": "sparse",
            "x": -0.21014224077411361,
            "y": -0.35839124092611446
        },
        {
            "id": "s_151",
            "name": "Andreas Just",
            "type": "sparse",
            "x": 0.2569618398965572,
            "y": 0.3217173930132032
        },
        {
            "id": "s_152",
            "name": "Erik Trostmann",
            "type": "sparse",
            "x": -0.3844168588884551,
            "y": -0.3911270768450071
        },
        {
            "id": "s_153",
            "name": "Xinyu Zhu",
            "type": "sparse",
            "x": -0.01301972190109026,
            "y": 0.19929105680885914
        },
        {
            "id": "s_154",
            "name": "Ruizhen Hu",
            "type": "sparse",
            "x": 0.276716836635578,
            "y": 0.09156434562506009
        },
        {
            "id": "s_155",
            "name": "Shamal Al-Dohuki",
            "type": "sparse",
            "x": 0.050916112549305816,
            "y": 0.4511060123111367
        },
        {
            "id": "s_156",
            "name": "Colin Fredericks",
            "type": "sparse",
            "x": -0.2818273302515799,
            "y": -0.19675075933626768
        },
        {
            "id": "s_157",
            "name": "Ievgeniia Gutenko",
            "type": "sparse",
            "x": 0.47765847971327635,
            "y": 0.8409935240507064
        },
        {
            "id": "s_158",
            "name": "Michael Gleicher",
            "type": "sparse",
            "x": -0.18228127123887006,
            "y": -0.05096615300379229
        },
        {
            "id": "s_159",
            "name": "Martin Wattenberg",
            "type": "sparse",
            "x": 0.024783236841789156,
            "y": -0.6101092807595235
        },
        {
            "id": "s_160",
            "name": "Alireza Entezari",
            "type": "sparse",
            "x": -0.16219153119638985,
            "y": -0.4981627447000671
        },
        {
            "id": "s_161",
            "name": "Anastasia Bezerianos",
            "type": "sparse",
            "x": 0.037885444863157466,
            "y": -0.3201371509511487
        },
        {
            "id": "s_162",
            "name": "Tiffany Wun",
            "type": "sparse",
            "x": -0.25578150873424127,
            "y": -0.07904758292517233
        },
        {
            "id": "s_163",
            "name": "Eveline H. J. Mestrom",
            "type": "sparse",
            "x": 0.1378754206530422,
            "y": 0.6530554669550108
        },
        {
            "id": "s_164",
            "name": "Karoline Villinger",
            "type": "sparse",
            "x": -0.20016516580908195,
            "y": 0.11542953412196208
        },
        {
            "id": "s_165",
            "name": "Feng Wang 0013",
            "type": "sparse",
            "x": 0.20219262122150022,
            "y": -0.35000406815313495
        },
        {
            "id": "s_166",
            "name": "Alan Wilson 0004",
            "type": "sparse",
            "x": -0.07746342189166232,
            "y": 0.0426848393714219
        },
        {
            "id": "s_167",
            "name": "Yang Wang",
            "type": "sparse",
            "x": -0.027335366714893918,
            "y": 0.1329740475909836
        },
        {
            "id": "s_168",
            "name": "Reem Hourieh",
            "type": "sparse",
            "x": -0.1783582908381913,
            "y": -0.4626371339303174
        },
        {
            "id": "s_169",
            "name": "Biswaksen Patnaik",
            "type": "sparse",
            "x": -0.15038020615513603,
            "y": -0.13926349591373352
        },
        {
            "id": "s_170",
            "name": "Jeremy Millar",
            "type": "sparse",
            "x": -0.38439156687520426,
            "y": 0.053425344918131314
        },
        {
            "id": "s_171",
            "name": "Ashley D. Spear",
            "type": "sparse",
            "x": 0.3790007315646098,
            "y": -0.3002125032711951
        },
        {
            "id": "s_172",
            "name": "Vivek Kothari",
            "type": "sparse",
            "x": -0.3586747662901758,
            "y": -0.15076563411926291
        },
        {
            "id": "s_173",
            "name": "Chen Bao",
            "type": "sparse",
            "x": -0.014399291332255926,
            "y": 0.07681301389968637
        },
        {
            "id": "s_174",
            "name": "David Scarlatti",
            "type": "sparse",
            "x": 0.0051666252758892995,
            "y": 0.3696246291008983
        },
        {
            "id": "s_175",
            "name": "Tadija Kekic",
            "type": "sparse",
            "x": -0.09690367887547864,
            "y": -0.2414009783715671
        },
        {
            "id": "s_176",
            "name": "Piero Molino",
            "type": "sparse",
            "x": 0.03230021020202582,
            "y": 0.23406162717303347
        },
        {
            "id": "s_177",
            "name": "Sung-Hee Kim",
            "type": "sparse",
            "x": 0.19364393623846493,
            "y": -0.1970956488130179
        },
        {
            "id": "s_178",
            "name": "Mala Ananth 0001",
            "type": "sparse",
            "x": 0.39485293108520514,
            "y": 0.7738332768369365
        },
        {
            "id": "s_179",
            "name": "Mingliang Xue",
            "type": "sparse",
            "x": -0.034899652736983866,
            "y": 0.1414712985438179
        },
        {
            "id": "s_180",
            "name": "Shuaiqi Wang",
            "type": "sparse",
            "x": 0.40848685500548654,
            "y": 0.12434053160554921
        },
        {
            "id": "s_181",
            "name": "Theresia Gschwandtner",
            "type": "sparse",
            "x": -0.1508597601044559,
            "y": -0.4797128151623953
        },
        {
            "id": "s_182",
            "name": "Fanny Chevalier",
            "type": "sparse",
            "x": 0.27400471760047895,
            "y": -0.12401621950241752
        },
        {
            "id": "s_183",
            "name": "Zhicong Lu",
            "type": "sparse",
            "x": 0.35561338355415434,
            "y": -0.013466098717112601
        },
        {
            "id": "s_184",
            "name": "Bruce H. Thomas",
            "type": "sparse",
            "x": -0.22361884217986167,
            "y": 0.012246500863795345
        },
        {
            "id": "s_185",
            "name": "Andreas Mathisen",
            "type": "sparse",
            "x": -0.10637492278909036,
            "y": -0.24367311583801524
        },
        {
            "id": "s_186",
            "name": "M. Leila Mays",
            "type": "sparse",
            "x": -0.35202742357472044,
            "y": 0.30808255598715795
        },
        {
            "id": "s_187",
            "name": "Francesca Samsel",
            "type": "sparse",
            "x": 0.20768059792612398,
            "y": -0.3390308226136713
        },
        {
            "id": "s_188",
            "name": "Ben Eysenbach",
            "type": "sparse",
            "x": -0.022016924017072143,
            "y": -0.12835831456867386
        },
        {
            "id": "s_189",
            "name": "Silvia Mabel Castro",
            "type": "sparse",
            "x": 0.07652969474921187,
            "y": -0.13245812193319836
        },
        {
            "id": "s_190",
            "name": "John J. Baglino",
            "type": "sparse",
            "x": 0.37132058534220774,
            "y": -0.3015999969309963
        },
        {
            "id": "s_191",
            "name": "Xiaoming Liu 0002",
            "type": "sparse",
            "x": 0.029409509970402835,
            "y": -0.12973298242038897
        },
        {
            "id": "s_192",
            "name": "Stefania Forlini",
            "type": "sparse",
            "x": -0.308357801200932,
            "y": 0.17483169642882979
        },
        {
            "id": "s_193",
            "name": "Tiankai Xie",
            "type": "sparse",
            "x": 0.062472262683559675,
            "y": 0.4550863625367852
        },
        {
            "id": "s_194",
            "name": "Evanthia Dimara",
            "type": "sparse",
            "x": -0.031059861052138406,
            "y": -0.2957500060203817
        },
        {
            "id": "s_195",
            "name": "Joon-Yong Lee",
            "type": "sparse",
            "x": 0.0357008100624154,
            "y": 0.021512253776096866
        },
        {
            "id": "s_196",
            "name": "David J. Israel",
            "type": "sparse",
            "x": 0.04096392955322681,
            "y": -0.07852396456564163
        },
        {
            "id": "s_197",
            "name": "Albert Amor-Amoros",
            "type": "sparse",
            "x": -0.15263511960600368,
            "y": -0.5430719046404671
        },
        {
            "id": "s_198",
            "name": "Rusheng Pan",
            "type": "sparse",
            "x": 0.34335623940562804,
            "y": 0.18416256034036538
        },
        {
            "id": "s_199",
            "name": "Silvia Fademrecht",
            "type": "sparse",
            "x": 0.038430892327438106,
            "y": -0.4718412157741352
        },
        {
            "id": "s_200",
            "name": "Eric Sauda",
            "type": "sparse",
            "x": 0.3925765950383884,
            "y": -0.4945300837551159
        },
        {
            "id": "s_201",
            "name": "Michael Ankele",
            "type": "sparse",
            "x": -0.30016549215555827,
            "y": -0.6715723569686494
        },
        {
            "id": "s_202",
            "name": "Tim Dwyer",
            "type": "sparse",
            "x": -0.2856132459210858,
            "y": 0.0546440979277788
        },
        {
            "id": "s_203",
            "name": "Timothy Major",
            "type": "sparse",
            "x": -0.03788128189782887,
            "y": -0.09171440739990389
        },
        {
            "id": "s_204",
            "name": "Dennis Dingen",
            "type": "sparse",
            "x": 0.14844722285588113,
            "y": 0.6495346070981339
        },
        {
            "id": "s_205",
            "name": "Xing Mu",
            "type": "sparse",
            "x": 0.3016648276979552,
            "y": 0.06940510410451518
        },
        {
            "id": "s_206",
            "name": "Julian Thijssen",
            "type": "sparse",
            "x": -0.13710640901400642,
            "y": -0.5169391829734865
        },
        {
            "id": "s_207",
            "name": "Chris North 0001",
            "type": "sparse",
            "x": 0.40270499611356086,
            "y": 0.2093777448313423
        },
        {
            "id": "s_208",
            "name": "Baldur van Lew",
            "type": "sparse",
            "x": -0.14054521277664933,
            "y": -0.511199859919056
        },
        {
            "id": "s_209",
            "name": "Timo Ropinski",
            "type": "sparse",
            "x": -0.34486037013842524,
            "y": 0.1866027092346509
        },
        {
            "id": "s_210",
            "name": "Preeti Raghavan",
            "type": "sparse",
            "x": 0.08188723131570204,
            "y": 0.31600483139397345
        },
        {
            "id": "s_211",
            "name": "Alexander Lex",
            "type": "sparse",
            "x": -0.18308459732395596,
            "y": -0.16931783695166647
        },
        {
            "id": "s_212",
            "name": "Harsh Bhatia",
            "type": "sparse",
            "x": 0.2305345596088345,
            "y": -0.27050410592926805
        },
        {
            "id": "s_213",
            "name": "Joseph Botros",
            "type": "sparse",
            "x": -0.15999994600864184,
            "y": -0.20901559333257855
        },
        {
            "id": "s_214",
            "name": "Wenping Wang",
            "type": "sparse",
            "x": 0.16650546672778227,
            "y": -0.4894204802962563
        },
        {
            "id": "s_215",
            "name": "Sujin Jang",
            "type": "sparse",
            "x": 0.037222830338966034,
            "y": -0.358400257897236
        },
        {
            "id": "s_216",
            "name": "Susan M. Mniszewski",
            "type": "sparse",
            "x": 0.4883140726710926,
            "y": 0.10077899361664162
        },
        {
            "id": "s_217",
            "name": "Eston Schweickart",
            "type": "sparse",
            "x": -0.18680791278928716,
            "y": -0.06924305491187711
        },
        {
            "id": "s_218",
            "name": "Lace M. K. Padilla",
            "type": "sparse",
            "x": -0.4148792219022191,
            "y": 0.36051469398191655
        },
        {
            "id": "s_219",
            "name": "Joel Lanir",
            "type": "sparse",
            "x": 0.44270195149009606,
            "y": 0.09662194368855409
        },
        {
            "id": "s_220",
            "name": "Zhiguang Yang",
            "type": "sparse",
            "x": 0.10717014400304208,
            "y": 0.13114556527682183
        },
        {
            "id": "s_221",
            "name": "Laurent Lessard",
            "type": "sparse",
            "x": 0.17779155815452433,
            "y": 0.4544263198038825
        },
        {
            "id": "s_222",
            "name": "Halden Lin",
            "type": "sparse",
            "x": -0.05762285807027213,
            "y": -0.40515718003631707
        },
        {
            "id": "s_223",
            "name": "Ben Shneiderman",
            "type": "sparse",
            "x": 0.4690660947124251,
            "y": 0.059114379233050754
        },
        {
            "id": "s_224",
            "name": "Jurim Lee",
            "type": "sparse",
            "x": 0.12422270238263555,
            "y": -0.2762494176723509
        },
        {
            "id": "s_225",
            "name": "Siwei Fu",
            "type": "sparse",
            "x": 0.23012411665059632,
            "y": 0.14921572603968855
        },
        {
            "id": "s_226",
            "name": "Stefan M\u00fcller Arisona",
            "type": "sparse",
            "x": 0.1412657594472186,
            "y": 0.179732419941381
        },
        {
            "id": "s_227",
            "name": "Julie Gerdes",
            "type": "sparse",
            "x": -0.39713715968192836,
            "y": 0.27002412437939044
        },
        {
            "id": "s_228",
            "name": "Thomas M\u00fcller 0005",
            "type": "sparse",
            "x": 0.33719864898496227,
            "y": 0.3194514358663668
        },
        {
            "id": "s_229",
            "name": "Bastien Liutkus",
            "type": "sparse",
            "x": 0.12162033419123933,
            "y": -0.5218504967753321
        },
        {
            "id": "s_230",
            "name": "Won-Dong Jang",
            "type": "sparse",
            "x": -0.31126809820448653,
            "y": -0.16914536877690722
        },
        {
            "id": "s_231",
            "name": "Pierre Dragicevic",
            "type": "sparse",
            "x": -0.13126812195045565,
            "y": -0.17401568910153284
        },
        {
            "id": "s_232",
            "name": "Jochen G\u00f6rtler",
            "type": "sparse",
            "x": 0.05071233864479508,
            "y": -0.015249403803675364
        },
        {
            "id": "s_233",
            "name": "Fabian G\u00fcnther",
            "type": "sparse",
            "x": -0.8245058652865598,
            "y": -0.4377857182519172
        },
        {
            "id": "s_234",
            "name": "Mark A. Whiting",
            "type": "sparse",
            "x": 0.008240823549321845,
            "y": 0.009463753946578153
        },
        {
            "id": "s_235",
            "name": "Samuel Leventhal",
            "type": "sparse",
            "x": 0.37059301506803133,
            "y": -0.3072883208834116
        },
        {
            "id": "s_236",
            "name": "Elisa De Llano",
            "type": "sparse",
            "x": -0.08728086210187379,
            "y": -0.2399575383304563
        },
        {
            "id": "s_237",
            "name": "William Ribarsky",
            "type": "sparse",
            "x": 0.3453133298167524,
            "y": -0.45639948669311814
        },
        {
            "id": "s_238",
            "name": "James Harris",
            "type": "sparse",
            "x": -0.35316881988226856,
            "y": -0.15776235683375942
        },
        {
            "id": "s_239",
            "name": "Anjul Kumar Tyagi",
            "type": "sparse",
            "x": -0.7557443314749571,
            "y": -0.05019235445868658
        },
        {
            "id": "s_240",
            "name": "Tatiana von Landesberger",
            "type": "sparse",
            "x": 0.023271223012875274,
            "y": 0.3753021179761558
        },
        {
            "id": "s_241",
            "name": "Sarkis Halladjian",
            "type": "sparse",
            "x": -0.0911631403826674,
            "y": -0.21680631886240365
        },
        {
            "id": "s_242",
            "name": "Steven R. Gomez",
            "type": "sparse",
            "x": -0.2684931118677873,
            "y": 0.22482323706498722
        },
        {
            "id": "s_243",
            "name": "Amira Chalbi",
            "type": "sparse",
            "x": 0.17382662661728904,
            "y": -0.20270670506395688
        },
        {
            "id": "s_244",
            "name": "Rongwen Zhao",
            "type": "sparse",
            "x": 0.32884042058204505,
            "y": 0.06436252810479509
        },
        {
            "id": "s_245",
            "name": "Ji Hwan Park",
            "type": "sparse",
            "x": 0.45051721341064166,
            "y": 0.8764612189423235
        },
        {
            "id": "s_246",
            "name": "Kasper Hornb\u00e6k",
            "type": "sparse",
            "x": -0.2698769630348881,
            "y": -0.42280174884863997
        },
        {
            "id": "s_247",
            "name": "Bridger Herman",
            "type": "sparse",
            "x": 0.1728476693632541,
            "y": -0.3714237478983747
        },
        {
            "id": "s_248",
            "name": "Mark R. Petersen",
            "type": "sparse",
            "x": 0.3695685933096314,
            "y": -0.19417380602083414
        },
        {
            "id": "s_249",
            "name": "Gabriel Ryan",
            "type": "sparse",
            "x": -0.42851215806037496,
            "y": 0.04218719892775406
        },
        {
            "id": "s_250",
            "name": "Bongshin Lee",
            "type": "sparse",
            "x": -0.034917864609809224,
            "y": -0.08911311862616889
        },
        {
            "id": "s_251",
            "name": "Steffen Frey",
            "type": "sparse",
            "x": 0.16032026725223011,
            "y": -0.46840344938748385
        },
        {
            "id": "s_252",
            "name": "Klaus Mueller 0001",
            "type": "sparse",
            "x": -0.7479951648577605,
            "y": -0.1008085577358763
        },
        {
            "id": "s_253",
            "name": "Azam Khan",
            "type": "sparse",
            "x": 0.30546577407182024,
            "y": -0.0912609416910392
        },
        {
            "id": "s_254",
            "name": "Enamul Hoque",
            "type": "sparse",
            "x": 0.2231489316972804,
            "y": 0.024289736838247953
        },
        {
            "id": "s_255",
            "name": "Xiaolong Zhang 0001",
            "type": "sparse",
            "x": 0.22964334102762962,
            "y": 0.37007863731751856
        },
        {
            "id": "s_256",
            "name": "Mingming Fan 0001",
            "type": "sparse",
            "x": 0.37495909116016923,
            "y": -0.000342387330182825
        },
        {
            "id": "s_257",
            "name": "Andrew Wentzel",
            "type": "sparse",
            "x": -0.7924982486750273,
            "y": 0.16268380019074177
        },
        {
            "id": "s_258",
            "name": "Hanna Sch\u00e4fer",
            "type": "sparse",
            "x": 0.01967132577453061,
            "y": 0.0747389736686781
        },
        {
            "id": "s_259",
            "name": "Ulrik Brandes",
            "type": "sparse",
            "x": 0.003993273171396305,
            "y": 0.05125492372048955
        },
        {
            "id": "s_260",
            "name": "Anders Ynnerman",
            "type": "sparse",
            "x": -0.3290285506958303,
            "y": 0.35815062646425605
        },
        {
            "id": "s_261",
            "name": "Qing Chen 0001",
            "type": "sparse",
            "x": 0.26723087362006487,
            "y": 0.17131219836549505
        },
        {
            "id": "s_262",
            "name": "Anja C. Schunke",
            "type": "sparse",
            "x": -0.27890671687992263,
            "y": -0.675344286235395
        },
        {
            "id": "s_263",
            "name": "Lionel M. Ni",
            "type": "sparse",
            "x": 0.15630315465268135,
            "y": 0.02004901999276607
        },
        {
            "id": "s_264",
            "name": "Bastian Goldl\u00fccke",
            "type": "sparse",
            "x": -0.06811509421304207,
            "y": 0.19827186757921
        },
        {
            "id": "s_265",
            "name": "Finale Doshi-Velez",
            "type": "sparse",
            "x": 0.3578693840379196,
            "y": -0.09901531916206086
        },
        {
            "id": "s_266",
            "name": "Min Chen 0001",
            "type": "sparse",
            "x": -0.2693038644535842,
            "y": -0.10968761113990758
        },
        {
            "id": "s_267",
            "name": "Erik Sund\u00e9n",
            "type": "sparse",
            "x": -0.4337425346212374,
            "y": 0.27455645343683227
        },
        {
            "id": "s_268",
            "name": "Nadir Weibel",
            "type": "sparse",
            "x": 0.031937363095757114,
            "y": -0.18288788582200388
        },
        {
            "id": "s_269",
            "name": "Xianfeng Gu",
            "type": "sparse",
            "x": 0.47131437361321954,
            "y": 0.8819652834582429
        },
        {
            "id": "s_270",
            "name": "Harald Piringer",
            "type": "sparse",
            "x": -0.0966286892637959,
            "y": -0.33912080407768963
        },
        {
            "id": "s_271",
            "name": "Mark Wallace 0001",
            "type": "sparse",
            "x": -0.35579760892739565,
            "y": 0.15657200539070862
        },
        {
            "id": "s_272",
            "name": "Haiyang Wang",
            "type": "sparse",
            "x": 0.2939868431185753,
            "y": 0.29602938755410446
        },
        {
            "id": "s_273",
            "name": "Simon Butscher",
            "type": "sparse",
            "x": -0.19210735139545781,
            "y": 0.11444850410659356
        },
        {
            "id": "s_274",
            "name": "Xiaoke Huang",
            "type": "sparse",
            "x": 0.0213769207565163,
            "y": 0.4814267696066559
        },
        {
            "id": "s_275",
            "name": "Christina Niederer",
            "type": "sparse",
            "x": -0.18190316242119334,
            "y": -0.45826930092169144
        },
        {
            "id": "s_276",
            "name": "Xu-Meng Wang",
            "type": "sparse",
            "x": 0.32642598638199216,
            "y": 0.19044688843475094
        },
        {
            "id": "s_277",
            "name": "Maria Florencia Gargiulo",
            "type": "sparse",
            "x": 0.07857851694289074,
            "y": -0.12564593247276196
        },
        {
            "id": "s_278",
            "name": "Vanessa Pe\u00f1a Araya",
            "type": "sparse",
            "x": 0.11993453327306056,
            "y": -0.4197545304256663
        },
        {
            "id": "s_279",
            "name": "Jiazhi Xia",
            "type": "sparse",
            "x": 0.14542039903773654,
            "y": 0.38049308900994305
        },
        {
            "id": "s_280",
            "name": "Ronell Sicat",
            "type": "sparse",
            "x": -0.2875079781482223,
            "y": -0.06957277238179843
        },
        {
            "id": "s_281",
            "name": "Johannes Kehrer",
            "type": "sparse",
            "x": 0.1514117395531923,
            "y": 0.061226862925033945
        },
        {
            "id": "s_282",
            "name": "Michael Markl 0001",
            "type": "sparse",
            "x": -0.3328517595966838,
            "y": 0.6938551421059377
        },
        {
            "id": "s_283",
            "name": "Ruixiang Zhang",
            "type": "sparse",
            "x": 0.20334076954053407,
            "y": 0.2166508065282426
        },
        {
            "id": "s_284",
            "name": "Michael Glueck",
            "type": "sparse",
            "x": 0.30549933612408664,
            "y": -0.09171458158192783
        },
        {
            "id": "s_285",
            "name": "Michael J. McGuffin",
            "type": "sparse",
            "x": 0.2580182472567115,
            "y": 0.3285286222852604
        },
        {
            "id": "s_286",
            "name": "Cheng Deng",
            "type": "sparse",
            "x": -0.47353237979395246,
            "y": 0.1281493509621187
        },
        {
            "id": "s_287",
            "name": "Xi Ye",
            "type": "sparse",
            "x": 0.11707439795999126,
            "y": 0.48690175692376686
        },
        {
            "id": "s_288",
            "name": "Peter J. Polack Jr.",
            "type": "sparse",
            "x": 0.13862706355989163,
            "y": -0.6346746567207825
        },
        {
            "id": "s_289",
            "name": "Frits Koning",
            "type": "sparse",
            "x": -0.1144812999332885,
            "y": -0.5173118547855176
        },
        {
            "id": "s_290",
            "name": "Omar ElTayeby",
            "type": "sparse",
            "x": 0.3124518004038049,
            "y": -0.42969812141023317
        },
        {
            "id": "s_291",
            "name": "Dmitry A. Storchak",
            "type": "sparse",
            "x": -0.3443273105073883,
            "y": -0.16237871491711847
        },
        {
            "id": "s_292",
            "name": "Katar\u00edna Furmanov\u00e1",
            "type": "sparse",
            "x": -0.0844577076897681,
            "y": -0.08243410943817259
        },
        {
            "id": "s_293",
            "name": "Wenwen Dou",
            "type": "sparse",
            "x": 0.35462453179464304,
            "y": -0.4704491832489076
        },
        {
            "id": "s_294",
            "name": "Thomas Wilde",
            "type": "sparse",
            "x": -0.49262652032592735,
            "y": 0.27183993648703264
        },
        {
            "id": "s_295",
            "name": "Duen Horng Chau",
            "type": "sparse",
            "x": 0.1225840257099531,
            "y": -0.5597432935194343
        },
        {
            "id": "s_296",
            "name": "Frederick Federer",
            "type": "sparse",
            "x": 0.29687934967307245,
            "y": -0.30398267974825854
        },
        {
            "id": "s_297",
            "name": "Valerio Pascucci",
            "type": "sparse",
            "x": 0.27672818003559063,
            "y": -0.25610025038114903
        },
        {
            "id": "s_298",
            "name": "Daniel Toloudis",
            "type": "sparse",
            "x": -0.07477003791855952,
            "y": -0.20956601919845516
        },
        {
            "id": "s_299",
            "name": "Thomas Ertl",
            "type": "sparse",
            "x": 0.09737412987969421,
            "y": -0.3694400401126375
        },
        {
            "id": "s_300",
            "name": "Bettina Speckmann",
            "type": "sparse",
            "x": -0.3326028176802717,
            "y": 0.4009402559362585
        },
        {
            "id": "s_301",
            "name": "Shixia Liu",
            "type": "sparse",
            "x": 0.09850227842034977,
            "y": 0.490119479977956
        },
        {
            "id": "s_302",
            "name": "Stefan J\u00e4nicke",
            "type": "sparse",
            "x": -0.7611852726309273,
            "y": -0.4939151856452246
        },
        {
            "id": "s_303",
            "name": "Sihang Li",
            "type": "sparse",
            "x": 0.27249694659810453,
            "y": 0.34504405054144366
        },
        {
            "id": "s_304",
            "name": "Xinxin Huang",
            "type": "sparse",
            "x": 0.20708982778447632,
            "y": 0.46317328650740175
        },
        {
            "id": "s_305",
            "name": "Jie Lu 0002",
            "type": "sparse",
            "x": 0.07977038126699047,
            "y": 0.051827151428250146
        },
        {
            "id": "s_306",
            "name": "Zening Qu",
            "type": "sparse",
            "x": -0.2382739706734038,
            "y": -0.41937161479951796
        },
        {
            "id": "s_307",
            "name": "Christof Seeger",
            "type": "sparse",
            "x": 0.10392088813841727,
            "y": -0.24002892724866023
        },
        {
            "id": "s_308",
            "name": "Alexander Kumpf",
            "type": "sparse",
            "x": 0.5377764797697809,
            "y": 0.1790856969260993
        },
        {
            "id": "s_309",
            "name": "Elisabeth Andr\u00e9",
            "type": "sparse",
            "x": -0.3079273838601836,
            "y": -0.09057276220107036
        },
        {
            "id": "s_310",
            "name": "Tobias Czauderna",
            "type": "sparse",
            "x": -0.5255087292372693,
            "y": 0.0308501583340241
        },
        {
            "id": "s_311",
            "name": "Ludovic Autin",
            "type": "sparse",
            "x": -0.0906020165571741,
            "y": -0.13176924476950158
        },
        {
            "id": "s_312",
            "name": "Lijing Lin",
            "type": "sparse",
            "x": 0.2578379706415536,
            "y": 0.34473637524794165
        },
        {
            "id": "s_313",
            "name": "Robert A. Bridges",
            "type": "sparse",
            "x": 0.42316406196113376,
            "y": -0.24769930359806222
        },
        {
            "id": "s_314",
            "name": "Mathieu Le Goc",
            "type": "sparse",
            "x": -0.210412220807701,
            "y": -0.13209350551863763
        },
        {
            "id": "s_315",
            "name": "Winter Wei",
            "type": "sparse",
            "x": 0.4104433527229685,
            "y": 0.005777257269782965
        },
        {
            "id": "s_316",
            "name": "Siming Chen 0001",
            "type": "sparse",
            "x": 0.1912749648633569,
            "y": 0.292490445353527
        },
        {
            "id": "s_317",
            "name": "Jose Manuel Cordero Garcia",
            "type": "sparse",
            "x": 0.0046666375320284755,
            "y": 0.35076708389406214
        },
        {
            "id": "s_318",
            "name": "Brian D. Ondov",
            "type": "sparse",
            "x": -0.003177045397652801,
            "y": -0.30051370302714325
        },
        {
            "id": "s_319",
            "name": "Xin Chen",
            "type": "sparse",
            "x": 0.007735568231914728,
            "y": 0.08882340739542763
        },
        {
            "id": "s_320",
            "name": "Jaegul Choo",
            "type": "sparse",
            "x": 0.18760679666265745,
            "y": -0.2951268030215012
        },
        {
            "id": "s_321",
            "name": "Remco Chang",
            "type": "sparse",
            "x": -0.32490074574116956,
            "y": 0.03976381276941403
        },
        {
            "id": "s_322",
            "name": "Jean Krivine",
            "type": "sparse",
            "x": -0.8030942016755432,
            "y": 0.08605398853815001
        },
        {
            "id": "s_323",
            "name": "Mai Elshehaly",
            "type": "sparse",
            "x": 0.2463037666234805,
            "y": 0.0966415537259285
        },
        {
            "id": "s_324",
            "name": "Ian T. Ruginski",
            "type": "sparse",
            "x": -0.42714243590766265,
            "y": 0.39083552803249133
        },
        {
            "id": "s_325",
            "name": "Dieter W. Fellner",
            "type": "sparse",
            "x": -0.09504374129079242,
            "y": 0.049341096365618106
        },
        {
            "id": "s_326",
            "name": "Ren Liu",
            "type": "sparse",
            "x": 0.09456093104616013,
            "y": 0.2883535580211622
        },
        {
            "id": "s_327",
            "name": "Josef Focht",
            "type": "sparse",
            "x": -0.7977132415638911,
            "y": -0.4961776113501327
        },
        {
            "id": "s_328",
            "name": "Haipeng Zeng",
            "type": "sparse",
            "x": 0.15267630037753527,
            "y": -0.0030929892567516993
        },
        {
            "id": "s_329",
            "name": "Marlene Baumgart",
            "type": "sparse",
            "x": 0.5571319859902875,
            "y": 0.18343729133946063
        },
        {
            "id": "s_330",
            "name": "Alireza Karduni",
            "type": "sparse",
            "x": 0.39765459723173124,
            "y": -0.5215925683343394
        },
        {
            "id": "s_331",
            "name": "Guozheng Li 0002",
            "type": "sparse",
            "x": 0.25351423029313286,
            "y": 0.3240214215615255
        },
        {
            "id": "s_332",
            "name": "Victor Guallar",
            "type": "sparse",
            "x": -0.32968455572932526,
            "y": 0.12766428999680074
        },
        {
            "id": "s_333",
            "name": "Arthur van Goethem",
            "type": "sparse",
            "x": -0.31384131050147485,
            "y": 0.35788999679135786
        },
        {
            "id": "s_334",
            "name": "Charles Perin",
            "type": "sparse",
            "x": -0.19926049911026444,
            "y": -0.10588838214082637
        },
        {
            "id": "s_335",
            "name": "Mike Sips",
            "type": "sparse",
            "x": -0.6562861933053621,
            "y": 0.1499552860259028
        },
        {
            "id": "s_336",
            "name": "Markus Hadwiger",
            "type": "sparse",
            "x": -0.19613430927146475,
            "y": -0.30617214875634935
        },
        {
            "id": "s_337",
            "name": "Alexander Rind",
            "type": "sparse",
            "x": -0.14935852154489893,
            "y": -0.5116555517260578
        },
        {
            "id": "s_338",
            "name": "Siyuan Liu",
            "type": "sparse",
            "x": -0.014951863622338406,
            "y": 0.20423249147570807
        },
        {
            "id": "s_339",
            "name": "Wesley Willett",
            "type": "sparse",
            "x": -0.2638187092025808,
            "y": -0.176438663386993
        },
        {
            "id": "s_340",
            "name": "Shusen Liu 0001",
            "type": "sparse",
            "x": 0.23115516020567908,
            "y": -0.24372550349668398
        },
        {
            "id": "s_341",
            "name": "Nils Gehlenborg",
            "type": "sparse",
            "x": -0.25488280928304113,
            "y": -0.04006237113848576
        },
        {
            "id": "s_342",
            "name": "Chris Bryan",
            "type": "sparse",
            "x": 0.40581509789412645,
            "y": 0.111805237346558
        },
        {
            "id": "s_343",
            "name": "Chris R. Johnson 0001",
            "type": "sparse",
            "x": 0.131975539446887,
            "y": -0.3660639241240999
        },
        {
            "id": "s_344",
            "name": "Daniel Cohen-Or",
            "type": "sparse",
            "x": 0.4072211325870857,
            "y": 0.1289153840010136
        },
        {
            "id": "s_345",
            "name": "J. Luc Peterson",
            "type": "sparse",
            "x": 0.2269921756931952,
            "y": -0.2726230868565696
        },
        {
            "id": "s_346",
            "name": "Anqi Cao",
            "type": "sparse",
            "x": 0.020469942725274227,
            "y": 0.31820111763504677
        },
        {
            "id": "s_347",
            "name": "Timothy Luciani",
            "type": "sparse",
            "x": -0.7636927821245639,
            "y": 0.15174962019283955
        },
        {
            "id": "s_348",
            "name": "Xinnan Du",
            "type": "sparse",
            "x": 0.033357210671133265,
            "y": 0.15942005640955625
        },
        {
            "id": "s_349",
            "name": "Luis Gustavo Nonato",
            "type": "sparse",
            "x": 0.15734882355273566,
            "y": 0.3194715603541888
        },
        {
            "id": "s_350",
            "name": "Fr\u00e9d\u00e9ric Vernier",
            "type": "sparse",
            "x": -0.22514308326751103,
            "y": -0.1352609909056529
        },
        {
            "id": "s_351",
            "name": "Haidong Zhang",
            "type": "sparse",
            "x": 0.2559107047931694,
            "y": 0.026377862874561786
        },
        {
            "id": "s_352",
            "name": "Constance May Bainbridge",
            "type": "sparse",
            "x": -0.3823254462252273,
            "y": -0.17965822607448298
        },
        {
            "id": "s_353",
            "name": "Han-Wei Shen",
            "type": "sparse",
            "x": 0.3512209283021013,
            "y": 0.17332535306704352
        },
        {
            "id": "s_354",
            "name": "Naveen Pitipornvivat",
            "type": "sparse",
            "x": 0.224530104248561,
            "y": 0.09418483822335208
        },
        {
            "id": "s_355",
            "name": "Charles Rozhon",
            "type": "sparse",
            "x": 0.5795187056919047,
            "y": 0.09250883193585285
        },
        {
            "id": "s_356",
            "name": "Eric D. Ragan",
            "type": "sparse",
            "x": 0.3658435332535422,
            "y": -0.20942966600799348
        },
        {
            "id": "s_357",
            "name": "Pierre J. Magistretti",
            "type": "sparse",
            "x": -0.2717040802172434,
            "y": -0.2681397224345998
        },
        {
            "id": "s_358",
            "name": "Darren Treanor",
            "type": "sparse",
            "x": -0.4327328152122133,
            "y": 0.3654956838645984
        },
        {
            "id": "s_359",
            "name": "Shoubin Cheng",
            "type": "sparse",
            "x": 0.04821194844533682,
            "y": 0.32677042861276523
        },
        {
            "id": "s_360",
            "name": "Carolyn L. Phillips",
            "type": "sparse",
            "x": 0.1640002614993556,
            "y": 0.40168925664695515
        },
        {
            "id": "s_361",
            "name": "Jiannan Xiao",
            "type": "sparse",
            "x": 0.08697430611737336,
            "y": 0.5336729252777225
        },
        {
            "id": "s_362",
            "name": "Di Wang",
            "type": "sparse",
            "x": 0.21869844560432225,
            "y": -0.2648513405572329
        },
        {
            "id": "s_363",
            "name": "Raffaele De Simone",
            "type": "sparse",
            "x": -0.3859578874675225,
            "y": -0.4559428996785269
        },
        {
            "id": "s_364",
            "name": "Shuai Chen 0001",
            "type": "sparse",
            "x": 0.24671847814847034,
            "y": 0.2954543480315563
        },
        {
            "id": "s_365",
            "name": "Doug Fritz",
            "type": "sparse",
            "x": 0.0009389245805122428,
            "y": -0.5858878124449289
        },
        {
            "id": "s_366",
            "name": "Jinsong Wang",
            "type": "sparse",
            "x": 0.21192147175829829,
            "y": 0.3186236405203102
        },
        {
            "id": "s_367",
            "name": "Helwig Hauser",
            "type": "sparse",
            "x": -0.017605934503261325,
            "y": -0.055137635774388925
        },
        {
            "id": "s_368",
            "name": "Jingmin Chen",
            "type": "sparse",
            "x": 0.07229617573879898,
            "y": 0.31632228887511477
        },
        {
            "id": "s_369",
            "name": "Stefan Jordan",
            "type": "sparse",
            "x": 0.2817209951340236,
            "y": 0.31718942655244625
        },
        {
            "id": "s_370",
            "name": "Gordon Woodhull",
            "type": "sparse",
            "x": 0.09525277152615498,
            "y": 0.022484723340427726
        },
        {
            "id": "s_371",
            "name": "Lorenz Linhardt",
            "type": "sparse",
            "x": -0.049316416072005556,
            "y": -0.31465555680954777
        },
        {
            "id": "s_372",
            "name": "Christian Tominski",
            "type": "sparse",
            "x": -0.17921924732323644,
            "y": -0.47433824972194805
        },
        {
            "id": "s_373",
            "name": "Theophanis Tsandilas",
            "type": "sparse",
            "x": 0.09702550964449606,
            "y": -0.42356365930222445
        },
        {
            "id": "s_374",
            "name": "Shouxing Xiang",
            "type": "sparse",
            "x": 0.1114845460602849,
            "y": 0.48442879043347786
        },
        {
            "id": "s_375",
            "name": "Krist Wongsuphasawat",
            "type": "sparse",
            "x": -0.020286410127432444,
            "y": -0.32257358900550814
        },
        {
            "id": "s_376",
            "name": "Dana Higgins",
            "type": "sparse",
            "x": -0.279007027925904,
            "y": -0.1909146585415899
        },
        {
            "id": "s_377",
            "name": "Noa Fish",
            "type": "sparse",
            "x": 0.40313772821839094,
            "y": 0.12605097033301454
        },
        {
            "id": "s_378",
            "name": "Jiewen Lai",
            "type": "sparse",
            "x": 0.07229166939674661,
            "y": 0.10222661887503517
        },
        {
            "id": "s_379",
            "name": "Mosab Khayat",
            "type": "sparse",
            "x": 0.17177012977622022,
            "y": 0.4802479739701269
        },
        {
            "id": "s_380",
            "name": "Cl\u00e1udio T. Silva",
            "type": "sparse",
            "x": -0.0983131717667435,
            "y": 0.3509709047265305
        },
        {
            "id": "s_381",
            "name": "Derek Xiaoyu Wang",
            "type": "sparse",
            "x": 0.38724435598645246,
            "y": -0.49685426862769666
        },
        {
            "id": "s_382",
            "name": "Artem Amirkhanov",
            "type": "sparse",
            "x": -0.15847043084277618,
            "y": -0.23749574373355078
        },
        {
            "id": "s_383",
            "name": "Fabian Beck 0001",
            "type": "sparse",
            "x": 0.11701276133654558,
            "y": -0.23842119001338677
        },
        {
            "id": "s_384",
            "name": "Louis Eveillard",
            "type": "sparse",
            "x": -0.2948493275690802,
            "y": -0.03595357677675687
        },
        {
            "id": "s_385",
            "name": "Hessam Sokooti",
            "type": "sparse",
            "x": -0.2008476407306473,
            "y": -0.39773391153347354
        },
        {
            "id": "s_386",
            "name": "Dirk Streeb",
            "type": "sparse",
            "x": 0.06871120126180383,
            "y": -0.005775688837240433
        },
        {
            "id": "s_387",
            "name": "Johannes Fuchs 0001",
            "type": "sparse",
            "x": 0.05904648544977299,
            "y": 0.03357065281771583
        },
        {
            "id": "s_388",
            "name": "Won-Ki Jeong",
            "type": "sparse",
            "x": -0.3342517876077427,
            "y": -0.05491563138705314
        },
        {
            "id": "s_389",
            "name": "Brian Summa",
            "type": "sparse",
            "x": 0.60637584305651,
            "y": -0.38847964000432894
        },
        {
            "id": "s_390",
            "name": "Steffen Koch 0001",
            "type": "sparse",
            "x": 0.10905332787200261,
            "y": -0.3335877695443252
        },
        {
            "id": "s_391",
            "name": "Ivan Koles\u00e1r",
            "type": "sparse",
            "x": -0.04884669869790826,
            "y": -0.1733361712318129
        },
        {
            "id": "s_392",
            "name": "Simone Kriglstein",
            "type": "sparse",
            "x": -0.1723669657115019,
            "y": -0.5560261535721654
        },
        {
            "id": "s_393",
            "name": "Zhimin Li",
            "type": "sparse",
            "x": 0.2671745225930747,
            "y": -0.2556187221268788
        },
        {
            "id": "s_394",
            "name": "Simon J. Walton",
            "type": "sparse",
            "x": -0.3529813399711574,
            "y": -0.16376836434744363
        },
        {
            "id": "s_395",
            "name": "Udo Schlegel",
            "type": "sparse",
            "x": 0.013912275106513234,
            "y": 0.07474165716281117
        },
        {
            "id": "s_396",
            "name": "Ivan Barisic",
            "type": "sparse",
            "x": -0.09490161156956255,
            "y": -0.23726705373177628
        },
        {
            "id": "s_397",
            "name": "P. Samuel Quinan",
            "type": "sparse",
            "x": -0.39308516204803845,
            "y": 0.3306506815538341
        },
        {
            "id": "s_398",
            "name": "Tan Tang",
            "type": "sparse",
            "x": 0.07672459324174241,
            "y": 0.10295858469920972
        },
        {
            "id": "s_399",
            "name": "Marco Hutter 0002",
            "type": "sparse",
            "x": -0.09828075764421428,
            "y": 0.040033269112185084
        },
        {
            "id": "s_400",
            "name": "Shunan Guo",
            "type": "sparse",
            "x": 0.33555547028055943,
            "y": 0.07248554057505166
        },
        {
            "id": "s_401",
            "name": "Kelly M. T. Huffer",
            "type": "sparse",
            "x": 0.43195663238480236,
            "y": -0.24354292354756923
        },
        {
            "id": "s_402",
            "name": "Daniel Archambault",
            "type": "sparse",
            "x": 0.12607657525501295,
            "y": 0.13985318036462105
        },
        {
            "id": "s_403",
            "name": "Qianwen Wang",
            "type": "sparse",
            "x": 0.18915708579938884,
            "y": 0.14854594325849116
        },
        {
            "id": "s_404",
            "name": "Yating Wei",
            "type": "sparse",
            "x": 0.18238769181663833,
            "y": 0.3350736786001391
        },
        {
            "id": "s_405",
            "name": "Tim Lammarsch",
            "type": "sparse",
            "x": -0.13874047891241043,
            "y": -0.5010626006267023
        },
        {
            "id": "s_406",
            "name": "Denis M. Larkin",
            "type": "sparse",
            "x": 0.4816292753012008,
            "y": 0.11400521038239585
        },
        {
            "id": "s_407",
            "name": "Hossein Siadati",
            "type": "sparse",
            "x": 0.187556584480402,
            "y": -0.12493200382853004
        },
        {
            "id": "s_408",
            "name": "Anna Vilanova",
            "type": "sparse",
            "x": -0.1750461786856584,
            "y": -0.4433723830363273
        },
        {
            "id": "s_409",
            "name": "Jos\u00e9 Matute",
            "type": "sparse",
            "x": -0.27351021582967194,
            "y": 0.5437693796350418
        },
        {
            "id": "s_410",
            "name": "Thomas M\u00fchlbacher",
            "type": "sparse",
            "x": -0.0639999107009779,
            "y": -0.3306021048703913
        },
        {
            "id": "s_411",
            "name": "Yuanzhe Chen",
            "type": "sparse",
            "x": 0.20864150526272168,
            "y": 0.1542637425446473
        },
        {
            "id": "s_412",
            "name": "Zachary Leggon",
            "type": "sparse",
            "x": 0.18485210968518423,
            "y": 0.45072567329664553
        },
        {
            "id": "s_413",
            "name": "Daniel Orban",
            "type": "sparse",
            "x": 0.26171789591641914,
            "y": -0.21265007409085213
        },
        {
            "id": "s_414",
            "name": "Omniah H. Nagoor",
            "type": "sparse",
            "x": -0.2146775456391829,
            "y": -0.4141491640011679
        },
        {
            "id": "s_415",
            "name": "Lorna Role",
            "type": "sparse",
            "x": 0.39212615542213397,
            "y": 0.7627338840142309
        },
        {
            "id": "s_416",
            "name": "Sriram Karthik Badam",
            "type": "sparse",
            "x": -0.10504270597837259,
            "y": -0.18385180338379115
        },
        {
            "id": "s_417",
            "name": "Luke S. Snyder",
            "type": "sparse",
            "x": 0.19067702001543171,
            "y": 0.4774788154986561
        },
        {
            "id": "s_418",
            "name": "Peter Kerpedjiev",
            "type": "sparse",
            "x": -0.2501190788370702,
            "y": -0.035037119546103855
        },
        {
            "id": "s_419",
            "name": "Alexander Mordvintsev",
            "type": "sparse",
            "x": -0.14495825045359417,
            "y": -0.5138833005782525
        },
        {
            "id": "s_420",
            "name": "Hongye Liang",
            "type": "sparse",
            "x": 0.053716020774536945,
            "y": 0.32504186298072857
        },
        {
            "id": "s_421",
            "name": "Kyle Wm. Hall",
            "type": "sparse",
            "x": -0.229479581577149,
            "y": 0.0640015572236837
        },
        {
            "id": "s_422",
            "name": "Clifton D. Fuller",
            "type": "sparse",
            "x": -0.7889212111613879,
            "y": 0.1720459417684753
        },
        {
            "id": "s_423",
            "name": "John R. Goodall",
            "type": "sparse",
            "x": 0.42877608695732994,
            "y": -0.2485406173626806
        },
        {
            "id": "s_424",
            "name": "Mehmet Adil Yal\u00e7in",
            "type": "sparse",
            "x": 0.018590236841974663,
            "y": -0.35561425595867074
        },
        {
            "id": "s_425",
            "name": "Chufan Lai",
            "type": "sparse",
            "x": 0.3442922288534526,
            "y": 0.2395697850600933
        },
        {
            "id": "s_426",
            "name": "Niklas Weiler",
            "type": "sparse",
            "x": 0.06014235214073826,
            "y": 0.033512756310626335
        },
        {
            "id": "s_427",
            "name": "Alice Thudt",
            "type": "sparse",
            "x": -0.2765018536979557,
            "y": -0.004136609416247697
        },
        {
            "id": "s_428",
            "name": "Mukund Raj",
            "type": "sparse",
            "x": 0.2679303630987619,
            "y": 0.2775885321403115
        },
        {
            "id": "s_429",
            "name": "Min-Je Choi",
            "type": "sparse",
            "x": 0.1989757387204453,
            "y": -0.2279006035560583
        },
        {
            "id": "s_430",
            "name": "Mayya Komisarchik",
            "type": "sparse",
            "x": -0.2853227643913692,
            "y": -0.19989285151506528
        },
        {
            "id": "s_431",
            "name": "Huihua Guan",
            "type": "sparse",
            "x": 0.32215526007994255,
            "y": 0.18856596200471337
        },
        {
            "id": "s_432",
            "name": "Dilip Krishnan",
            "type": "sparse",
            "x": 0.0027113412031488206,
            "y": -0.5808582315583695
        },
        {
            "id": "s_433",
            "name": "Jeremy Boy",
            "type": "sparse",
            "x": -0.26884062620243915,
            "y": -0.010977396354014384
        },
        {
            "id": "s_434",
            "name": "Zeyu Wang 0005",
            "type": "sparse",
            "x": 0.01116008608299604,
            "y": -0.008886984061857216
        },
        {
            "id": "s_435",
            "name": "Sebastian Baltes",
            "type": "sparse",
            "x": 0.1001126947192557,
            "y": -0.2787382307091899
        },
        {
            "id": "s_436",
            "name": "David Borland",
            "type": "sparse",
            "x": 0.43872183007208465,
            "y": 0.044517798660915875
        },
        {
            "id": "s_437",
            "name": "Kevin Tate",
            "type": "sparse",
            "x": 0.08543641707537128,
            "y": -0.21380111320801107
        },
        {
            "id": "s_438",
            "name": "Peer-Timo Bremer",
            "type": "sparse",
            "x": 0.2566392059262226,
            "y": -0.24870929649003068
        },
        {
            "id": "s_439",
            "name": "Sean Follmer",
            "type": "sparse",
            "x": -0.21614649355677396,
            "y": -0.13017554016944682
        },
        {
            "id": "s_440",
            "name": "Alexandre X. Falc\u00e3o",
            "type": "sparse",
            "x": -0.27024152918799765,
            "y": 0.501802736006057
        },
        {
            "id": "s_441",
            "name": "Guowei Huang 0002",
            "type": "sparse",
            "x": 0.22414594535092916,
            "y": 0.09970990606886526
        },
        {
            "id": "s_442",
            "name": "Peter K. Sorger",
            "type": "sparse",
            "x": -0.3055910553133935,
            "y": -0.17523268411948997
        },
        {
            "id": "s_443",
            "name": "Ivan Viola",
            "type": "sparse",
            "x": -0.07768991123812542,
            "y": -0.1713023057755865
        },
        {
            "id": "s_444",
            "name": "Jayaraman J. Thiagarajan",
            "type": "sparse",
            "x": 0.22280024206324556,
            "y": -0.24408227326101822
        },
        {
            "id": "s_445",
            "name": "Alexander M. Rush",
            "type": "sparse",
            "x": -0.20772438984058172,
            "y": -0.10364106862958739
        },
        {
            "id": "s_446",
            "name": "Asher Pembroke",
            "type": "sparse",
            "x": -0.3574991302525065,
            "y": 0.3061040159852945
        },
        {
            "id": "s_447",
            "name": "Xiaolong Luke Zhang",
            "type": "sparse",
            "x": 0.28708749064664957,
            "y": 0.3302571631597687
        },
        {
            "id": "s_448",
            "name": "Josua Krause",
            "type": "sparse",
            "x": 0.02126815433800481,
            "y": -0.015093473048590877
        },
        {
            "id": "s_449",
            "name": "Tobias Klein",
            "type": "sparse",
            "x": -0.09170154362547403,
            "y": -0.1484911148594907
        },
        {
            "id": "s_450",
            "name": "Aditeya Pandey",
            "type": "sparse",
            "x": -0.48971910246574135,
            "y": -0.2546562122250256
        },
        {
            "id": "s_451",
            "name": "Lutz Rastaetter",
            "type": "sparse",
            "x": -0.35861096707523904,
            "y": 0.3113081583395552
        },
        {
            "id": "s_452",
            "name": "Ashley Suh 0001",
            "type": "sparse",
            "x": 0.10770504636993504,
            "y": -0.028136052247151447
        },
        {
            "id": "s_453",
            "name": "Yanhong Wu",
            "type": "sparse",
            "x": 0.13220613607564596,
            "y": 0.07851364112125156
        },
        {
            "id": "s_454",
            "name": "Chaoguang Lin",
            "type": "sparse",
            "x": 0.1588057143849703,
            "y": 0.11514005025651594
        },
        {
            "id": "s_455",
            "name": "Sanjeev Balakrishnan",
            "type": "sparse",
            "x": 0.08196390266092442,
            "y": -0.16802294602809364
        },
        {
            "id": "s_456",
            "name": "Phillip J. Wolfram",
            "type": "sparse",
            "x": 0.17175869640087274,
            "y": -0.36651684101100324
        },
        {
            "id": "s_457",
            "name": "Enrico Bertini",
            "type": "sparse",
            "x": 0.12293804597944213,
            "y": -0.06092432590610467
        },
        {
            "id": "s_458",
            "name": "Lezhi Li",
            "type": "sparse",
            "x": 0.029499345659118006,
            "y": 0.23617434070673432
        },
        {
            "id": "s_459",
            "name": "G. Elisabeta Marai",
            "type": "sparse",
            "x": -0.770981772564674,
            "y": 0.15153090444681572
        },
        {
            "id": "s_460",
            "name": "Peter B. Robinson",
            "type": "sparse",
            "x": 0.2227555500774135,
            "y": -0.27331924057146245
        },
        {
            "id": "s_461",
            "name": "Ragini Rathore",
            "type": "sparse",
            "x": 0.18199085440072157,
            "y": 0.44548465432138895
        },
        {
            "id": "s_462",
            "name": "Vidya Setlur",
            "type": "sparse",
            "x": 0.19358309538876906,
            "y": 0.0025428265225727075
        },
        {
            "id": "s_463",
            "name": "Katherine Coles",
            "type": "sparse",
            "x": -0.40473123039719644,
            "y": 0.26308327459618824
        },
        {
            "id": "s_464",
            "name": "Lyndsey Franklin",
            "type": "sparse",
            "x": -0.045006533921485764,
            "y": -0.14489405708117256
        },
        {
            "id": "s_465",
            "name": "Paolo Federico 0001",
            "type": "sparse",
            "x": -0.1417846693361554,
            "y": -0.5191894677176947
        },
        {
            "id": "s_466",
            "name": "Francine Chen 0001",
            "type": "sparse",
            "x": 0.3897767561125084,
            "y": 0.19140080188303196
        },
        {
            "id": "s_467",
            "name": "Osamu Saeki",
            "type": "sparse",
            "x": 0.14233750862251102,
            "y": -0.17586759849497788
        },
        {
            "id": "s_468",
            "name": "Claudio Delrieux",
            "type": "sparse",
            "x": 0.17713326308728666,
            "y": -0.03522364829874977
        },
        {
            "id": "s_469",
            "name": "Paul Rosen 0001",
            "type": "sparse",
            "x": 0.10475535346340621,
            "y": -0.03311623168584261
        },
        {
            "id": "s_470",
            "name": "Wouter Meulemans",
            "type": "sparse",
            "x": -0.17927906092915008,
            "y": 0.20995866121786613
        },
        {
            "id": "s_471",
            "name": "Xiaotong Liu",
            "type": "sparse",
            "x": 0.3800253245829487,
            "y": 0.12491305360791773
        },
        {
            "id": "s_472",
            "name": "Wolfgang Aigner",
            "type": "sparse",
            "x": -0.1571255789305974,
            "y": -0.48804311490038715
        },
        {
            "id": "s_473",
            "name": "Frederik L. Dennig",
            "type": "sparse",
            "x": -0.1455516411170584,
            "y": 0.11522105175881667
        },
        {
            "id": "s_474",
            "name": "Adam M. Smith 0001",
            "type": "sparse",
            "x": -0.05206738885802627,
            "y": -0.40628753876828594
        },
        {
            "id": "s_475",
            "name": "Christine Nothelfer",
            "type": "sparse",
            "x": 0.03172619508233264,
            "y": -0.44627328360286267
        },
        {
            "id": "s_476",
            "name": "Mengchen Liu",
            "type": "sparse",
            "x": 0.14377991319414385,
            "y": 0.4930135613361752
        },
        {
            "id": "s_477",
            "name": "Quan Li",
            "type": "sparse",
            "x": 0.18156298116330935,
            "y": -0.02886638350379529
        },
        {
            "id": "s_478",
            "name": "Emil Axelsson",
            "type": "sparse",
            "x": -0.28161342037522874,
            "y": 0.39456744607355526
        },
        {
            "id": "s_479",
            "name": "Minghui Chen",
            "type": "sparse",
            "x": 0.123380869189545,
            "y": 0.3003829763200194
        },
        {
            "id": "s_480",
            "name": "Mahima Pushkarna",
            "type": "sparse",
            "x": 0.015719631809589584,
            "y": -0.6409849229457574
        },
        {
            "id": "s_481",
            "name": "Holger Steeb",
            "type": "sparse",
            "x": 0.1732914758044825,
            "y": -0.4892152938369781
        },
        {
            "id": "s_482",
            "name": "Chelsea S. Yeh",
            "type": "sparse",
            "x": -0.38736746750749457,
            "y": -0.18119717425923612
        },
        {
            "id": "s_483",
            "name": "Fereshteh Amini",
            "type": "sparse",
            "x": -0.1288401202989145,
            "y": -0.10080585105037485
        },
        {
            "id": "s_484",
            "name": "Zhifang Jiang",
            "type": "sparse",
            "x": -0.7544262315167855,
            "y": -0.14172658636288443
        },
        {
            "id": "s_485",
            "name": "Xizhou Zhu",
            "type": "sparse",
            "x": 0.14234563302042114,
            "y": 0.5709086349496489
        },
        {
            "id": "s_486",
            "name": "Jan C. van Gemert",
            "type": "sparse",
            "x": -0.13155795707596007,
            "y": -0.5139571953182811
        },
        {
            "id": "s_487",
            "name": "Jordan Swartz",
            "type": "sparse",
            "x": 0.06692938760094479,
            "y": 0.015217497916621869
        },
        {
            "id": "s_488",
            "name": "Tengfei Ma 0001",
            "type": "sparse",
            "x": 0.2101367770613616,
            "y": 0.12721192942532547
        },
        {
            "id": "s_489",
            "name": "Harris A. Lewin",
            "type": "sparse",
            "x": 0.48432880601251993,
            "y": 0.12595446373886024
        },
        {
            "id": "s_490",
            "name": "Cicero Augusto de Lara Pahins",
            "type": "sparse",
            "x": -0.11686973148170535,
            "y": 0.17632217140468384
        },
        {
            "id": "s_491",
            "name": "Robin Sk\u00e5nberg",
            "type": "sparse",
            "x": -0.3606482419724556,
            "y": 0.1428887596895376
        },
        {
            "id": "s_492",
            "name": "Heidi Lam",
            "type": "sparse",
            "x": 0.1929998423613998,
            "y": -0.1614808279020647
        },
        {
            "id": "s_493",
            "name": "Tobias Schreck",
            "type": "sparse",
            "x": -0.10245713636368117,
            "y": 0.07197643100554793
        },
        {
            "id": "s_494",
            "name": "Xin Tong",
            "type": "sparse",
            "x": 0.5122536943697711,
            "y": 0.24800546502314158
        },
        {
            "id": "s_495",
            "name": "Marc Streit",
            "type": "sparse",
            "x": -0.15068196255339705,
            "y": -0.3724745804383008
        },
        {
            "id": "s_496",
            "name": "Hsiang-Yun Wu",
            "type": "sparse",
            "x": 0.05623262979304865,
            "y": -0.1548033172586766
        },
        {
            "id": "s_497",
            "name": "Susannah Burrows",
            "type": "sparse",
            "x": 0.02225687971984777,
            "y": 0.22955151391779627
        },
        {
            "id": "s_498",
            "name": "Juraj P\u00e1lenik",
            "type": "sparse",
            "x": -0.06771297032474052,
            "y": -0.1574836228856269
        },
        {
            "id": "s_499",
            "name": "Jason Dykes",
            "type": "sparse",
            "x": -0.23855920584500498,
            "y": 0.2293552133838997
        },
        {
            "id": "s_500",
            "name": "Bei Chen",
            "type": "sparse",
            "x": 0.2641413355542238,
            "y": 0.013641572493783866
        },
        {
            "id": "s_501",
            "name": "Pepe Eulzer",
            "type": "sparse",
            "x": -0.3945559779150527,
            "y": -0.4544925093167642
        },
        {
            "id": "s_502",
            "name": "Robert M. Kirby",
            "type": "sparse",
            "x": 0.6995330017828533,
            "y": -0.4755089339973676
        },
        {
            "id": "s_503",
            "name": "David S. Goodsell",
            "type": "sparse",
            "x": -0.056833492507370124,
            "y": -0.13223548058242038
        },
        {
            "id": "s_504",
            "name": "Vahan Yoghourdjian",
            "type": "sparse",
            "x": -0.3483663868740235,
            "y": 0.06223707427145767
        },
        {
            "id": "s_505",
            "name": "Jaakko Peltonen",
            "type": "sparse",
            "x": 0.009996237301735338,
            "y": -0.01962049245935011
        },
        {
            "id": "s_506",
            "name": "Zhaosong Huang",
            "type": "sparse",
            "x": 0.11410881983394784,
            "y": 0.4947846512966484
        },
        {
            "id": "s_507",
            "name": "Yifang Wang 0001",
            "type": "sparse",
            "x": 0.29456974246494955,
            "y": 0.020070511593177336
        },
        {
            "id": "s_508",
            "name": "Richard Alligier",
            "type": "sparse",
            "x": -0.1764908921822369,
            "y": 0.05832541377448515
        },
        {
            "id": "s_509",
            "name": "Emmanuel Pietriga",
            "type": "sparse",
            "x": 0.125403708546386,
            "y": -0.4151491671774158
        },
        {
            "id": "s_510",
            "name": "John Aldo Lee",
            "type": "sparse",
            "x": 0.0045118837674053645,
            "y": -0.02109827550922795
        },
        {
            "id": "s_511",
            "name": "Caleb Robinson",
            "type": "sparse",
            "x": 0.1169319599575497,
            "y": -0.6423526579323944
        },
        {
            "id": "s_512",
            "name": "Andreas Kerren",
            "type": "sparse",
            "x": 0.023491468588863074,
            "y": 0.36899733838716064
        },
        {
            "id": "s_513",
            "name": "Steve Petruzza",
            "type": "sparse",
            "x": 0.37718228470176346,
            "y": -0.30646956907474115
        },
        {
            "id": "s_514",
            "name": "Johannes G\u00fcnther 0001",
            "type": "sparse",
            "x": 0.32600906194969165,
            "y": -0.3268274425006493
        },
        {
            "id": "s_515",
            "name": "Eugen Zizer",
            "type": "sparse",
            "x": -0.45203608409574497,
            "y": 0.18845718191031344
        },
        {
            "id": "s_516",
            "name": "Xue Wu",
            "type": "sparse",
            "x": 0.4938792264441461,
            "y": 0.10485183095091312
        },
        {
            "id": "s_517",
            "name": "Amin Abbasloo",
            "type": "sparse",
            "x": -0.28457843583080844,
            "y": -0.6643947988987297
        },
        {
            "id": "s_518",
            "name": "Rebecca Faust",
            "type": "sparse",
            "x": -0.10178344420648408,
            "y": 0.18350131543017328
        },
        {
            "id": "s_519",
            "name": "M. Eduard Gr\u00f6ller",
            "type": "sparse",
            "x": -0.06627920097888605,
            "y": -0.20073266774926302
        },
        {
            "id": "s_520",
            "name": "Vivian Trakinski",
            "type": "sparse",
            "x": -0.25673387915243534,
            "y": 0.39451275224926236
        },
        {
            "id": "s_521",
            "name": "Johannes Sorger",
            "type": "sparse",
            "x": -0.08988461939602231,
            "y": -0.25574180083627973
        },
        {
            "id": "s_522",
            "name": "Hui Zhang 0027",
            "type": "sparse",
            "x": 0.17114191836847883,
            "y": -0.48304704302239754
        },
        {
            "id": "s_523",
            "name": "Deborah R. Wahl",
            "type": "sparse",
            "x": -0.1936665405353766,
            "y": 0.10968117743172882
        },
        {
            "id": "s_524",
            "name": "Michael Michaux",
            "type": "sparse",
            "x": 0.6076047253762846,
            "y": -0.40277761756621683
        },
        {
            "id": "s_525",
            "name": "Kejian Zhao",
            "type": "sparse",
            "x": 0.015304059049164917,
            "y": 0.2956303632917729
        },
        {
            "id": "s_526",
            "name": "Sheizaf Rafaeli",
            "type": "sparse",
            "x": 0.5105246686811281,
            "y": 0.06748636790979387
        },
        {
            "id": "s_527",
            "name": "Fangzhou Guo",
            "type": "sparse",
            "x": 0.21698202987550191,
            "y": 0.4048951719790397
        },
        {
            "id": "s_528",
            "name": "Martin Luboschik",
            "type": "sparse",
            "x": -0.0860417308710233,
            "y": -0.37686989813174404
        },
        {
            "id": "s_529",
            "name": "Carlos Scheidegger",
            "type": "sparse",
            "x": -0.02384282008098218,
            "y": 0.09898064172985331
        },
        {
            "id": "s_530",
            "name": "Chaoli Wang 0001",
            "type": "sparse",
            "x": 0.10389888015400946,
            "y": 0.437023090141482
        },
        {
            "id": "s_531",
            "name": "Alessandra Angelucci",
            "type": "sparse",
            "x": 0.29462214992527735,
            "y": -0.30801841938285346
        },
        {
            "id": "s_532",
            "name": "Yarden Livnat",
            "type": "sparse",
            "x": 0.2413285955383023,
            "y": -0.21367599555396619
        },
        {
            "id": "s_533",
            "name": "Christoph Schulz 0001",
            "type": "sparse",
            "x": 0.04104880444874902,
            "y": -0.013117567763267866
        },
        {
            "id": "s_534",
            "name": "Graeme Gange",
            "type": "sparse",
            "x": -0.348615319358304,
            "y": 0.059404473121049746
        },
        {
            "id": "s_535",
            "name": "Oliver van Kaick",
            "type": "sparse",
            "x": 0.2808960790284332,
            "y": 0.08837600982718626
        },
        {
            "id": "s_536",
            "name": "Ashok Jallepalli",
            "type": "sparse",
            "x": 0.6956498131665925,
            "y": -0.47913315570104464
        },
        {
            "id": "s_537",
            "name": "Joshua Shrestha",
            "type": "sparse",
            "x": 0.4391544660365899,
            "y": 0.03316217164582332
        },
        {
            "id": "s_538",
            "name": "Pourang Irani",
            "type": "sparse",
            "x": -0.1312424502619003,
            "y": -0.10229161551865715
        },
        {
            "id": "s_539",
            "name": "Seung Hyun Kim",
            "type": "sparse",
            "x": 0.12644260618808859,
            "y": 0.4204127521718928
        },
        {
            "id": "s_540",
            "name": "Michael Behrisch 0001",
            "type": "sparse",
            "x": -0.16329464510066535,
            "y": 0.02966837341274698
        },
        {
            "id": "s_541",
            "name": "Jules Vidal",
            "type": "sparse",
            "x": 0.6203029633136765,
            "y": -0.37444380237118763
        },
        {
            "id": "s_542",
            "name": "He Huang",
            "type": "sparse",
            "x": 0.2586580966743739,
            "y": 0.02237250088188785
        },
        {
            "id": "s_543",
            "name": "Jinson Zhang",
            "type": "sparse",
            "x": 0.2526897132101974,
            "y": 0.3147338698930156
        },
        {
            "id": "s_544",
            "name": "Zhan Guo",
            "type": "sparse",
            "x": -0.15547996751818896,
            "y": 0.48803621900649924
        },
        {
            "id": "s_545",
            "name": "Michael Hund",
            "type": "sparse",
            "x": -0.1574892836999876,
            "y": 0.04485116401126508
        },
        {
            "id": "s_546",
            "name": "Chaitanya Chandurkar",
            "type": "sparse",
            "x": 0.386521593023531,
            "y": 0.3589169641902902
        },
        {
            "id": "s_547",
            "name": "Bingru Lin",
            "type": "sparse",
            "x": 0.18514795736581877,
            "y": 0.3306333326624941
        },
        {
            "id": "s_548",
            "name": "Gene Payne",
            "type": "sparse",
            "x": -0.26098403530954123,
            "y": 0.3918682607333132
        },
        {
            "id": "s_549",
            "name": "Haejin Jeong",
            "type": "sparse",
            "x": -0.3830282010451283,
            "y": -0.0557729792001792
        },
        {
            "id": "s_550",
            "name": "Mathias Neugebauer",
            "type": "sparse",
            "x": -0.33491029111497495,
            "y": 0.6876822153592629
        },
        {
            "id": "s_551",
            "name": "Rafael Henkin",
            "type": "sparse",
            "x": 0.028724181320026177,
            "y": 0.27773962168207
        },
        {
            "id": "s_552",
            "name": "Hesham Elhalawani",
            "type": "sparse",
            "x": -0.7990260484987923,
            "y": 0.17086728774296903
        },
        {
            "id": "s_553",
            "name": "Konstantin Dmitriev",
            "type": "sparse",
            "x": 0.47175897906535724,
            "y": 0.8115243203921945
        },
        {
            "id": "s_554",
            "name": "Dominik Moritz",
            "type": "sparse",
            "x": -0.04089503925528753,
            "y": -0.39320986210123493
        },
        {
            "id": "s_555",
            "name": "Andrew T. Wilson",
            "type": "sparse",
            "x": 0.06797481822557033,
            "y": -0.42384983824362554
        },
        {
            "id": "s_556",
            "name": "Hui Zhang 0051",
            "type": "sparse",
            "x": 0.040017852254086925,
            "y": 0.28602243335129773
        },
        {
            "id": "s_557",
            "name": "Andre Schmei\u00dfer",
            "type": "sparse",
            "x": 0.3641263513406935,
            "y": -0.19107316650646386
        },
        {
            "id": "s_558",
            "name": "Dongming Han",
            "type": "sparse",
            "x": 0.22263561966788553,
            "y": 0.4118843112935791
        },
        {
            "id": "s_559",
            "name": "Jock D. Mackinlay",
            "type": "sparse",
            "x": -0.03756024731462868,
            "y": -0.430165406061617
        },
        {
            "id": "s_560",
            "name": "Aude Oliva",
            "type": "sparse",
            "x": -0.3843043891633319,
            "y": -0.18674640962947928
        },
        {
            "id": "s_561",
            "name": "Oh-Hyun Kwon",
            "type": "sparse",
            "x": 0.45477101392433256,
            "y": 0.04392676981970409
        },
        {
            "id": "s_562",
            "name": "Yuanzhe Hu",
            "type": "sparse",
            "x": 0.18469635642684493,
            "y": 0.34790046347749454
        },
        {
            "id": "s_563",
            "name": "Patrick Houthuizen",
            "type": "sparse",
            "x": 0.14359999660813058,
            "y": 0.6470109767350042
        },
        {
            "id": "s_564",
            "name": "Kevin A. Roundy",
            "type": "sparse",
            "x": 0.1093751832327125,
            "y": -0.4968558903783713
        },
        {
            "id": "s_565",
            "name": "Stephan Pajer",
            "type": "sparse",
            "x": -0.07803332119214994,
            "y": -0.31834925479730913
        },
        {
            "id": "s_566",
            "name": "Jeff Yarch",
            "type": "sparse",
            "x": 0.2922477966286122,
            "y": -0.311954962148779
        },
        {
            "id": "s_567",
            "name": "Muchan Park",
            "type": "sparse",
            "x": -0.1578907368014825,
            "y": 0.3453799604972943
        },
        {
            "id": "s_568",
            "name": "Xiaoli Qiao",
            "type": "sparse",
            "x": -0.1471097366120281,
            "y": -0.22403239674475547
        },
        {
            "id": "s_569",
            "name": "Duen Horng (Polo) Chau",
            "type": "sparse",
            "x": 0.08776411534212922,
            "y": -0.6537306180619281
        },
        {
            "id": "s_570",
            "name": "Paul Klemm",
            "type": "sparse",
            "x": -0.3309606432363667,
            "y": -0.3760153632747832
        },
        {
            "id": "s_571",
            "name": "Chun-Ming Chen",
            "type": "sparse",
            "x": 0.4314512243025848,
            "y": 0.16976534184955203
        },
        {
            "id": "s_572",
            "name": "Reinhard Klein",
            "type": "sparse",
            "x": -0.271561024253062,
            "y": -0.6729663508769812
        },
        {
            "id": "s_573",
            "name": "Cong Wu 0004",
            "type": "sparse",
            "x": 0.19848246887622312,
            "y": 0.28194283923655516
        },
        {
            "id": "s_574",
            "name": "Sean A. Stephens",
            "type": "sparse",
            "x": -0.10824378681987348,
            "y": 0.17928895614381346
        },
        {
            "id": "s_575",
            "name": "Johannes Zagermann",
            "type": "sparse",
            "x": -0.27287800518261157,
            "y": 0.15461752888328714
        },
        {
            "id": "s_576",
            "name": "Marcel Hlawatsch",
            "type": "sparse",
            "x": 0.08616642311951477,
            "y": -0.2137168090536911
        },
        {
            "id": "s_577",
            "name": "Chad A. Steed",
            "type": "sparse",
            "x": 0.4226393949831758,
            "y": -0.24067735128476345
        },
        {
            "id": "s_578",
            "name": "Michael Riemer",
            "type": "sparse",
            "x": 0.5377267576581697,
            "y": 0.1736344590996971
        },
        {
            "id": "s_579",
            "name": "Duong Hoang",
            "type": "sparse",
            "x": 0.25195936661402973,
            "y": -0.2969371143168221
        },
        {
            "id": "s_580",
            "name": "Yong Xu 0010",
            "type": "sparse",
            "x": 0.2915608106092078,
            "y": 0.023789029588448703
        },
        {
            "id": "s_581",
            "name": "Khai N. Truong",
            "type": "sparse",
            "x": 0.41503097938974665,
            "y": 0.0017680317582241536
        },
        {
            "id": "s_582",
            "name": "John E. Wenskovitch",
            "type": "sparse",
            "x": 0.44320218933236905,
            "y": 0.2703474667082793
        },
        {
            "id": "s_583",
            "name": "Holly A. Taylor",
            "type": "sparse",
            "x": -0.37176183275957053,
            "y": 0.1214857282772093
        },
        {
            "id": "s_584",
            "name": "Christopher S. Gates",
            "type": "sparse",
            "x": 0.11785624093076966,
            "y": -0.4944735234832965
        },
        {
            "id": "s_585",
            "name": "Antoni Sagrist\u00e0",
            "type": "sparse",
            "x": 0.2817390894347152,
            "y": 0.3164969712860464
        },
        {
            "id": "s_586",
            "name": "Julia Docampo-S\u00e1nchez",
            "type": "sparse",
            "x": 0.7133461485691157,
            "y": -0.4990740299847148
        },
        {
            "id": "s_587",
            "name": "Jundong Li",
            "type": "sparse",
            "x": 0.055032942679760305,
            "y": 0.44684870656792375
        },
        {
            "id": "s_588",
            "name": "Kartik Chanana",
            "type": "sparse",
            "x": -0.5609302411170328,
            "y": -0.3003602372869379
        },
        {
            "id": "s_589",
            "name": "Christophe Hurter",
            "type": "sparse",
            "x": -0.09766842364890765,
            "y": 0.19258237239391923
        },
        {
            "id": "s_590",
            "name": "Ali K. Al-Awami",
            "type": "sparse",
            "x": -0.2479392068293025,
            "y": -0.24795549163508468
        },
        {
            "id": "s_591",
            "name": "Maik Schulze",
            "type": "sparse",
            "x": -0.5410268417981198,
            "y": 0.0521322126724002
        },
        {
            "id": "s_592",
            "name": "Martin Falk",
            "type": "sparse",
            "x": -0.41556380232079715,
            "y": 0.3087362390539847
        },
        {
            "id": "s_593",
            "name": "Anzu Hakone",
            "type": "sparse",
            "x": -0.4075150209351347,
            "y": 0.08512988045736754
        },
        {
            "id": "s_594",
            "name": "Pedro Hermosilla",
            "type": "sparse",
            "x": -0.2847267017966387,
            "y": 0.09090203099452851
        },
        {
            "id": "s_595",
            "name": "Harald Reiterer",
            "type": "sparse",
            "x": -0.2249274432932251,
            "y": 0.13193370455700917
        },
        {
            "id": "s_596",
            "name": "Michael Delz",
            "type": "sparse",
            "x": -0.1878487394422849,
            "y": 0.01633952543301219
        },
        {
            "id": "s_597",
            "name": "Clemens Arbesser",
            "type": "sparse",
            "x": -0.09143442740822041,
            "y": -0.36023105570510017
        },
        {
            "id": "s_598",
            "name": "Johannes Weissenbock",
            "type": "sparse",
            "x": -0.13908641013152476,
            "y": -0.24268958280492986
        },
        {
            "id": "s_599",
            "name": "Anshul Vikram Pandey",
            "type": "sparse",
            "x": 0.18284122680839646,
            "y": -0.11398323782566387
        },
        {
            "id": "s_600",
            "name": "Ralph Wickenh\u00f6fer",
            "type": "sparse",
            "x": -0.40181072569646714,
            "y": -0.23714343929207415
        },
        {
            "id": "s_601",
            "name": "Chenyang Ji",
            "type": "sparse",
            "x": 0.0014046992745318768,
            "y": 0.2842874884586584
        },
        {
            "id": "s_602",
            "name": "Seyedkoosha Mirhosseini",
            "type": "sparse",
            "x": 0.45558770931391523,
            "y": 0.8849041088202378
        },
        {
            "id": "s_603",
            "name": "Johanna Beyer",
            "type": "sparse",
            "x": -0.2556306512154247,
            "y": -0.20317807852897204
        },
        {
            "id": "s_604",
            "name": "Dylan Cashman",
            "type": "sparse",
            "x": -0.22321926070399437,
            "y": -0.05162014273602844
        },
        {
            "id": "s_605",
            "name": "Thomas Schultz 0001",
            "type": "sparse",
            "x": -0.2580110379178263,
            "y": -0.589019890176503
        },
        {
            "id": "s_606",
            "name": "Sam Ade Jacobs",
            "type": "sparse",
            "x": 0.22981074272027927,
            "y": -0.2683306612607886
        },
        {
            "id": "s_607",
            "name": "Markus Gross 0001",
            "type": "sparse",
            "x": -0.5454646747449796,
            "y": 0.01778950557605321
        },
        {
            "id": "s_608",
            "name": "Joyce Ma",
            "type": "sparse",
            "x": 0.579417897542587,
            "y": 0.11410683044625863
        },
        {
            "id": "s_609",
            "name": "Nils Rodrigues",
            "type": "sparse",
            "x": 0.1919637272664859,
            "y": -0.23324096717673162
        },
        {
            "id": "s_610",
            "name": "Jian Ma 0004",
            "type": "sparse",
            "x": 0.4876479580835652,
            "y": 0.1221463665681367
        },
        {
            "id": "s_611",
            "name": "Julien Tierny",
            "type": "sparse",
            "x": 0.5329439264714756,
            "y": -0.35256356014352525
        },
        {
            "id": "s_612",
            "name": "Will Usher 0001",
            "type": "sparse",
            "x": 0.24809939101979564,
            "y": -0.30158184368434726
        },
        {
            "id": "s_613",
            "name": "Tong Ge",
            "type": "sparse",
            "x": 0.007791246103239228,
            "y": 0.09478089098615562
        },
        {
            "id": "s_614",
            "name": "Frank Kr\u00fcger 0001",
            "type": "sparse",
            "x": -0.09731921495628229,
            "y": -0.36655427671521323
        },
        {
            "id": "s_615",
            "name": "Maria Luj\u00e1n Ganuza",
            "type": "sparse",
            "x": 0.07195358891937254,
            "y": -0.13058073110559615
        },
        {
            "id": "s_616",
            "name": "Matthew A. Barish",
            "type": "sparse",
            "x": 0.48693246485926667,
            "y": 0.8435233831506129
        },
        {
            "id": "s_617",
            "name": "Eric C. Alexander",
            "type": "sparse",
            "x": -0.2758991288603746,
            "y": -0.04427782591269773
        },
        {
            "id": "s_618",
            "name": "Larry A. Curtiss",
            "type": "sparse",
            "x": 0.29271214509031335,
            "y": -0.20456152783012885
        },
        {
            "id": "s_619",
            "name": "Bernhard Preim",
            "type": "sparse",
            "x": -0.3418686899097145,
            "y": -0.3162241054582977
        },
        {
            "id": "s_620",
            "name": "Max Hermann",
            "type": "sparse",
            "x": -0.27319948067451444,
            "y": -0.6544162108722886
        },
        {
            "id": "s_621",
            "name": "Brian C. Van Essen",
            "type": "sparse",
            "x": 0.2153753543669645,
            "y": -0.2688906783871582
        },
        {
            "id": "s_622",
            "name": "Wenyuan Wang",
            "type": "sparse",
            "x": 0.44070447535926655,
            "y": 0.0389240574243439
        },
        {
            "id": "s_623",
            "name": "Joel Shapiro",
            "type": "sparse",
            "x": -0.06082770552424802,
            "y": -0.3853678781978455
        },
        {
            "id": "s_624",
            "name": "Zhuochen Jin",
            "type": "sparse",
            "x": 0.3584363827492173,
            "y": 0.08131719978080758
        },
        {
            "id": "s_625",
            "name": "Ryan Russell",
            "type": "sparse",
            "x": -0.05119784475971715,
            "y": -0.33850111324662646
        },
        {
            "id": "s_626",
            "name": "Kasper Dinkla",
            "type": "sparse",
            "x": -0.22774442816891774,
            "y": -0.20552556742101458
        },
        {
            "id": "s_627",
            "name": "Peter Bak",
            "type": "sparse",
            "x": 0.5133095728412779,
            "y": 0.07621552733461684
        },
        {
            "id": "s_628",
            "name": "Klaus Hildebrandt",
            "type": "sparse",
            "x": -0.38748760315870623,
            "y": -0.38437090714140204
        },
        {
            "id": "s_629",
            "name": "Meeshu Agnihotri",
            "type": "sparse",
            "x": 0.035904540384653745,
            "y": -0.3094711163756696
        },
        {
            "id": "s_630",
            "name": "Kelei Cao",
            "type": "sparse",
            "x": 0.1421381536207496,
            "y": 0.5156972997744607
        },
        {
            "id": "s_631",
            "name": "Juliana Freire",
            "type": "sparse",
            "x": -0.1629556441998694,
            "y": 0.4917350771788212
        },
        {
            "id": "s_632",
            "name": "Raymond Huang",
            "type": "sparse",
            "x": -0.4944514224805612,
            "y": -0.24504998977043158
        },
        {
            "id": "s_633",
            "name": "Christopher P. Kappe",
            "type": "sparse",
            "x": 0.2894198938590275,
            "y": 0.48991643857408096
        },
        {
            "id": "s_634",
            "name": "Artem Sokolov",
            "type": "sparse",
            "x": -0.30957095137492685,
            "y": -0.17353369603696053
        },
        {
            "id": "s_635",
            "name": "S\u00f8ren Knudsen",
            "type": "sparse",
            "x": -0.28920698675777845,
            "y": -0.1408153998039702
        },
        {
            "id": "s_636",
            "name": "Markus Wagner 0008",
            "type": "sparse",
            "x": -0.1493786978682401,
            "y": -0.5462480476754615
        },
        {
            "id": "s_637",
            "name": "Xiaotao Nie",
            "type": "sparse",
            "x": 0.21582214000459823,
            "y": 0.4102572247637252
        },
        {
            "id": "s_638",
            "name": "Di Weng",
            "type": "sparse",
            "x": 0.09500533951573863,
            "y": 0.2749178933535592
        },
        {
            "id": "s_639",
            "name": "Min Shih",
            "type": "sparse",
            "x": 0.5756364932240713,
            "y": 0.08543335007855662
        },
        {
            "id": "s_640",
            "name": "Yun Wang 0012",
            "type": "sparse",
            "x": 0.2539442157304923,
            "y": 0.032198171611874715
        },
        {
            "id": "s_641",
            "name": "Jefferson Amstutz",
            "type": "sparse",
            "x": 0.3196476383729058,
            "y": -0.32630797587903176
        },
        {
            "id": "s_642",
            "name": "Charles D. Hansen",
            "type": "sparse",
            "x": -0.2578180212909389,
            "y": 0.400912263229388
        },
        {
            "id": "s_643",
            "name": "Shade T. Shutters",
            "type": "sparse",
            "x": 0.001018669350329568,
            "y": 0.43494436894063787
        },
        {
            "id": "s_644",
            "name": "Nivan Ferreira",
            "type": "sparse",
            "x": -0.12655867290746003,
            "y": 0.2899654727625136
        },
        {
            "id": "s_645",
            "name": "Bruno Gon\u00e7alves",
            "type": "sparse",
            "x": -0.1740577907736496,
            "y": 0.3759128324590741
        },
        {
            "id": "s_646",
            "name": "Samana Shrestha",
            "type": "sparse",
            "x": -0.21032966086374966,
            "y": -0.34424106852093406
        },
        {
            "id": "s_647",
            "name": "Mark Borowsky",
            "type": "sparse",
            "x": -0.1863959598798174,
            "y": -0.20359155557688605
        },
        {
            "id": "s_648",
            "name": "Thomas Butkiewicz",
            "type": "sparse",
            "x": 0.2657099773293169,
            "y": -0.4495460371372528
        },
        {
            "id": "s_649",
            "name": "Jonathan Woodring",
            "type": "sparse",
            "x": 0.3706546663596802,
            "y": -0.12552854131144642
        },
        {
            "id": "s_650",
            "name": "Paul Issartel",
            "type": "sparse",
            "x": -0.006556320949659366,
            "y": -0.26032831630409314
        },
        {
            "id": "s_651",
            "name": "Walter F. Stewart",
            "type": "sparse",
            "x": -0.02577546954443502,
            "y": -0.13204110217620446
        },
        {
            "id": "s_652",
            "name": "Adam James Bradley",
            "type": "sparse",
            "x": -0.2262788814996867,
            "y": 0.0642826458329983
        },
        {
            "id": "s_653",
            "name": "Maneesh Agrawala",
            "type": "sparse",
            "x": 0.29683543239407295,
            "y": 0.0637543570226691
        },
        {
            "id": "s_654",
            "name": "Haojing Jiang",
            "type": "sparse",
            "x": 0.20030264493347577,
            "y": 0.3284224399352004
        },
        {
            "id": "s_655",
            "name": "Ting-Chuen Pong",
            "type": "sparse",
            "x": 0.19574623240771874,
            "y": 0.09538188360350024
        },
        {
            "id": "s_656",
            "name": "Michael E. Papka",
            "type": "sparse",
            "x": 0.3305657223032808,
            "y": -0.1621249229873742
        },
        {
            "id": "s_657",
            "name": "Paul A. Navr\u00e1til",
            "type": "sparse",
            "x": 0.31969708824294923,
            "y": -0.3324857443934403
        },
        {
            "id": "s_658",
            "name": "Isaac Dykeman",
            "type": "sparse",
            "x": 0.18503281845151628,
            "y": -0.0037642681698518672
        },
        {
            "id": "s_659",
            "name": "Sebastian Koch",
            "type": "sparse",
            "x": 0.162710954150549,
            "y": -0.2272179667375055
        },
        {
            "id": "s_660",
            "name": "Corrado Cal\u00ec",
            "type": "sparse",
            "x": -0.26781564487048704,
            "y": -0.2720816815381066
        },
        {
            "id": "s_661",
            "name": "Angus Graeme Forbes",
            "type": "sparse",
            "x": -0.8054830052178962,
            "y": 0.09647879355160005
        },
        {
            "id": "s_662",
            "name": "Holger Stitz",
            "type": "sparse",
            "x": -0.16121935847914456,
            "y": -0.4336697586190908
        },
        {
            "id": "s_663",
            "name": "David Duran",
            "type": "sparse",
            "x": -0.26280059150718144,
            "y": 0.06800975680209159
        },
        {
            "id": "s_664",
            "name": "Yindalon Aphinyanaphongs",
            "type": "sparse",
            "x": 0.06386816824087793,
            "y": 0.012098442890824861
        },
        {
            "id": "s_665",
            "name": "Haoyu Li",
            "type": "sparse",
            "x": 0.3348903925470708,
            "y": 0.2112190880382631
        },
        {
            "id": "s_666",
            "name": "Richard Pusch",
            "type": "sparse",
            "x": -0.2507638788455786,
            "y": -0.08145431458017775
        },
        {
            "id": "s_667",
            "name": "Fei Wang 0016",
            "type": "sparse",
            "x": 0.054047730374217916,
            "y": 0.46226031758351466
        },
        {
            "id": "s_668",
            "name": "Robert S. Pienta",
            "type": "sparse",
            "x": 0.11221716944736393,
            "y": -0.4947729212606984
        },
        {
            "id": "s_669",
            "name": "Andrea Batch",
            "type": "sparse",
            "x": -0.16839784158874313,
            "y": -0.07241844059073006
        },
        {
            "id": "s_670",
            "name": "Kristin A. Cook",
            "type": "sparse",
            "x": 0.004090656818564291,
            "y": -0.039691195136032524
        },
        {
            "id": "s_671",
            "name": "Youfeng Hao",
            "type": "sparse",
            "x": 0.2891933516881724,
            "y": 0.2948342009147332
        },
        {
            "id": "s_672",
            "name": "Ryan Wilson",
            "type": "sparse",
            "x": 0.02967289600745372,
            "y": 0.023364343913833435
        },
        {
            "id": "s_673",
            "name": "Fan Hong",
            "type": "sparse",
            "x": 0.23420955412539268,
            "y": 0.38425852726219917
        },
        {
            "id": "s_674",
            "name": "Philipp Koytek",
            "type": "sparse",
            "x": -0.30211009044613657,
            "y": -0.10067597746077675
        },
        {
            "id": "s_675",
            "name": "Peter Hamilton",
            "type": "sparse",
            "x": 0.3501300015722161,
            "y": -0.11754619165138057
        },
        {
            "id": "s_676",
            "name": "Manuel Stein",
            "type": "sparse",
            "x": -0.06568433169265067,
            "y": 0.19465900988727555
        },
        {
            "id": "s_677",
            "name": "Samuel Vo\u00df",
            "type": "sparse",
            "x": -0.3923488291497292,
            "y": -0.3202271640918133
        },
        {
            "id": "s_678",
            "name": "Andreas Walch",
            "type": "sparse",
            "x": -0.13802428796401356,
            "y": -0.41033593065069907
        },
        {
            "id": "s_679",
            "name": "Filip Sadlo",
            "type": "sparse",
            "x": 0.334501124438404,
            "y": 0.28130766379899713
        },
        {
            "id": "s_680",
            "name": "Paulo E. Rauber",
            "type": "sparse",
            "x": -0.2621762205619807,
            "y": 0.5060247231070394
        },
        {
            "id": "s_681",
            "name": "Hua Guo",
            "type": "sparse",
            "x": -0.2716152675691377,
            "y": 0.22904522847474884
        },
        {
            "id": "s_682",
            "name": "Lamont Samuels",
            "type": "sparse",
            "x": -0.1964856615291601,
            "y": -0.015444193267745888
        },
        {
            "id": "s_683",
            "name": "Sara Jones 0001",
            "type": "sparse",
            "x": -0.32436121079521446,
            "y": 0.23177285622762817
        },
        {
            "id": "s_684",
            "name": "Daniel J. Wigdor",
            "type": "sparse",
            "x": 0.35546302160073356,
            "y": -0.11231300411844117
        },
        {
            "id": "s_685",
            "name": "Shuyue Zhou",
            "type": "sparse",
            "x": 0.18807069838443682,
            "y": 0.33264189577560543
        },
        {
            "id": "s_686",
            "name": "Jun Tao 0002",
            "type": "sparse",
            "x": 0.12023780722065146,
            "y": 0.44452892715188597
        },
        {
            "id": "s_687",
            "name": "Kwan-Liu Ma",
            "type": "sparse",
            "x": 0.3944686016449845,
            "y": 0.09174494003754032
        },
        {
            "id": "s_688",
            "name": "Peng Mi",
            "type": "sparse",
            "x": 0.43698127108764934,
            "y": 0.29450517658781256
        },
        {
            "id": "s_689",
            "name": "Zeyu Li 0003",
            "type": "sparse",
            "x": 0.3291775971666059,
            "y": 0.40914740963125373
        },
        {
            "id": "s_690",
            "name": "Kaiyi Zhang 0003",
            "type": "sparse",
            "x": -0.037244537280239465,
            "y": 0.07320931429196054
        },
        {
            "id": "s_691",
            "name": "David Koop",
            "type": "sparse",
            "x": 0.38038162214267157,
            "y": 0.3634095856576077
        },
        {
            "id": "s_692",
            "name": "Wei Xu 0020",
            "type": "sparse",
            "x": -0.8486854818704307,
            "y": -0.1190367531207697
        },
        {
            "id": "s_693",
            "name": "Shengjie Gao",
            "type": "sparse",
            "x": 0.10892499604417166,
            "y": 0.49505813186696435
        },
        {
            "id": "s_694",
            "name": "James Wexler",
            "type": "sparse",
            "x": 0.007544418182806097,
            "y": -0.6012916116178625
        },
        {
            "id": "s_695",
            "name": "Rainer Splechtna",
            "type": "sparse",
            "x": 0.14019118027957275,
            "y": -0.028754932154485033
        },
        {
            "id": "s_696",
            "name": "Dongjin Choi",
            "type": "sparse",
            "x": 0.13495537461837437,
            "y": -0.30672360331763965
        },
        {
            "id": "s_697",
            "name": "Holger Theisel",
            "type": "sparse",
            "x": -0.517143267259143,
            "y": 0.12804700705350044
        },
        {
            "id": "s_698",
            "name": "Geoffrey P. Ellis",
            "type": "sparse",
            "x": -0.004623941892706822,
            "y": -0.07159827416384958
        },
        {
            "id": "s_699",
            "name": "Lifeng Zhu",
            "type": "sparse",
            "x": -0.0004120938004865262,
            "y": 0.0834571931348856
        },
        {
            "id": "s_700",
            "name": "David Schroeder",
            "type": "sparse",
            "x": 0.17700265031915074,
            "y": -0.38915138463305965
        },
        {
            "id": "s_701",
            "name": "Sana Malik",
            "type": "sparse",
            "x": -0.00464940864984112,
            "y": -0.05520269646726369
        },
        {
            "id": "s_702",
            "name": "Andreas J. Lind",
            "type": "sparse",
            "x": 0.028941558073050728,
            "y": -0.3448783089374307
        },
        {
            "id": "s_703",
            "name": "Changjian Chen",
            "type": "sparse",
            "x": 0.006564083302301117,
            "y": 0.5226461366322125
        },
        {
            "id": "s_704",
            "name": "Mi Feng",
            "type": "sparse",
            "x": -0.457640817893259,
            "y": 0.1242975351632348
        },
        {
            "id": "s_705",
            "name": "Michael Kern",
            "type": "sparse",
            "x": 0.4606327059044567,
            "y": 0.22769527227697237
        },
        {
            "id": "s_706",
            "name": "Mieka West",
            "type": "sparse",
            "x": -0.2817732122244622,
            "y": -0.13852310724743852
        },
        {
            "id": "s_707",
            "name": "Nicole Sultanum",
            "type": "sparse",
            "x": 0.385853893729206,
            "y": -0.14158143937263304
        },
        {
            "id": "s_708",
            "name": "Ross Maciejewski",
            "type": "sparse",
            "x": 0.06203432338621715,
            "y": 0.42089367367948266
        },
        {
            "id": "s_709",
            "name": "Bo Qiao 0001",
            "type": "sparse",
            "x": 0.28662998325324923,
            "y": 0.0169096147463466
        },
        {
            "id": "s_710",
            "name": "Zheng Zhou",
            "type": "sparse",
            "x": 0.016082472078048433,
            "y": 0.3171104706157312
        },
        {
            "id": "s_711",
            "name": "Yiyang Tang",
            "type": "sparse",
            "x": -0.039986975761658626,
            "y": 0.18166509229298325
        },
        {
            "id": "s_712",
            "name": "Jens Schneider 0002",
            "type": "sparse",
            "x": -0.16319441784908179,
            "y": -0.4716358221342084
        },
        {
            "id": "s_713",
            "name": "Chong Zhang",
            "type": "sparse",
            "x": 0.025889002079745337,
            "y": 0.4834027797170646
        },
        {
            "id": "s_714",
            "name": "Gary King",
            "type": "sparse",
            "x": -0.2899912513925877,
            "y": -0.2006579735316029
        },
        {
            "id": "s_715",
            "name": "Gabriela Ferracutti",
            "type": "sparse",
            "x": 0.08307484633282967,
            "y": -0.13070769384512587
        },
        {
            "id": "s_716",
            "name": "Kalyan Veeramachaneni",
            "type": "sparse",
            "x": 0.2615930574387702,
            "y": 0.17986172207650922
        },
        {
            "id": "s_717",
            "name": "Nina McCurdy",
            "type": "sparse",
            "x": -0.38406646013513196,
            "y": 0.25358642934199
        },
        {
            "id": "s_718",
            "name": "Junyoung Choi",
            "type": "sparse",
            "x": -0.3340002037025938,
            "y": -0.05283255812011621
        },
        {
            "id": "s_719",
            "name": "Mingxuan Yuan",
            "type": "sparse",
            "x": 0.17137767533362186,
            "y": 0.05524787108403051
        },
        {
            "id": "s_720",
            "name": "Samuel H. Payne",
            "type": "sparse",
            "x": 0.02785316340600939,
            "y": 0.01935249740583024
        },
        {
            "id": "s_721",
            "name": "Yi-Shan Lin",
            "type": "sparse",
            "x": 0.19827611916366322,
            "y": 0.5002955746106663
        },
        {
            "id": "s_722",
            "name": "Bei Wang 0001",
            "type": "sparse",
            "x": 0.20980327699577625,
            "y": -0.13984838985078907
        },
        {
            "id": "s_723",
            "name": "Jungu Choi",
            "type": "sparse",
            "x": 0.17019236266495938,
            "y": -0.19419792873230324
        },
        {
            "id": "s_724",
            "name": "Yu Zhang 0043",
            "type": "sparse",
            "x": 0.25742011625581585,
            "y": 0.33353374358582344
        },
        {
            "id": "s_725",
            "name": "Lucas Schutz",
            "type": "sparse",
            "x": 0.2936168450872072,
            "y": 0.4867787510952877
        },
        {
            "id": "s_726",
            "name": "Hannah Kim 0001",
            "type": "sparse",
            "x": 0.09421763442906596,
            "y": -0.2604209971059465
        },
        {
            "id": "s_727",
            "name": "Tom Peterka",
            "type": "sparse",
            "x": 0.2009571766115135,
            "y": 0.3402868755362487
        },
        {
            "id": "s_728",
            "name": "Lyn Bartram",
            "type": "sparse",
            "x": -0.058977104069744817,
            "y": -0.024390982939863595
        },
        {
            "id": "s_729",
            "name": "Charles Gueunet",
            "type": "sparse",
            "x": 0.6103794319089034,
            "y": -0.3970494804602732
        },
        {
            "id": "s_730",
            "name": "Yingyu Wu",
            "type": "sparse",
            "x": 0.05588915973727249,
            "y": 0.4582191666994431
        },
        {
            "id": "s_731",
            "name": "Marco C. DeRuiter",
            "type": "sparse",
            "x": -0.19748282403698786,
            "y": -0.3914136064028468
        },
        {
            "id": "s_732",
            "name": "Arie E. Kaufman",
            "type": "sparse",
            "x": 0.44050269650544843,
            "y": 0.8237808394492364
        },
        {
            "id": "s_733",
            "name": "Guido Reina",
            "type": "sparse",
            "x": -0.017441185380311978,
            "y": -0.440601575436326
        },
        {
            "id": "s_734",
            "name": "Michael Brudno",
            "type": "sparse",
            "x": 0.33677154285934563,
            "y": -0.1119854664548212
        },
        {
            "id": "s_735",
            "name": "John T. Stasko",
            "type": "sparse",
            "x": -0.01905433752996813,
            "y": -0.22725742746844407
        },
        {
            "id": "s_736",
            "name": "Jae-Seung Yeom",
            "type": "sparse",
            "x": 0.2156057474662182,
            "y": -0.2692962880117156
        },
        {
            "id": "s_737",
            "name": "Heidrun Schumann",
            "type": "sparse",
            "x": -0.09139642049595345,
            "y": -0.3771897341163657
        },
        {
            "id": "s_738",
            "name": "Roxana Bujack",
            "type": "sparse",
            "x": 0.2562251876980038,
            "y": -0.3159312798791961
        },
        {
            "id": "s_739",
            "name": "Harry Stavropoulos",
            "type": "sparse",
            "x": -0.03248687801325875,
            "y": -0.04913715870219374
        },
        {
            "id": "s_740",
            "name": "Tingkai Sha",
            "type": "sparse",
            "x": 0.2761939612601338,
            "y": 0.08570832956672525
        },
        {
            "id": "s_741",
            "name": "Xiaowei Chu",
            "type": "sparse",
            "x": -0.02524043197716993,
            "y": 0.07512565082642784
        },
        {
            "id": "s_742",
            "name": "Patrick Chiu",
            "type": "sparse",
            "x": 0.38420611862873383,
            "y": 0.1923119441273381
        },
        {
            "id": "s_743",
            "name": "Yifan Zhang 0007",
            "type": "sparse",
            "x": -0.04034354942140448,
            "y": 0.40901467625648336
        },
        {
            "id": "s_744",
            "name": "Takahiro Yamamoto",
            "type": "sparse",
            "x": 0.1456190791277236,
            "y": -0.180911605544474
        },
        {
            "id": "s_745",
            "name": "Jing Yang 0001",
            "type": "sparse",
            "x": 0.017053859182756627,
            "y": 0.4049396351070642
        },
        {
            "id": "s_746",
            "name": "Kecheng Lu",
            "type": "sparse",
            "x": 0.006057402862261861,
            "y": 0.10274564459754153
        },
        {
            "id": "s_747",
            "name": "Yusu Wang 0001",
            "type": "sparse",
            "x": 0.28667412506077034,
            "y": -0.15875854189109734
        },
        {
            "id": "s_748",
            "name": "Luc Wilson",
            "type": "sparse",
            "x": -0.15974088865718222,
            "y": 0.35727012419282095
        },
        {
            "id": "s_749",
            "name": "Qi Wu 0015",
            "type": "sparse",
            "x": 0.1988025775043612,
            "y": -0.34532248364020907
        },
        {
            "id": "s_750",
            "name": "Qinying Liao",
            "type": "sparse",
            "x": 0.1351565814529267,
            "y": 0.5764696585767052
        },
        {
            "id": "s_751",
            "name": "Karsten Klein 0001",
            "type": "sparse",
            "x": -0.3015560340063954,
            "y": 0.060885199226426004
        },
        {
            "id": "s_752",
            "name": "Kui Wu 0003",
            "type": "sparse",
            "x": 0.30786613128378854,
            "y": -0.2563358158390561
        },
        {
            "id": "s_753",
            "name": "Hicham G. Elmongui",
            "type": "sparse",
            "x": 0.25073357037708377,
            "y": 0.0986590012704236
        },
        {
            "id": "s_754",
            "name": "Zheqing Yu",
            "type": "sparse",
            "x": -0.007392906463289659,
            "y": 0.2003869508655302
        },
        {
            "id": "s_755",
            "name": "Pierre Y. Andrews",
            "type": "sparse",
            "x": 0.10291682931685404,
            "y": -0.7086465065530524
        },
        {
            "id": "s_756",
            "name": "Brant Peterson",
            "type": "sparse",
            "x": -0.15843231404479244,
            "y": -0.2046886800159859
        },
        {
            "id": "s_757",
            "name": "Saad Nadeem",
            "type": "sparse",
            "x": 0.4633958279552286,
            "y": 0.8750487822777825
        },
        {
            "id": "s_758",
            "name": "Alvitta Ottley",
            "type": "sparse",
            "x": -0.38236418722547283,
            "y": 0.10326804893730246
        },
        {
            "id": "s_759",
            "name": "Alex Kale",
            "type": "sparse",
            "x": -0.1616567915678943,
            "y": -0.2520174859499391
        },
        {
            "id": "s_760",
            "name": "Aoyu Wu",
            "type": "sparse",
            "x": 0.15139177348419358,
            "y": -0.02905272596468807
        },
        {
            "id": "s_761",
            "name": "Yixuan Zhang 0001",
            "type": "sparse",
            "x": -0.5562244355361964,
            "y": -0.3062607682373066
        },
        {
            "id": "s_762",
            "name": "Nikhil Thorat",
            "type": "sparse",
            "x": 0.055027799586171905,
            "y": -0.6577618895615289
        },
        {
            "id": "s_763",
            "name": "Shang-Tse Chen",
            "type": "sparse",
            "x": 0.13803277330711808,
            "y": -0.6422583920427861
        },
        {
            "id": "s_764",
            "name": "Russ Burtner",
            "type": "sparse",
            "x": 0.04666520264369481,
            "y": -0.07695390865234926
        },
        {
            "id": "s_765",
            "name": "Junpeng Wang 0001",
            "type": "sparse",
            "x": 0.281432021310633,
            "y": 0.18756672572696648
        },
        {
            "id": "s_766",
            "name": "Andreas Sch\u00e4fler",
            "type": "sparse",
            "x": 0.5070241569902868,
            "y": 0.22111776359819624
        },
        {
            "id": "s_767",
            "name": "Jie Liu 0046",
            "type": "sparse",
            "x": -0.3796565455215082,
            "y": 0.058783954798553695
        },
        {
            "id": "s_768",
            "name": "Jiachen Wang",
            "type": "sparse",
            "x": 0.02999616359044107,
            "y": 0.2987676195939407
        },
        {
            "id": "s_769",
            "name": "Vincent Bloemen",
            "type": "sparse",
            "x": 0.06450123658907636,
            "y": 0.5406307464657795
        },
        {
            "id": "s_770",
            "name": "Mingjie Tang",
            "type": "sparse",
            "x": 0.11357460616974065,
            "y": 0.5029411965267954
        },
        {
            "id": "s_771",
            "name": "Chittayong Surakitbanharn",
            "type": "sparse",
            "x": 0.18903778886112904,
            "y": 0.49765675953025307
        },
        {
            "id": "s_772",
            "name": "J. T. Fry",
            "type": "sparse",
            "x": 0.4701801256891128,
            "y": 0.27892936891974535
        },
        {
            "id": "s_773",
            "name": "Mennatallah El-Assady",
            "type": "sparse",
            "x": -0.06255676658150924,
            "y": 0.07313876063650188
        },
        {
            "id": "s_774",
            "name": "Zudi Lin",
            "type": "sparse",
            "x": -0.14948594038942584,
            "y": 0.11801205550039523
        },
        {
            "id": "s_775",
            "name": "Nadine Drager",
            "type": "sparse",
            "x": -0.6496025805293981,
            "y": 0.14240826364242512
        },
        {
            "id": "s_776",
            "name": "Yafeng Lu",
            "type": "sparse",
            "x": 0.00875112517454124,
            "y": 0.45246800212951516
        },
        {
            "id": "s_777",
            "name": "Furui Cheng",
            "type": "sparse",
            "x": 0.21297203354770597,
            "y": 0.12206218057853088
        },
        {
            "id": "s_778",
            "name": "Sebastian Gehrmann",
            "type": "sparse",
            "x": -0.20927979432925053,
            "y": -0.09995939218913077
        },
        {
            "id": "s_779",
            "name": "Simon Urbanek",
            "type": "sparse",
            "x": 0.09028866346171921,
            "y": 0.025363475964811203
        },
        {
            "id": "s_780",
            "name": "Davide Ceneda",
            "type": "sparse",
            "x": -0.18505200200156902,
            "y": -0.4737890933417077
        },
        {
            "id": "s_781",
            "name": "Michael Sedlmair",
            "type": "sparse",
            "x": -0.0027312590841840866,
            "y": 0.02154073966035298
        },
        {
            "id": "s_782",
            "name": "Lei Fang 0004",
            "type": "sparse",
            "x": 0.2625287196623089,
            "y": 0.0202418965146185
        },
        {
            "id": "s_783",
            "name": "Jia-Kai Chou",
            "type": "sparse",
            "x": 0.3055116645735895,
            "y": 0.1515758000905799
        },
        {
            "id": "s_784",
            "name": "Yi Gu",
            "type": "sparse",
            "x": 0.1365483551232011,
            "y": 0.4152966243042362
        },
        {
            "id": "s_785",
            "name": "Ji Qi",
            "type": "sparse",
            "x": 0.05538140740295885,
            "y": 0.5437659077552491
        },
        {
            "id": "s_786",
            "name": "Gemma Sanderson",
            "type": "sparse",
            "x": -0.2663239463805867,
            "y": 0.1594204872981388
        },
        {
            "id": "s_787",
            "name": "Hongsen Liao",
            "type": "sparse",
            "x": 0.06696154999880369,
            "y": 0.2681282317587176
        },
        {
            "id": "s_788",
            "name": "Jagoda Walny",
            "type": "sparse",
            "x": -0.2642499491074033,
            "y": -0.11094273223751619
        },
        {
            "id": "s_789",
            "name": "Ayan Biswas",
            "type": "sparse",
            "x": 0.2939630889896008,
            "y": -0.07999826630325022
        },
        {
            "id": "s_790",
            "name": "Florian Friess",
            "type": "sparse",
            "x": 0.0368001625538284,
            "y": -0.4673118692636448
        },
        {
            "id": "s_791",
            "name": "Charilaos Papadopoulos",
            "type": "sparse",
            "x": 0.49000510274495196,
            "y": 0.8335158051998681
        },
        {
            "id": "s_792",
            "name": "Richen Liu",
            "type": "sparse",
            "x": 0.2711396652281193,
            "y": 0.40656193418688913
        },
        {
            "id": "s_793",
            "name": "Doris Kosminsky",
            "type": "sparse",
            "x": -0.2858052722562443,
            "y": -0.13530701974280546
        },
        {
            "id": "s_794",
            "name": "Laura von R\u00fcden",
            "type": "sparse",
            "x": -0.19194917773968184,
            "y": 0.017867961755522174
        },
        {
            "id": "s_795",
            "name": "Arvind Satyanarayan",
            "type": "sparse",
            "x": -0.042594807016722254,
            "y": -0.27766326553968057
        },
        {
            "id": "s_796",
            "name": "Aditya Kalro",
            "type": "sparse",
            "x": 0.10898730165882911,
            "y": -0.705226738554239
        },
        {
            "id": "s_797",
            "name": "Eun Kyoung Choe",
            "type": "sparse",
            "x": 0.032294471913884785,
            "y": -0.14566377259281135
        },
        {
            "id": "s_798",
            "name": "Spencer C. Castro",
            "type": "sparse",
            "x": -0.4329100734378236,
            "y": 0.3871746639992648
        },
        {
            "id": "s_799",
            "name": "Thomas M. Hamill",
            "type": "sparse",
            "x": 0.05963142211288012,
            "y": 0.2679583962183145
        },
        {
            "id": "s_800",
            "name": "Elham Sakhaee",
            "type": "sparse",
            "x": -0.1211937984240968,
            "y": -0.47954192532821505
        },
        {
            "id": "s_801",
            "name": "Santhosh Nandhakumar",
            "type": "sparse",
            "x": 0.4285250234530836,
            "y": 0.10246232649533533
        },
        {
            "id": "s_802",
            "name": "Tamara Munzner",
            "type": "sparse",
            "x": 0.10144646218838953,
            "y": -0.17421470532279207
        },
        {
            "id": "s_803",
            "name": "Jason A. Laska",
            "type": "sparse",
            "x": 0.42993188309575747,
            "y": -0.23637052679462073
        },
        {
            "id": "s_804",
            "name": "Saeed Boorboor",
            "type": "sparse",
            "x": 0.40026251253722345,
            "y": 0.7708400636236745
        },
        {
            "id": "s_805",
            "name": "Matthias Kraus 0002",
            "type": "sparse",
            "x": -0.03314514162093149,
            "y": 0.010690572018160241
        },
        {
            "id": "s_806",
            "name": "Tim Biedert",
            "type": "sparse",
            "x": 0.1642826681011891,
            "y": 0.4434577460467406
        },
        {
            "id": "s_807",
            "name": "Francesco Parisio",
            "type": "sparse",
            "x": -0.8255732655894358,
            "y": -0.449284451323985
        },
        {
            "id": "s_808",
            "name": "Monique Meuschke",
            "type": "sparse",
            "x": -0.3853570366325122,
            "y": -0.2663174042306725
        },
        {
            "id": "s_809",
            "name": "Eric Lecolinet",
            "type": "sparse",
            "x": 0.12761275604469977,
            "y": -0.5179833970942301
        },
        {
            "id": "s_810",
            "name": "Mahdi Pakdaman Naeini",
            "type": "sparse",
            "x": 0.3538077920047554,
            "y": -0.10340007968516064
        },
        {
            "id": "s_811",
            "name": "Steven Franconeri",
            "type": "sparse",
            "x": 0.022590647522249874,
            "y": -0.3197984040152076
        },
        {
            "id": "s_812",
            "name": "Zhen Li 0044",
            "type": "sparse",
            "x": 0.17262999189585065,
            "y": 0.29300226029065307
        },
        {
            "id": "s_813",
            "name": "Connor Gramazio",
            "type": "sparse",
            "x": 0.053355510694197465,
            "y": 0.38857680798972216
        },
        {
            "id": "s_814",
            "name": "Tao Li 0039",
            "type": "sparse",
            "x": 0.2623670378604763,
            "y": -0.2608186644421633
        },
        {
            "id": "s_815",
            "name": "Christian R\u00f6ssl",
            "type": "sparse",
            "x": -0.512525391124571,
            "y": 0.23838042914455532
        },
        {
            "id": "s_816",
            "name": "Christian Richter",
            "type": "sparse",
            "x": -0.07861736844445465,
            "y": -0.41284328392624026
        },
        {
            "id": "s_817",
            "name": "Evan M. Peck",
            "type": "sparse",
            "x": -0.40834671552673496,
            "y": 0.12129278550147105
        },
        {
            "id": "s_818",
            "name": "Kresimir Matkovic",
            "type": "sparse",
            "x": 0.114333790456065,
            "y": -0.06991168514960747
        },
        {
            "id": "s_819",
            "name": "Patrick Wickenhauser",
            "type": "sparse",
            "x": -0.09256427781883877,
            "y": -0.42540428715587925
        },
        {
            "id": "s_820",
            "name": "Fearn Bishop",
            "type": "sparse",
            "x": -0.27243286810131856,
            "y": 0.1600256229075476
        },
        {
            "id": "s_821",
            "name": "Susanne Schnell",
            "type": "sparse",
            "x": -0.32776314714911975,
            "y": 0.6884350396889447
        },
        {
            "id": "s_822",
            "name": "Jiang Zhang 0002",
            "type": "sparse",
            "x": 0.24075218378919735,
            "y": 0.37396316083447295
        },
        {
            "id": "s_823",
            "name": "Maarten L\u00f6ffler",
            "type": "sparse",
            "x": -0.31885813531314927,
            "y": 0.3600315163410046
        },
        {
            "id": "s_824",
            "name": "Hendrikus H. M. Korsten",
            "type": "sparse",
            "x": 0.13817668927818386,
            "y": 0.6479585499365837
        },
        {
            "id": "s_825",
            "name": "Jina Huh",
            "type": "sparse",
            "x": 0.20023973434713713,
            "y": -0.21513044732293346
        },
        {
            "id": "s_826",
            "name": "Renata G. Raidou",
            "type": "sparse",
            "x": -0.148264064533137,
            "y": -0.5291349839962213
        },
        {
            "id": "s_827",
            "name": "Nancy O'Brien",
            "type": "sparse",
            "x": 0.017353721241891502,
            "y": 0.2301313410958701
        },
        {
            "id": "s_828",
            "name": "Nikolina Ban",
            "type": "sparse",
            "x": -0.5643456022300966,
            "y": 0.016988227746560617
        },
        {
            "id": "s_829",
            "name": "Michael Traor\u00e9",
            "type": "sparse",
            "x": -0.1898542700837495,
            "y": 0.3344280932396389
        },
        {
            "id": "s_830",
            "name": "Rita Sevastjanova",
            "type": "sparse",
            "x": -0.1084089023670469,
            "y": 0.09736450486323917
        },
        {
            "id": "s_831",
            "name": "Sarang Joshi",
            "type": "sparse",
            "x": 0.32202891376714465,
            "y": 0.05470076979110383
        },
        {
            "id": "s_832",
            "name": "Daniel Afergan",
            "type": "sparse",
            "x": -0.3758586660205792,
            "y": 0.12015069156695883
        },
        {
            "id": "s_833",
            "name": "Caroline Ziemkiewicz",
            "type": "sparse",
            "x": -0.3236200368175933,
            "y": 0.16211002481600081
        },
        {
            "id": "s_834",
            "name": "Mingwei Li",
            "type": "sparse",
            "x": -0.10502155867671534,
            "y": 0.00987260674163785
        },
        {
            "id": "s_835",
            "name": "Feng Wang 0012",
            "type": "sparse",
            "x": 0.009848814529301869,
            "y": 0.4304052498796806
        },
        {
            "id": "s_836",
            "name": "Guadalupe Canahuate",
            "type": "sparse",
            "x": -0.8029778320290109,
            "y": 0.16700585988457592
        },
        {
            "id": "s_837",
            "name": "Ellen Gasparovic",
            "type": "sparse",
            "x": 0.2957852425027076,
            "y": -0.15632885295001778
        },
        {
            "id": "s_838",
            "name": "Johann Kastner",
            "type": "sparse",
            "x": -0.1437203099962249,
            "y": -0.23877857426427257
        },
        {
            "id": "s_839",
            "name": "Philipp Roskosch",
            "type": "sparse",
            "x": 0.030218039508259218,
            "y": 0.3668211357363159
        },
        {
            "id": "s_840",
            "name": "Michael J. Haass",
            "type": "sparse",
            "x": 0.06990736870082383,
            "y": -0.41935620222182624
        },
        {
            "id": "s_841",
            "name": "Gerik Scheuermann",
            "type": "sparse",
            "x": -0.7836933823709297,
            "y": -0.44244030964073455
        },
        {
            "id": "s_842",
            "name": "Jibonananda Sanyal",
            "type": "sparse",
            "x": 0.2840157118183693,
            "y": -0.16449865347952583
        },
        {
            "id": "s_843",
            "name": "Sarah E. Burke",
            "type": "sparse",
            "x": -0.004688380163996586,
            "y": 0.4300937091638571
        },
        {
            "id": "s_844",
            "name": "Anton Strezhnev",
            "type": "sparse",
            "x": -0.2900718315963807,
            "y": -0.1933541819569034
        },
        {
            "id": "s_845",
            "name": "Nathalie Henry Riche",
            "type": "sparse",
            "x": -0.18108449161345458,
            "y": 0.014898980260890756
        },
        {
            "id": "s_846",
            "name": "Xun Zhao",
            "type": "sparse",
            "x": 0.09724199259254683,
            "y": 0.11474005935434192
        },
        {
            "id": "s_847",
            "name": "Rama Akkiraju",
            "type": "sparse",
            "x": 0.4379001403707655,
            "y": 0.18943616785291967
        },
        {
            "id": "s_848",
            "name": "R. Jordan Crouser",
            "type": "sparse",
            "x": -0.17327740407529568,
            "y": -0.04617122675346703
        },
        {
            "id": "s_849",
            "name": "Arthur J. Olson",
            "type": "sparse",
            "x": -0.05687478471241761,
            "y": -0.13231495891650594
        },
        {
            "id": "s_850",
            "name": "Patrice Y. Simard",
            "type": "sparse",
            "x": -0.12766078229408517,
            "y": -0.08974282446864669
        },
        {
            "id": "s_851",
            "name": "David Gotz",
            "type": "sparse",
            "x": 0.3740828534868664,
            "y": 0.05340204984308811
        },
        {
            "id": "s_852",
            "name": "Hao Yang 0007",
            "type": "sparse",
            "x": 0.3743284460835021,
            "y": 0.2145826674714722
        },
        {
            "id": "s_853",
            "name": "Sheelagh Carpendale",
            "type": "sparse",
            "x": -0.24811488718162159,
            "y": -0.046228025781527515
        },
        {
            "id": "s_854",
            "name": "Erik Broberg",
            "type": "sparse",
            "x": -0.3184235212910497,
            "y": 0.41018742724241714
        },
        {
            "id": "s_855",
            "name": "Denis Koschichow",
            "type": "sparse",
            "x": -0.7661075523571643,
            "y": -0.4570672153959207
        },
        {
            "id": "s_856",
            "name": "Martina Ziefle",
            "type": "sparse",
            "x": -0.1103610484857041,
            "y": 0.07205931369089845
        },
        {
            "id": "s_857",
            "name": "Adam Summers",
            "type": "sparse",
            "x": -0.2128915384174906,
            "y": 0.37578574117797775
        },
        {
            "id": "s_858",
            "name": "Furu Wei",
            "type": "sparse",
            "x": 0.13217520634804103,
            "y": 0.5726070100745096
        },
        {
            "id": "s_859",
            "name": "Fabian Fischer 0001",
            "type": "sparse",
            "x": -0.1170775079811758,
            "y": 0.14422243599691195
        },
        {
            "id": "s_860",
            "name": "Christian Blecha",
            "type": "sparse",
            "x": -0.8306180942386351,
            "y": -0.44443650400292295
        },
        {
            "id": "s_861",
            "name": "Nick Cramer",
            "type": "sparse",
            "x": 0.03907639194989912,
            "y": -0.025912540377255603
        },
        {
            "id": "s_862",
            "name": "Magnus Paulson Erga",
            "type": "sparse",
            "x": -0.015500496756854045,
            "y": -0.36599743826587117
        },
        {
            "id": "s_863",
            "name": "Jovan Popovic",
            "type": "sparse",
            "x": -0.18596384859965417,
            "y": -0.06953413774639025
        },
        {
            "id": "s_864",
            "name": "Samuel Huron",
            "type": "sparse",
            "x": -0.19633372334725138,
            "y": -0.02470938344341629
        },
        {
            "id": "s_865",
            "name": "Hanqi Guo 0001",
            "type": "sparse",
            "x": 0.19780453033695228,
            "y": 0.35443996779967685
        },
        {
            "id": "s_866",
            "name": "Nan Cao 0001",
            "type": "sparse",
            "x": 0.22807909531993506,
            "y": 0.11250548400944735
        },
        {
            "id": "s_867",
            "name": "J\u00fcrgen Bernard",
            "type": "sparse",
            "x": -0.08648297297158929,
            "y": 0.028148614851469387
        },
        {
            "id": "s_868",
            "name": "Michal Marcinkowski",
            "type": "sparse",
            "x": -0.341142338775821,
            "y": 0.41146127641575525
        },
        {
            "id": "s_869",
            "name": "Hendrik Strobelt",
            "type": "sparse",
            "x": -0.19148439765183395,
            "y": -0.13553720039694098
        },
        {
            "id": "s_870",
            "name": "Yinqi Sun",
            "type": "sparse",
            "x": 0.0010935342036782253,
            "y": 0.08944660151750224
        },
        {
            "id": "s_871",
            "name": "Noreen Kamal",
            "type": "sparse",
            "x": -0.2833978268336627,
            "y": -0.08209940705408605
        },
        {
            "id": "s_872",
            "name": "Xidao Wen",
            "type": "sparse",
            "x": 0.16410497130808083,
            "y": 0.11301917869154596
        },
        {
            "id": "s_873",
            "name": "Michelle Dowling",
            "type": "sparse",
            "x": 0.4561954119110246,
            "y": 0.2713783112341308
        },
        {
            "id": "s_874",
            "name": "Jan Byska",
            "type": "sparse",
            "x": -0.08241418187066829,
            "y": -0.11978460478307358
        },
        {
            "id": "s_875",
            "name": "David J. Duke",
            "type": "sparse",
            "x": 0.1414947859766778,
            "y": -0.1810936642774045
        },
        {
            "id": "s_876",
            "name": "Peter Rautek",
            "type": "sparse",
            "x": -0.13257952458093064,
            "y": -0.38536800566885854
        },
        {
            "id": "s_877",
            "name": "Yue Li",
            "type": "sparse",
            "x": 0.40900512842399783,
            "y": -0.00039620121445394684
        },
        {
            "id": "s_878",
            "name": "Huub van de Wetering",
            "type": "sparse",
            "x": 0.03290195935679639,
            "y": 0.4749620642569278
        },
        {
            "id": "s_879",
            "name": "Cody Dunne",
            "type": "sparse",
            "x": -0.5097718108691983,
            "y": -0.2691023561317991
        },
        {
            "id": "s_880",
            "name": "Lingyun Yu 0005",
            "type": "sparse",
            "x": 0.10808769150428689,
            "y": 0.1369366677234427
        },
        {
            "id": "s_881",
            "name": "Ke Wu",
            "type": "sparse",
            "x": 0.41167820861642707,
            "y": 0.010305325791052863
        },
        {
            "id": "s_882",
            "name": "Yi Chen 0007",
            "type": "sparse",
            "x": 0.14999929421271024,
            "y": 0.30411983941588727
        },
        {
            "id": "s_883",
            "name": "Bianca Tost",
            "type": "sparse",
            "x": 0.5584739036030048,
            "y": 0.1752968965460079
        },
        {
            "id": "s_884",
            "name": "Xiaoyu Zhang 0014",
            "type": "sparse",
            "x": 0.2494712679603197,
            "y": 0.013771329316884967
        },
        {
            "id": "s_885",
            "name": "Stephan Reiling",
            "type": "sparse",
            "x": -0.22686020812486796,
            "y": -0.21372516982203082
        },
        {
            "id": "s_886",
            "name": "Danny Holten",
            "type": "sparse",
            "x": 0.1628999252781149,
            "y": 0.637256315264533
        },
        {
            "id": "s_887",
            "name": "Dominikus Baur",
            "type": "sparse",
            "x": -0.28073477039399314,
            "y": -0.010302192330202745
        },
        {
            "id": "s_888",
            "name": "Wanqi Hu",
            "type": "sparse",
            "x": 0.19717106523911945,
            "y": 0.4601426868784247
        },
        {
            "id": "s_889",
            "name": "Xin Li",
            "type": "sparse",
            "x": 0.05706912992768729,
            "y": 0.4526738962231722
        },
        {
            "id": "s_890",
            "name": "Lars Linsen",
            "type": "sparse",
            "x": -0.3039739183903223,
            "y": 0.6228699234316633
        },
        {
            "id": "s_891",
            "name": "Kate Isaacs",
            "type": "sparse",
            "x": -0.321947230262758,
            "y": -0.01592937734163996
        },
        {
            "id": "s_892",
            "name": "Samuel G. Fadel",
            "type": "sparse",
            "x": -0.2639891514361045,
            "y": 0.49977620631757974
        },
        {
            "id": "s_893",
            "name": "Michael Brooks",
            "type": "sparse",
            "x": -0.11974891710197229,
            "y": -0.09026174456354288
        },
        {
            "id": "s_894",
            "name": "Jun Han 0010",
            "type": "sparse",
            "x": 0.04090808764003702,
            "y": 0.515838897623626
        },
        {
            "id": "s_895",
            "name": "Jiawan Zhang",
            "type": "sparse",
            "x": 0.2992605218167864,
            "y": 0.3593894861982759
        },
        {
            "id": "s_896",
            "name": "Weiwei Cui",
            "type": "sparse",
            "x": 0.18166428579145533,
            "y": 0.09185400384859599
        },
        {
            "id": "s_897",
            "name": "Yue Wang",
            "type": "sparse",
            "x": -0.7616286159290547,
            "y": -0.1393002481606225
        },
        {
            "id": "s_898",
            "name": "Amin Jourabloo",
            "type": "sparse",
            "x": 0.0259608928692066,
            "y": -0.1244463409680422
        },
        {
            "id": "s_899",
            "name": "Hang Su 0006",
            "type": "sparse",
            "x": 0.1423581243967232,
            "y": 0.5440685415666361
        },
        {
            "id": "s_900",
            "name": "Stefan Bruckner",
            "type": "sparse",
            "x": -0.09975050633750553,
            "y": -0.29026612773877997
        },
        {
            "id": "s_901",
            "name": "Tim Gerrits",
            "type": "sparse",
            "x": -0.5324520112538029,
            "y": 0.252412856616463
        },
        {
            "id": "s_902",
            "name": "Rushil Anirudh",
            "type": "sparse",
            "x": 0.21994180229182886,
            "y": -0.2600937183191838
        },
        {
            "id": "s_903",
            "name": "Zhipeng Wang",
            "type": "sparse",
            "x": 0.22423387606135964,
            "y": -0.01596752018822673
        },
        {
            "id": "s_904",
            "name": "Marcos Lage",
            "type": "sparse",
            "x": -0.15832933779498412,
            "y": 0.35787257153667845
        },
        {
            "id": "s_905",
            "name": "Dominik Sacha",
            "type": "sparse",
            "x": -0.0513017046499414,
            "y": -0.02739916491977273
        },
        {
            "id": "s_906",
            "name": "Bireswar Laha",
            "type": "sparse",
            "x": -0.23719382933318733,
            "y": 0.13212845372669998
        },
        {
            "id": "s_907",
            "name": "Bo Ma 0002",
            "type": "sparse",
            "x": -0.22545936388874915,
            "y": -0.5407995336648848
        },
        {
            "id": "s_908",
            "name": "Miriah D. Meyer",
            "type": "sparse",
            "x": -0.31410123776873194,
            "y": 0.1992785674772152
        },
        {
            "id": "s_909",
            "name": "Abigail Mosca",
            "type": "sparse",
            "x": -0.4363382044206805,
            "y": 0.041469478714440344
        },
        {
            "id": "s_910",
            "name": "Leilani Battle",
            "type": "sparse",
            "x": -0.2523189329454785,
            "y": -0.015081861006972322
        },
        {
            "id": "s_911",
            "name": "Daniel Olson",
            "type": "sparse",
            "x": 0.1794308517034005,
            "y": -0.3733427884330476
        },
        {
            "id": "s_912",
            "name": "Daniel Wigdor",
            "type": "sparse",
            "x": 0.3374111674655701,
            "y": -0.061089139918714244
        },
        {
            "id": "s_913",
            "name": "Paolo Simonetto",
            "type": "sparse",
            "x": 0.05688891083304141,
            "y": 0.15479368878805494
        },
        {
            "id": "s_914",
            "name": "Shreeraj Jadhav",
            "type": "sparse",
            "x": 0.4059006025140595,
            "y": 0.7914191272228202
        },
        {
            "id": "s_915",
            "name": "Dan Goldwasser",
            "type": "sparse",
            "x": 0.20439539011918473,
            "y": 0.49451490982420665
        },
        {
            "id": "s_916",
            "name": "Joseph Budin",
            "type": "sparse",
            "x": 0.6169261104100654,
            "y": -0.37989584490952044
        },
        {
            "id": "s_917",
            "name": "Hao Dong 0008",
            "type": "sparse",
            "x": 0.2702458492172972,
            "y": 0.12169592350826977
        },
        {
            "id": "s_918",
            "name": "Jun Wang",
            "type": "sparse",
            "x": -0.7166950398339872,
            "y": -0.11741980170671554
        },
        {
            "id": "s_919",
            "name": "Jaebum Kim",
            "type": "sparse",
            "x": 0.4827990494359625,
            "y": 0.11698498549430401
        },
        {
            "id": "s_920",
            "name": "Matthew Kay 0001",
            "type": "sparse",
            "x": -0.1646658883873428,
            "y": -0.27714095616487217
        },
        {
            "id": "s_921",
            "name": "David S. Ebert",
            "type": "sparse",
            "x": 0.1347434120549267,
            "y": 0.39402616863037515
        },
        {
            "id": "s_922",
            "name": "Mao Ye",
            "type": "sparse",
            "x": 0.03127708247737176,
            "y": -0.1257788040510895
        },
        {
            "id": "s_923",
            "name": "Christoph M. Sch\u00e4r",
            "type": "sparse",
            "x": -0.5602782221685135,
            "y": 0.010978889190953831
        },
        {
            "id": "s_924",
            "name": "Ke Li",
            "type": "sparse",
            "x": -0.03406242215972449,
            "y": 0.18632858355771298
        },
        {
            "id": "s_925",
            "name": "Alex Bigelow",
            "type": "sparse",
            "x": -0.23869689152870452,
            "y": -0.026647139151913336
        },
        {
            "id": "s_926",
            "name": "Michael Krone",
            "type": "sparse",
            "x": 0.028918423287722515,
            "y": -0.4703383758393337
        },
        {
            "id": "s_927",
            "name": "Cristian Felix",
            "type": "sparse",
            "x": 0.1452488381905265,
            "y": -0.14075702724539815
        },
        {
            "id": "s_928",
            "name": "Stefan Gumhold",
            "type": "sparse",
            "x": -0.7738915804990323,
            "y": -0.46582336504078914
        },
        {
            "id": "s_929",
            "name": "Gunther H. Weber",
            "type": "sparse",
            "x": 0.16853806865967813,
            "y": 0.44159867420607574
        },
        {
            "id": "s_930",
            "name": "Guillaume Favelier",
            "type": "sparse",
            "x": 0.5951753195303783,
            "y": -0.388301083238996
        },
        {
            "id": "s_931",
            "name": "Deokgun Park 0001",
            "type": "sparse",
            "x": 0.13902077543546668,
            "y": -0.23467899324040747
        },
        {
            "id": "s_932",
            "name": "Thomas Kirste",
            "type": "sparse",
            "x": -0.0940220660200054,
            "y": -0.3667732850689945
        },
        {
            "id": "s_933",
            "name": "Bridget Moynihan",
            "type": "sparse",
            "x": -0.3028941930877662,
            "y": 0.1784930823650728
        },
        {
            "id": "s_934",
            "name": "Li Chen 0031",
            "type": "sparse",
            "x": 0.05929751947751154,
            "y": 0.2612750494673099
        },
        {
            "id": "s_935",
            "name": "Jina Suh",
            "type": "sparse",
            "x": -0.09323155399988033,
            "y": -0.15447063802689856
        },
        {
            "id": "s_936",
            "name": "Guang Lin",
            "type": "sparse",
            "x": 0.3368622989227485,
            "y": 0.07358922799705446
        },
        {
            "id": "s_937",
            "name": "Peter Steneteg",
            "type": "sparse",
            "x": -0.4265244788617458,
            "y": 0.2699678983443596
        },
        {
            "id": "s_938",
            "name": "Young Bin Kim",
            "type": "sparse",
            "x": 0.20423476988113037,
            "y": -0.23351152893893595
        },
        {
            "id": "s_939",
            "name": "Kai Zhao",
            "type": "sparse",
            "x": -0.16608880704011272,
            "y": 0.38195952077875484
        },
        {
            "id": "s_940",
            "name": "Katy Williams",
            "type": "sparse",
            "x": -0.3235046815065922,
            "y": -0.023119850633610994
        },
        {
            "id": "s_941",
            "name": "Ye Zhao 0003",
            "type": "sparse",
            "x": 0.07292698839084356,
            "y": 0.4682799349696119
        },
        {
            "id": "s_942",
            "name": "Linhao Meng",
            "type": "sparse",
            "x": 0.17628892757717982,
            "y": 0.25938227642454775
        },
        {
            "id": "s_943",
            "name": "Romain Vuillemot",
            "type": "sparse",
            "x": -0.20061863391648446,
            "y": 0.0466576733869646
        },
        {
            "id": "s_944",
            "name": "Claes Lundstr\u00f6m",
            "type": "sparse",
            "x": -0.43707740080637936,
            "y": 0.36099232329428677
        },
        {
            "id": "s_945",
            "name": "Stefan Gunther",
            "type": "sparse",
            "x": 0.2894778217536391,
            "y": 0.47893420569044987
        },
        {
            "id": "s_946",
            "name": "Zhenyu Cheryl Qian",
            "type": "sparse",
            "x": 0.19550634323590524,
            "y": 0.49221173898914855
        },
        {
            "id": "s_947",
            "name": "Martin Eisemann",
            "type": "sparse",
            "x": -0.16308101790086035,
            "y": -0.5253888199876442
        },
        {
            "id": "s_948",
            "name": "Stefanie Schmid",
            "type": "sparse",
            "x": -0.19480469775635909,
            "y": 0.11833188106997526
        },
        {
            "id": "s_949",
            "name": "Guido Tack",
            "type": "sparse",
            "x": -0.34834500908193666,
            "y": 0.14986018203679458
        },
        {
            "id": "s_950",
            "name": "Jason D. Williams",
            "type": "sparse",
            "x": -0.0977873608659209,
            "y": -0.15079929404220008
        },
        {
            "id": "s_951",
            "name": "Yeukyin Chan",
            "type": "sparse",
            "x": 0.22601521384381978,
            "y": -0.010115860683199987
        },
        {
            "id": "s_952",
            "name": "Christopher deFilippi",
            "type": "sparse",
            "x": -0.02567386540901702,
            "y": -0.13943410322385005
        },
        {
            "id": "s_953",
            "name": "Denis Gracanin",
            "type": "sparse",
            "x": 0.17619801197152632,
            "y": 0.02139533388653832
        },
        {
            "id": "s_954",
            "name": "Seth Johnson",
            "type": "sparse",
            "x": 0.17588984219433168,
            "y": -0.3634407629983177
        },
        {
            "id": "s_955",
            "name": "Jianping Kelvin Li",
            "type": "sparse",
            "x": 0.5675271047259272,
            "y": 0.12508579300299347
        },
        {
            "id": "s_956",
            "name": "Ching-Yung Lin",
            "type": "sparse",
            "x": 0.08052961119342167,
            "y": 0.047515740416277465
        },
        {
            "id": "s_957",
            "name": "Laura E. Matzen",
            "type": "sparse",
            "x": 0.05050199454677013,
            "y": -0.3584456179198692
        },
        {
            "id": "s_958",
            "name": "Andr\u00e9 Calero Valdez",
            "type": "sparse",
            "x": -0.1067061768907512,
            "y": 0.08024781377003709
        },
        {
            "id": "s_959",
            "name": "Dan Maljovec",
            "type": "sparse",
            "x": 0.22123131719568714,
            "y": -0.2689554659899244
        },
        {
            "id": "s_960",
            "name": "Ernesto A. Bjerg",
            "type": "sparse",
            "x": 0.08004236587566806,
            "y": -0.1364762248207175
        },
        {
            "id": "s_961",
            "name": "Christoph Garth",
            "type": "sparse",
            "x": 0.16681680614189198,
            "y": 0.43592024507384636
        },
        {
            "id": "s_962",
            "name": "Robert A. Lafrance",
            "type": "sparse",
            "x": 0.034283682370255884,
            "y": 0.017472795671001035
        },
        {
            "id": "s_963",
            "name": "Michail Schwab",
            "type": "sparse",
            "x": -0.2873548878315829,
            "y": -0.18970960026874986
        },
        {
            "id": "s_964",
            "name": "Minfeng Zhu 0001",
            "type": "sparse",
            "x": 0.11310014491370499,
            "y": 0.503459375072035
        },
        {
            "id": "s_965",
            "name": "Wenchao Wu",
            "type": "sparse",
            "x": 0.14065229136253804,
            "y": 0.02305141973216055
        },
        {
            "id": "s_966",
            "name": "Nicola Pezzotti",
            "type": "sparse",
            "x": -0.12876368120036397,
            "y": -0.49216690314435446
        },
        {
            "id": "s_967",
            "name": "Bowen Yu 0004",
            "type": "sparse",
            "x": -0.14260932642085125,
            "y": 0.44966435629509766
        },
        {
            "id": "s_968",
            "name": "Oliver Beuing",
            "type": "sparse",
            "x": -0.38705119174323327,
            "y": -0.31890426648916775
        },
        {
            "id": "s_969",
            "name": "Zeng Dai",
            "type": "sparse",
            "x": 0.14683646230448874,
            "y": 0.18888719571171142
        },
        {
            "id": "s_970",
            "name": "Meng Xia 0002",
            "type": "sparse",
            "x": 0.30041048097843964,
            "y": 0.06227127740828299
        },
        {
            "id": "s_971",
            "name": "Seungyeon Kim",
            "type": "sparse",
            "x": 0.12142052182825396,
            "y": -0.28203823598188155
        },
        {
            "id": "s_972",
            "name": "Hui Huang 0004",
            "type": "sparse",
            "x": 0.34510698644944926,
            "y": 0.1056936832736108
        },
        {
            "id": "s_973",
            "name": "Tarik Crnovrsanin",
            "type": "sparse",
            "x": 0.5192458901068877,
            "y": 0.049431022362952666
        },
        {
            "id": "s_974",
            "name": "Jay Koven",
            "type": "sparse",
            "x": 0.18858495956245455,
            "y": -0.11854199859677098
        },
        {
            "id": "s_975",
            "name": "Doug A. Bowman",
            "type": "sparse",
            "x": -0.2285308426053374,
            "y": 0.20924110463675583
        },
        {
            "id": "s_976",
            "name": "Yixian Zheng",
            "type": "sparse",
            "x": 0.13421700639321785,
            "y": 0.0168483784044912
        },
        {
            "id": "s_977",
            "name": "Benjamin J. Isaac",
            "type": "sparse",
            "x": 0.31002448998865095,
            "y": -0.25023018768967104
        },
        {
            "id": "s_978",
            "name": "David Kouril",
            "type": "sparse",
            "x": -0.06044339480942539,
            "y": -0.17611004208571615
        },
        {
            "id": "s_979",
            "name": "Kristine Lee",
            "type": "sparse",
            "x": -0.8133925594618533,
            "y": 0.09154434304860537
        },
        {
            "id": "s_980",
            "name": "Jeffrey T. Morisette",
            "type": "sparse",
            "x": -0.14525470738420793,
            "y": 0.21527677050698094
        },
        {
            "id": "s_981",
            "name": "Jia Zeng",
            "type": "sparse",
            "x": 0.19755095381378465,
            "y": 0.09006583770898627
        },
        {
            "id": "s_982",
            "name": "Jim Gaffney",
            "type": "sparse",
            "x": 0.21797892717244124,
            "y": -0.26424160639794875
        },
        {
            "id": "s_983",
            "name": "Olivier Thonnard",
            "type": "sparse",
            "x": 0.026866781688536394,
            "y": 0.2838219893333942
        },
        {
            "id": "s_984",
            "name": "Shigeo Takahashi",
            "type": "sparse",
            "x": 0.1457307850621677,
            "y": -0.17189978018103616
        },
        {
            "id": "s_985",
            "name": "Khairi Reda",
            "type": "sparse",
            "x": 0.38363312575933345,
            "y": -0.08932687770936344
        },
        {
            "id": "s_986",
            "name": "Max Sondag",
            "type": "sparse",
            "x": -0.36660070437914316,
            "y": 0.4633481771929891
        },
        {
            "id": "s_987",
            "name": "Sai Prashanth Dasari",
            "type": "sparse",
            "x": 0.4232404718215075,
            "y": 0.11060700195062895
        },
        {
            "id": "s_988",
            "name": "Joe Bruce",
            "type": "sparse",
            "x": 0.04696680708594201,
            "y": -0.07181941472736138
        },
        {
            "id": "s_989",
            "name": "Vivek Srikumar",
            "type": "sparse",
            "x": 0.2483713482183009,
            "y": -0.22836088397282273
        },
        {
            "id": "s_990",
            "name": "Daniel Smilkov",
            "type": "sparse",
            "x": 0.006288050716258862,
            "y": -0.5830679517076252
        },
        {
            "id": "s_991",
            "name": "Bram C. M. Cappers",
            "type": "sparse",
            "x": 0.11950860967040193,
            "y": 0.6598442398453223
        },
        {
            "id": "s_992",
            "name": "Leslie M. Blaha",
            "type": "sparse",
            "x": -0.00627321945309557,
            "y": -0.2076583461241436
        },
        {
            "id": "s_993",
            "name": "Xian Teng",
            "type": "sparse",
            "x": 0.16468563523635155,
            "y": 0.10625612960504052
        },
        {
            "id": "s_994",
            "name": "Gregory P. Johnson",
            "type": "sparse",
            "x": 0.3114824621393711,
            "y": -0.296556337750522
        },
        {
            "id": "s_995",
            "name": "Wenbin He",
            "type": "sparse",
            "x": 0.26778509065879946,
            "y": 0.2776102343054253
        },
        {
            "id": "s_996",
            "name": "Teodora Chitiboi",
            "type": "sparse",
            "x": -0.32632064347615153,
            "y": 0.6950391360580594
        },
        {
            "id": "s_997",
            "name": "Sukwon Lee",
            "type": "sparse",
            "x": 0.1936576866491848,
            "y": -0.19865552788531934
        },
        {
            "id": "s_998",
            "name": "Johannes Kuntner",
            "type": "sparse",
            "x": -0.16860897516946222,
            "y": -0.5374376437286079
        },
        {
            "id": "s_999",
            "name": "Jieqiong Zhao",
            "type": "sparse",
            "x": 0.18173935865119256,
            "y": 0.4769461514774932
        },
        {
            "id": "s_1000",
            "name": "Neil Spring",
            "type": "sparse",
            "x": 0.4649051065663128,
            "y": 0.06293146142437304
        },
        {
            "id": "s_1001",
            "name": "Brian K. Spears",
            "type": "sparse",
            "x": 0.22450845917811044,
            "y": -0.2612216453072942
        },
        {
            "id": "s_1002",
            "name": "Caitlin Gutheil",
            "type": "sparse",
            "x": -0.4116013225132997,
            "y": 0.08914932792251117
        },
        {
            "id": "s_1003",
            "name": "Daniel A. Keim",
            "type": "sparse",
            "x": -0.06770503216398238,
            "y": 0.0641990492573603
        },
        {
            "id": "s_1004",
            "name": "Xiaoru Lin",
            "type": "sparse",
            "x": 0.18037145810549804,
            "y": 0.32158740605198355
        },
        {
            "id": "s_1005",
            "name": "Fred Hohman",
            "type": "sparse",
            "x": 0.11484947372662137,
            "y": -0.5571167314928127
        },
        {
            "id": "s_1006",
            "name": "Florence Nicol",
            "type": "sparse",
            "x": -0.1795936044751273,
            "y": 0.3388286091525834
        },
        {
            "id": "s_1007",
            "name": "Adam Jurc\u00edk",
            "type": "sparse",
            "x": -0.07894668967743043,
            "y": -0.08027232990580813
        },
        {
            "id": "s_1008",
            "name": "Youn-Ah Kang",
            "type": "sparse",
            "x": 0.23723313746568045,
            "y": -0.19186614618456535
        },
        {
            "id": "s_1009",
            "name": "Xuanwu Yue",
            "type": "sparse",
            "x": -0.01777050012431318,
            "y": 0.18954535170701753
        },
        {
            "id": "s_1010",
            "name": "Peter Lindstrom 0001",
            "type": "sparse",
            "x": 0.26091641955885553,
            "y": -0.3186363496186322
        },
        {
            "id": "s_1011",
            "name": "Danyel Fisher",
            "type": "sparse",
            "x": -0.12681739576766246,
            "y": 0.011973402995028805
        },
        {
            "id": "s_1012",
            "name": "Yongqun He",
            "type": "sparse",
            "x": -0.5202409007517226,
            "y": 0.025502878141358747
        },
        {
            "id": "s_1013",
            "name": "Roeland Scheepens",
            "type": "sparse",
            "x": 0.005438675384991373,
            "y": 0.4181867794105363
        },
        {
            "id": "s_1014",
            "name": "Wenlong Chen",
            "type": "sparse",
            "x": 0.3239481852002267,
            "y": 0.186856589478987
        },
        {
            "id": "s_1015",
            "name": "Simon Schubiger",
            "type": "sparse",
            "x": 0.13678635384652982,
            "y": 0.1752012729086567
        },
        {
            "id": "s_1016",
            "name": "Tongshuang Wu",
            "type": "sparse",
            "x": 0.1790884344782713,
            "y": 0.13411699523064005
        },
        {
            "id": "s_1017",
            "name": "Georg Fuchs",
            "type": "sparse",
            "x": 0.0134379458951484,
            "y": 0.36783598276438106
        },
        {
            "id": "s_1018",
            "name": "Zhenhuang Wang",
            "type": "sparse",
            "x": 0.26565896429153313,
            "y": 0.29221038374184277
        },
        {
            "id": "s_1019",
            "name": "Remo Burkhard",
            "type": "sparse",
            "x": 0.1287621033002608,
            "y": 0.17737750057649979
        },
        {
            "id": "s_1020",
            "name": "Zhiguang Zhou",
            "type": "sparse",
            "x": 0.16462173238697425,
            "y": 0.2051007519773288
        },
        {
            "id": "s_1021",
            "name": "Steven R. Corman",
            "type": "sparse",
            "x": -0.007403558553326706,
            "y": 0.4235758384664438
        },
        {
            "id": "s_1022",
            "name": "Yadong Wu",
            "type": "sparse",
            "x": 0.2699379414452493,
            "y": 0.2711712301972726
        },
        {
            "id": "s_1023",
            "name": "David Joseph Wrisley",
            "type": "sparse",
            "x": -0.7295322614050723,
            "y": -0.5092007133509523
        },
        {
            "id": "s_1024",
            "name": "Jorge Poco",
            "type": "sparse",
            "x": -0.12669279753766843,
            "y": 0.10871805728329156
        },
        {
            "id": "s_1025",
            "name": "Uli Niemann",
            "type": "sparse",
            "x": -0.32625814490793964,
            "y": -0.37067206831363353
        },
        {
            "id": "s_1026",
            "name": "Joanne Taery Kim",
            "type": "sparse",
            "x": 0.19740068132205926,
            "y": -0.2324101377207231
        },
        {
            "id": "s_1027",
            "name": "Jordi Ventosa-Molina",
            "type": "sparse",
            "x": -0.7508092260773296,
            "y": -0.44791245845770034
        },
        {
            "id": "s_1028",
            "name": "Yang Chen",
            "type": "sparse",
            "x": 0.07360193259954365,
            "y": 0.48368885612384116
        },
        {
            "id": "s_1029",
            "name": "Petra Isenberg",
            "type": "sparse",
            "x": 0.09272789131844542,
            "y": -0.12831199935189677
        },
        {
            "id": "s_1030",
            "name": "Shichao Jia",
            "type": "sparse",
            "x": 0.32356344208899224,
            "y": 0.4092828661584739
        },
        {
            "id": "s_1031",
            "name": "Steven Mark Drucker",
            "type": "sparse",
            "x": -0.13658777348831871,
            "y": -0.028674749393694708
        },
        {
            "id": "s_1032",
            "name": "Xiting Wang",
            "type": "sparse",
            "x": 0.06566910504015798,
            "y": 0.5080944651225155
        },
        {
            "id": "s_1033",
            "name": "Terece L. Turton",
            "type": "sparse",
            "x": 0.26035456761651915,
            "y": -0.3182981375861695
        },
        {
            "id": "s_1034",
            "name": "Jeff W. Lichtman",
            "type": "sparse",
            "x": -0.2794949323197231,
            "y": -0.2664768912747412
        },
        {
            "id": "s_1035",
            "name": "James Eagan",
            "type": "sparse",
            "x": 0.12340794360732414,
            "y": -0.5147234630350732
        },
        {
            "id": "s_1036",
            "name": "Andrew Burks",
            "type": "sparse",
            "x": -0.7684673445072,
            "y": 0.10092461443437166
        },
        {
            "id": "s_1037",
            "name": "Carlos Eduardo Scheidegger",
            "type": "sparse",
            "x": 0.09619493290084062,
            "y": 0.016683637917952403
        },
        {
            "id": "s_1038",
            "name": "Kristin M. Divis",
            "type": "sparse",
            "x": 0.06126391540030056,
            "y": -0.4194468173010051
        },
        {
            "id": "s_1039",
            "name": "Shamkant B. Navathe",
            "type": "sparse",
            "x": 0.12075379660622475,
            "y": -0.4926804438555324
        },
        {
            "id": "s_1040",
            "name": "Mengdie Hu",
            "type": "sparse",
            "x": -0.028741173126825684,
            "y": -0.3207742992654628
        },
        {
            "id": "s_1041",
            "name": "Audace Nakeshimana",
            "type": "sparse",
            "x": -0.24933701197948402,
            "y": -0.019623269484900046
        },
        {
            "id": "s_1042",
            "name": "Mira Dontcheva",
            "type": "sparse",
            "x": -0.13138215053898553,
            "y": -0.011071522583681806
        },
        {
            "id": "s_1043",
            "name": "Haichao Miao",
            "type": "sparse",
            "x": -0.08482518842089636,
            "y": -0.22518407405416357
        },
        {
            "id": "s_1044",
            "name": "Kelly P. Gaither",
            "type": "sparse",
            "x": -0.36653389884975357,
            "y": -0.1322207503773333
        },
        {
            "id": "s_1045",
            "name": "Yang Yue 0001",
            "type": "sparse",
            "x": 0.4047077112665552,
            "y": 0.12130992398678829
        },
        {
            "id": "s_1046",
            "name": "Jorge Estrada",
            "type": "sparse",
            "x": -0.32302697992409984,
            "y": 0.11946690233557875
        },
        {
            "id": "s_1047",
            "name": "Alexandra Diehl",
            "type": "sparse",
            "x": 0.16960948551025362,
            "y": -0.030792255767951908
        },
        {
            "id": "s_1048",
            "name": "Isaac Cho",
            "type": "sparse",
            "x": 0.35461810305952496,
            "y": -0.47045598562421925
        },
        {
            "id": "s_1049",
            "name": "Dan Zhang",
            "type": "sparse",
            "x": -0.7577248711330458,
            "y": -0.14818745174437178
        },
        {
            "id": "s_1050",
            "name": "Yuhong Li",
            "type": "sparse",
            "x": 0.11436971357750057,
            "y": 0.24395939996585211
        },
        {
            "id": "s_1051",
            "name": "Sadia Rubab",
            "type": "sparse",
            "x": 0.07977034729369067,
            "y": 0.09957592832176726
        },
        {
            "id": "s_1052",
            "name": "Christophe Lenglet",
            "type": "sparse",
            "x": 0.18103363655569563,
            "y": -0.37230501267355637
        },
        {
            "id": "s_1053",
            "name": "Ji Lan",
            "type": "sparse",
            "x": -0.002070433624543756,
            "y": 0.28844912235089276
        },
        {
            "id": "s_1054",
            "name": "Boudewijn P. F. Lelieveldt",
            "type": "sparse",
            "x": -0.13371299088796829,
            "y": -0.4935948585132779
        },
        {
            "id": "s_1055",
            "name": "Min Zhu",
            "type": "sparse",
            "x": 0.16408495942306794,
            "y": 0.15685566094847467
        },
        {
            "id": "s_1056",
            "name": "Christopher M. White",
            "type": "sparse",
            "x": -0.2658091349251229,
            "y": 0.02720226314269415
        },
        {
            "id": "s_1057",
            "name": "Walter Fontana",
            "type": "sparse",
            "x": -0.810632036176369,
            "y": 0.09686452449252138
        },
        {
            "id": "s_1058",
            "name": "Yu Dong",
            "type": "sparse",
            "x": 0.2479241337724155,
            "y": 0.3173279705434558
        },
        {
            "id": "s_1059",
            "name": "Yunhai Wang",
            "type": "sparse",
            "x": 0.035878118858179694,
            "y": 0.13186697707532546
        },
        {
            "id": "s_1060",
            "name": "Ryan Wesslen",
            "type": "sparse",
            "x": 0.3803493255385532,
            "y": -0.5037778856033083
        },
        {
            "id": "s_1061",
            "name": "Nicholas Seltzer",
            "type": "sparse",
            "x": -0.19108059919762396,
            "y": -0.020925316821473167
        },
        {
            "id": "s_1062",
            "name": "Will Epperson",
            "type": "sparse",
            "x": 0.1294960993921215,
            "y": -0.6115921003990633
        },
        {
            "id": "s_1063",
            "name": "Daniel Borkin",
            "type": "sparse",
            "x": -0.3878365312581111,
            "y": -0.17842782470364899
        },
        {
            "id": "s_1064",
            "name": "Markus Stommel",
            "type": "sparse",
            "x": -0.8146099480851966,
            "y": -0.4465827607721395
        },
        {
            "id": "s_1065",
            "name": "Eugene Wu 0002",
            "type": "sparse",
            "x": -0.432634006751286,
            "y": 0.0502129111019796
        },
        {
            "id": "s_1066",
            "name": "Haiyan Yang",
            "type": "sparse",
            "x": 0.18082651926578874,
            "y": 0.14027489548202463
        },
        {
            "id": "s_1067",
            "name": "Dongyu Liu",
            "type": "sparse",
            "x": 0.1247953354467158,
            "y": 0.21107632427769749
        },
        {
            "id": "s_1068",
            "name": "Haesun Park",
            "type": "sparse",
            "x": 0.11638956529074039,
            "y": -0.2736698846534852
        },
        {
            "id": "s_1069",
            "name": "Michelle A. Borkin",
            "type": "sparse",
            "x": -0.43960826278404924,
            "y": -0.2175068268123449
        },
        {
            "id": "s_1070",
            "name": "James P. Ahrens",
            "type": "sparse",
            "x": 0.2974079930393317,
            "y": -0.240097588467374
        },
        {
            "id": "s_1071",
            "name": "Emily Delahaye",
            "type": "sparse",
            "x": -0.34719213614349453,
            "y": -0.15625492355883092
        },
        {
            "id": "s_1072",
            "name": "Heidi Werner",
            "type": "sparse",
            "x": -0.1525531972933919,
            "y": 0.34206080581861337
        },
        {
            "id": "s_1073",
            "name": "Mihaela Jarema",
            "type": "sparse",
            "x": 0.26965098185898195,
            "y": 0.0959751526897683
        },
        {
            "id": "s_1074",
            "name": "Preeti Malakar",
            "type": "sparse",
            "x": 0.3762039649086293,
            "y": -0.059423758695072146
        },
        {
            "id": "s_1075",
            "name": "Cheng Li",
            "type": "sparse",
            "x": 0.5087334612871184,
            "y": 0.25595948572486343
        },
        {
            "id": "s_1076",
            "name": "Michael Blumenschein",
            "type": "sparse",
            "x": -0.19826025121702642,
            "y": 0.10725147406265584
        },
        {
            "id": "s_1077",
            "name": "Bryan Genest",
            "type": "sparse",
            "x": -0.23275267146698633,
            "y": -0.20971576413929915
        },
        {
            "id": "s_1078",
            "name": "Andrew H. Stevens",
            "type": "sparse",
            "x": 0.2725642518414075,
            "y": -0.42981585244555764
        },
        {
            "id": "s_1079",
            "name": "Annelot Kraima",
            "type": "sparse",
            "x": -0.19372357222107273,
            "y": -0.39498323516796663
        },
        {
            "id": "s_1080",
            "name": "Zhicheng Liu 0001",
            "type": "sparse",
            "x": -0.0838156097816726,
            "y": -0.07964721658751996
        },
        {
            "id": "s_1081",
            "name": "Robert Kr\u00fcger",
            "type": "sparse",
            "x": -0.27267032217506015,
            "y": -0.15260658795641854
        },
        {
            "id": "s_1082",
            "name": "Jie Bao 0003",
            "type": "sparse",
            "x": 0.10224810709632505,
            "y": 0.2619498752090556
        },
        {
            "id": "s_1083",
            "name": "Martin Imre",
            "type": "sparse",
            "x": 0.11738243197809929,
            "y": 0.4386290365267995
        },
        {
            "id": "s_1084",
            "name": "Henry V\u00f6lzke",
            "type": "sparse",
            "x": -0.32549431873959994,
            "y": -0.37723093959251036
        },
        {
            "id": "s_1085",
            "name": "Baher Elgohari",
            "type": "sparse",
            "x": -0.7902621949501475,
            "y": 0.16719930277092535
        },
        {
            "id": "s_1086",
            "name": "Hansi Senaratne",
            "type": "sparse",
            "x": -0.00015214696434113364,
            "y": -0.06758780115347313
        },
        {
            "id": "s_1087",
            "name": "Silvia Miksch",
            "type": "sparse",
            "x": -0.1360245177081287,
            "y": -0.46488052556424964
        },
        {
            "id": "s_1088",
            "name": "Thilo Spinner",
            "type": "sparse",
            "x": 0.03513942031739869,
            "y": 0.03429500846195257
        },
        {
            "id": "s_1089",
            "name": "Jo Wood",
            "type": "sparse",
            "x": -0.2024160340394882,
            "y": 0.1489422048406265
        },
        {
            "id": "s_1090",
            "name": "Yumeng Hou",
            "type": "sparse",
            "x": 0.205630772630547,
            "y": 0.45877174138470905
        },
        {
            "id": "s_1091",
            "name": "Marian Talbert",
            "type": "sparse",
            "x": -0.14530833514874525,
            "y": 0.21355078966436417
        },
        {
            "id": "s_1092",
            "name": "Steffen Lemke 0002",
            "type": "sparse",
            "x": 0.29771786973321546,
            "y": 0.4822187036962594
        },
        {
            "id": "s_1093",
            "name": "Ko-Chih Wang",
            "type": "sparse",
            "x": 0.2880430382074209,
            "y": 0.24554775474386994
        },
        {
            "id": "s_1094",
            "name": "Connor Huff",
            "type": "sparse",
            "x": -0.29250823030057865,
            "y": -0.19621080877026187
        },
        {
            "id": "s_1095",
            "name": "Carson Brownlee",
            "type": "sparse",
            "x": 0.3237840576168987,
            "y": -0.3308251362842309
        },
        {
            "id": "s_1096",
            "name": "Ulrike Pfeil",
            "type": "sparse",
            "x": -0.2674809141409049,
            "y": 0.15385691067253277
        },
        {
            "id": "s_1097",
            "name": "Katrin Scharnowski",
            "type": "sparse",
            "x": 0.03015736064757694,
            "y": -0.4752013703376005
        },
        {
            "id": "s_1098",
            "name": "Konstantinos Efstathiou 0001",
            "type": "sparse",
            "x": 0.02764871061695825,
            "y": -0.07045156969819702
        },
        {
            "id": "s_1099",
            "name": "Andrea Unger",
            "type": "sparse",
            "x": -0.6566843281768949,
            "y": 0.14054502356450785
        },
        {
            "id": "s_1100",
            "name": "Alice Chu",
            "type": "sparse",
            "x": 0.07999126011519446,
            "y": 0.32070702047233346
        },
        {
            "id": "s_1101",
            "name": "Justin Talbot",
            "type": "sparse",
            "x": -0.03123180331083558,
            "y": -0.5205605288993665
        },
        {
            "id": "s_1102",
            "name": "Britta Renner",
            "type": "sparse",
            "x": -0.2015892457084634,
            "y": 0.11071135733089517
        },
        {
            "id": "s_1103",
            "name": "Feiran Wu",
            "type": "sparse",
            "x": 0.0772709779649755,
            "y": 0.31218874965534527
        },
        {
            "id": "s_1104",
            "name": "Liangge Hsu",
            "type": "sparse",
            "x": -0.49465970624677424,
            "y": -0.25120905255592396
        },
        {
            "id": "s_1105",
            "name": "Jochen Fr\u00f6hlich",
            "type": "sparse",
            "x": -0.7779593186168767,
            "y": -0.4598631611428562
        },
        {
            "id": "s_1106",
            "name": "Peter Hanula",
            "type": "sparse",
            "x": -0.7978115339343957,
            "y": 0.16334770191284698
        },
        {
            "id": "s_1107",
            "name": "Feng Luo 0002",
            "type": "sparse",
            "x": 0.12707695203235328,
            "y": 0.2932425541980828
        },
        {
            "id": "s_1108",
            "name": "Dong Sun 0001",
            "type": "sparse",
            "x": 0.2029109868986206,
            "y": 0.0958377923390145
        },
        {
            "id": "s_1109",
            "name": "Panpan Xu",
            "type": "sparse",
            "x": 0.1986604466744777,
            "y": 0.12853870350172406
        },
        {
            "id": "s_1110",
            "name": "Xiaobo Luo",
            "type": "sparse",
            "x": 0.17360700640551163,
            "y": 0.3264611553461829
        },
        {
            "id": "s_1111",
            "name": "Colin Ware",
            "type": "sparse",
            "x": 0.2614677448314794,
            "y": -0.36005986419461944
        },
        {
            "id": "s_1112",
            "name": "Zhiyuan Wang",
            "type": "sparse",
            "x": 0.061662689633923094,
            "y": -0.42562464409862444
        },
        {
            "id": "s_1113",
            "name": "Margit Pohl",
            "type": "sparse",
            "x": -0.16415022503830098,
            "y": -0.5560310833747368
        },
        {
            "id": "s_1114",
            "name": "Erez Zadok",
            "type": "sparse",
            "x": -0.7583911401020395,
            "y": -0.05799492821583539
        },
        {
            "id": "s_1115",
            "name": "Yong Wang 0021",
            "type": "sparse",
            "x": 0.15358551935330716,
            "y": 0.08090852632748627
        },
        {
            "id": "s_1116",
            "name": "Barry L. Drake",
            "type": "sparse",
            "x": 0.12786366193111862,
            "y": -0.31097553867302685
        },
        {
            "id": "s_1117",
            "name": "Yuki Asano 0003",
            "type": "sparse",
            "x": -0.0872036317536561,
            "y": 0.01754294523803338
        },
        {
            "id": "s_1118",
            "name": "Angela Mayhua",
            "type": "sparse",
            "x": -0.11291801825353927,
            "y": -0.01846635466070487
        },
        {
            "id": "s_1119",
            "name": "Cindy Xiong",
            "type": "sparse",
            "x": -0.030115671963496966,
            "y": -0.39444757526441576
        },
        {
            "id": "s_1120",
            "name": "Jiaxin Shi",
            "type": "sparse",
            "x": 0.14968065652655776,
            "y": 0.44477371060906523
        },
        {
            "id": "s_1121",
            "name": "Jen-Ping Chen",
            "type": "sparse",
            "x": 0.4095515032996476,
            "y": 0.16496512445611014
        },
        {
            "id": "s_1122",
            "name": "Zuchao Wang",
            "type": "sparse",
            "x": 0.2790132213154992,
            "y": 0.3096943240004543
        },
        {
            "id": "s_1123",
            "name": "Jing Xia",
            "type": "sparse",
            "x": 0.20968120890514055,
            "y": 0.45588888534782074
        },
        {
            "id": "s_1124",
            "name": "Daisuke Sakurai",
            "type": "sparse",
            "x": 0.14750686182659342,
            "y": -0.1760681782341153
        },
        {
            "id": "s_1125",
            "name": "Harsh Shukla",
            "type": "sparse",
            "x": -0.4989733617767155,
            "y": -0.24621894287011561
        },
        {
            "id": "s_1126",
            "name": "Benjamin Bach",
            "type": "sparse",
            "x": -0.20916400236559263,
            "y": -0.03817804592089981
        },
        {
            "id": "s_1127",
            "name": "Haohui Chen",
            "type": "sparse",
            "x": -0.3269269168219881,
            "y": 0.04826608463769039
        },
        {
            "id": "s_1128",
            "name": "Jian Chen 0006",
            "type": "sparse",
            "x": 0.21963775509870836,
            "y": -0.0686482334775246
        },
        {
            "id": "s_1129",
            "name": "Valentin Zobel",
            "type": "sparse",
            "x": -0.844905484993957,
            "y": -0.4701462514340026
        },
        {
            "id": "s_1130",
            "name": "Olaf Kolditz",
            "type": "sparse",
            "x": -0.816512628337729,
            "y": -0.43511483839994575
        },
        {
            "id": "s_1131",
            "name": "Fan Du",
            "type": "sparse",
            "x": 0.4015603659691587,
            "y": 0.06819210205052392
        },
        {
            "id": "s_1132",
            "name": "Qi Han 0006",
            "type": "sparse",
            "x": 0.11860976108415196,
            "y": -0.34973664612095956
        },
        {
            "id": "s_1133",
            "name": "Bing Ni",
            "type": "sparse",
            "x": 0.14978506297280203,
            "y": 0.015414501007510927
        },
        {
            "id": "s_1134",
            "name": "Simon Breslav",
            "type": "sparse",
            "x": 0.3361813389043068,
            "y": -0.08997300463255564
        },
        {
            "id": "s_1135",
            "name": "Adam Perer",
            "type": "sparse",
            "x": -0.10322657692256543,
            "y": -0.0866383830732485
        },
        {
            "id": "s_1136",
            "name": "Stephen C. North",
            "type": "sparse",
            "x": 0.04559684681738454,
            "y": 0.0009132779241891988
        },
        {
            "id": "s_1137",
            "name": "Bin Wang 0021",
            "type": "sparse",
            "x": 0.0042287327356553064,
            "y": 0.5278974149610698
        },
        {
            "id": "s_1138",
            "name": "Markus H. Gross",
            "type": "sparse",
            "x": -0.32155558627655967,
            "y": -0.09339706190897734
        },
        {
            "id": "s_1139",
            "name": "Karthik Ramani",
            "type": "sparse",
            "x": 0.028774136718255936,
            "y": -0.36100441217165397
        },
        {
            "id": "s_1140",
            "name": "Paola Valdivia",
            "type": "sparse",
            "x": 0.11469205313800183,
            "y": 0.35637660595780285
        },
        {
            "id": "s_1141",
            "name": "Haris Mumtaz",
            "type": "sparse",
            "x": 0.15429455658723737,
            "y": -0.24843269021137757
        },
        {
            "id": "s_1142",
            "name": "Xingbo Wang 0001",
            "type": "sparse",
            "x": 0.1568996049441389,
            "y": -0.026453585013462573
        },
        {
            "id": "s_1143",
            "name": "Niklas Elmqvist",
            "type": "sparse",
            "x": -0.012131548498329857,
            "y": -0.18952891552285028
        },
        {
            "id": "s_1144",
            "name": "Carolina Nobre",
            "type": "sparse",
            "x": -0.2077173678173435,
            "y": -0.15585915421077348
        },
        {
            "id": "s_1145",
            "name": "Martin Rohlig",
            "type": "sparse",
            "x": -0.09166445531968884,
            "y": -0.37780119518780636
        },
        {
            "id": "s_1146",
            "name": "Scott Houde",
            "type": "sparse",
            "x": -0.20858660819689942,
            "y": 0.35243362165817854
        },
        {
            "id": "s_1147",
            "name": "Xiaotong Li",
            "type": "sparse",
            "x": -0.037590216332094924,
            "y": 0.07784889412383139
        },
        {
            "id": "s_1148",
            "name": "David H. Laidlaw",
            "type": "sparse",
            "x": -0.1583045673017949,
            "y": 0.2771677658520553
        },
        {
            "id": "s_1149",
            "name": "Gennady L. Andrienko",
            "type": "sparse",
            "x": -0.013621416062669898,
            "y": 0.27532256066663596
        },
        {
            "id": "s_1150",
            "name": "Hans Hagen",
            "type": "sparse",
            "x": 0.3671280780741219,
            "y": -0.19825595680894784
        },
        {
            "id": "s_1151",
            "name": "Baining Guo",
            "type": "sparse",
            "x": 0.043759992199512494,
            "y": 0.49831579113740304
        },
        {
            "id": "s_1152",
            "name": "Matej Mlejnek",
            "type": "sparse",
            "x": -0.1941417358485722,
            "y": -0.41330411463147027
        },
        {
            "id": "s_1153",
            "name": "Seth Walker",
            "type": "sparse",
            "x": -0.08214857413297864,
            "y": 0.04542874118551215
        },
        {
            "id": "s_1154",
            "name": "Hans-J\u00f6rg Schulz",
            "type": "sparse",
            "x": -0.18596849561802048,
            "y": -0.4690174631286117
        },
        {
            "id": "s_1155",
            "name": "Kenneth Weiss 0001",
            "type": "sparse",
            "x": 0.29348200693497895,
            "y": -0.3724877379726588
        },
        {
            "id": "s_1156",
            "name": "Shihan Wang 0001",
            "type": "sparse",
            "x": 0.060769287411970394,
            "y": 0.5451369321882364
        },
        {
            "id": "s_1157",
            "name": "Thomas H\u00f6llt",
            "type": "sparse",
            "x": -0.13407594776211043,
            "y": -0.49375724479540767
        },
        {
            "id": "s_1158",
            "name": "Yuan-Fang Li",
            "type": "sparse",
            "x": -0.5245212382201123,
            "y": 0.02126955035962491
        },
        {
            "id": "s_1159",
            "name": "Jacheng Pan",
            "type": "sparse",
            "x": 0.22314234155333965,
            "y": 0.40605603808890284
        },
        {
            "id": "s_1160",
            "name": "Fabian Sperrle",
            "type": "sparse",
            "x": -0.09692092305951833,
            "y": 0.082757777668938
        },
        {
            "id": "s_1161",
            "name": "Baoquan Chen",
            "type": "sparse",
            "x": -0.0029027075771520285,
            "y": 0.09689885012709488
        },
        {
            "id": "s_1162",
            "name": "John Patchett",
            "type": "sparse",
            "x": 0.36297208051549124,
            "y": -0.1952752001817469
        },
        {
            "id": "s_1163",
            "name": "Donghao Ren",
            "type": "sparse",
            "x": -0.05591469551702155,
            "y": -0.17245317739967328
        },
        {
            "id": "s_1164",
            "name": "Bharath Kalidindi",
            "type": "sparse",
            "x": 0.06763166143603405,
            "y": -0.3043464110458504
        },
        {
            "id": "s_1165",
            "name": "Zhida Sun",
            "type": "sparse",
            "x": 0.24811997240207465,
            "y": 0.007951626421405279
        },
        {
            "id": "s_1166",
            "name": "Ian Crandell",
            "type": "sparse",
            "x": 0.4568189188297973,
            "y": 0.2917722218420923
        },
        {
            "id": "s_1167",
            "name": "Jiayi Xu 0001",
            "type": "sparse",
            "x": 0.14412668322362396,
            "y": 0.012842816688776685
        },
        {
            "id": "s_1168",
            "name": "Thorsten May",
            "type": "sparse",
            "x": -0.17851358842173184,
            "y": -0.475574879870238
        },
        {
            "id": "s_1169",
            "name": "Gromit Yeuk-Yin Chan",
            "type": "sparse",
            "x": 0.1046007687817496,
            "y": 0.25475233260737284
        },
        {
            "id": "s_1170",
            "name": "Elizabeth Munch",
            "type": "sparse",
            "x": 0.2898715477412282,
            "y": -0.15231217168055539
        },
        {
            "id": "s_1171",
            "name": "Zhihua Jin",
            "type": "sparse",
            "x": 0.20654162745161483,
            "y": 0.13338062237661719
        },
        {
            "id": "s_1172",
            "name": "Johanna Fulda",
            "type": "sparse",
            "x": 0.09085388091979264,
            "y": -0.22035962303379478
        },
        {
            "id": "s_1173",
            "name": "Aur\u00e9lie Coh\u00e9",
            "type": "sparse",
            "x": 0.11689124084624457,
            "y": -0.5190827914569551
        },
        {
            "id": "s_1174",
            "name": "Lingyun Yu 0001",
            "type": "sparse",
            "x": 0.053917973499962836,
            "y": 0.02368719927367567
        },
        {
            "id": "s_1175",
            "name": "Bill Howe",
            "type": "sparse",
            "x": -0.042457719656851654,
            "y": -0.4086058541799264
        },
        {
            "id": "s_1176",
            "name": "Daniel Haehn",
            "type": "sparse",
            "x": -0.2776109483422366,
            "y": -0.24606397539598146
        },
        {
            "id": "s_1177",
            "name": "Yu-Ru Lin",
            "type": "sparse",
            "x": 0.11802204068408415,
            "y": 0.08104952646975153
        },
        {
            "id": "s_1178",
            "name": "Changhong Zhang",
            "type": "sparse",
            "x": 0.33510732172019053,
            "y": 0.4069648679511123
        },
        {
            "id": "s_1179",
            "name": "Ran Chen",
            "type": "sparse",
            "x": 0.0763147051659656,
            "y": 0.322258685698583
        },
        {
            "id": "s_1180",
            "name": "Yusi Wang",
            "type": "sparse",
            "x": 0.1323119015693922,
            "y": 0.4372665714867616
        },
        {
            "id": "s_1181",
            "name": "Bum Chul Kwon",
            "type": "sparse",
            "x": 0.07947506174565182,
            "y": -0.1632639632643645
        },
        {
            "id": "s_1182",
            "name": "Kan Dai",
            "type": "sparse",
            "x": 0.06606474465989762,
            "y": 0.2619585846535007
        },
        {
            "id": "s_1183",
            "name": "Jennifer Frazier",
            "type": "sparse",
            "x": 0.577641919878053,
            "y": 0.10670712990203354
        },
        {
            "id": "s_1184",
            "name": "Roger Beecham",
            "type": "sparse",
            "x": -0.1896049325704036,
            "y": 0.22127238793026274
        },
        {
            "id": "s_1185",
            "name": "Xiaoyan Kui",
            "type": "sparse",
            "x": 0.17821542007258398,
            "y": 0.3255910378460606
        },
        {
            "id": "s_1186",
            "name": "Bernhard Fr\u00f6hler",
            "type": "sparse",
            "x": -0.15837383109935213,
            "y": -0.24577977203107282
        },
        {
            "id": "s_1187",
            "name": "Jean Scholtz",
            "type": "sparse",
            "x": 0.01721866666910657,
            "y": 0.017605110573805878
        },
        {
            "id": "s_1188",
            "name": "Kim Marriott",
            "type": "sparse",
            "x": -0.2807631805206226,
            "y": 0.03620326181342696
        },
        {
            "id": "s_1189",
            "name": "Charisee Chiw",
            "type": "sparse",
            "x": -0.1912825138591406,
            "y": -0.010194919673599956
        },
        {
            "id": "s_1190",
            "name": "Zikun Deng",
            "type": "sparse",
            "x": 0.08597297645315076,
            "y": 0.29560477511083744
        },
        {
            "id": "s_1191",
            "name": "Qinhan Liu",
            "type": "sparse",
            "x": -0.031269461456135565,
            "y": 0.1811033109170531
        },
        {
            "id": "s_1192",
            "name": "Shilpika",
            "type": "sparse",
            "x": 0.2907935230991865,
            "y": 0.08296114833611327
        },
        {
            "id": "s_1193",
            "name": "Hasan Davulcu",
            "type": "sparse",
            "x": -0.008834312399578487,
            "y": 0.42956960030699315
        },
        {
            "id": "s_1194",
            "name": "Leni Yang",
            "type": "sparse",
            "x": 0.28917237654255495,
            "y": 0.028902702482258135
        },
        {
            "id": "s_1195",
            "name": "Vitalis Wiens",
            "type": "sparse",
            "x": -0.28525750134327615,
            "y": -0.6704166022330482
        },
        {
            "id": "s_1196",
            "name": "Chongxuan Li",
            "type": "sparse",
            "x": 0.15116034735556164,
            "y": 0.41442055864528443
        },
        {
            "id": "s_1197",
            "name": "Dik Lun Lee",
            "type": "sparse",
            "x": 0.09904329616607309,
            "y": 0.11923627572083298
        },
        {
            "id": "s_1198",
            "name": "Ismail Demir",
            "type": "sparse",
            "x": 0.2678125569849038,
            "y": 0.10018025700104838
        },
        {
            "id": "s_1199",
            "name": "Daniel F. Keefe",
            "type": "sparse",
            "x": 0.20566573666381296,
            "y": -0.3036888565794042
        },
        {
            "id": "s_1200",
            "name": "Ji Soo Yi",
            "type": "sparse",
            "x": 0.20878096152626283,
            "y": -0.2016179454655491
        },
        {
            "id": "s_1201",
            "name": "Takanori Fujiwara",
            "type": "sparse",
            "x": 0.33741018394648736,
            "y": 0.010658332647360127
        },
        {
            "id": "s_1202",
            "name": "Andrew J. Solis",
            "type": "sparse",
            "x": 0.1846655765425169,
            "y": -0.3687660812159808
        },
        {
            "id": "s_1203",
            "name": "Fabio Miranda 0001",
            "type": "sparse",
            "x": -0.16603773417433035,
            "y": 0.3803906695910954
        },
        {
            "id": "s_1204",
            "name": "Maxime Cordeil",
            "type": "sparse",
            "x": -0.24333668296000846,
            "y": -0.0025268444131515645
        },
        {
            "id": "s_1205",
            "name": "Tobias Isenberg 0001",
            "type": "sparse",
            "x": -0.05973874551760038,
            "y": -0.20075782824571076
        },
        {
            "id": "s_1206",
            "name": "Jie Liang 0004",
            "type": "sparse",
            "x": 0.2712184701256761,
            "y": 0.2864061130009732
        },
        {
            "id": "s_1207",
            "name": "Cristina R. Ceja",
            "type": "sparse",
            "x": -0.0015311733541116391,
            "y": -0.4237210803623928
        },
        {
            "id": "s_1208",
            "name": "Melanie Tory",
            "type": "sparse",
            "x": 0.07380239385363656,
            "y": -0.04885467741869649
        },
        {
            "id": "s_1209",
            "name": "Chenglong Wang",
            "type": "sparse",
            "x": -0.06013218377221072,
            "y": -0.4013351399252987
        },
        {
            "id": "s_1210",
            "name": "Vladimir Molchanov",
            "type": "sparse",
            "x": -0.3456722230754864,
            "y": 0.6851741583725864
        },
        {
            "id": "s_1211",
            "name": "Andreas Lamprecht",
            "type": "sparse",
            "x": -0.06627616846385079,
            "y": 0.1907120806973976
        },
        {
            "id": "s_1212",
            "name": "Fernanda B. Vi\u00e9gas",
            "type": "sparse",
            "x": 0.02496166397010842,
            "y": -0.6100660562396201
        },
        {
            "id": "s_1213",
            "name": "Katrin Hegenscheid",
            "type": "sparse",
            "x": -0.33200315032543753,
            "y": -0.36992557907914103
        },
        {
            "id": "s_1214",
            "name": "Xiaoru Yuan",
            "type": "sparse",
            "x": 0.25732226006393016,
            "y": 0.3022573496239221
        },
        {
            "id": "s_1215",
            "name": "Michael Correll",
            "type": "sparse",
            "x": -0.0792515099311517,
            "y": -0.08628085589965376
        },
        {
            "id": "s_1216",
            "name": "Venkatram Vishwanath",
            "type": "sparse",
            "x": 0.3809499456788662,
            "y": -0.059495869413378526
        },
        {
            "id": "s_1217",
            "name": "Dazhen Deng",
            "type": "sparse",
            "x": 0.03928354469309776,
            "y": 0.3129435500830163
        },
        {
            "id": "s_1218",
            "name": "Cesar Palomo",
            "type": "sparse",
            "x": -0.1552804771886011,
            "y": 0.4960016982753746
        },
        {
            "id": "s_1219",
            "name": "Fritz Lekschas",
            "type": "sparse",
            "x": -0.25759535504865355,
            "y": -0.03374817336095851
        },
        {
            "id": "s_1220",
            "name": "Oliver Deussen",
            "type": "sparse",
            "x": 0.04072315591471808,
            "y": 0.0524348491467406
        },
        {
            "id": "s_1221",
            "name": "Tanja Blascheck",
            "type": "sparse",
            "x": 0.07340145517170037,
            "y": -0.25938657634781204
        },
        {
            "id": "s_1222",
            "name": "Kristanto Sean Njotoprawiro",
            "type": "sparse",
            "x": 0.1728889830183561,
            "y": -0.07737771519237341
        },
        {
            "id": "s_1223",
            "name": "Benjamin B. Bederson",
            "type": "sparse",
            "x": 0.02142059238399494,
            "y": -0.34863730726292863
        },
        {
            "id": "s_1224",
            "name": "Zhe Wang",
            "type": "sparse",
            "x": -0.10216169304602758,
            "y": 0.2381365948239361
        },
        {
            "id": "s_1225",
            "name": "Tobias G\u00fcnther",
            "type": "sparse",
            "x": -0.4737251125807873,
            "y": -0.06631685132679209
        },
        {
            "id": "s_1226",
            "name": "Hamish A. Carr",
            "type": "sparse",
            "x": 0.23425519885757617,
            "y": -0.2159208480384713
        },
        {
            "id": "s_1227",
            "name": "Barbora Kozl\u00edkov\u00e1",
            "type": "sparse",
            "x": -0.1324224118003603,
            "y": -0.06096342345182715
        },
        {
            "id": "s_1228",
            "name": "Hongyuan Zha",
            "type": "sparse",
            "x": 0.3341341812774393,
            "y": 0.0775346007135777
        },
        {
            "id": "s_1229",
            "name": "Junlin Liu",
            "type": "sparse",
            "x": 0.09224694209604853,
            "y": 0.5338454558413556
        },
        {
            "id": "s_1230",
            "name": "Allison T. Silverman",
            "type": "sparse",
            "x": 0.11884943740987552,
            "y": 0.42111101454982824
        },
        {
            "id": "s_1231",
            "name": "Dmitry A. Karpeyev",
            "type": "sparse",
            "x": 0.17525297661208053,
            "y": 0.4055571353056361
        },
        {
            "id": "s_1232",
            "name": "Robert L. Jacob",
            "type": "sparse",
            "x": 0.13301537297063235,
            "y": 0.4103379586243327
        },
        {
            "id": "s_1233",
            "name": "Alexander Bock 0002",
            "type": "sparse",
            "x": -0.2860117082146331,
            "y": 0.3610005412030866
        },
        {
            "id": "s_1234",
            "name": "Greg L. Nelson",
            "type": "sparse",
            "x": -0.05665488449854522,
            "y": -0.4107569500701521
        },
        {
            "id": "s_1235",
            "name": "Samira Shaikh",
            "type": "sparse",
            "x": 0.39970807045802226,
            "y": -0.5166957834909521
        },
        {
            "id": "s_1236",
            "name": "Christopher Mears",
            "type": "sparse",
            "x": -0.3494281993189862,
            "y": 0.15982196389944678
        },
        {
            "id": "s_1237",
            "name": "Maureen C. Stone",
            "type": "sparse",
            "x": 0.28248800609458935,
            "y": 0.04156395192147714
        },
        {
            "id": "s_1238",
            "name": "Stef van den Elzen",
            "type": "sparse",
            "x": 0.1591747585349735,
            "y": 0.6441870191395832
        },
        {
            "id": "s_1239",
            "name": "Jiun-Yi Tsai",
            "type": "sparse",
            "x": -0.0012625967965648054,
            "y": 0.4256709490948595
        },
        {
            "id": "s_1240",
            "name": "Erdem Kaya",
            "type": "sparse",
            "x": -0.06528025777817505,
            "y": 0.08459211816273876
        },
        {
            "id": "s_1241",
            "name": "Qiuhan Zhu",
            "type": "sparse",
            "x": 0.17039536792673765,
            "y": 0.11166413692555646
        },
        {
            "id": "s_1242",
            "name": "Hansj\u00f6rg Schmauder",
            "type": "sparse",
            "x": 0.060764329171787375,
            "y": -0.09958782627202313
        },
        {
            "id": "s_1243",
            "name": "Marcel van 't Veer",
            "type": "sparse",
            "x": 0.14717804796343686,
            "y": 0.654705220716464
        },
        {
            "id": "s_1244",
            "name": "Po-Ming Law",
            "type": "sparse",
            "x": 0.06256667131169165,
            "y": -0.01162257141226395
        },
        {
            "id": "s_1245",
            "name": "Mario Jelovic",
            "type": "sparse",
            "x": 0.11438791569232734,
            "y": -0.02983019436186403
        },
        {
            "id": "s_1246",
            "name": "Darren Edge",
            "type": "sparse",
            "x": -0.2723688289111244,
            "y": 0.028341739407937844
        },
        {
            "id": "s_1247",
            "name": "Jian-Guang Lou",
            "type": "sparse",
            "x": 0.2600248266602652,
            "y": 0.015612099569966337
        },
        {
            "id": "s_1248",
            "name": "Florian Ferstl",
            "type": "sparse",
            "x": 0.5479583230937194,
            "y": 0.1774163892727206
        },
        {
            "id": "s_1249",
            "name": "Nicholas Diakopoulos",
            "type": "sparse",
            "x": 0.12485305304522956,
            "y": -0.2850882750012915
        },
        {
            "id": "s_1250",
            "name": "Wei Chen 0001",
            "type": "sparse",
            "x": 0.16615829517977648,
            "y": 0.32247389197293375
        },
        {
            "id": "s_1251",
            "name": "Jeffrey Heer",
            "type": "sparse",
            "x": -0.06065244533220539,
            "y": -0.27655625221010693
        },
        {
            "id": "s_1252",
            "name": "Shimei Pan",
            "type": "sparse",
            "x": 0.14093501015931992,
            "y": 0.5737402396517657
        },
        {
            "id": "s_1253",
            "name": "Xinyue Ye",
            "type": "sparse",
            "x": 0.04041420101439236,
            "y": 0.45723878430981996
        },
        {
            "id": "s_1254",
            "name": "Alexander Kachkaev",
            "type": "sparse",
            "x": -0.2581373633274851,
            "y": 0.22400328843191544
        },
        {
            "id": "s_1255",
            "name": "Hong Wang",
            "type": "sparse",
            "x": 0.010936544904554361,
            "y": 0.3563367690842897
        },
        {
            "id": "s_1256",
            "name": "Noura Faraj",
            "type": "sparse",
            "x": 0.6077418081193644,
            "y": -0.38197202986392775
        },
        {
            "id": "s_1257",
            "name": "Svitlana Volkova",
            "type": "sparse",
            "x": 0.38473121835151136,
            "y": -0.5050442336427976
        },
        {
            "id": "s_1258",
            "name": "Cong Xie",
            "type": "sparse",
            "x": -0.8281348336528257,
            "y": -0.11118623953486999
        },
        {
            "id": "s_1259",
            "name": "Steffen Oeltze-Jafra",
            "type": "sparse",
            "x": -0.43070512073973677,
            "y": -0.3832934107916348
        },
        {
            "id": "s_1260",
            "name": "Peng Xu",
            "type": "sparse",
            "x": 0.20930571139946372,
            "y": -0.005598997064675641
        },
        {
            "id": "s_1261",
            "name": "Cong Guo 0004",
            "type": "sparse",
            "x": 0.2827927985117585,
            "y": 0.33311035938077976
        },
        {
            "id": "s_1262",
            "name": "Lars Hufnagel",
            "type": "sparse",
            "x": 0.29539849406607427,
            "y": 0.47765057075648415
        },
        {
            "id": "s_1263",
            "name": "Nam Wook Kim",
            "type": "sparse",
            "x": -0.28633092886650635,
            "y": -0.12870118704291175
        },
        {
            "id": "s_1264",
            "name": "Acar Tamersoy",
            "type": "sparse",
            "x": 0.11391805338234778,
            "y": -0.49234156879579405
        },
        {
            "id": "s_1265",
            "name": "Maria Garcia de la Banda",
            "type": "sparse",
            "x": -0.35088246338052353,
            "y": 0.15363896215890982
        },
        {
            "id": "s_1266",
            "name": "Sandy Engelhardt",
            "type": "sparse",
            "x": -0.3840094473549759,
            "y": -0.4499621099793378
        },
        {
            "id": "s_1267",
            "name": "Christian Frisson",
            "type": "sparse",
            "x": -0.29154141010869056,
            "y": -0.1362329650575649
        },
        {
            "id": "s_1268",
            "name": "Fang-Xin Ou-Yang",
            "type": "sparse",
            "x": 0.011399935448415886,
            "y": 0.5283444804130379
        },
        {
            "id": "s_1269",
            "name": "Sheila Bonde",
            "type": "sparse",
            "x": -0.19902161983639102,
            "y": 0.35396513827499615
        },
        {
            "id": "s_1270",
            "name": "J\u00fcrgen Pleiss",
            "type": "sparse",
            "x": 0.032249867375119416,
            "y": -0.4665648781366703
        },
        {
            "id": "s_1271",
            "name": "Youhao Wei",
            "type": "sparse",
            "x": -0.10504748378716725,
            "y": 0.23306050283118065
        },
        {
            "id": "s_1272",
            "name": "Jessica Hullman",
            "type": "sparse",
            "x": -0.14865454392378327,
            "y": -0.318692174141305
        },
        {
            "id": "s_1273",
            "name": "Yingcai Wu",
            "type": "sparse",
            "x": 0.0636864254144952,
            "y": 0.24387483105123353
        },
        {
            "id": "s_1274",
            "name": "Gregory Guterman",
            "type": "sparse",
            "x": 0.490103220907904,
            "y": 0.1207939974943114
        },
        {
            "id": "s_1275",
            "name": "Bahador Saket",
            "type": "sparse",
            "x": -0.02444908124466351,
            "y": -0.17060034677567282
        },
        {
            "id": "s_1276",
            "name": "Jocelyn Ng",
            "type": "sparse",
            "x": 0.09169647635532593,
            "y": -0.2107313562037951
        },
        {
            "id": "s_1277",
            "name": "Michael Haass",
            "type": "sparse",
            "x": 0.031606571582591184,
            "y": -0.3111231627348417
        },
        {
            "id": "s_1278",
            "name": "Akhilesh Camisetty",
            "type": "sparse",
            "x": 0.3906899188610333,
            "y": 0.36330092964721444
        },
        {
            "id": "s_1279",
            "name": "Ching-Shan Chou",
            "type": "sparse",
            "x": 0.33026288043839314,
            "y": 0.2090879647955578
        },
        {
            "id": "s_1280",
            "name": "Shaozu Cao",
            "type": "sparse",
            "x": 0.20021308914340374,
            "y": 0.21899641259347935
        },
        {
            "id": "s_1281",
            "name": "Sergej Stoppel",
            "type": "sparse",
            "x": -0.03362223712315605,
            "y": -0.3502301901509788
        },
        {
            "id": "s_1282",
            "name": "Ya-Hsin Hung",
            "type": "sparse",
            "x": 0.23699234135198186,
            "y": -0.187034037625121
        },
        {
            "id": "s_1283",
            "name": "Jian Zhao 0010",
            "type": "sparse",
            "x": 0.3018114990013682,
            "y": 0.040443992601911956
        },
        {
            "id": "s_1284",
            "name": "Aritra Dasgupta",
            "type": "sparse",
            "x": 0.03710223032322168,
            "y": 0.09480930722990803
        },
        {
            "id": "s_1285",
            "name": "Ying Zhao 0001",
            "type": "sparse",
            "x": 0.1593696294970158,
            "y": 0.29700804403337555
        },
        {
            "id": "s_1286",
            "name": "Mehdi Ammi",
            "type": "sparse",
            "x": -0.012627747410676557,
            "y": -0.25832500135095304
        },
        {
            "id": "s_1287",
            "name": "Nitesh V. Chawla",
            "type": "sparse",
            "x": 0.12247199921888938,
            "y": 0.44273706877313873
        },
        {
            "id": "s_1288",
            "name": "Anthony K. H. Tung",
            "type": "sparse",
            "x": 0.13755913272485845,
            "y": 0.4398499176055152
        },
        {
            "id": "s_1289",
            "name": "Patric Ljung",
            "type": "sparse",
            "x": -0.3162925324243136,
            "y": 0.41434289412999514
        },
        {
            "id": "s_1290",
            "name": "Fran\u00e7oise D\u00e9tienne",
            "type": "sparse",
            "x": -0.2927805614524476,
            "y": -0.041965499071064984
        },
        {
            "id": "s_1291",
            "name": "Anushka Anand",
            "type": "sparse",
            "x": -0.03635703683095075,
            "y": -0.4513987412041573
        },
        {
            "id": "s_1292",
            "name": "Hyunwoo Park",
            "type": "sparse",
            "x": 0.014659184450612369,
            "y": -0.13990103828706604
        },
        {
            "id": "s_1293",
            "name": "David M. Vock",
            "type": "sparse",
            "x": -0.7941779310840038,
            "y": 0.1732277490902263
        },
        {
            "id": "s_1294",
            "name": "Fabiano Petronetto",
            "type": "sparse",
            "x": 0.11332379532846956,
            "y": 0.35187557688437837
        },
        {
            "id": "s_1295",
            "name": "Katharina Reinecke",
            "type": "sparse",
            "x": -0.23385956164006053,
            "y": -0.39136177218474466
        },
        {
            "id": "s_1296",
            "name": "Clemens Nylandsted Klokmose",
            "type": "sparse",
            "x": -0.10559449505031272,
            "y": -0.25004799832695424
        },
        {
            "id": "s_1297",
            "name": "Xinyuan Yan",
            "type": "sparse",
            "x": -0.0296006281591414,
            "y": 0.1450277810580475
        },
        {
            "id": "s_1298",
            "name": "Wei Zeng 0004",
            "type": "sparse",
            "x": 0.11967965913069185,
            "y": 0.15415540758840235
        },
        {
            "id": "s_1299",
            "name": "Frank Staals",
            "type": "sparse",
            "x": -0.32111867756993234,
            "y": 0.35413535321175915
        },
        {
            "id": "s_1300",
            "name": "Stefan R\u00fcdis\u00fchli",
            "type": "sparse",
            "x": -0.5598266542016289,
            "y": 0.02174081660452808
        },
        {
            "id": "s_1301",
            "name": "Soumya Dutta",
            "type": "sparse",
            "x": 0.40776496389503847,
            "y": 0.1713193805097467
        },
        {
            "id": "s_1302",
            "name": "Felix Raith",
            "type": "sparse",
            "x": -0.8298537826789424,
            "y": -0.43795473980492594
        },
        {
            "id": "s_1303",
            "name": "Tran Minh Quan",
            "type": "sparse",
            "x": -0.38166462206659774,
            "y": -0.06175856719714831
        },
        {
            "id": "s_1304",
            "name": "Xiaoyu Wang 0001",
            "type": "sparse",
            "x": 0.31629417395368503,
            "y": -0.42571047274294205
        },
        {
            "id": "s_1305",
            "name": "Minjeong Kim",
            "type": "sparse",
            "x": 0.13174109781819288,
            "y": -0.2756644979975799
        },
        {
            "id": "s_1306",
            "name": "Jian Zhang 0070",
            "type": "sparse",
            "x": -0.021913286033854822,
            "y": 0.08051917532451673
        },
        {
            "id": "s_1307",
            "name": "Gary K. L. Tam",
            "type": "sparse",
            "x": -0.3598043757133394,
            "y": -0.1444459789308933
        },
        {
            "id": "s_1308",
            "name": "Jaemin Jo",
            "type": "sparse",
            "x": -0.2202347511074626,
            "y": -0.14012381227369192
        },
        {
            "id": "s_1309",
            "name": "Mingqian Zhao",
            "type": "sparse",
            "x": 0.2688775046171113,
            "y": 0.17590368347975952
        },
        {
            "id": "s_1310",
            "name": "Eren Cakmak",
            "type": "sparse",
            "x": -0.028509061669368563,
            "y": 0.10619078365686073
        },
        {
            "id": "s_1311",
            "name": "Markus Jakobsson",
            "type": "sparse",
            "x": 0.1804978146162241,
            "y": -0.12254450680567121
        },
        {
            "id": "s_1312",
            "name": "Zhen Cao",
            "type": "sparse",
            "x": -0.7470475112498695,
            "y": -0.05751531230946584
        },
        {
            "id": "s_1313",
            "name": "Ali Sarvghad",
            "type": "sparse",
            "x": 0.05162315807840865,
            "y": -0.1451334655434294
        },
        {
            "id": "s_1314",
            "name": "Jim Jeffers",
            "type": "sparse",
            "x": 0.3264022883694825,
            "y": -0.3229292180752141
        },
        {
            "id": "s_1315",
            "name": "Jonathan Larson",
            "type": "sparse",
            "x": -0.2650088965273992,
            "y": 0.03415829290482757
        },
        {
            "id": "s_1316",
            "name": "Yasaman Ahmadi",
            "type": "sparse",
            "x": -0.0905341715472014,
            "y": -0.2455843264143651
        },
        {
            "id": "s_1317",
            "name": "Rafael Veras",
            "type": "sparse",
            "x": -0.22415981834734253,
            "y": 0.08959461328405824
        },
        {
            "id": "s_1318",
            "name": "David Talmage",
            "type": "sparse",
            "x": 0.40861808496768004,
            "y": 0.7868651971382501
        },
        {
            "id": "s_1319",
            "name": "Ying Yang",
            "type": "sparse",
            "x": -0.5249392452818025,
            "y": 0.031260814177897035
        },
        {
            "id": "s_1320",
            "name": "Wen Zhong",
            "type": "sparse",
            "x": -0.8519016963394032,
            "y": -0.10949017170511265
        },
        {
            "id": "s_1321",
            "name": "Michael Grossniklaus",
            "type": "sparse",
            "x": -0.07511641440063341,
            "y": 0.19180247322388
        },
        {
            "id": "s_1322",
            "name": "Can Liu 0004",
            "type": "sparse",
            "x": 0.19336392449322698,
            "y": 0.2911814663689772
        },
        {
            "id": "s_1323",
            "name": "Markus John",
            "type": "sparse",
            "x": 0.10542441689265271,
            "y": -0.3278243331978338
        },
        {
            "id": "s_1324",
            "name": "Le Liu 0007",
            "type": "sparse",
            "x": -0.44411543408172977,
            "y": 0.4125881762279187
        },
        {
            "id": "s_1325",
            "name": "Liu Ren",
            "type": "sparse",
            "x": 0.13834565747366073,
            "y": 0.0467410253832737
        },
        {
            "id": "s_1326",
            "name": "Michael Wybrow",
            "type": "sparse",
            "x": -0.4615202368923235,
            "y": 0.03228212137896293
        },
        {
            "id": "s_1327",
            "name": "Morteza Karimzadeh",
            "type": "sparse",
            "x": 0.17821271415174847,
            "y": 0.46211416181053605
        },
        {
            "id": "s_1328",
            "name": "Wilmot Li",
            "type": "sparse",
            "x": -0.18362278447426017,
            "y": -0.07391151784827137
        },
        {
            "id": "s_1329",
            "name": "Jorik Blaas",
            "type": "sparse",
            "x": 0.16665294829985933,
            "y": 0.6435941647643292
        },
        {
            "id": "s_1330",
            "name": "Lane T. Harrison",
            "type": "sparse",
            "x": -0.3765355275384871,
            "y": 0.12471927198898121
        },
        {
            "id": "s_1331",
            "name": "Mustafa Hajij",
            "type": "sparse",
            "x": 0.10056675507894777,
            "y": -0.029689179666105354
        },
        {
            "id": "s_1332",
            "name": "Saleema Amershi",
            "type": "sparse",
            "x": -0.10417773678243226,
            "y": -0.11439810977344834
        },
        {
            "id": "s_1333",
            "name": "Kah Chun Lau",
            "type": "sparse",
            "x": 0.29045966866877854,
            "y": -0.20809332504494782
        },
        {
            "id": "s_1334",
            "name": "Jo\u00e3o Luiz Dihl Comba",
            "type": "sparse",
            "x": -0.1118005830970607,
            "y": 0.17534193376987736
        },
        {
            "id": "s_1335",
            "name": "Bilal Alsallakh",
            "type": "sparse",
            "x": -0.06080423054977319,
            "y": -0.23267888068000864
        },
        {
            "id": "s_1336",
            "name": "Catherine Plaisant",
            "type": "sparse",
            "x": 0.4672678207248815,
            "y": 0.05335819565856148
        },
        {
            "id": "s_1337",
            "name": "Michael Burch",
            "type": "sparse",
            "x": 0.08312917881632448,
            "y": -0.20728754606382405
        },
        {
            "id": "s_1338",
            "name": "Faisal Taher",
            "type": "sparse",
            "x": -0.2861197018393505,
            "y": -0.43643743270722074
        },
        {
            "id": "s_1339",
            "name": "Chi-Wing Fu",
            "type": "sparse",
            "x": 0.01960199839066328,
            "y": 0.08082233565693574
        },
        {
            "id": "s_1340",
            "name": "David Glickenstein",
            "type": "sparse",
            "x": -0.09161739111241836,
            "y": 0.19218100860809392
        },
        {
            "id": "s_1341",
            "name": "Johannes H\u00e4u\u00dfler",
            "type": "sparse",
            "x": -0.060329079098411856,
            "y": 0.2697915497482188
        },
        {
            "id": "s_1342",
            "name": "Thorsten Breitkreutz",
            "type": "sparse",
            "x": -0.07153804411349442,
            "y": 0.19954551324653483
        },
        {
            "id": "s_1343",
            "name": "Brian C. McCann",
            "type": "sparse",
            "x": -0.3597668408365477,
            "y": -0.12942845306254142
        },
        {
            "id": "s_1344",
            "name": "Yingchao Wang",
            "type": "sparse",
            "x": 0.12646140859329633,
            "y": 0.29743720937762724
        },
        {
            "id": "s_1345",
            "name": "G\u00e1bor Janiga",
            "type": "sparse",
            "x": -0.437960745618371,
            "y": -0.3738714820761174
        },
        {
            "id": "s_1346",
            "name": "Hanspeter Pfister",
            "type": "sparse",
            "x": -0.22712319640408857,
            "y": -0.12418004888910086
        },
        {
            "id": "s_1347",
            "name": "Yvonne Jansen",
            "type": "sparse",
            "x": -0.24452196151728994,
            "y": -0.34843336425044247
        },
        {
            "id": "s_1348",
            "name": "Thomas Torsney-Weir",
            "type": "sparse",
            "x": -0.07417036516570098,
            "y": -0.32229631487786753
        },
        {
            "id": "s_1349",
            "name": "Aidan Slingsby",
            "type": "sparse",
            "x": -0.18422624711830685,
            "y": 0.20756746235829993
        },
        {
            "id": "s_1350",
            "name": "Vincent van Unen",
            "type": "sparse",
            "x": -0.11825358227169326,
            "y": -0.5217329378216498
        },
        {
            "id": "s_1351",
            "name": "David Hysom",
            "type": "sparse",
            "x": 0.22390066637226225,
            "y": -0.2627862798659912
        },
        {
            "id": "s_1352",
            "name": "Douglas C. Montgomery",
            "type": "sparse",
            "x": -0.0009344356531057554,
            "y": 0.42534929420384215
        },
        {
            "id": "s_1353",
            "name": "Mondrian Hsieh",
            "type": "sparse",
            "x": -0.1737377417322537,
            "y": 0.38201537033043853
        },
        {
            "id": "s_1354",
            "name": "Pere-Pau V\u00e1zquez",
            "type": "sparse",
            "x": -0.29812455172810065,
            "y": 0.10416613224214606
        },
        {
            "id": "s_1355",
            "name": "James Tompkin 0001",
            "type": "sparse",
            "x": -0.28159088927294823,
            "y": -0.20679156417850958
        },
        {
            "id": "s_1356",
            "name": "Yongsu Ahn",
            "type": "sparse",
            "x": 0.09013681920342818,
            "y": 0.1345209173715519
        },
        {
            "id": "s_1357",
            "name": "Honghui Mei",
            "type": "sparse",
            "x": 0.18094000112696276,
            "y": 0.28565003427167934
        },
        {
            "id": "s_1358",
            "name": "Florian Grassinger",
            "type": "sparse",
            "x": -0.1869252060814208,
            "y": -0.4593834372293062
        },
        {
            "id": "s_1359",
            "name": "Thomas Theu\u00dfl",
            "type": "sparse",
            "x": -0.20027784965194942,
            "y": -0.40985328655792186
        },
        {
            "id": "s_1360",
            "name": "Philipp Zimmermann",
            "type": "sparse",
            "x": -0.07526178768114533,
            "y": 0.19673685162867283
        },
        {
            "id": "s_1361",
            "name": "Ulderico Fugacci",
            "type": "sparse",
            "x": 0.24416203686176197,
            "y": 0.4606952297876498
        },
        {
            "id": "s_1362",
            "name": "Graham Johnson",
            "type": "sparse",
            "x": -0.05395531144037522,
            "y": -0.16957600606827838
        },
        {
            "id": "s_1363",
            "name": "Andrew Cunningham",
            "type": "sparse",
            "x": -0.20353782076104457,
            "y": -0.03493610658986898
        },
        {
            "id": "s_1364",
            "name": "Hyejin Im",
            "type": "sparse",
            "x": -0.319546263276796,
            "y": -0.10201189659416679
        },
        {
            "id": "s_1365",
            "name": "Soonwook Kwon",
            "type": "sparse",
            "x": 0.19823251686770132,
            "y": -0.23645068392232046
        },
        {
            "id": "s_1366",
            "name": "Joshua A. Levine",
            "type": "sparse",
            "x": 0.6348014964919327,
            "y": -0.4226014475559444
        },
        {
            "id": "s_1367",
            "name": "Conglei Shi",
            "type": "sparse",
            "x": 0.0006009317463277566,
            "y": -0.0008486055195780607
        },
        {
            "id": "s_1368",
            "name": "Marco Agus",
            "type": "sparse",
            "x": -0.25134828469708004,
            "y": -0.254293966641733
        },
        {
            "id": "s_1369",
            "name": "Kevin Verbeek",
            "type": "sparse",
            "x": -0.3724715016280277,
            "y": 0.45957590330109177
        },
        {
            "id": "s_1370",
            "name": "Gokhan Sever",
            "type": "sparse",
            "x": 0.11707178639729096,
            "y": 0.4475141678511323
        },
        {
            "id": "s_1371",
            "name": "Sasha Schriber",
            "type": "sparse",
            "x": -0.31685140186497723,
            "y": -0.0970187821687947
        },
        {
            "id": "s_1372",
            "name": "Edward Choi",
            "type": "sparse",
            "x": 0.19615542174128492,
            "y": -0.227730380783858
        },
        {
            "id": "s_1373",
            "name": "Manuela Waldner",
            "type": "sparse",
            "x": 0.17176733799714525,
            "y": -0.03698483391602308
        },
        {
            "id": "s_1374",
            "name": "Yao Ming",
            "type": "sparse",
            "x": 0.18219461406568369,
            "y": 0.15455333221654974
        },
        {
            "id": "s_1375",
            "name": "Haeyong Chung",
            "type": "sparse",
            "x": 0.36589743796845264,
            "y": 0.07627466941524248
        },
        {
            "id": "s_1376",
            "name": "Si Qin",
            "type": "sparse",
            "x": 0.28621647799304023,
            "y": 0.023746982183530142
        },
        {
            "id": "s_1377",
            "name": "Xinhuan Shu",
            "type": "sparse",
            "x": -0.003371740517420422,
            "y": 0.2382637776326367
        },
        {
            "id": "s_1378",
            "name": "Miriah Meyer",
            "type": "sparse",
            "x": -0.2219524242092678,
            "y": -0.03705256771276513
        },
        {
            "id": "s_1379",
            "name": "Joel W. Reed",
            "type": "sparse",
            "x": 0.42631675382632783,
            "y": -0.243735780924088
        },
        {
            "id": "s_1380",
            "name": "Weifeng Chen 0002",
            "type": "sparse",
            "x": 0.13674084332281183,
            "y": 0.446713871161673
        },
        {
            "id": "s_1381",
            "name": "Rebecca Kehlbeck",
            "type": "sparse",
            "x": -0.08999305606846979,
            "y": 0.0918100890278018
        },
        {
            "id": "s_1382",
            "name": "Arlind Nocaj",
            "type": "sparse",
            "x": 0.03383859442060118,
            "y": 0.0009807548758680115
        },
        {
            "id": "s_1383",
            "name": "Zoya Bylinskii",
            "type": "sparse",
            "x": -0.38295081946330817,
            "y": -0.18711678675256477
        },
        {
            "id": "s_1384",
            "name": "Jing Wu 0004",
            "type": "sparse",
            "x": 0.09813565192807806,
            "y": 0.5031303290671252
        },
        {
            "id": "s_1385",
            "name": "Hammad Haleem",
            "type": "sparse",
            "x": 0.17783174428087173,
            "y": -0.0784849618299395
        },
        {
            "id": "s_1386",
            "name": "Thomas Nagel",
            "type": "sparse",
            "x": -0.8242897556075383,
            "y": -0.444419661742111
        },
        {
            "id": "s_1387",
            "name": "Farah Kamw",
            "type": "sparse",
            "x": 0.05191632016118129,
            "y": 0.45695170303081556
        },
        {
            "id": "s_1388",
            "name": "Sixiao Yang",
            "type": "sparse",
            "x": 0.18856394899490958,
            "y": 0.1255875434032046
        },
        {
            "id": "s_1389",
            "name": "Kyeongpil Kang",
            "type": "sparse",
            "x": 0.13207819234490584,
            "y": -0.28249914115888985
        },
        {
            "id": "s_1390",
            "name": "Marc Rautenhaus",
            "type": "sparse",
            "x": 0.49639039498409715,
            "y": 0.19837138909337199
        },
        {
            "id": "s_1391",
            "name": "Blair Lyons",
            "type": "sparse",
            "x": -0.07635156224645186,
            "y": -0.20437878045258528
        },
        {
            "id": "s_1392",
            "name": "Peter Filzmoser",
            "type": "sparse",
            "x": -0.14627164998182451,
            "y": -0.49952480592952525
        },
        {
            "id": "s_1393",
            "name": "Tobias Kulschewski",
            "type": "sparse",
            "x": 0.03504442465725007,
            "y": -0.4754195686517047
        },
        {
            "id": "s_1394",
            "name": "Thomas Zichner",
            "type": "sparse",
            "x": -0.14595548267075997,
            "y": -0.41812028286871417
        },
        {
            "id": "s_1395",
            "name": "Nigel W. John",
            "type": "sparse",
            "x": -0.35839084982282987,
            "y": -0.1364509632993062
        },
        {
            "id": "s_1396",
            "name": "Chao Ma 0023",
            "type": "sparse",
            "x": 0.04040307738005624,
            "y": 0.45721917760590497
        },
        {
            "id": "s_1397",
            "name": "Lei Qin",
            "type": "sparse",
            "x": -0.49915011253082514,
            "y": -0.25174344041183927
        },
        {
            "id": "s_1398",
            "name": "Tyler Estro",
            "type": "sparse",
            "x": -0.7490728992514215,
            "y": -0.05020857534987194
        },
        {
            "id": "s_1399",
            "name": "Fenjin Ye",
            "type": "sparse",
            "x": 0.12977307206975663,
            "y": 0.44237658905077976
        },
        {
            "id": "s_1400",
            "name": "Gregory Heinlein",
            "type": "sparse",
            "x": 0.4277013413472126,
            "y": 0.17501822985831678
        },
        {
            "id": "s_1401",
            "name": "Alina Gvozdik",
            "type": "sparse",
            "x": 0.35114070514811385,
            "y": -0.09804323312594086
        },
        {
            "id": "s_1402",
            "name": "Nicolas Heulot",
            "type": "sparse",
            "x": -0.07834201199929454,
            "y": -0.05101401837847246
        },
        {
            "id": "s_1403",
            "name": "Changgong Zhang",
            "type": "sparse",
            "x": -0.24478638428853117,
            "y": -0.4956946538003132
        },
        {
            "id": "s_1404",
            "name": "Juan R. Cebral",
            "type": "sparse",
            "x": -0.4338991416490039,
            "y": -0.37825557351549416
        },
        {
            "id": "s_1405",
            "name": "Uta Hinrichs",
            "type": "sparse",
            "x": -0.25575796328316425,
            "y": 0.11179077744718838
        },
        {
            "id": "s_1406",
            "name": "David Uribe",
            "type": "sparse",
            "x": 0.17838566258373476,
            "y": -0.48582875454723656
        },
        {
            "id": "s_1407",
            "name": "Naren Ramakrishnan",
            "type": "sparse",
            "x": 0.4348483731175135,
            "y": 0.28004569986874017
        },
        {
            "id": "s_1408",
            "name": "Shenghui Cheng",
            "type": "sparse",
            "x": -0.7420000003255149,
            "y": -0.1361269753574255
        },
        {
            "id": "s_1409",
            "name": "Cassiano Sugiyama",
            "type": "sparse",
            "x": -0.7841637186794843,
            "y": 0.12855505573349854
        },
        {
            "id": "s_1410",
            "name": "Kai B\u00fcrger",
            "type": "sparse",
            "x": 0.5781881416758536,
            "y": 0.17616300816470634
        },
        {
            "id": "s_1411",
            "name": "Joakim Kilby",
            "type": "sparse",
            "x": -0.33727771055829875,
            "y": 0.4172257758834352
        },
        {
            "id": "s_1412",
            "name": "Minsuk Kahng",
            "type": "sparse",
            "x": 0.10330100287990379,
            "y": -0.6372863253802651
        },
        {
            "id": "s_1413",
            "name": "Anna Gogolou",
            "type": "sparse",
            "x": 0.10329738043054064,
            "y": -0.42757029982545947
        },
        {
            "id": "s_1414",
            "name": "Carter Emmart",
            "type": "sparse",
            "x": -0.2904742664519181,
            "y": 0.39379753511431737
        },
        {
            "id": "s_1415",
            "name": "Xiaojuan Ma",
            "type": "sparse",
            "x": 0.2085599639651664,
            "y": -0.023225408535903137
        },
        {
            "id": "s_1416",
            "name": "John Hardy",
            "type": "sparse",
            "x": -0.28067395346865975,
            "y": -0.4426401947289815
        },
        {
            "id": "s_1417",
            "name": "Min Lu 0002",
            "type": "sparse",
            "x": 0.37142262064067955,
            "y": 0.1713667951650117
        },
        {
            "id": "s_1418",
            "name": "Jamie Morgenstern",
            "type": "sparse",
            "x": 0.12439049206289698,
            "y": -0.6127077492754409
        },
        {
            "id": "s_1419",
            "name": "Robin Bader",
            "type": "sparse",
            "x": -0.5660650664066169,
            "y": 0.008770953322394607
        },
        {
            "id": "s_1420",
            "name": "Youssef S. G. Nashed",
            "type": "sparse",
            "x": 0.26281104206842265,
            "y": 0.27493357024387816
        },
        {
            "id": "s_1421",
            "name": "Peter Mindek",
            "type": "sparse",
            "x": -0.08562441781387696,
            "y": -0.1981495992007911
        },
        {
            "id": "s_1422",
            "name": "Jonathan Woodruff",
            "type": "sparse",
            "x": -0.27966983288707303,
            "y": -0.4360546793879798
        },
        {
            "id": "s_1423",
            "name": "Qiaomu Shen",
            "type": "sparse",
            "x": 0.14945787053268658,
            "y": 0.14340707467160943
        },
        {
            "id": "s_1424",
            "name": "Devin Singh",
            "type": "sparse",
            "x": 0.3832012406338621,
            "y": -0.14919941082011196
        },
        {
            "id": "s_1425",
            "name": "Michael Sprenger",
            "type": "sparse",
            "x": -0.5695640328781225,
            "y": 0.014519498385640794
        },
        {
            "id": "s_1426",
            "name": "Jimbo Wilson",
            "type": "sparse",
            "x": 0.00410888655466266,
            "y": -0.6033210009239579
        },
        {
            "id": "s_1427",
            "name": "Jiabao Li",
            "type": "sparse",
            "x": -0.30880498380494686,
            "y": -0.051569387091012636
        },
        {
            "id": "s_1428",
            "name": "Andr\u00e9s Monroy-Hern\u00e1ndez",
            "type": "sparse",
            "x": -0.14571218052202706,
            "y": -0.068174722281845
        },
        {
            "id": "s_1429",
            "name": "Ingo Wald",
            "type": "sparse",
            "x": 0.2688735557283567,
            "y": -0.3137062573006779
        },
        {
            "id": "s_1430",
            "name": "Noeska N. Smit",
            "type": "sparse",
            "x": -0.20239701300227625,
            "y": -0.39238704098326127
        },
        {
            "id": "s_1431",
            "name": "Gordon L. Kindlmann",
            "type": "sparse",
            "x": -0.14845738284159718,
            "y": -0.001377668129138391
        },
        {
            "id": "s_1432",
            "name": "Anbang Xu",
            "type": "sparse",
            "x": 0.43468490285139016,
            "y": 0.18499744018416858
        },
        {
            "id": "s_1433",
            "name": "Michael Wolverton",
            "type": "sparse",
            "x": 0.03981335187339758,
            "y": -0.07474751086686544
        },
        {
            "id": "s_1434",
            "name": "Torsten M\u00f6ller",
            "type": "sparse",
            "x": -0.012852878083669713,
            "y": -0.24568884325782495
        },
        {
            "id": "s_1435",
            "name": "Christian Luksch",
            "type": "sparse",
            "x": -0.1260758100374392,
            "y": -0.36152314455307055
        },
        {
            "id": "s_1436",
            "name": "Jun Zhu 0001",
            "type": "sparse",
            "x": 0.1291238170828881,
            "y": 0.4755913444264418
        },
        {
            "id": "s_1437",
            "name": "Kanit Wongsuphasawat",
            "type": "sparse",
            "x": -0.015893877665516042,
            "y": -0.48626475648721695
        },
        {
            "id": "s_1438",
            "name": "Xiao Xie",
            "type": "sparse",
            "x": 0.03944552620977333,
            "y": 0.3127130576970693
        },
        {
            "id": "s_1439",
            "name": "Huy T. Vo",
            "type": "sparse",
            "x": -0.15060836846399545,
            "y": 0.34546158254374865
        },
        {
            "id": "s_1440",
            "name": "John H. Reppy",
            "type": "sparse",
            "x": -0.1865647263656935,
            "y": -0.017235488047391848
        },
        {
            "id": "s_1441",
            "name": "Christopher Andrews 0001",
            "type": "sparse",
            "x": 0.42837278388846445,
            "y": 0.11057453049312854
        },
        {
            "id": "s_1442",
            "name": "Nils Lichtenberg",
            "type": "sparse",
            "x": -0.390222675920108,
            "y": -0.4457213970127625
        },
        {
            "id": "s_1443",
            "name": "Tianyi Lao",
            "type": "sparse",
            "x": 0.3226041671347768,
            "y": 0.20494241170436703
        },
        {
            "id": "s_1444",
            "name": "Fabio Dias 0001",
            "type": "sparse",
            "x": 0.19211990768372095,
            "y": 0.3331020896420639
        },
        {
            "id": "s_1445",
            "name": "Narges Mahyar",
            "type": "sparse",
            "x": 0.09121155579541215,
            "y": -0.11588372981964654
        },
        {
            "id": "s_1446",
            "name": "Alberto Gonzalez",
            "type": "sparse",
            "x": 0.4124798967363486,
            "y": -0.13763017746627323
        },
        {
            "id": "s_1447",
            "name": "Moushumi Sharmin",
            "type": "sparse",
            "x": 0.1442513482790649,
            "y": -0.6385797261706898
        },
        {
            "id": "s_1448",
            "name": "Audrey S. Wang",
            "type": "sparse",
            "x": 0.11431828636928205,
            "y": 0.4269866059507106
        },
        {
            "id": "s_1449",
            "name": "Roman R\u00e4dle",
            "type": "sparse",
            "x": -0.11228759919450232,
            "y": -0.24425452383552873
        },
        {
            "id": "s_1450",
            "name": "Haifeng Zhang",
            "type": "sparse",
            "x": -0.007825429460330463,
            "y": 0.099979238370898
        },
        {
            "id": "s_1451",
            "name": "Cheng Tang",
            "type": "sparse",
            "x": 0.16880869161576134,
            "y": 0.2564070993063836
        },
        {
            "id": "s_1452",
            "name": "Lin Yan 0003",
            "type": "sparse",
            "x": 0.29241929478000817,
            "y": -0.16223395713287334
        },
        {
            "id": "s_1453",
            "name": "Robert Haimes",
            "type": "sparse",
            "x": 0.7196494032555851,
            "y": -0.5010071652920396
        },
        {
            "id": "s_1454",
            "name": "Jason Alexander",
            "type": "sparse",
            "x": -0.2747359429611976,
            "y": -0.4410504489532596
        },
        {
            "id": "s_1455",
            "name": "Jonathas Costa",
            "type": "sparse",
            "x": -0.26248623719384756,
            "y": 0.3966198315344834
        },
        {
            "id": "s_1456",
            "name": "Jason Leigh",
            "type": "sparse",
            "x": 0.41137854580705635,
            "y": -0.13210959189697968
        },
        {
            "id": "s_1457",
            "name": "Zhutian Chen",
            "type": "sparse",
            "x": 0.1383782922767327,
            "y": 0.11527716692333018
        },
        {
            "id": "s_1458",
            "name": "Dimitrios Papadopoulos 0001",
            "type": "sparse",
            "x": -0.009180329972164806,
            "y": 0.20543729477427788
        },
        {
            "id": "s_1459",
            "name": "Felix Brodkorb",
            "type": "sparse",
            "x": 0.028708806627819907,
            "y": 0.37234106981445775
        },
        {
            "id": "s_1460",
            "name": "Michael Stonebraker",
            "type": "sparse",
            "x": -0.248274652841114,
            "y": -0.009722743847983788
        },
        {
            "id": "s_1461",
            "name": "Michael Steptoe",
            "type": "sparse",
            "x": 0.006274429818627358,
            "y": 0.41737873373760037
        },
        {
            "id": "s_1462",
            "name": "Amir A. Zamani",
            "type": "sparse",
            "x": -0.49392640471386834,
            "y": -0.25650989202728297
        },
        {
            "id": "s_1463",
            "name": "Kuno Kurzhals",
            "type": "sparse",
            "x": 0.09558544462655451,
            "y": -0.2662760139228032
        },
        {
            "id": "s_1464",
            "name": "Weixia Xu",
            "type": "sparse",
            "x": 0.1065549144124052,
            "y": 0.49928218671367347
        },
        {
            "id": "s_1465",
            "name": "Chris Yi",
            "type": "sparse",
            "x": 0.17422359514818236,
            "y": -0.07169865470383704
        },
        {
            "id": "s_1466",
            "name": "Jing Su",
            "type": "sparse",
            "x": 0.042537217438855746,
            "y": 0.5034738028232286
        },
        {
            "id": "s_1467",
            "name": "Kai Lawonn",
            "type": "sparse",
            "x": -0.2973917324600648,
            "y": -0.35777458607172574
        },
        {
            "id": "s_1468",
            "name": "Juri Buchm\u00fcller",
            "type": "sparse",
            "x": -0.02368419342493901,
            "y": 0.11006455672160993
        },
        {
            "id": "s_1469",
            "name": "Alex Endert",
            "type": "sparse",
            "x": 0.06867043658594148,
            "y": -0.21686206604207217
        },
        {
            "id": "s_1470",
            "name": "Florian Spechtenhauser",
            "type": "sparse",
            "x": -0.07889813720737844,
            "y": -0.3320159498421486
        },
        {
            "id": "s_1471",
            "name": "Sashank Santhanam",
            "type": "sparse",
            "x": 0.39285803650474205,
            "y": -0.52310504195871
        },
        {
            "id": "s_1472",
            "name": "Attila Gyulassy",
            "type": "sparse",
            "x": 0.3201071028274507,
            "y": -0.2539603642995135
        },
        {
            "id": "s_1473",
            "name": "Hui Fang 0003",
            "type": "sparse",
            "x": -0.3475750989647514,
            "y": -0.16740017994391687
        },
        {
            "id": "s_1474",
            "name": "Yangqiu Song",
            "type": "sparse",
            "x": 0.19675257016070877,
            "y": 0.21626181868550123
        },
        {
            "id": "s_1475",
            "name": "Gregory D. Abram",
            "type": "sparse",
            "x": 0.17740201719433055,
            "y": -0.3675273348968974
        },
        {
            "id": "s_1476",
            "name": "Christopher Collins 0001",
            "type": "sparse",
            "x": -0.15532508979387455,
            "y": 0.05006870706528455
        },
        {
            "id": "s_1477",
            "name": "Hairong Wang",
            "type": "sparse",
            "x": 0.17405976112132573,
            "y": 0.3314024196226805
        },
        {
            "id": "s_1478",
            "name": "Sebastien Boyer",
            "type": "sparse",
            "x": 0.2679273713300533,
            "y": 0.18304750563217373
        },
        {
            "id": "s_1479",
            "name": "Qiusheng Li",
            "type": "sparse",
            "x": 0.2907623747252563,
            "y": 0.30007616527415754
        },
        {
            "id": "s_1480",
            "name": "Aaron Knoll",
            "type": "sparse",
            "x": 0.2955084922768964,
            "y": -0.26632485856334703
        },
        {
            "id": "s_1481",
            "name": "Themis Palpanas",
            "type": "sparse",
            "x": 0.09408200355387376,
            "y": -0.4304801060693332
        },
        {
            "id": "s_1482",
            "name": "Renfei Huang",
            "type": "sparse",
            "x": 0.20314193064904493,
            "y": 0.08958956325843845
        },
        {
            "id": "s_1483",
            "name": "Matthias Zeppelzauer",
            "type": "sparse",
            "x": -0.1014197278056019,
            "y": 0.046904024113939985
        },
        {
            "id": "s_1484",
            "name": "Halldor Janetzko",
            "type": "sparse",
            "x": -0.0714170233246175,
            "y": 0.18923386330120168
        },
        {
            "id": "s_1485",
            "name": "Tushar M. Athawale",
            "type": "sparse",
            "x": -0.04834367350734045,
            "y": -0.4395708761476795
        },
        {
            "id": "s_1486",
            "name": "Mathieu Le Muzic",
            "type": "sparse",
            "x": -0.11931884036050236,
            "y": -0.1349966240087428
        },
        {
            "id": "s_1487",
            "name": "Viswanath Aluru",
            "type": "sparse",
            "x": 0.08404932943847297,
            "y": 0.32144289763546535
        },
        {
            "id": "s_1488",
            "name": "Daniel J\u00f6nsson",
            "type": "sparse",
            "x": -0.40722208076538813,
            "y": 0.29670719193285483
        },
        {
            "id": "s_1489",
            "name": "G. David Richardson",
            "type": "sparse",
            "x": 0.42949129527931934,
            "y": -0.23852956821662094
        }
    ],
    "links": [
        {
            "source": "r_0",
            "target": "s_795"
        },
        {
            "source": "r_0",
            "target": "s_554"
        },
        {
            "source": "r_0",
            "target": "s_1437"
        },
        {
            "source": "r_0",
            "target": "s_1251"
        },
        {
            "source": "r_1",
            "target": "s_476"
        },
        {
            "source": "r_1",
            "target": "s_1120"
        },
        {
            "source": "r_1",
            "target": "s_812"
        },
        {
            "source": "r_1",
            "target": "s_1196"
        },
        {
            "source": "r_1",
            "target": "s_1436"
        },
        {
            "source": "r_1",
            "target": "s_301"
        },
        {
            "source": "r_2",
            "target": "s_1437"
        },
        {
            "source": "r_2",
            "target": "s_554"
        },
        {
            "source": "r_2",
            "target": "s_1291"
        },
        {
            "source": "r_2",
            "target": "s_559"
        },
        {
            "source": "r_2",
            "target": "s_1175"
        },
        {
            "source": "r_2",
            "target": "s_1251"
        },
        {
            "source": "r_3",
            "target": "s_1437"
        },
        {
            "source": "r_3",
            "target": "s_990"
        },
        {
            "source": "r_3",
            "target": "s_694"
        },
        {
            "source": "r_3",
            "target": "s_1426"
        },
        {
            "source": "r_3",
            "target": "s_31"
        },
        {
            "source": "r_3",
            "target": "s_365"
        },
        {
            "source": "r_3",
            "target": "s_432"
        },
        {
            "source": "r_3",
            "target": "s_1212"
        },
        {
            "source": "r_3",
            "target": "s_159"
        },
        {
            "source": "r_4",
            "target": "s_1412"
        },
        {
            "source": "r_4",
            "target": "s_755"
        },
        {
            "source": "r_4",
            "target": "s_796"
        },
        {
            "source": "r_4",
            "target": "s_569"
        },
        {
            "source": "r_5",
            "target": "s_869"
        },
        {
            "source": "r_5",
            "target": "s_778"
        },
        {
            "source": "r_5",
            "target": "s_1346"
        },
        {
            "source": "r_5",
            "target": "s_445"
        },
        {
            "source": "r_6",
            "target": "s_1069"
        },
        {
            "source": "r_6",
            "target": "s_1383"
        },
        {
            "source": "r_6",
            "target": "s_1263"
        },
        {
            "source": "r_6",
            "target": "s_352"
        },
        {
            "source": "r_6",
            "target": "s_482"
        },
        {
            "source": "r_6",
            "target": "s_1063"
        },
        {
            "source": "r_6",
            "target": "s_1346"
        },
        {
            "source": "r_6",
            "target": "s_560"
        },
        {
            "source": "r_7",
            "target": "s_39"
        },
        {
            "source": "r_7",
            "target": "s_1215"
        },
        {
            "source": "r_7",
            "target": "s_728"
        },
        {
            "source": "r_7",
            "target": "s_1208"
        },
        {
            "source": "r_7",
            "target": "s_1011"
        },
        {
            "source": "r_8",
            "target": "s_680"
        },
        {
            "source": "r_8",
            "target": "s_892"
        },
        {
            "source": "r_8",
            "target": "s_440"
        },
        {
            "source": "r_8",
            "target": "s_133"
        },
        {
            "source": "r_9",
            "target": "s_1181"
        },
        {
            "source": "r_9",
            "target": "s_429"
        },
        {
            "source": "r_9",
            "target": "s_1026"
        },
        {
            "source": "r_9",
            "target": "s_1372"
        },
        {
            "source": "r_9",
            "target": "s_938"
        },
        {
            "source": "r_9",
            "target": "s_1365"
        },
        {
            "source": "r_9",
            "target": "s_109"
        },
        {
            "source": "r_9",
            "target": "s_320"
        },
        {
            "source": "r_10",
            "target": "s_795"
        },
        {
            "source": "r_10",
            "target": "s_625"
        },
        {
            "source": "r_10",
            "target": "s_56"
        },
        {
            "source": "r_10",
            "target": "s_1251"
        },
        {
            "source": "r_11",
            "target": "s_905"
        },
        {
            "source": "r_11",
            "target": "s_1086"
        },
        {
            "source": "r_11",
            "target": "s_1181"
        },
        {
            "source": "r_11",
            "target": "s_698"
        },
        {
            "source": "r_11",
            "target": "s_1003"
        },
        {
            "source": "r_12",
            "target": "s_554"
        },
        {
            "source": "r_12",
            "target": "s_1209"
        },
        {
            "source": "r_12",
            "target": "s_1234"
        },
        {
            "source": "r_12",
            "target": "s_222"
        },
        {
            "source": "r_12",
            "target": "s_474"
        },
        {
            "source": "r_12",
            "target": "s_1175"
        },
        {
            "source": "r_12",
            "target": "s_1251"
        },
        {
            "source": "r_13",
            "target": "s_339"
        },
        {
            "source": "r_13",
            "target": "s_1347"
        },
        {
            "source": "r_13",
            "target": "s_231"
        },
        {
            "source": "r_14",
            "target": "s_1374"
        },
        {
            "source": "r_14",
            "target": "s_137"
        },
        {
            "source": "r_14",
            "target": "s_457"
        },
        {
            "source": "r_15",
            "target": "s_1126"
        },
        {
            "source": "r_15",
            "target": "s_280"
        },
        {
            "source": "r_15",
            "target": "s_603"
        },
        {
            "source": "r_15",
            "target": "s_1204"
        },
        {
            "source": "r_15",
            "target": "s_1346"
        },
        {
            "source": "r_16",
            "target": "s_780"
        },
        {
            "source": "r_16",
            "target": "s_181"
        },
        {
            "source": "r_16",
            "target": "s_1168"
        },
        {
            "source": "r_16",
            "target": "s_1087"
        },
        {
            "source": "r_16",
            "target": "s_1154"
        },
        {
            "source": "r_16",
            "target": "s_495"
        },
        {
            "source": "r_16",
            "target": "s_372"
        },
        {
            "source": "r_17",
            "target": "s_905"
        },
        {
            "source": "r_17",
            "target": "s_120"
        },
        {
            "source": "r_17",
            "target": "s_781"
        },
        {
            "source": "r_17",
            "target": "s_510"
        },
        {
            "source": "r_17",
            "target": "s_505"
        },
        {
            "source": "r_17",
            "target": "s_138"
        },
        {
            "source": "r_17",
            "target": "s_1136"
        },
        {
            "source": "r_17",
            "target": "s_1003"
        },
        {
            "source": "r_18",
            "target": "s_240"
        },
        {
            "source": "r_18",
            "target": "s_1459"
        },
        {
            "source": "r_18",
            "target": "s_839"
        },
        {
            "source": "r_18",
            "target": "s_9"
        },
        {
            "source": "r_18",
            "target": "s_1149"
        },
        {
            "source": "r_18",
            "target": "s_512"
        },
        {
            "source": "r_19",
            "target": "s_356"
        },
        {
            "source": "r_19",
            "target": "s_1469"
        },
        {
            "source": "r_19",
            "target": "s_842"
        },
        {
            "source": "r_19",
            "target": "s_1128"
        },
        {
            "source": "r_20",
            "target": "s_1067"
        },
        {
            "source": "r_20",
            "target": "s_638"
        },
        {
            "source": "r_20",
            "target": "s_1050"
        },
        {
            "source": "r_20",
            "target": "s_1082"
        },
        {
            "source": "r_20",
            "target": "s_98"
        },
        {
            "source": "r_20",
            "target": "s_137"
        },
        {
            "source": "r_20",
            "target": "s_1273"
        },
        {
            "source": "r_21",
            "target": "s_1163"
        },
        {
            "source": "r_21",
            "target": "s_1332"
        },
        {
            "source": "r_21",
            "target": "s_250"
        },
        {
            "source": "r_21",
            "target": "s_935"
        },
        {
            "source": "r_21",
            "target": "s_950"
        },
        {
            "source": "r_22",
            "target": "s_694"
        },
        {
            "source": "r_22",
            "target": "s_480"
        },
        {
            "source": "r_22",
            "target": "s_42"
        },
        {
            "source": "r_22",
            "target": "s_159"
        },
        {
            "source": "r_22",
            "target": "s_1212"
        },
        {
            "source": "r_22",
            "target": "s_1426"
        },
        {
            "source": "r_23",
            "target": "s_1204"
        },
        {
            "source": "r_23",
            "target": "s_202"
        },
        {
            "source": "r_23",
            "target": "s_751"
        },
        {
            "source": "r_23",
            "target": "s_906"
        },
        {
            "source": "r_23",
            "target": "s_1188"
        },
        {
            "source": "r_23",
            "target": "s_184"
        },
        {
            "source": "r_24",
            "target": "s_274"
        },
        {
            "source": "r_24",
            "target": "s_941"
        },
        {
            "source": "r_24",
            "target": "s_1396"
        },
        {
            "source": "r_24",
            "target": "s_745"
        },
        {
            "source": "r_24",
            "target": "s_1253"
        },
        {
            "source": "r_24",
            "target": "s_713"
        },
        {
            "source": "r_25",
            "target": "s_611"
        },
        {
            "source": "r_25",
            "target": "s_930"
        },
        {
            "source": "r_25",
            "target": "s_1366"
        },
        {
            "source": "r_25",
            "target": "s_729"
        },
        {
            "source": "r_25",
            "target": "s_524"
        },
        {
            "source": "r_26",
            "target": "s_1126"
        },
        {
            "source": "r_26",
            "target": "s_1367"
        },
        {
            "source": "r_26",
            "target": "s_1402"
        },
        {
            "source": "r_26",
            "target": "s_128"
        },
        {
            "source": "r_26",
            "target": "s_97"
        },
        {
            "source": "r_26",
            "target": "s_231"
        },
        {
            "source": "r_27",
            "target": "s_158"
        },
        {
            "source": "r_28",
            "target": "s_65"
        },
        {
            "source": "r_28",
            "target": "s_167"
        },
        {
            "source": "r_28",
            "target": "s_176"
        },
        {
            "source": "r_28",
            "target": "s_458"
        },
        {
            "source": "r_28",
            "target": "s_921"
        },
        {
            "source": "r_29",
            "target": "s_280"
        },
        {
            "source": "r_29",
            "target": "s_1427"
        },
        {
            "source": "r_29",
            "target": "s_718"
        },
        {
            "source": "r_29",
            "target": "s_1204"
        },
        {
            "source": "r_29",
            "target": "s_388"
        },
        {
            "source": "r_29",
            "target": "s_1126"
        },
        {
            "source": "r_29",
            "target": "s_1346"
        },
        {
            "source": "r_30",
            "target": "s_1374"
        },
        {
            "source": "r_30",
            "target": "s_1280"
        },
        {
            "source": "r_30",
            "target": "s_283"
        },
        {
            "source": "r_30",
            "target": "s_812"
        },
        {
            "source": "r_30",
            "target": "s_411"
        },
        {
            "source": "r_30",
            "target": "s_1474"
        },
        {
            "source": "r_30",
            "target": "s_137"
        },
        {
            "source": "r_31",
            "target": "s_1005"
        },
        {
            "source": "r_31",
            "target": "s_27"
        },
        {
            "source": "r_31",
            "target": "s_511"
        },
        {
            "source": "r_31",
            "target": "s_569"
        },
        {
            "source": "r_32",
            "target": "s_676"
        },
        {
            "source": "r_32",
            "target": "s_1484"
        },
        {
            "source": "r_32",
            "target": "s_1211"
        },
        {
            "source": "r_32",
            "target": "s_1342"
        },
        {
            "source": "r_32",
            "target": "s_1360"
        },
        {
            "source": "r_32",
            "target": "s_264"
        },
        {
            "source": "r_32",
            "target": "s_493"
        },
        {
            "source": "r_32",
            "target": "s_1149"
        },
        {
            "source": "r_32",
            "target": "s_1321"
        },
        {
            "source": "r_32",
            "target": "s_1003"
        },
        {
            "source": "r_33",
            "target": "s_966"
        },
        {
            "source": "r_33",
            "target": "s_1157"
        },
        {
            "source": "r_33",
            "target": "s_486"
        },
        {
            "source": "r_33",
            "target": "s_1054"
        },
        {
            "source": "r_33",
            "target": "s_145"
        },
        {
            "source": "r_33",
            "target": "s_408"
        },
        {
            "source": "r_34",
            "target": "s_1429"
        },
        {
            "source": "r_34",
            "target": "s_994"
        },
        {
            "source": "r_34",
            "target": "s_641"
        },
        {
            "source": "r_34",
            "target": "s_1095"
        },
        {
            "source": "r_34",
            "target": "s_1480"
        },
        {
            "source": "r_34",
            "target": "s_1314"
        },
        {
            "source": "r_34",
            "target": "s_514"
        },
        {
            "source": "r_34",
            "target": "s_657"
        },
        {
            "source": "r_35",
            "target": "s_28"
        },
        {
            "source": "r_35",
            "target": "s_1031"
        },
        {
            "source": "r_35",
            "target": "s_1469"
        },
        {
            "source": "r_35",
            "target": "s_735"
        },
        {
            "source": "r_36",
            "target": "s_846"
        },
        {
            "source": "r_36",
            "target": "s_453"
        },
        {
            "source": "r_36",
            "target": "s_1197"
        },
        {
            "source": "r_36",
            "target": "s_896"
        },
        {
            "source": "r_37",
            "target": "s_869"
        },
        {
            "source": "r_37",
            "target": "s_778"
        },
        {
            "source": "r_37",
            "target": "s_540"
        },
        {
            "source": "r_37",
            "target": "s_1135"
        },
        {
            "source": "r_37",
            "target": "s_1346"
        },
        {
            "source": "r_37",
            "target": "s_445"
        },
        {
            "source": "r_38",
            "target": "s_1335"
        },
        {
            "source": "r_38",
            "target": "s_898"
        },
        {
            "source": "r_38",
            "target": "s_922"
        },
        {
            "source": "r_38",
            "target": "s_191"
        },
        {
            "source": "r_38",
            "target": "s_1325"
        },
        {
            "source": "r_39",
            "target": "s_997"
        },
        {
            "source": "r_39",
            "target": "s_177"
        },
        {
            "source": "r_39",
            "target": "s_1181"
        },
        {
            "source": "r_40",
            "target": "s_476"
        },
        {
            "source": "r_40",
            "target": "s_1120"
        },
        {
            "source": "r_40",
            "target": "s_630"
        },
        {
            "source": "r_40",
            "target": "s_1436"
        },
        {
            "source": "r_40",
            "target": "s_301"
        },
        {
            "source": "r_41",
            "target": "s_1238"
        },
        {
            "source": "r_41",
            "target": "s_886"
        },
        {
            "source": "r_41",
            "target": "s_1329"
        },
        {
            "source": "r_41",
            "target": "s_61"
        },
        {
            "source": "r_42",
            "target": "s_39"
        },
        {
            "source": "r_42",
            "target": "s_158"
        },
        {
            "source": "r_43",
            "target": "s_866"
        },
        {
            "source": "r_43",
            "target": "s_1367"
        },
        {
            "source": "r_43",
            "target": "s_93"
        },
        {
            "source": "r_43",
            "target": "s_305"
        },
        {
            "source": "r_43",
            "target": "s_1177"
        },
        {
            "source": "r_43",
            "target": "s_956"
        },
        {
            "source": "r_44",
            "target": "s_147"
        },
        {
            "source": "r_44",
            "target": "s_1062"
        },
        {
            "source": "r_44",
            "target": "s_1005"
        },
        {
            "source": "r_44",
            "target": "s_1412"
        },
        {
            "source": "r_44",
            "target": "s_1418"
        },
        {
            "source": "r_44",
            "target": "s_295"
        },
        {
            "source": "r_45",
            "target": "s_813"
        },
        {
            "source": "r_45",
            "target": "s_1148"
        },
        {
            "source": "r_45",
            "target": "s_30"
        },
        {
            "source": "r_46",
            "target": "s_1272"
        },
        {
            "source": "r_46",
            "target": "s_568"
        },
        {
            "source": "r_46",
            "target": "s_1215"
        },
        {
            "source": "r_46",
            "target": "s_759"
        },
        {
            "source": "r_46",
            "target": "s_920"
        },
        {
            "source": "r_47",
            "target": "s_316"
        },
        {
            "source": "r_47",
            "target": "s_1214"
        },
        {
            "source": "r_47",
            "target": "s_1018"
        },
        {
            "source": "r_47",
            "target": "s_1261"
        },
        {
            "source": "r_47",
            "target": "s_1206"
        },
        {
            "source": "r_47",
            "target": "s_1122"
        },
        {
            "source": "r_47",
            "target": "s_447"
        },
        {
            "source": "r_47",
            "target": "s_895"
        },
        {
            "source": "r_48",
            "target": "s_1088"
        },
        {
            "source": "r_48",
            "target": "s_395"
        },
        {
            "source": "r_48",
            "target": "s_258"
        },
        {
            "source": "r_48",
            "target": "s_773"
        },
        {
            "source": "r_49",
            "target": "s_1181"
        },
        {
            "source": "r_49",
            "target": "s_188"
        },
        {
            "source": "r_49",
            "target": "s_20"
        },
        {
            "source": "r_49",
            "target": "s_113"
        },
        {
            "source": "r_49",
            "target": "s_952"
        },
        {
            "source": "r_49",
            "target": "s_651"
        },
        {
            "source": "r_49",
            "target": "s_1135"
        },
        {
            "source": "r_50",
            "target": "s_1412"
        },
        {
            "source": "r_50",
            "target": "s_762"
        },
        {
            "source": "r_50",
            "target": "s_569"
        },
        {
            "source": "r_50",
            "target": "s_1212"
        },
        {
            "source": "r_50",
            "target": "s_159"
        },
        {
            "source": "r_51",
            "target": "s_1109"
        },
        {
            "source": "r_51",
            "target": "s_1357"
        },
        {
            "source": "r_51",
            "target": "s_1325"
        },
        {
            "source": "r_51",
            "target": "s_1250"
        },
        {
            "source": "r_52",
            "target": "s_1263"
        },
        {
            "source": "r_52",
            "target": "s_217"
        },
        {
            "source": "r_52",
            "target": "s_1080"
        },
        {
            "source": "r_52",
            "target": "s_1042"
        },
        {
            "source": "r_52",
            "target": "s_1328"
        },
        {
            "source": "r_52",
            "target": "s_863"
        },
        {
            "source": "r_52",
            "target": "s_1346"
        },
        {
            "source": "r_53",
            "target": "s_765"
        },
        {
            "source": "r_53",
            "target": "s_122"
        },
        {
            "source": "r_53",
            "target": "s_353"
        },
        {
            "source": "r_53",
            "target": "s_852"
        },
        {
            "source": "r_54",
            "target": "s_997"
        },
        {
            "source": "r_54",
            "target": "s_177"
        },
        {
            "source": "r_54",
            "target": "s_1282"
        },
        {
            "source": "r_54",
            "target": "s_492"
        },
        {
            "source": "r_54",
            "target": "s_1008"
        },
        {
            "source": "r_54",
            "target": "s_1200"
        },
        {
            "source": "r_55",
            "target": "s_867"
        },
        {
            "source": "r_55",
            "target": "s_399"
        },
        {
            "source": "r_55",
            "target": "s_1483"
        },
        {
            "source": "r_55",
            "target": "s_325"
        },
        {
            "source": "r_55",
            "target": "s_781"
        },
        {
            "source": "r_56",
            "target": "s_1272"
        },
        {
            "source": "r_57",
            "target": "s_483"
        },
        {
            "source": "r_57",
            "target": "s_845"
        },
        {
            "source": "r_57",
            "target": "s_250"
        },
        {
            "source": "r_57",
            "target": "s_1428"
        },
        {
            "source": "r_57",
            "target": "s_538"
        },
        {
            "source": "r_58",
            "target": "s_1080"
        },
        {
            "source": "r_58",
            "target": "s_167"
        },
        {
            "source": "r_58",
            "target": "s_1042"
        },
        {
            "source": "r_58",
            "target": "s_33"
        },
        {
            "source": "r_58",
            "target": "s_1153"
        },
        {
            "source": "r_58",
            "target": "s_166"
        },
        {
            "source": "r_59",
            "target": "s_28"
        },
        {
            "source": "r_59",
            "target": "s_735"
        },
        {
            "source": "r_60",
            "target": "s_383"
        },
        {
            "source": "r_60",
            "target": "s_659"
        },
        {
            "source": "r_60",
            "target": "s_138"
        },
        {
            "source": "r_61",
            "target": "s_640"
        },
        {
            "source": "r_61",
            "target": "s_1165"
        },
        {
            "source": "r_61",
            "target": "s_351"
        },
        {
            "source": "r_61",
            "target": "s_896"
        },
        {
            "source": "r_61",
            "target": "s_26"
        },
        {
            "source": "r_61",
            "target": "s_1415"
        },
        {
            "source": "r_61",
            "target": "s_135"
        },
        {
            "source": "r_62",
            "target": "s_589"
        },
        {
            "source": "r_62",
            "target": "s_845"
        },
        {
            "source": "r_62",
            "target": "s_1031"
        },
        {
            "source": "r_62",
            "target": "s_1204"
        },
        {
            "source": "r_62",
            "target": "s_508"
        },
        {
            "source": "r_62",
            "target": "s_943"
        },
        {
            "source": "r_63",
            "target": "s_1248"
        },
        {
            "source": "r_63",
            "target": "s_1410"
        },
        {
            "source": "r_63",
            "target": "s_111"
        },
        {
            "source": "r_64",
            "target": "s_669"
        },
        {
            "source": "r_64",
            "target": "s_1363"
        },
        {
            "source": "r_64",
            "target": "s_1204"
        },
        {
            "source": "r_64",
            "target": "s_1143"
        },
        {
            "source": "r_64",
            "target": "s_202"
        },
        {
            "source": "r_64",
            "target": "s_184"
        },
        {
            "source": "r_64",
            "target": "s_1188"
        },
        {
            "source": "r_65",
            "target": "s_1273"
        },
        {
            "source": "r_65",
            "target": "s_1438"
        },
        {
            "source": "r_65",
            "target": "s_768"
        },
        {
            "source": "r_65",
            "target": "s_1217"
        },
        {
            "source": "r_65",
            "target": "s_420"
        },
        {
            "source": "r_65",
            "target": "s_556"
        },
        {
            "source": "r_65",
            "target": "s_359"
        },
        {
            "source": "r_65",
            "target": "s_1250"
        },
        {
            "source": "r_66",
            "target": "s_967"
        },
        {
            "source": "r_66",
            "target": "s_380"
        },
        {
            "source": "r_67",
            "target": "s_1020"
        },
        {
            "source": "r_67",
            "target": "s_942"
        },
        {
            "source": "r_67",
            "target": "s_1451"
        },
        {
            "source": "r_67",
            "target": "s_1285"
        },
        {
            "source": "r_67",
            "target": "s_74"
        },
        {
            "source": "r_67",
            "target": "s_4"
        },
        {
            "source": "r_67",
            "target": "s_1250"
        },
        {
            "source": "r_68",
            "target": "s_155"
        },
        {
            "source": "r_68",
            "target": "s_730"
        },
        {
            "source": "r_68",
            "target": "s_1387"
        },
        {
            "source": "r_68",
            "target": "s_745"
        },
        {
            "source": "r_68",
            "target": "s_889"
        },
        {
            "source": "r_68",
            "target": "s_941"
        },
        {
            "source": "r_68",
            "target": "s_1253"
        },
        {
            "source": "r_68",
            "target": "s_1250"
        },
        {
            "source": "r_68",
            "target": "s_1396"
        },
        {
            "source": "r_68",
            "target": "s_667"
        },
        {
            "source": "r_69",
            "target": "s_1163"
        },
        {
            "source": "r_69",
            "target": "s_250"
        },
        {
            "source": "r_69",
            "target": "s_12"
        },
        {
            "source": "r_70",
            "target": "s_400"
        },
        {
            "source": "r_70",
            "target": "s_26"
        },
        {
            "source": "r_70",
            "target": "s_244"
        },
        {
            "source": "r_70",
            "target": "s_851"
        },
        {
            "source": "r_70",
            "target": "s_1228"
        },
        {
            "source": "r_70",
            "target": "s_866"
        },
        {
            "source": "r_71",
            "target": "s_36"
        },
        {
            "source": "r_71",
            "target": "s_202"
        },
        {
            "source": "r_71",
            "target": "s_48"
        },
        {
            "source": "r_71",
            "target": "s_1188"
        },
        {
            "source": "r_71",
            "target": "s_1204"
        },
        {
            "source": "r_71",
            "target": "s_1127"
        },
        {
            "source": "r_72",
            "target": "s_411"
        },
        {
            "source": "r_72",
            "target": "s_1109"
        },
        {
            "source": "r_72",
            "target": "s_1325"
        },
        {
            "source": "r_73",
            "target": "s_1423"
        },
        {
            "source": "r_73",
            "target": "s_1298"
        },
        {
            "source": "r_73",
            "target": "s_95"
        },
        {
            "source": "r_73",
            "target": "s_226"
        },
        {
            "source": "r_73",
            "target": "s_1015"
        },
        {
            "source": "r_73",
            "target": "s_1019"
        },
        {
            "source": "r_73",
            "target": "s_137"
        },
        {
            "source": "r_74",
            "target": "s_1203"
        },
        {
            "source": "r_74",
            "target": "s_29"
        },
        {
            "source": "r_74",
            "target": "s_904"
        },
        {
            "source": "r_74",
            "target": "s_939"
        },
        {
            "source": "r_74",
            "target": "s_645"
        },
        {
            "source": "r_74",
            "target": "s_748"
        },
        {
            "source": "r_74",
            "target": "s_1353"
        },
        {
            "source": "r_74",
            "target": "s_380"
        },
        {
            "source": "r_75",
            "target": "s_453"
        },
        {
            "source": "r_75",
            "target": "s_354"
        },
        {
            "source": "r_75",
            "target": "s_1283"
        },
        {
            "source": "r_75",
            "target": "s_1388"
        },
        {
            "source": "r_75",
            "target": "s_441"
        },
        {
            "source": "r_75",
            "target": "s_137"
        },
        {
            "source": "r_76",
            "target": "s_758"
        },
        {
            "source": "r_76",
            "target": "s_817"
        },
        {
            "source": "r_76",
            "target": "s_1330"
        },
        {
            "source": "r_76",
            "target": "s_832"
        },
        {
            "source": "r_76",
            "target": "s_833"
        },
        {
            "source": "r_76",
            "target": "s_583"
        },
        {
            "source": "r_76",
            "target": "s_114"
        },
        {
            "source": "r_76",
            "target": "s_321"
        },
        {
            "source": "r_77",
            "target": "s_905"
        },
        {
            "source": "r_77",
            "target": "s_805"
        },
        {
            "source": "r_77",
            "target": "s_1003"
        },
        {
            "source": "r_77",
            "target": "s_266"
        },
        {
            "source": "r_78",
            "target": "s_866"
        },
        {
            "source": "r_78",
            "target": "s_454"
        },
        {
            "source": "r_78",
            "target": "s_1241"
        },
        {
            "source": "r_78",
            "target": "s_1177"
        },
        {
            "source": "r_78",
            "target": "s_993"
        },
        {
            "source": "r_78",
            "target": "s_872"
        },
        {
            "source": "r_79",
            "target": "s_765"
        },
        {
            "source": "r_79",
            "target": "s_471"
        },
        {
            "source": "r_79",
            "target": "s_353"
        },
        {
            "source": "r_79",
            "target": "s_936"
        },
        {
            "source": "r_80",
            "target": "s_62"
        },
        {
            "source": "r_80",
            "target": "s_1132"
        },
        {
            "source": "r_80",
            "target": "s_390"
        },
        {
            "source": "r_80",
            "target": "s_299"
        },
        {
            "source": "r_81",
            "target": "s_920"
        },
        {
            "source": "r_81",
            "target": "s_1251"
        },
        {
            "source": "r_82",
            "target": "s_896"
        },
        {
            "source": "r_82",
            "target": "s_884"
        },
        {
            "source": "r_82",
            "target": "s_640"
        },
        {
            "source": "r_82",
            "target": "s_542"
        },
        {
            "source": "r_82",
            "target": "s_500"
        },
        {
            "source": "r_82",
            "target": "s_782"
        },
        {
            "source": "r_82",
            "target": "s_351"
        },
        {
            "source": "r_82",
            "target": "s_1247"
        },
        {
            "source": "r_82",
            "target": "s_135"
        },
        {
            "source": "r_83",
            "target": "s_306"
        },
        {
            "source": "r_83",
            "target": "s_1272"
        },
        {
            "source": "r_84",
            "target": "s_726"
        },
        {
            "source": "r_84",
            "target": "s_320"
        },
        {
            "source": "r_84",
            "target": "s_1068"
        },
        {
            "source": "r_84",
            "target": "s_1469"
        },
        {
            "source": "r_85",
            "target": "s_8"
        },
        {
            "source": "r_85",
            "target": "s_992"
        },
        {
            "source": "r_85",
            "target": "s_464"
        },
        {
            "source": "r_85",
            "target": "s_1469"
        },
        {
            "source": "r_86",
            "target": "s_30"
        },
        {
            "source": "r_86",
            "target": "s_813"
        },
        {
            "source": "r_86",
            "target": "s_1230"
        },
        {
            "source": "r_86",
            "target": "s_139"
        },
        {
            "source": "r_86",
            "target": "s_1448"
        },
        {
            "source": "r_87",
            "target": "s_565"
        },
        {
            "source": "r_87",
            "target": "s_495"
        },
        {
            "source": "r_87",
            "target": "s_1348"
        },
        {
            "source": "r_87",
            "target": "s_1470"
        },
        {
            "source": "r_87",
            "target": "s_1434"
        },
        {
            "source": "r_87",
            "target": "s_270"
        },
        {
            "source": "r_88",
            "target": "s_582"
        },
        {
            "source": "r_88",
            "target": "s_1166"
        },
        {
            "source": "r_88",
            "target": "s_1407"
        },
        {
            "source": "r_88",
            "target": "s_13"
        },
        {
            "source": "r_88",
            "target": "s_55"
        },
        {
            "source": "r_88",
            "target": "s_207"
        },
        {
            "source": "r_89",
            "target": "s_1248"
        },
        {
            "source": "r_89",
            "target": "s_103"
        },
        {
            "source": "r_89",
            "target": "s_1390"
        },
        {
            "source": "r_89",
            "target": "s_111"
        },
        {
            "source": "r_90",
            "target": "s_1221"
        },
        {
            "source": "r_90",
            "target": "s_1323"
        },
        {
            "source": "r_90",
            "target": "s_1463"
        },
        {
            "source": "r_90",
            "target": "s_390"
        },
        {
            "source": "r_90",
            "target": "s_299"
        },
        {
            "source": "r_91",
            "target": "s_965"
        },
        {
            "source": "r_91",
            "target": "s_1167"
        },
        {
            "source": "r_91",
            "target": "s_328"
        },
        {
            "source": "r_91",
            "target": "s_976"
        },
        {
            "source": "r_91",
            "target": "s_137"
        },
        {
            "source": "r_91",
            "target": "s_1133"
        },
        {
            "source": "r_91",
            "target": "s_719"
        },
        {
            "source": "r_91",
            "target": "s_263"
        },
        {
            "source": "r_92",
            "target": "s_254"
        },
        {
            "source": "r_92",
            "target": "s_462"
        },
        {
            "source": "r_92",
            "target": "s_1208"
        },
        {
            "source": "r_92",
            "target": "s_658"
        },
        {
            "source": "r_93",
            "target": "s_738"
        },
        {
            "source": "r_93",
            "target": "s_1033"
        },
        {
            "source": "r_93",
            "target": "s_187"
        },
        {
            "source": "r_93",
            "target": "s_1111"
        },
        {
            "source": "r_93",
            "target": "s_80"
        },
        {
            "source": "r_93",
            "target": "s_1070"
        },
        {
            "source": "r_94",
            "target": "s_1201"
        },
        {
            "source": "r_94",
            "target": "s_561"
        },
        {
            "source": "r_94",
            "target": "s_687"
        },
        {
            "source": "r_95",
            "target": "s_8"
        },
        {
            "source": "r_95",
            "target": "s_629"
        },
        {
            "source": "r_95",
            "target": "s_957"
        },
        {
            "source": "r_95",
            "target": "s_107"
        },
        {
            "source": "r_95",
            "target": "s_1277"
        },
        {
            "source": "r_95",
            "target": "s_1469"
        },
        {
            "source": "r_95",
            "target": "s_735"
        },
        {
            "source": "r_96",
            "target": "s_1273"
        },
        {
            "source": "r_96",
            "target": "s_1053"
        },
        {
            "source": "r_96",
            "target": "s_1377"
        },
        {
            "source": "r_96",
            "target": "s_601"
        },
        {
            "source": "r_96",
            "target": "s_525"
        },
        {
            "source": "r_96",
            "target": "s_768"
        },
        {
            "source": "r_96",
            "target": "s_556"
        },
        {
            "source": "r_97",
            "target": "s_612"
        },
        {
            "source": "r_97",
            "target": "s_83"
        },
        {
            "source": "r_97",
            "target": "s_296"
        },
        {
            "source": "r_97",
            "target": "s_438"
        },
        {
            "source": "r_97",
            "target": "s_1480"
        },
        {
            "source": "r_97",
            "target": "s_566"
        },
        {
            "source": "r_97",
            "target": "s_531"
        },
        {
            "source": "r_97",
            "target": "s_297"
        },
        {
            "source": "r_98",
            "target": "s_342"
        },
        {
            "source": "r_98",
            "target": "s_687"
        },
        {
            "source": "r_98",
            "target": "s_649"
        },
        {
            "source": "r_99",
            "target": "s_490"
        },
        {
            "source": "r_99",
            "target": "s_574"
        },
        {
            "source": "r_99",
            "target": "s_529"
        },
        {
            "source": "r_99",
            "target": "s_1334"
        },
        {
            "source": "r_100",
            "target": "s_462"
        },
        {
            "source": "r_100",
            "target": "s_1237"
        },
        {
            "source": "r_101",
            "target": "s_759"
        },
        {
            "source": "r_101",
            "target": "s_58"
        },
        {
            "source": "r_101",
            "target": "s_920"
        },
        {
            "source": "r_101",
            "target": "s_1272"
        },
        {
            "source": "r_102",
            "target": "s_448"
        },
        {
            "source": "r_102",
            "target": "s_1135"
        },
        {
            "source": "r_102",
            "target": "s_739"
        },
        {
            "source": "r_103",
            "target": "s_761"
        },
        {
            "source": "r_103",
            "target": "s_588"
        },
        {
            "source": "r_103",
            "target": "s_879"
        },
        {
            "source": "r_104",
            "target": "s_340"
        },
        {
            "source": "r_104",
            "target": "s_438"
        },
        {
            "source": "r_104",
            "target": "s_444"
        },
        {
            "source": "r_104",
            "target": "s_989"
        },
        {
            "source": "r_104",
            "target": "s_722"
        },
        {
            "source": "r_104",
            "target": "s_532"
        },
        {
            "source": "r_104",
            "target": "s_297"
        },
        {
            "source": "r_105",
            "target": "s_301"
        },
        {
            "source": "r_105",
            "target": "s_361"
        },
        {
            "source": "r_105",
            "target": "s_1229"
        },
        {
            "source": "r_105",
            "target": "s_1032"
        },
        {
            "source": "r_105",
            "target": "s_1384"
        },
        {
            "source": "r_105",
            "target": "s_1436"
        },
        {
            "source": "r_106",
            "target": "s_36"
        },
        {
            "source": "r_106",
            "target": "s_202"
        },
        {
            "source": "r_106",
            "target": "s_91"
        },
        {
            "source": "r_106",
            "target": "s_1188"
        },
        {
            "source": "r_107",
            "target": "s_788"
        },
        {
            "source": "r_107",
            "target": "s_1267"
        },
        {
            "source": "r_107",
            "target": "s_706"
        },
        {
            "source": "r_107",
            "target": "s_793"
        },
        {
            "source": "r_107",
            "target": "s_635"
        },
        {
            "source": "r_107",
            "target": "s_853"
        },
        {
            "source": "r_107",
            "target": "s_339"
        },
        {
            "source": "r_108",
            "target": "s_1149"
        },
        {
            "source": "r_108",
            "target": "s_9"
        },
        {
            "source": "r_108",
            "target": "s_1017"
        },
        {
            "source": "r_108",
            "target": "s_317"
        },
        {
            "source": "r_109",
            "target": "s_194"
        },
        {
            "source": "r_109",
            "target": "s_161"
        },
        {
            "source": "r_109",
            "target": "s_231"
        },
        {
            "source": "r_110",
            "target": "s_681"
        },
        {
            "source": "r_110",
            "target": "s_242"
        },
        {
            "source": "r_110",
            "target": "s_833"
        },
        {
            "source": "r_110",
            "target": "s_1148"
        },
        {
            "source": "r_111",
            "target": "s_1067"
        },
        {
            "source": "r_111",
            "target": "s_1109"
        },
        {
            "source": "r_111",
            "target": "s_1325"
        },
        {
            "source": "r_112",
            "target": "s_669"
        },
        {
            "source": "r_112",
            "target": "s_1143"
        },
        {
            "source": "r_113",
            "target": "s_85"
        },
        {
            "source": "r_113",
            "target": "s_1240"
        },
        {
            "source": "r_113",
            "target": "s_0"
        },
        {
            "source": "r_113",
            "target": "s_367"
        },
        {
            "source": "r_114",
            "target": "s_805"
        },
        {
            "source": "r_114",
            "target": "s_426"
        },
        {
            "source": "r_114",
            "target": "s_34"
        },
        {
            "source": "r_114",
            "target": "s_281"
        },
        {
            "source": "r_114",
            "target": "s_1003"
        },
        {
            "source": "r_114",
            "target": "s_387"
        },
        {
            "source": "r_115",
            "target": "s_1307"
        },
        {
            "source": "r_115",
            "target": "s_172"
        },
        {
            "source": "r_115",
            "target": "s_266"
        },
        {
            "source": "r_116",
            "target": "s_1463"
        },
        {
            "source": "r_116",
            "target": "s_576"
        },
        {
            "source": "r_116",
            "target": "s_307"
        },
        {
            "source": "r_116",
            "target": "s_138"
        },
        {
            "source": "r_117",
            "target": "s_194"
        },
        {
            "source": "r_117",
            "target": "s_334"
        },
        {
            "source": "r_118",
            "target": "s_905"
        },
        {
            "source": "r_118",
            "target": "s_805"
        },
        {
            "source": "r_118",
            "target": "s_867"
        },
        {
            "source": "r_118",
            "target": "s_540"
        },
        {
            "source": "r_118",
            "target": "s_493"
        },
        {
            "source": "r_118",
            "target": "s_1117"
        },
        {
            "source": "r_118",
            "target": "s_1003"
        },
        {
            "source": "r_119",
            "target": "s_931"
        },
        {
            "source": "r_119",
            "target": "s_971"
        },
        {
            "source": "r_119",
            "target": "s_224"
        },
        {
            "source": "r_119",
            "target": "s_320"
        },
        {
            "source": "r_119",
            "target": "s_1249"
        },
        {
            "source": "r_119",
            "target": "s_1143"
        },
        {
            "source": "r_120",
            "target": "s_1275"
        },
        {
            "source": "r_120",
            "target": "s_726"
        },
        {
            "source": "r_120",
            "target": "s_45"
        },
        {
            "source": "r_120",
            "target": "s_1469"
        },
        {
            "source": "r_121",
            "target": "s_1013"
        },
        {
            "source": "r_121",
            "target": "s_589"
        },
        {
            "source": "r_121",
            "target": "s_878"
        },
        {
            "source": "r_121",
            "target": "s_61"
        },
        {
            "source": "r_122",
            "target": "s_991"
        },
        {
            "source": "r_122",
            "target": "s_61"
        },
        {
            "source": "r_123",
            "target": "s_1305"
        },
        {
            "source": "r_123",
            "target": "s_1389"
        },
        {
            "source": "r_123",
            "target": "s_931"
        },
        {
            "source": "r_123",
            "target": "s_320"
        },
        {
            "source": "r_123",
            "target": "s_1143"
        },
        {
            "source": "r_124",
            "target": "s_768"
        },
        {
            "source": "r_124",
            "target": "s_525"
        },
        {
            "source": "r_124",
            "target": "s_1217"
        },
        {
            "source": "r_124",
            "target": "s_346"
        },
        {
            "source": "r_124",
            "target": "s_1438"
        },
        {
            "source": "r_124",
            "target": "s_710"
        },
        {
            "source": "r_124",
            "target": "s_556"
        },
        {
            "source": "r_124",
            "target": "s_1273"
        },
        {
            "source": "r_125",
            "target": "s_1024"
        },
        {
            "source": "r_125",
            "target": "s_1118"
        },
        {
            "source": "r_125",
            "target": "s_1251"
        },
        {
            "source": "r_126",
            "target": "s_958"
        },
        {
            "source": "r_126",
            "target": "s_856"
        },
        {
            "source": "r_126",
            "target": "s_781"
        },
        {
            "source": "r_127",
            "target": "s_561"
        },
        {
            "source": "r_127",
            "target": "s_973"
        },
        {
            "source": "r_127",
            "target": "s_687"
        },
        {
            "source": "r_128",
            "target": "s_1463"
        },
        {
            "source": "r_128",
            "target": "s_576"
        },
        {
            "source": "r_128",
            "target": "s_62"
        },
        {
            "source": "r_128",
            "target": "s_1337"
        },
        {
            "source": "r_128",
            "target": "s_299"
        },
        {
            "source": "r_128",
            "target": "s_138"
        },
        {
            "source": "r_129",
            "target": "s_130"
        },
        {
            "source": "r_129",
            "target": "s_202"
        },
        {
            "source": "r_129",
            "target": "s_1188"
        },
        {
            "source": "r_129",
            "target": "s_1326"
        },
        {
            "source": "r_130",
            "target": "s_37"
        },
        {
            "source": "r_130",
            "target": "s_859"
        },
        {
            "source": "r_130",
            "target": "s_493"
        },
        {
            "source": "r_130",
            "target": "s_1003"
        },
        {
            "source": "r_131",
            "target": "s_1201"
        },
        {
            "source": "r_131",
            "target": "s_783"
        },
        {
            "source": "r_131",
            "target": "s_1192"
        },
        {
            "source": "r_131",
            "target": "s_1109"
        },
        {
            "source": "r_131",
            "target": "s_1325"
        },
        {
            "source": "r_131",
            "target": "s_687"
        },
        {
            "source": "r_132",
            "target": "s_1413"
        },
        {
            "source": "r_132",
            "target": "s_373"
        },
        {
            "source": "r_132",
            "target": "s_1481"
        },
        {
            "source": "r_132",
            "target": "s_161"
        },
        {
            "source": "r_133",
            "target": "s_1285"
        },
        {
            "source": "r_133",
            "target": "s_1107"
        },
        {
            "source": "r_133",
            "target": "s_479"
        },
        {
            "source": "r_133",
            "target": "s_1344"
        },
        {
            "source": "r_133",
            "target": "s_279"
        },
        {
            "source": "r_133",
            "target": "s_116"
        },
        {
            "source": "r_133",
            "target": "s_1059"
        },
        {
            "source": "r_133",
            "target": "s_882"
        },
        {
            "source": "r_133",
            "target": "s_1250"
        },
        {
            "source": "r_134",
            "target": "s_8"
        },
        {
            "source": "r_134",
            "target": "s_25"
        },
        {
            "source": "r_134",
            "target": "s_17"
        },
        {
            "source": "r_134",
            "target": "s_1164"
        },
        {
            "source": "r_134",
            "target": "s_45"
        },
        {
            "source": "r_134",
            "target": "s_1469"
        },
        {
            "source": "r_135",
            "target": "s_279"
        },
        {
            "source": "r_135",
            "target": "s_1399"
        },
        {
            "source": "r_135",
            "target": "s_1250"
        },
        {
            "source": "r_135",
            "target": "s_1180"
        },
        {
            "source": "r_135",
            "target": "s_1380"
        },
        {
            "source": "r_135",
            "target": "s_41"
        },
        {
            "source": "r_135",
            "target": "s_1288"
        },
        {
            "source": "r_136",
            "target": "s_232"
        },
        {
            "source": "r_136",
            "target": "s_533"
        },
        {
            "source": "r_136",
            "target": "s_138"
        },
        {
            "source": "r_136",
            "target": "s_1220"
        },
        {
            "source": "r_137",
            "target": "s_225"
        },
        {
            "source": "r_137",
            "target": "s_1283"
        },
        {
            "source": "r_137",
            "target": "s_896"
        },
        {
            "source": "r_137",
            "target": "s_137"
        },
        {
            "source": "r_138",
            "target": "s_181"
        },
        {
            "source": "r_138",
            "target": "s_81"
        },
        {
            "source": "r_138",
            "target": "s_465"
        },
        {
            "source": "r_138",
            "target": "s_1087"
        },
        {
            "source": "r_139",
            "target": "s_1029"
        },
        {
            "source": "r_139",
            "target": "s_1205"
        },
        {
            "source": "r_139",
            "target": "s_781"
        },
        {
            "source": "r_139",
            "target": "s_1128"
        },
        {
            "source": "r_139",
            "target": "s_1434"
        },
        {
            "source": "r_140",
            "target": "s_427"
        },
        {
            "source": "r_140",
            "target": "s_887"
        },
        {
            "source": "r_140",
            "target": "s_864"
        },
        {
            "source": "r_140",
            "target": "s_853"
        },
        {
            "source": "r_141",
            "target": "s_110"
        },
        {
            "source": "r_141",
            "target": "s_334"
        },
        {
            "source": "r_141",
            "target": "s_871"
        },
        {
            "source": "r_141",
            "target": "s_99"
        },
        {
            "source": "r_141",
            "target": "s_853"
        },
        {
            "source": "r_142",
            "target": "s_448"
        },
        {
            "source": "r_142",
            "target": "s_1284"
        },
        {
            "source": "r_142",
            "target": "s_487"
        },
        {
            "source": "r_142",
            "target": "s_664"
        },
        {
            "source": "r_142",
            "target": "s_457"
        },
        {
            "source": "r_143",
            "target": "s_918"
        },
        {
            "source": "r_143",
            "target": "s_252"
        },
        {
            "source": "r_144",
            "target": "s_893"
        },
        {
            "source": "r_144",
            "target": "s_1332"
        },
        {
            "source": "r_144",
            "target": "s_250"
        },
        {
            "source": "r_144",
            "target": "s_1031"
        },
        {
            "source": "r_144",
            "target": "s_66"
        },
        {
            "source": "r_144",
            "target": "s_850"
        },
        {
            "source": "r_145",
            "target": "s_1221"
        },
        {
            "source": "r_145",
            "target": "s_59"
        },
        {
            "source": "r_145",
            "target": "s_161"
        },
        {
            "source": "r_145",
            "target": "s_250"
        },
        {
            "source": "r_145",
            "target": "s_1029"
        },
        {
            "source": "r_146",
            "target": "s_773"
        },
        {
            "source": "r_146",
            "target": "s_830"
        },
        {
            "source": "r_146",
            "target": "s_1160"
        },
        {
            "source": "r_146",
            "target": "s_1003"
        },
        {
            "source": "r_146",
            "target": "s_1476"
        },
        {
            "source": "r_147",
            "target": "s_597"
        },
        {
            "source": "r_147",
            "target": "s_1470"
        },
        {
            "source": "r_147",
            "target": "s_410"
        },
        {
            "source": "r_147",
            "target": "s_270"
        },
        {
            "source": "r_148",
            "target": "s_476"
        },
        {
            "source": "r_148",
            "target": "s_301"
        },
        {
            "source": "r_148",
            "target": "s_485"
        },
        {
            "source": "r_148",
            "target": "s_750"
        },
        {
            "source": "r_148",
            "target": "s_858"
        },
        {
            "source": "r_148",
            "target": "s_1252"
        },
        {
            "source": "r_149",
            "target": "s_1131"
        },
        {
            "source": "r_149",
            "target": "s_1336"
        },
        {
            "source": "r_149",
            "target": "s_1000"
        },
        {
            "source": "r_149",
            "target": "s_223"
        },
        {
            "source": "r_150",
            "target": "s_41"
        },
        {
            "source": "r_150",
            "target": "s_193"
        },
        {
            "source": "r_150",
            "target": "s_587"
        },
        {
            "source": "r_150",
            "target": "s_708"
        },
        {
            "source": "r_151",
            "target": "s_400"
        },
        {
            "source": "r_151",
            "target": "s_624"
        },
        {
            "source": "r_151",
            "target": "s_851"
        },
        {
            "source": "r_151",
            "target": "s_1131"
        },
        {
            "source": "r_151",
            "target": "s_1228"
        },
        {
            "source": "r_151",
            "target": "s_866"
        },
        {
            "source": "r_152",
            "target": "s_453"
        },
        {
            "source": "r_152",
            "target": "s_866"
        },
        {
            "source": "r_152",
            "target": "s_402"
        },
        {
            "source": "r_152",
            "target": "s_1423"
        },
        {
            "source": "r_152",
            "target": "s_137"
        },
        {
            "source": "r_152",
            "target": "s_896"
        },
        {
            "source": "r_153",
            "target": "s_1485"
        },
        {
            "source": "r_153",
            "target": "s_800"
        },
        {
            "source": "r_153",
            "target": "s_160"
        },
        {
            "source": "r_154",
            "target": "s_617"
        },
        {
            "source": "r_154",
            "target": "s_158"
        },
        {
            "source": "r_155",
            "target": "s_649"
        },
        {
            "source": "r_155",
            "target": "s_248"
        },
        {
            "source": "r_155",
            "target": "s_557"
        },
        {
            "source": "r_155",
            "target": "s_1162"
        },
        {
            "source": "r_155",
            "target": "s_1070"
        },
        {
            "source": "r_155",
            "target": "s_1150"
        },
        {
            "source": "r_156",
            "target": "s_1457"
        },
        {
            "source": "r_156",
            "target": "s_640"
        },
        {
            "source": "r_156",
            "target": "s_403"
        },
        {
            "source": "r_156",
            "target": "s_1115"
        },
        {
            "source": "r_156",
            "target": "s_137"
        },
        {
            "source": "r_157",
            "target": "s_423"
        },
        {
            "source": "r_157",
            "target": "s_356"
        },
        {
            "source": "r_157",
            "target": "s_577"
        },
        {
            "source": "r_157",
            "target": "s_1379"
        },
        {
            "source": "r_157",
            "target": "s_1489"
        },
        {
            "source": "r_157",
            "target": "s_401"
        },
        {
            "source": "r_157",
            "target": "s_313"
        },
        {
            "source": "r_157",
            "target": "s_803"
        },
        {
            "source": "r_158",
            "target": "s_1215"
        },
        {
            "source": "r_158",
            "target": "s_834"
        },
        {
            "source": "r_158",
            "target": "s_1431"
        },
        {
            "source": "r_158",
            "target": "s_529"
        },
        {
            "source": "r_159",
            "target": "s_593"
        },
        {
            "source": "r_159",
            "target": "s_124"
        },
        {
            "source": "r_159",
            "target": "s_758"
        },
        {
            "source": "r_159",
            "target": "s_143"
        },
        {
            "source": "r_159",
            "target": "s_1002"
        },
        {
            "source": "r_159",
            "target": "s_114"
        },
        {
            "source": "r_159",
            "target": "s_321"
        },
        {
            "source": "r_160",
            "target": "s_1172"
        },
        {
            "source": "r_160",
            "target": "s_12"
        },
        {
            "source": "r_160",
            "target": "s_802"
        },
        {
            "source": "r_161",
            "target": "s_1285"
        },
        {
            "source": "r_161",
            "target": "s_1110"
        },
        {
            "source": "r_161",
            "target": "s_1004"
        },
        {
            "source": "r_161",
            "target": "s_1477"
        },
        {
            "source": "r_161",
            "target": "s_1185"
        },
        {
            "source": "r_161",
            "target": "s_116"
        },
        {
            "source": "r_161",
            "target": "s_366"
        },
        {
            "source": "r_161",
            "target": "s_882"
        },
        {
            "source": "r_161",
            "target": "s_1250"
        },
        {
            "source": "r_162",
            "target": "s_87"
        },
        {
            "source": "r_162",
            "target": "s_91"
        },
        {
            "source": "r_162",
            "target": "s_499"
        },
        {
            "source": "r_162",
            "target": "s_683"
        },
        {
            "source": "r_162",
            "target": "s_908"
        },
        {
            "source": "r_163",
            "target": "s_169"
        },
        {
            "source": "r_163",
            "target": "s_669"
        },
        {
            "source": "r_163",
            "target": "s_1143"
        },
        {
            "source": "r_164",
            "target": "s_318"
        },
        {
            "source": "r_164",
            "target": "s_75"
        },
        {
            "source": "r_164",
            "target": "s_1143"
        },
        {
            "source": "r_164",
            "target": "s_811"
        },
        {
            "source": "r_165",
            "target": "s_1338"
        },
        {
            "source": "r_165",
            "target": "s_1347"
        },
        {
            "source": "r_165",
            "target": "s_1422"
        },
        {
            "source": "r_165",
            "target": "s_1416"
        },
        {
            "source": "r_165",
            "target": "s_246"
        },
        {
            "source": "r_165",
            "target": "s_1454"
        },
        {
            "source": "r_166",
            "target": "s_121"
        },
        {
            "source": "r_166",
            "target": "s_219"
        },
        {
            "source": "r_166",
            "target": "s_627"
        },
        {
            "source": "r_166",
            "target": "s_526"
        },
        {
            "source": "r_167",
            "target": "s_374"
        },
        {
            "source": "r_167",
            "target": "s_287"
        },
        {
            "source": "r_167",
            "target": "s_279"
        },
        {
            "source": "r_167",
            "target": "s_1384"
        },
        {
            "source": "r_167",
            "target": "s_1028"
        },
        {
            "source": "r_167",
            "target": "s_301"
        },
        {
            "source": "r_168",
            "target": "s_1048"
        },
        {
            "source": "r_168",
            "target": "s_1060"
        },
        {
            "source": "r_168",
            "target": "s_330"
        },
        {
            "source": "r_168",
            "target": "s_1471"
        },
        {
            "source": "r_168",
            "target": "s_1235"
        },
        {
            "source": "r_168",
            "target": "s_293"
        },
        {
            "source": "r_169",
            "target": "s_966"
        },
        {
            "source": "r_169",
            "target": "s_206"
        },
        {
            "source": "r_169",
            "target": "s_419"
        },
        {
            "source": "r_169",
            "target": "s_1157"
        },
        {
            "source": "r_169",
            "target": "s_208"
        },
        {
            "source": "r_169",
            "target": "s_1054"
        },
        {
            "source": "r_169",
            "target": "s_145"
        },
        {
            "source": "r_169",
            "target": "s_408"
        },
        {
            "source": "r_170",
            "target": "s_301"
        },
        {
            "source": "r_170",
            "target": "s_703"
        },
        {
            "source": "r_170",
            "target": "s_776"
        },
        {
            "source": "r_170",
            "target": "s_1268"
        },
        {
            "source": "r_170",
            "target": "s_1137"
        },
        {
            "source": "r_171",
            "target": "s_927"
        },
        {
            "source": "r_171",
            "target": "s_811"
        },
        {
            "source": "r_171",
            "target": "s_457"
        },
        {
            "source": "r_172",
            "target": "s_59"
        },
        {
            "source": "r_172",
            "target": "s_650"
        },
        {
            "source": "r_172",
            "target": "s_1286"
        },
        {
            "source": "r_172",
            "target": "s_1205"
        },
        {
            "source": "r_173",
            "target": "s_1181"
        },
        {
            "source": "r_173",
            "target": "s_726"
        },
        {
            "source": "r_173",
            "target": "s_8"
        },
        {
            "source": "r_173",
            "target": "s_320"
        },
        {
            "source": "r_173",
            "target": "s_1068"
        },
        {
            "source": "r_173",
            "target": "s_1469"
        },
        {
            "source": "r_174",
            "target": "s_1378"
        },
        {
            "source": "r_174",
            "target": "s_499"
        },
        {
            "source": "r_175",
            "target": "s_12"
        },
        {
            "source": "r_175",
            "target": "s_250"
        },
        {
            "source": "r_175",
            "target": "s_1029"
        },
        {
            "source": "r_175",
            "target": "s_797"
        },
        {
            "source": "r_176",
            "target": "s_1324"
        },
        {
            "source": "r_176",
            "target": "s_218"
        },
        {
            "source": "r_176",
            "target": "s_84"
        },
        {
            "source": "r_176",
            "target": "s_10"
        },
        {
            "source": "r_177",
            "target": "s_492"
        },
        {
            "source": "r_177",
            "target": "s_1208"
        },
        {
            "source": "r_177",
            "target": "s_802"
        },
        {
            "source": "r_178",
            "target": "s_1115"
        },
        {
            "source": "r_178",
            "target": "s_1423"
        },
        {
            "source": "r_178",
            "target": "s_402"
        },
        {
            "source": "r_178",
            "target": "s_1020"
        },
        {
            "source": "r_178",
            "target": "s_1055"
        },
        {
            "source": "r_178",
            "target": "s_1388"
        },
        {
            "source": "r_178",
            "target": "s_137"
        },
        {
            "source": "r_179",
            "target": "s_1059"
        },
        {
            "source": "r_179",
            "target": "s_319"
        },
        {
            "source": "r_179",
            "target": "s_613"
        },
        {
            "source": "r_179",
            "target": "s_173"
        },
        {
            "source": "r_179",
            "target": "s_781"
        },
        {
            "source": "r_179",
            "target": "s_1339"
        },
        {
            "source": "r_179",
            "target": "s_1220"
        },
        {
            "source": "r_179",
            "target": "s_1161"
        },
        {
            "source": "r_180",
            "target": "s_1009"
        },
        {
            "source": "r_180",
            "target": "s_1377"
        },
        {
            "source": "r_180",
            "target": "s_153"
        },
        {
            "source": "r_180",
            "target": "s_348"
        },
        {
            "source": "r_180",
            "target": "s_754"
        },
        {
            "source": "r_180",
            "target": "s_1458"
        },
        {
            "source": "r_180",
            "target": "s_338"
        },
        {
            "source": "r_181",
            "target": "s_1301"
        },
        {
            "source": "r_181",
            "target": "s_571"
        },
        {
            "source": "r_181",
            "target": "s_1400"
        },
        {
            "source": "r_181",
            "target": "s_353"
        },
        {
            "source": "r_181",
            "target": "s_1121"
        },
        {
            "source": "r_182",
            "target": "s_1040"
        },
        {
            "source": "r_182",
            "target": "s_375"
        },
        {
            "source": "r_182",
            "target": "s_735"
        },
        {
            "source": "r_183",
            "target": "s_967"
        },
        {
            "source": "r_183",
            "target": "s_380"
        },
        {
            "source": "r_184",
            "target": "s_411"
        },
        {
            "source": "r_184",
            "target": "s_261"
        },
        {
            "source": "r_184",
            "target": "s_1309"
        },
        {
            "source": "r_184",
            "target": "s_1478"
        },
        {
            "source": "r_184",
            "target": "s_716"
        },
        {
            "source": "r_184",
            "target": "s_137"
        },
        {
            "source": "r_185",
            "target": "s_795"
        },
        {
            "source": "r_185",
            "target": "s_250"
        },
        {
            "source": "r_185",
            "target": "s_1163"
        },
        {
            "source": "r_185",
            "target": "s_1251"
        },
        {
            "source": "r_185",
            "target": "s_735"
        },
        {
            "source": "r_185",
            "target": "s_106"
        },
        {
            "source": "r_185",
            "target": "s_12"
        },
        {
            "source": "r_185",
            "target": "s_1080"
        },
        {
            "source": "r_186",
            "target": "s_1126"
        },
        {
            "source": "r_186",
            "target": "s_845"
        },
        {
            "source": "r_186",
            "target": "s_589"
        },
        {
            "source": "r_186",
            "target": "s_1188"
        },
        {
            "source": "r_186",
            "target": "s_202"
        },
        {
            "source": "r_187",
            "target": "s_1408"
        },
        {
            "source": "r_187",
            "target": "s_252"
        },
        {
            "source": "r_188",
            "target": "s_476"
        },
        {
            "source": "r_188",
            "target": "s_301"
        },
        {
            "source": "r_188",
            "target": "s_899"
        },
        {
            "source": "r_188",
            "target": "s_630"
        },
        {
            "source": "r_188",
            "target": "s_1436"
        },
        {
            "source": "r_189",
            "target": "s_1081"
        },
        {
            "source": "r_189",
            "target": "s_603"
        },
        {
            "source": "r_189",
            "target": "s_230"
        },
        {
            "source": "r_189",
            "target": "s_1263"
        },
        {
            "source": "r_189",
            "target": "s_634"
        },
        {
            "source": "r_189",
            "target": "s_442"
        },
        {
            "source": "r_189",
            "target": "s_1346"
        },
        {
            "source": "r_190",
            "target": "s_204"
        },
        {
            "source": "r_190",
            "target": "s_1243"
        },
        {
            "source": "r_190",
            "target": "s_563"
        },
        {
            "source": "r_190",
            "target": "s_163"
        },
        {
            "source": "r_190",
            "target": "s_824"
        },
        {
            "source": "r_190",
            "target": "s_140"
        },
        {
            "source": "r_190",
            "target": "s_61"
        },
        {
            "source": "r_191",
            "target": "s_1283"
        },
        {
            "source": "r_191",
            "target": "s_284"
        },
        {
            "source": "r_191",
            "target": "s_1029"
        },
        {
            "source": "r_191",
            "target": "s_182"
        },
        {
            "source": "r_191",
            "target": "s_253"
        },
        {
            "source": "r_192",
            "target": "s_1284"
        },
        {
            "source": "r_192",
            "target": "s_195"
        },
        {
            "source": "r_192",
            "target": "s_672"
        },
        {
            "source": "r_192",
            "target": "s_962"
        },
        {
            "source": "r_192",
            "target": "s_861"
        },
        {
            "source": "r_192",
            "target": "s_670"
        },
        {
            "source": "r_192",
            "target": "s_720"
        },
        {
            "source": "r_193",
            "target": "s_717"
        },
        {
            "source": "r_193",
            "target": "s_14"
        },
        {
            "source": "r_193",
            "target": "s_463"
        },
        {
            "source": "r_193",
            "target": "s_908"
        },
        {
            "source": "r_194",
            "target": "s_1174"
        },
        {
            "source": "r_194",
            "target": "s_1098"
        },
        {
            "source": "r_194",
            "target": "s_1029"
        },
        {
            "source": "r_194",
            "target": "s_1205"
        },
        {
            "source": "r_195",
            "target": "s_433"
        },
        {
            "source": "r_195",
            "target": "s_384"
        },
        {
            "source": "r_195",
            "target": "s_1290"
        },
        {
            "source": "r_195",
            "target": "s_100"
        },
        {
            "source": "r_196",
            "target": "s_1374"
        },
        {
            "source": "r_196",
            "target": "s_1109"
        },
        {
            "source": "r_196",
            "target": "s_777"
        },
        {
            "source": "r_196",
            "target": "s_137"
        },
        {
            "source": "r_196",
            "target": "s_1325"
        },
        {
            "source": "r_197",
            "target": "s_773"
        },
        {
            "source": "r_197",
            "target": "s_1160"
        },
        {
            "source": "r_197",
            "target": "s_1220"
        },
        {
            "source": "r_197",
            "target": "s_1003"
        },
        {
            "source": "r_197",
            "target": "s_1476"
        },
        {
            "source": "r_198",
            "target": "s_52"
        },
        {
            "source": "r_198",
            "target": "s_1361"
        },
        {
            "source": "r_198",
            "target": "s_32"
        },
        {
            "source": "r_198",
            "target": "s_35"
        },
        {
            "source": "r_199",
            "target": "s_102"
        },
        {
            "source": "r_199",
            "target": "s_590"
        },
        {
            "source": "r_199",
            "target": "s_603"
        },
        {
            "source": "r_199",
            "target": "s_660"
        },
        {
            "source": "r_199",
            "target": "s_357"
        },
        {
            "source": "r_199",
            "target": "s_1346"
        },
        {
            "source": "r_199",
            "target": "s_336"
        },
        {
            "source": "r_200",
            "target": "s_957"
        },
        {
            "source": "r_200",
            "target": "s_840"
        },
        {
            "source": "r_200",
            "target": "s_1038"
        },
        {
            "source": "r_200",
            "target": "s_1112"
        },
        {
            "source": "r_200",
            "target": "s_555"
        },
        {
            "source": "r_201",
            "target": "s_533"
        },
        {
            "source": "r_201",
            "target": "s_1382"
        },
        {
            "source": "r_201",
            "target": "s_232"
        },
        {
            "source": "r_201",
            "target": "s_1220"
        },
        {
            "source": "r_201",
            "target": "s_259"
        },
        {
            "source": "r_201",
            "target": "s_138"
        },
        {
            "source": "r_202",
            "target": "s_1184"
        },
        {
            "source": "r_202",
            "target": "s_499"
        },
        {
            "source": "r_202",
            "target": "s_470"
        },
        {
            "source": "r_202",
            "target": "s_1349"
        },
        {
            "source": "r_202",
            "target": "s_85"
        },
        {
            "source": "r_202",
            "target": "s_1089"
        },
        {
            "source": "r_203",
            "target": "s_1048"
        },
        {
            "source": "r_203",
            "target": "s_293"
        },
        {
            "source": "r_203",
            "target": "s_381"
        },
        {
            "source": "r_203",
            "target": "s_200"
        },
        {
            "source": "r_203",
            "target": "s_237"
        },
        {
            "source": "r_204",
            "target": "s_561"
        },
        {
            "source": "r_204",
            "target": "s_687"
        },
        {
            "source": "r_205",
            "target": "s_1059"
        },
        {
            "source": "r_205",
            "target": "s_131"
        },
        {
            "source": "r_205",
            "target": "s_870"
        },
        {
            "source": "r_205",
            "target": "s_699"
        },
        {
            "source": "r_205",
            "target": "s_746"
        },
        {
            "source": "r_205",
            "target": "s_1339"
        },
        {
            "source": "r_205",
            "target": "s_781"
        },
        {
            "source": "r_205",
            "target": "s_1220"
        },
        {
            "source": "r_205",
            "target": "s_1161"
        },
        {
            "source": "r_206",
            "target": "s_410"
        },
        {
            "source": "r_206",
            "target": "s_371"
        },
        {
            "source": "r_206",
            "target": "s_1434"
        },
        {
            "source": "r_206",
            "target": "s_270"
        },
        {
            "source": "r_207",
            "target": "s_925"
        },
        {
            "source": "r_207",
            "target": "s_1031"
        },
        {
            "source": "r_207",
            "target": "s_1011"
        },
        {
            "source": "r_207",
            "target": "s_908"
        },
        {
            "source": "r_208",
            "target": "s_91"
        },
        {
            "source": "r_208",
            "target": "s_499"
        },
        {
            "source": "r_208",
            "target": "s_1349"
        },
        {
            "source": "r_208",
            "target": "s_85"
        },
        {
            "source": "r_209",
            "target": "s_308"
        },
        {
            "source": "r_209",
            "target": "s_883"
        },
        {
            "source": "r_209",
            "target": "s_329"
        },
        {
            "source": "r_209",
            "target": "s_578"
        },
        {
            "source": "r_209",
            "target": "s_111"
        },
        {
            "source": "r_209",
            "target": "s_1390"
        },
        {
            "source": "r_210",
            "target": "s_112"
        },
        {
            "source": "r_210",
            "target": "s_688"
        },
        {
            "source": "r_210",
            "target": "s_207"
        },
        {
            "source": "r_210",
            "target": "s_1407"
        },
        {
            "source": "r_211",
            "target": "s_869"
        },
        {
            "source": "r_211",
            "target": "s_34"
        },
        {
            "source": "r_211",
            "target": "s_1181"
        },
        {
            "source": "r_211",
            "target": "s_493"
        },
        {
            "source": "r_211",
            "target": "s_1346"
        },
        {
            "source": "r_212",
            "target": "s_1119"
        },
        {
            "source": "r_212",
            "target": "s_623"
        },
        {
            "source": "r_212",
            "target": "s_1272"
        },
        {
            "source": "r_212",
            "target": "s_811"
        },
        {
            "source": "r_213",
            "target": "s_1356"
        },
        {
            "source": "r_213",
            "target": "s_1177"
        },
        {
            "source": "r_214",
            "target": "s_1219"
        },
        {
            "source": "r_214",
            "target": "s_1126"
        },
        {
            "source": "r_214",
            "target": "s_418"
        },
        {
            "source": "r_214",
            "target": "s_341"
        },
        {
            "source": "r_214",
            "target": "s_1346"
        },
        {
            "source": "r_215",
            "target": "s_668"
        },
        {
            "source": "r_215",
            "target": "s_1005"
        },
        {
            "source": "r_215",
            "target": "s_1469"
        },
        {
            "source": "r_215",
            "target": "s_1264"
        },
        {
            "source": "r_215",
            "target": "s_564"
        },
        {
            "source": "r_215",
            "target": "s_584"
        },
        {
            "source": "r_215",
            "target": "s_1039"
        },
        {
            "source": "r_215",
            "target": "s_295"
        },
        {
            "source": "r_216",
            "target": "s_18"
        },
        {
            "source": "r_216",
            "target": "s_181"
        },
        {
            "source": "r_216",
            "target": "s_1087"
        },
        {
            "source": "r_216",
            "target": "s_392"
        },
        {
            "source": "r_216",
            "target": "s_1113"
        },
        {
            "source": "r_216",
            "target": "s_49"
        },
        {
            "source": "r_216",
            "target": "s_998"
        },
        {
            "source": "r_217",
            "target": "s_459"
        },
        {
            "source": "r_218",
            "target": "s_1224"
        },
        {
            "source": "r_218",
            "target": "s_644"
        },
        {
            "source": "r_218",
            "target": "s_1271"
        },
        {
            "source": "r_218",
            "target": "s_68"
        },
        {
            "source": "r_218",
            "target": "s_529"
        },
        {
            "source": "r_219",
            "target": "s_477"
        },
        {
            "source": "r_219",
            "target": "s_1260"
        },
        {
            "source": "r_219",
            "target": "s_951"
        },
        {
            "source": "r_219",
            "target": "s_640"
        },
        {
            "source": "r_219",
            "target": "s_903"
        },
        {
            "source": "r_219",
            "target": "s_137"
        },
        {
            "source": "r_219",
            "target": "s_1415"
        },
        {
            "source": "r_220",
            "target": "s_397"
        },
        {
            "source": "r_220",
            "target": "s_908"
        },
        {
            "source": "r_221",
            "target": "s_118"
        },
        {
            "source": "r_221",
            "target": "s_697"
        },
        {
            "source": "r_222",
            "target": "s_316"
        },
        {
            "source": "r_222",
            "target": "s_364"
        },
        {
            "source": "r_222",
            "target": "s_1018"
        },
        {
            "source": "r_222",
            "target": "s_1206"
        },
        {
            "source": "r_222",
            "target": "s_1214"
        },
        {
            "source": "r_222",
            "target": "s_866"
        },
        {
            "source": "r_222",
            "target": "s_1022"
        },
        {
            "source": "r_223",
            "target": "s_1176"
        },
        {
            "source": "r_223",
            "target": "s_1355"
        },
        {
            "source": "r_223",
            "target": "s_1346"
        },
        {
            "source": "r_224",
            "target": "s_276"
        },
        {
            "source": "r_224",
            "target": "s_783"
        },
        {
            "source": "r_224",
            "target": "s_1250"
        },
        {
            "source": "r_224",
            "target": "s_431"
        },
        {
            "source": "r_224",
            "target": "s_1014"
        },
        {
            "source": "r_224",
            "target": "s_1443"
        },
        {
            "source": "r_224",
            "target": "s_687"
        },
        {
            "source": "r_225",
            "target": "s_194"
        },
        {
            "source": "r_225",
            "target": "s_161"
        },
        {
            "source": "r_225",
            "target": "s_231"
        },
        {
            "source": "r_226",
            "target": "s_336"
        },
        {
            "source": "r_226",
            "target": "s_590"
        },
        {
            "source": "r_226",
            "target": "s_603"
        },
        {
            "source": "r_226",
            "target": "s_1368"
        },
        {
            "source": "r_226",
            "target": "s_1346"
        },
        {
            "source": "r_227",
            "target": "s_1218"
        },
        {
            "source": "r_227",
            "target": "s_544"
        },
        {
            "source": "r_227",
            "target": "s_380"
        },
        {
            "source": "r_227",
            "target": "s_631"
        },
        {
            "source": "r_228",
            "target": "s_1403"
        },
        {
            "source": "r_228",
            "target": "s_605"
        },
        {
            "source": "r_228",
            "target": "s_1467"
        },
        {
            "source": "r_228",
            "target": "s_145"
        },
        {
            "source": "r_228",
            "target": "s_408"
        },
        {
            "source": "r_229",
            "target": "s_471"
        },
        {
            "source": "r_229",
            "target": "s_353"
        },
        {
            "source": "r_230",
            "target": "s_995"
        },
        {
            "source": "r_230",
            "target": "s_765"
        },
        {
            "source": "r_230",
            "target": "s_865"
        },
        {
            "source": "r_230",
            "target": "s_1093"
        },
        {
            "source": "r_230",
            "target": "s_353"
        },
        {
            "source": "r_230",
            "target": "s_428"
        },
        {
            "source": "r_230",
            "target": "s_1420"
        },
        {
            "source": "r_230",
            "target": "s_727"
        },
        {
            "source": "r_231",
            "target": "s_314"
        },
        {
            "source": "r_231",
            "target": "s_334"
        },
        {
            "source": "r_231",
            "target": "s_439"
        },
        {
            "source": "r_231",
            "target": "s_100"
        },
        {
            "source": "r_231",
            "target": "s_231"
        },
        {
            "source": "r_232",
            "target": "s_638"
        },
        {
            "source": "r_232",
            "target": "s_1179"
        },
        {
            "source": "r_232",
            "target": "s_1190"
        },
        {
            "source": "r_232",
            "target": "s_1103"
        },
        {
            "source": "r_232",
            "target": "s_368"
        },
        {
            "source": "r_232",
            "target": "s_1273"
        },
        {
            "source": "r_233",
            "target": "s_1089"
        },
        {
            "source": "r_233",
            "target": "s_1254"
        },
        {
            "source": "r_233",
            "target": "s_499"
        },
        {
            "source": "r_234",
            "target": "s_449"
        },
        {
            "source": "r_234",
            "target": "s_311"
        },
        {
            "source": "r_234",
            "target": "s_1227"
        },
        {
            "source": "r_234",
            "target": "s_503"
        },
        {
            "source": "r_234",
            "target": "s_849"
        },
        {
            "source": "r_234",
            "target": "s_519"
        },
        {
            "source": "r_234",
            "target": "s_443"
        },
        {
            "source": "r_235",
            "target": "s_477"
        },
        {
            "source": "r_235",
            "target": "s_1222"
        },
        {
            "source": "r_235",
            "target": "s_1385"
        },
        {
            "source": "r_235",
            "target": "s_92"
        },
        {
            "source": "r_235",
            "target": "s_1465"
        },
        {
            "source": "r_235",
            "target": "s_1415"
        },
        {
            "source": "r_236",
            "target": "s_340"
        },
        {
            "source": "r_236",
            "target": "s_393"
        },
        {
            "source": "r_236",
            "target": "s_814"
        },
        {
            "source": "r_236",
            "target": "s_989"
        },
        {
            "source": "r_236",
            "target": "s_297"
        },
        {
            "source": "r_236",
            "target": "s_438"
        },
        {
            "source": "r_237",
            "target": "s_1250"
        },
        {
            "source": "r_237",
            "target": "s_527"
        },
        {
            "source": "r_237",
            "target": "s_558"
        },
        {
            "source": "r_237",
            "target": "s_1159"
        },
        {
            "source": "r_237",
            "target": "s_637"
        },
        {
            "source": "r_237",
            "target": "s_279"
        },
        {
            "source": "r_237",
            "target": "s_255"
        },
        {
            "source": "r_238",
            "target": "s_416"
        },
        {
            "source": "r_238",
            "target": "s_1080"
        },
        {
            "source": "r_238",
            "target": "s_1143"
        },
        {
            "source": "r_239",
            "target": "s_1258"
        },
        {
            "source": "r_239",
            "target": "s_692"
        },
        {
            "source": "r_239",
            "target": "s_252"
        },
        {
            "source": "r_240",
            "target": "s_15"
        },
        {
            "source": "r_240",
            "target": "s_383"
        },
        {
            "source": "r_241",
            "target": "s_846"
        },
        {
            "source": "r_241",
            "target": "s_453"
        },
        {
            "source": "r_241",
            "target": "s_896"
        },
        {
            "source": "r_241",
            "target": "s_348"
        },
        {
            "source": "r_241",
            "target": "s_127"
        },
        {
            "source": "r_241",
            "target": "s_1115"
        },
        {
            "source": "r_241",
            "target": "s_1197"
        },
        {
            "source": "r_241",
            "target": "s_137"
        },
        {
            "source": "r_242",
            "target": "s_1263"
        },
        {
            "source": "r_242",
            "target": "s_1126"
        },
        {
            "source": "r_242",
            "target": "s_1364"
        },
        {
            "source": "r_242",
            "target": "s_1371"
        },
        {
            "source": "r_242",
            "target": "s_1138"
        },
        {
            "source": "r_242",
            "target": "s_1346"
        },
        {
            "source": "r_243",
            "target": "s_789"
        },
        {
            "source": "r_243",
            "target": "s_936"
        },
        {
            "source": "r_243",
            "target": "s_471"
        },
        {
            "source": "r_243",
            "target": "s_353"
        },
        {
            "source": "r_244",
            "target": "s_590"
        },
        {
            "source": "r_244",
            "target": "s_603"
        },
        {
            "source": "r_244",
            "target": "s_1176"
        },
        {
            "source": "r_244",
            "target": "s_94"
        },
        {
            "source": "r_244",
            "target": "s_1034"
        },
        {
            "source": "r_244",
            "target": "s_1346"
        },
        {
            "source": "r_244",
            "target": "s_336"
        },
        {
            "source": "r_245",
            "target": "s_1301"
        },
        {
            "source": "r_245",
            "target": "s_353"
        },
        {
            "source": "r_246",
            "target": "s_644"
        },
        {
            "source": "r_246",
            "target": "s_904"
        },
        {
            "source": "r_246",
            "target": "s_29"
        },
        {
            "source": "r_246",
            "target": "s_1439"
        },
        {
            "source": "r_246",
            "target": "s_748"
        },
        {
            "source": "r_246",
            "target": "s_1072"
        },
        {
            "source": "r_246",
            "target": "s_567"
        },
        {
            "source": "r_246",
            "target": "s_380"
        },
        {
            "source": "r_247",
            "target": "s_12"
        },
        {
            "source": "r_247",
            "target": "s_250"
        },
        {
            "source": "r_247",
            "target": "s_1029"
        },
        {
            "source": "r_247",
            "target": "s_797"
        },
        {
            "source": "r_248",
            "target": "s_194"
        },
        {
            "source": "r_248",
            "target": "s_82"
        },
        {
            "source": "r_248",
            "target": "s_161"
        },
        {
            "source": "r_248",
            "target": "s_811"
        },
        {
            "source": "r_249",
            "target": "s_416"
        },
        {
            "source": "r_249",
            "target": "s_185"
        },
        {
            "source": "r_249",
            "target": "s_1449"
        },
        {
            "source": "r_249",
            "target": "s_1296"
        },
        {
            "source": "r_249",
            "target": "s_1143"
        },
        {
            "source": "r_250",
            "target": "s_398"
        },
        {
            "source": "r_250",
            "target": "s_1051"
        },
        {
            "source": "r_250",
            "target": "s_378"
        },
        {
            "source": "r_250",
            "target": "s_896"
        },
        {
            "source": "r_250",
            "target": "s_1174"
        },
        {
            "source": "r_250",
            "target": "s_1273"
        },
        {
            "source": "r_251",
            "target": "s_1059"
        },
        {
            "source": "r_251",
            "target": "s_741"
        },
        {
            "source": "r_251",
            "target": "s_173"
        },
        {
            "source": "r_251",
            "target": "s_699"
        },
        {
            "source": "r_251",
            "target": "s_1220"
        },
        {
            "source": "r_251",
            "target": "s_1161"
        },
        {
            "source": "r_251",
            "target": "s_781"
        },
        {
            "source": "r_252",
            "target": "s_1272"
        },
        {
            "source": "r_252",
            "target": "s_920"
        },
        {
            "source": "r_252",
            "target": "s_150"
        },
        {
            "source": "r_252",
            "target": "s_646"
        },
        {
            "source": "r_253",
            "target": "s_1283"
        },
        {
            "source": "r_253",
            "target": "s_284"
        },
        {
            "source": "r_253",
            "target": "s_1134"
        },
        {
            "source": "r_253",
            "target": "s_182"
        },
        {
            "source": "r_253",
            "target": "s_253"
        },
        {
            "source": "r_254",
            "target": "s_570"
        },
        {
            "source": "r_254",
            "target": "s_1467"
        },
        {
            "source": "r_254",
            "target": "s_132"
        },
        {
            "source": "r_254",
            "target": "s_1025"
        },
        {
            "source": "r_254",
            "target": "s_1213"
        },
        {
            "source": "r_254",
            "target": "s_1084"
        },
        {
            "source": "r_254",
            "target": "s_619"
        },
        {
            "source": "r_255",
            "target": "s_1259"
        },
        {
            "source": "r_255",
            "target": "s_1404"
        },
        {
            "source": "r_255",
            "target": "s_1345"
        },
        {
            "source": "r_255",
            "target": "s_619"
        },
        {
            "source": "r_256",
            "target": "s_1225"
        },
        {
            "source": "r_256",
            "target": "s_591"
        },
        {
            "source": "r_256",
            "target": "s_697"
        },
        {
            "source": "r_257",
            "target": "s_276"
        },
        {
            "source": "r_257",
            "target": "s_1250"
        },
        {
            "source": "r_257",
            "target": "s_783"
        },
        {
            "source": "r_257",
            "target": "s_342"
        },
        {
            "source": "r_257",
            "target": "s_431"
        },
        {
            "source": "r_257",
            "target": "s_1014"
        },
        {
            "source": "r_257",
            "target": "s_198"
        },
        {
            "source": "r_257",
            "target": "s_687"
        },
        {
            "source": "r_258",
            "target": "s_717"
        },
        {
            "source": "r_258",
            "target": "s_227"
        },
        {
            "source": "r_258",
            "target": "s_908"
        },
        {
            "source": "r_259",
            "target": "s_218"
        },
        {
            "source": "r_259",
            "target": "s_397"
        },
        {
            "source": "r_259",
            "target": "s_908"
        },
        {
            "source": "r_259",
            "target": "s_84"
        },
        {
            "source": "r_260",
            "target": "s_465"
        },
        {
            "source": "r_260",
            "target": "s_636"
        },
        {
            "source": "r_260",
            "target": "s_337"
        },
        {
            "source": "r_260",
            "target": "s_197"
        },
        {
            "source": "r_260",
            "target": "s_1087"
        },
        {
            "source": "r_260",
            "target": "s_472"
        },
        {
            "source": "r_261",
            "target": "s_421"
        },
        {
            "source": "r_261",
            "target": "s_652"
        },
        {
            "source": "r_261",
            "target": "s_1405"
        },
        {
            "source": "r_261",
            "target": "s_864"
        },
        {
            "source": "r_261",
            "target": "s_1089"
        },
        {
            "source": "r_261",
            "target": "s_1476"
        },
        {
            "source": "r_261",
            "target": "s_853"
        },
        {
            "source": "r_262",
            "target": "s_1190"
        },
        {
            "source": "r_262",
            "target": "s_638"
        },
        {
            "source": "r_262",
            "target": "s_117"
        },
        {
            "source": "r_262",
            "target": "s_326"
        },
        {
            "source": "r_262",
            "target": "s_6"
        },
        {
            "source": "r_262",
            "target": "s_1082"
        },
        {
            "source": "r_262",
            "target": "s_98"
        },
        {
            "source": "r_262",
            "target": "s_1273"
        },
        {
            "source": "r_263",
            "target": "s_1457"
        },
        {
            "source": "r_263",
            "target": "s_1298"
        },
        {
            "source": "r_263",
            "target": "s_220"
        },
        {
            "source": "r_263",
            "target": "s_880"
        },
        {
            "source": "r_263",
            "target": "s_1339"
        },
        {
            "source": "r_263",
            "target": "s_137"
        },
        {
            "source": "r_264",
            "target": "s_662"
        },
        {
            "source": "r_264",
            "target": "s_70"
        },
        {
            "source": "r_264",
            "target": "s_270"
        },
        {
            "source": "r_264",
            "target": "s_1394"
        },
        {
            "source": "r_264",
            "target": "s_495"
        },
        {
            "source": "r_265",
            "target": "s_1472"
        },
        {
            "source": "r_265",
            "target": "s_438"
        },
        {
            "source": "r_265",
            "target": "s_297"
        },
        {
            "source": "r_266",
            "target": "s_26"
        },
        {
            "source": "r_266",
            "target": "s_970"
        },
        {
            "source": "r_266",
            "target": "s_205"
        },
        {
            "source": "r_266",
            "target": "s_640"
        },
        {
            "source": "r_266",
            "target": "s_866"
        },
        {
            "source": "r_267",
            "target": "s_918"
        },
        {
            "source": "r_267",
            "target": "s_252"
        },
        {
            "source": "r_268",
            "target": "s_150"
        },
        {
            "source": "r_268",
            "target": "s_1295"
        },
        {
            "source": "r_268",
            "target": "s_1272"
        },
        {
            "source": "r_269",
            "target": "s_134"
        },
        {
            "source": "r_269",
            "target": "s_789"
        },
        {
            "source": "r_269",
            "target": "s_353"
        },
        {
            "source": "r_270",
            "target": "s_1233"
        },
        {
            "source": "r_270",
            "target": "s_29"
        },
        {
            "source": "r_270",
            "target": "s_857"
        },
        {
            "source": "r_270",
            "target": "s_380"
        },
        {
            "source": "r_271",
            "target": "s_1473"
        },
        {
            "source": "r_271",
            "target": "s_394"
        },
        {
            "source": "r_271",
            "target": "s_1071"
        },
        {
            "source": "r_271",
            "target": "s_238"
        },
        {
            "source": "r_271",
            "target": "s_291"
        },
        {
            "source": "r_271",
            "target": "s_266"
        },
        {
            "source": "r_272",
            "target": "s_611"
        },
        {
            "source": "r_272",
            "target": "s_1226"
        },
        {
            "source": "r_273",
            "target": "s_1347"
        },
        {
            "source": "r_273",
            "target": "s_246"
        },
        {
            "source": "r_274",
            "target": "s_1073"
        },
        {
            "source": "r_274",
            "target": "s_1198"
        },
        {
            "source": "r_274",
            "target": "s_281"
        },
        {
            "source": "r_274",
            "target": "s_111"
        },
        {
            "source": "r_275",
            "target": "s_75"
        },
        {
            "source": "r_275",
            "target": "s_318"
        },
        {
            "source": "r_275",
            "target": "s_1143"
        },
        {
            "source": "r_275",
            "target": "s_811"
        },
        {
            "source": "r_276",
            "target": "s_1144"
        },
        {
            "source": "r_276",
            "target": "s_495"
        },
        {
            "source": "r_276",
            "target": "s_211"
        },
        {
            "source": "r_277",
            "target": "s_707"
        },
        {
            "source": "r_277",
            "target": "s_1424"
        },
        {
            "source": "r_277",
            "target": "s_734"
        },
        {
            "source": "r_277",
            "target": "s_182"
        },
        {
            "source": "r_278",
            "target": "s_470"
        },
        {
            "source": "r_278",
            "target": "s_499"
        },
        {
            "source": "r_278",
            "target": "s_1349"
        },
        {
            "source": "r_278",
            "target": "s_85"
        },
        {
            "source": "r_278",
            "target": "s_1089"
        },
        {
            "source": "r_279",
            "target": "s_776"
        },
        {
            "source": "r_279",
            "target": "s_1461"
        },
        {
            "source": "r_279",
            "target": "s_843"
        },
        {
            "source": "r_279",
            "target": "s_1255"
        },
        {
            "source": "r_279",
            "target": "s_1239"
        },
        {
            "source": "r_279",
            "target": "s_1193"
        },
        {
            "source": "r_279",
            "target": "s_1352"
        },
        {
            "source": "r_279",
            "target": "s_1021"
        },
        {
            "source": "r_279",
            "target": "s_708"
        },
        {
            "source": "r_280",
            "target": "s_1467"
        },
        {
            "source": "r_280",
            "target": "s_132"
        },
        {
            "source": "r_280",
            "target": "s_408"
        },
        {
            "source": "r_280",
            "target": "s_619"
        },
        {
            "source": "r_280",
            "target": "s_1205"
        },
        {
            "source": "r_281",
            "target": "s_1405"
        },
        {
            "source": "r_281",
            "target": "s_192"
        },
        {
            "source": "r_281",
            "target": "s_933"
        },
        {
            "source": "r_282",
            "target": "s_518"
        },
        {
            "source": "r_282",
            "target": "s_1340"
        },
        {
            "source": "r_282",
            "target": "s_529"
        },
        {
            "source": "r_283",
            "target": "s_873"
        },
        {
            "source": "r_283",
            "target": "s_582"
        },
        {
            "source": "r_283",
            "target": "s_772"
        },
        {
            "source": "r_283",
            "target": "s_55"
        },
        {
            "source": "r_283",
            "target": "s_13"
        },
        {
            "source": "r_283",
            "target": "s_207"
        },
        {
            "source": "r_284",
            "target": "s_986"
        },
        {
            "source": "r_284",
            "target": "s_300"
        },
        {
            "source": "r_284",
            "target": "s_1369"
        },
        {
            "source": "r_285",
            "target": "s_148"
        },
        {
            "source": "r_285",
            "target": "s_478"
        },
        {
            "source": "r_285",
            "target": "s_854"
        },
        {
            "source": "r_285",
            "target": "s_1414"
        },
        {
            "source": "r_285",
            "target": "s_1289"
        },
        {
            "source": "r_285",
            "target": "s_1233"
        },
        {
            "source": "r_285",
            "target": "s_260"
        },
        {
            "source": "r_286",
            "target": "s_704"
        },
        {
            "source": "r_286",
            "target": "s_286"
        },
        {
            "source": "r_286",
            "target": "s_817"
        },
        {
            "source": "r_286",
            "target": "s_124"
        },
        {
            "source": "r_287",
            "target": "s_1215"
        },
        {
            "source": "r_287",
            "target": "s_158"
        },
        {
            "source": "r_288",
            "target": "s_416"
        },
        {
            "source": "r_288",
            "target": "s_483"
        },
        {
            "source": "r_288",
            "target": "s_1143"
        },
        {
            "source": "r_288",
            "target": "s_538"
        },
        {
            "source": "r_289",
            "target": "s_232"
        },
        {
            "source": "r_289",
            "target": "s_1088"
        },
        {
            "source": "r_289",
            "target": "s_386"
        },
        {
            "source": "r_289",
            "target": "s_138"
        },
        {
            "source": "r_289",
            "target": "s_1220"
        },
        {
            "source": "r_290",
            "target": "s_604"
        },
        {
            "source": "r_290",
            "target": "s_1135"
        },
        {
            "source": "r_290",
            "target": "s_321"
        },
        {
            "source": "r_290",
            "target": "s_869"
        },
        {
            "source": "r_291",
            "target": "s_1452"
        },
        {
            "source": "r_291",
            "target": "s_747"
        },
        {
            "source": "r_291",
            "target": "s_1170"
        },
        {
            "source": "r_291",
            "target": "s_837"
        },
        {
            "source": "r_291",
            "target": "s_722"
        },
        {
            "source": "r_292",
            "target": "s_336"
        },
        {
            "source": "r_292",
            "target": "s_1152"
        },
        {
            "source": "r_292",
            "target": "s_1359"
        },
        {
            "source": "r_292",
            "target": "s_876"
        },
        {
            "source": "r_293",
            "target": "s_674"
        },
        {
            "source": "r_293",
            "target": "s_334"
        },
        {
            "source": "r_293",
            "target": "s_129"
        },
        {
            "source": "r_293",
            "target": "s_309"
        },
        {
            "source": "r_293",
            "target": "s_853"
        },
        {
            "source": "r_294",
            "target": "s_808"
        },
        {
            "source": "r_294",
            "target": "s_677"
        },
        {
            "source": "r_294",
            "target": "s_968"
        },
        {
            "source": "r_294",
            "target": "s_619"
        },
        {
            "source": "r_294",
            "target": "s_1467"
        },
        {
            "source": "r_295",
            "target": "s_1313"
        },
        {
            "source": "r_295",
            "target": "s_1208"
        },
        {
            "source": "r_295",
            "target": "s_1445"
        },
        {
            "source": "r_296",
            "target": "s_215"
        },
        {
            "source": "r_296",
            "target": "s_1143"
        },
        {
            "source": "r_296",
            "target": "s_1139"
        },
        {
            "source": "r_297",
            "target": "s_504"
        },
        {
            "source": "r_297",
            "target": "s_202"
        },
        {
            "source": "r_297",
            "target": "s_534"
        },
        {
            "source": "r_297",
            "target": "s_130"
        },
        {
            "source": "r_297",
            "target": "s_751"
        },
        {
            "source": "r_297",
            "target": "s_1188"
        },
        {
            "source": "r_298",
            "target": "s_12"
        },
        {
            "source": "r_298",
            "target": "s_1276"
        },
        {
            "source": "r_298",
            "target": "s_437"
        },
        {
            "source": "r_298",
            "target": "s_802"
        },
        {
            "source": "r_299",
            "target": "s_62"
        },
        {
            "source": "r_299",
            "target": "s_1323"
        },
        {
            "source": "r_299",
            "target": "s_1132"
        },
        {
            "source": "r_299",
            "target": "s_390"
        },
        {
            "source": "r_299",
            "target": "s_299"
        },
        {
            "source": "r_300",
            "target": "s_894"
        },
        {
            "source": "r_300",
            "target": "s_530"
        },
        {
            "source": "r_301",
            "target": "s_540"
        },
        {
            "source": "r_301",
            "target": "s_1126"
        },
        {
            "source": "r_301",
            "target": "s_545"
        },
        {
            "source": "r_301",
            "target": "s_596"
        },
        {
            "source": "r_301",
            "target": "s_794"
        },
        {
            "source": "r_301",
            "target": "s_100"
        },
        {
            "source": "r_301",
            "target": "s_493"
        },
        {
            "source": "r_302",
            "target": "s_1423"
        },
        {
            "source": "r_302",
            "target": "s_1016"
        },
        {
            "source": "r_302",
            "target": "s_1066"
        },
        {
            "source": "r_302",
            "target": "s_453"
        },
        {
            "source": "r_302",
            "target": "s_137"
        },
        {
            "source": "r_302",
            "target": "s_896"
        },
        {
            "source": "r_303",
            "target": "s_302"
        },
        {
            "source": "r_303",
            "target": "s_327"
        },
        {
            "source": "r_303",
            "target": "s_841"
        },
        {
            "source": "r_304",
            "target": "s_1181"
        },
        {
            "source": "r_304",
            "target": "s_177"
        },
        {
            "source": "r_304",
            "target": "s_997"
        },
        {
            "source": "r_304",
            "target": "s_320"
        },
        {
            "source": "r_304",
            "target": "s_825"
        },
        {
            "source": "r_304",
            "target": "s_1200"
        },
        {
            "source": "r_305",
            "target": "s_670"
        },
        {
            "source": "r_305",
            "target": "s_861"
        },
        {
            "source": "r_305",
            "target": "s_196"
        },
        {
            "source": "r_305",
            "target": "s_1433"
        },
        {
            "source": "r_305",
            "target": "s_988"
        },
        {
            "source": "r_305",
            "target": "s_764"
        },
        {
            "source": "r_305",
            "target": "s_1469"
        },
        {
            "source": "r_306",
            "target": "s_1115"
        },
        {
            "source": "r_306",
            "target": "s_1171"
        },
        {
            "source": "r_306",
            "target": "s_403"
        },
        {
            "source": "r_306",
            "target": "s_896"
        },
        {
            "source": "r_306",
            "target": "s_488"
        },
        {
            "source": "r_306",
            "target": "s_137"
        },
        {
            "source": "r_307",
            "target": "s_5"
        },
        {
            "source": "r_307",
            "target": "s_551"
        },
        {
            "source": "r_307",
            "target": "s_316"
        },
        {
            "source": "r_307",
            "target": "s_9"
        },
        {
            "source": "r_307",
            "target": "s_1149"
        },
        {
            "source": "r_307",
            "target": "s_983"
        },
        {
            "source": "r_307",
            "target": "s_85"
        },
        {
            "source": "r_308",
            "target": "s_96"
        },
        {
            "source": "r_308",
            "target": "s_1225"
        },
        {
            "source": "r_309",
            "target": "s_1431"
        },
        {
            "source": "r_309",
            "target": "s_1189"
        },
        {
            "source": "r_309",
            "target": "s_1061"
        },
        {
            "source": "r_309",
            "target": "s_682"
        },
        {
            "source": "r_309",
            "target": "s_1440"
        },
        {
            "source": "r_310",
            "target": "s_316"
        },
        {
            "source": "r_310",
            "target": "s_364"
        },
        {
            "source": "r_310",
            "target": "s_312"
        },
        {
            "source": "r_310",
            "target": "s_1214"
        },
        {
            "source": "r_310",
            "target": "s_1206"
        },
        {
            "source": "r_310",
            "target": "s_255"
        },
        {
            "source": "r_311",
            "target": "s_1032"
        },
        {
            "source": "r_311",
            "target": "s_301"
        },
        {
            "source": "r_311",
            "target": "s_1028"
        },
        {
            "source": "r_311",
            "target": "s_40"
        },
        {
            "source": "r_311",
            "target": "s_1466"
        },
        {
            "source": "r_311",
            "target": "s_745"
        },
        {
            "source": "r_311",
            "target": "s_1151"
        },
        {
            "source": "r_312",
            "target": "s_930"
        },
        {
            "source": "r_312",
            "target": "s_1256"
        },
        {
            "source": "r_312",
            "target": "s_389"
        },
        {
            "source": "r_312",
            "target": "s_611"
        },
        {
            "source": "r_313",
            "target": "s_788"
        },
        {
            "source": "r_313",
            "target": "s_864"
        },
        {
            "source": "r_313",
            "target": "s_334"
        },
        {
            "source": "r_313",
            "target": "s_162"
        },
        {
            "source": "r_313",
            "target": "s_666"
        },
        {
            "source": "r_313",
            "target": "s_853"
        },
        {
            "source": "r_314",
            "target": "s_275"
        },
        {
            "source": "r_314",
            "target": "s_662"
        },
        {
            "source": "r_314",
            "target": "s_168"
        },
        {
            "source": "r_314",
            "target": "s_1358"
        },
        {
            "source": "r_314",
            "target": "s_472"
        },
        {
            "source": "r_314",
            "target": "s_495"
        },
        {
            "source": "r_315",
            "target": "s_822"
        },
        {
            "source": "r_315",
            "target": "s_865"
        },
        {
            "source": "r_315",
            "target": "s_673"
        },
        {
            "source": "r_315",
            "target": "s_1214"
        },
        {
            "source": "r_315",
            "target": "s_727"
        },
        {
            "source": "r_316",
            "target": "s_705"
        },
        {
            "source": "r_316",
            "target": "s_38"
        },
        {
            "source": "r_316",
            "target": "s_679"
        },
        {
            "source": "r_316",
            "target": "s_111"
        },
        {
            "source": "r_316",
            "target": "s_1390"
        },
        {
            "source": "r_317",
            "target": "s_1335"
        },
        {
            "source": "r_317",
            "target": "s_1325"
        },
        {
            "source": "r_318",
            "target": "s_927"
        },
        {
            "source": "r_318",
            "target": "s_599"
        },
        {
            "source": "r_318",
            "target": "s_457"
        },
        {
            "source": "r_319",
            "target": "s_826"
        },
        {
            "source": "r_319",
            "target": "s_947"
        },
        {
            "source": "r_319",
            "target": "s_77"
        },
        {
            "source": "r_319",
            "target": "s_145"
        },
        {
            "source": "r_319",
            "target": "s_408"
        },
        {
            "source": "r_320",
            "target": "s_1208"
        },
        {
            "source": "r_320",
            "target": "s_462"
        },
        {
            "source": "r_321",
            "target": "s_26"
        },
        {
            "source": "r_321",
            "target": "s_640"
        },
        {
            "source": "r_321",
            "target": "s_1194"
        },
        {
            "source": "r_321",
            "target": "s_507"
        },
        {
            "source": "r_321",
            "target": "s_709"
        },
        {
            "source": "r_321",
            "target": "s_1376"
        },
        {
            "source": "r_321",
            "target": "s_580"
        },
        {
            "source": "r_321",
            "target": "s_351"
        },
        {
            "source": "r_321",
            "target": "s_137"
        },
        {
            "source": "r_322",
            "target": "s_1119"
        },
        {
            "source": "r_322",
            "target": "s_1207"
        },
        {
            "source": "r_322",
            "target": "s_123"
        },
        {
            "source": "r_322",
            "target": "s_811"
        },
        {
            "source": "r_323",
            "target": "s_1244"
        },
        {
            "source": "r_323",
            "target": "s_108"
        },
        {
            "source": "r_323",
            "target": "s_453"
        },
        {
            "source": "r_324",
            "target": "s_1430"
        },
        {
            "source": "r_324",
            "target": "s_1467"
        },
        {
            "source": "r_324",
            "target": "s_1079"
        },
        {
            "source": "r_324",
            "target": "s_731"
        },
        {
            "source": "r_324",
            "target": "s_385"
        },
        {
            "source": "r_324",
            "target": "s_900"
        },
        {
            "source": "r_324",
            "target": "s_145"
        },
        {
            "source": "r_324",
            "target": "s_408"
        },
        {
            "source": "r_325",
            "target": "s_1233"
        },
        {
            "source": "r_325",
            "target": "s_446"
        },
        {
            "source": "r_325",
            "target": "s_186"
        },
        {
            "source": "r_325",
            "target": "s_451"
        },
        {
            "source": "r_325",
            "target": "s_209"
        },
        {
            "source": "r_325",
            "target": "s_260"
        },
        {
            "source": "r_326",
            "target": "s_347"
        },
        {
            "source": "r_326",
            "target": "s_1036"
        },
        {
            "source": "r_326",
            "target": "s_1409"
        },
        {
            "source": "r_326",
            "target": "s_144"
        },
        {
            "source": "r_326",
            "target": "s_459"
        },
        {
            "source": "r_327",
            "target": "s_284"
        },
        {
            "source": "r_327",
            "target": "s_810"
        },
        {
            "source": "r_327",
            "target": "s_265"
        },
        {
            "source": "r_327",
            "target": "s_182"
        },
        {
            "source": "r_327",
            "target": "s_253"
        },
        {
            "source": "r_327",
            "target": "s_912"
        },
        {
            "source": "r_327",
            "target": "s_734"
        },
        {
            "source": "r_328",
            "target": "s_409"
        },
        {
            "source": "r_328",
            "target": "s_133"
        },
        {
            "source": "r_328",
            "target": "s_890"
        },
        {
            "source": "r_329",
            "target": "s_284"
        },
        {
            "source": "r_329",
            "target": "s_675"
        },
        {
            "source": "r_329",
            "target": "s_182"
        },
        {
            "source": "r_329",
            "target": "s_1134"
        },
        {
            "source": "r_329",
            "target": "s_253"
        },
        {
            "source": "r_329",
            "target": "s_684"
        },
        {
            "source": "r_329",
            "target": "s_734"
        },
        {
            "source": "r_330",
            "target": "s_1429"
        },
        {
            "source": "r_330",
            "target": "s_1480"
        },
        {
            "source": "r_330",
            "target": "s_994"
        },
        {
            "source": "r_330",
            "target": "s_612"
        },
        {
            "source": "r_330",
            "target": "s_297"
        },
        {
            "source": "r_330",
            "target": "s_656"
        },
        {
            "source": "r_331",
            "target": "s_1059"
        },
        {
            "source": "r_331",
            "target": "s_434"
        },
        {
            "source": "r_331",
            "target": "s_44"
        },
        {
            "source": "r_331",
            "target": "s_1215"
        },
        {
            "source": "r_331",
            "target": "s_46"
        },
        {
            "source": "r_331",
            "target": "s_1220"
        },
        {
            "source": "r_331",
            "target": "s_781"
        },
        {
            "source": "r_332",
            "target": "s_1308"
        },
        {
            "source": "r_332",
            "target": "s_350"
        },
        {
            "source": "r_332",
            "target": "s_231"
        },
        {
            "source": "r_332",
            "target": "s_100"
        },
        {
            "source": "r_333",
            "target": "s_1302"
        },
        {
            "source": "r_333",
            "target": "s_860"
        },
        {
            "source": "r_333",
            "target": "s_1386"
        },
        {
            "source": "r_333",
            "target": "s_807"
        },
        {
            "source": "r_333",
            "target": "s_1130"
        },
        {
            "source": "r_333",
            "target": "s_233"
        },
        {
            "source": "r_333",
            "target": "s_1064"
        },
        {
            "source": "r_333",
            "target": "s_841"
        },
        {
            "source": "r_334",
            "target": "s_661"
        },
        {
            "source": "r_334",
            "target": "s_1036"
        },
        {
            "source": "r_334",
            "target": "s_979"
        },
        {
            "source": "r_334",
            "target": "s_136"
        },
        {
            "source": "r_334",
            "target": "s_69"
        },
        {
            "source": "r_334",
            "target": "s_322"
        },
        {
            "source": "r_334",
            "target": "s_1057"
        },
        {
            "source": "r_335",
            "target": "s_1043"
        },
        {
            "source": "r_335",
            "target": "s_236"
        },
        {
            "source": "r_335",
            "target": "s_521"
        },
        {
            "source": "r_335",
            "target": "s_1316"
        },
        {
            "source": "r_335",
            "target": "s_175"
        },
        {
            "source": "r_335",
            "target": "s_1205"
        },
        {
            "source": "r_335",
            "target": "s_519"
        },
        {
            "source": "r_335",
            "target": "s_396"
        },
        {
            "source": "r_335",
            "target": "s_443"
        },
        {
            "source": "r_336",
            "target": "s_91"
        },
        {
            "source": "r_336",
            "target": "s_1236"
        },
        {
            "source": "r_336",
            "target": "s_202"
        },
        {
            "source": "r_336",
            "target": "s_1265"
        },
        {
            "source": "r_336",
            "target": "s_949"
        },
        {
            "source": "r_336",
            "target": "s_271"
        },
        {
            "source": "r_337",
            "target": "s_963"
        },
        {
            "source": "r_337",
            "target": "s_869"
        },
        {
            "source": "r_337",
            "target": "s_1355"
        },
        {
            "source": "r_337",
            "target": "s_156"
        },
        {
            "source": "r_337",
            "target": "s_1094"
        },
        {
            "source": "r_337",
            "target": "s_376"
        },
        {
            "source": "r_337",
            "target": "s_844"
        },
        {
            "source": "r_337",
            "target": "s_430"
        },
        {
            "source": "r_337",
            "target": "s_714"
        },
        {
            "source": "r_337",
            "target": "s_1346"
        },
        {
            "source": "r_338",
            "target": "s_521"
        },
        {
            "source": "r_338",
            "target": "s_101"
        },
        {
            "source": "r_338",
            "target": "s_1435"
        },
        {
            "source": "r_338",
            "target": "s_57"
        },
        {
            "source": "r_338",
            "target": "s_519"
        },
        {
            "source": "r_338",
            "target": "s_270"
        },
        {
            "source": "r_339",
            "target": "s_1472"
        },
        {
            "source": "r_339",
            "target": "s_1480"
        },
        {
            "source": "r_339",
            "target": "s_1333"
        },
        {
            "source": "r_339",
            "target": "s_722"
        },
        {
            "source": "r_339",
            "target": "s_438"
        },
        {
            "source": "r_339",
            "target": "s_656"
        },
        {
            "source": "r_339",
            "target": "s_618"
        },
        {
            "source": "r_339",
            "target": "s_297"
        },
        {
            "source": "r_340",
            "target": "s_820"
        },
        {
            "source": "r_340",
            "target": "s_575"
        },
        {
            "source": "r_340",
            "target": "s_1096"
        },
        {
            "source": "r_340",
            "target": "s_786"
        },
        {
            "source": "r_340",
            "target": "s_595"
        },
        {
            "source": "r_340",
            "target": "s_1405"
        },
        {
            "source": "r_341",
            "target": "s_328"
        },
        {
            "source": "r_341",
            "target": "s_1142"
        },
        {
            "source": "r_341",
            "target": "s_760"
        },
        {
            "source": "r_341",
            "target": "s_1115"
        },
        {
            "source": "r_341",
            "target": "s_477"
        },
        {
            "source": "r_341",
            "target": "s_1469"
        },
        {
            "source": "r_341",
            "target": "s_137"
        },
        {
            "source": "r_342",
            "target": "s_778"
        },
        {
            "source": "r_342",
            "target": "s_869"
        },
        {
            "source": "r_342",
            "target": "s_1081"
        },
        {
            "source": "r_342",
            "target": "s_1346"
        },
        {
            "source": "r_342",
            "target": "s_445"
        },
        {
            "source": "r_343",
            "target": "s_254"
        },
        {
            "source": "r_343",
            "target": "s_653"
        },
        {
            "source": "r_344",
            "target": "s_249"
        },
        {
            "source": "r_344",
            "target": "s_909"
        },
        {
            "source": "r_344",
            "target": "s_321"
        },
        {
            "source": "r_344",
            "target": "s_1065"
        },
        {
            "source": "r_345",
            "target": "s_403"
        },
        {
            "source": "r_345",
            "target": "s_812"
        },
        {
            "source": "r_345",
            "target": "s_225"
        },
        {
            "source": "r_345",
            "target": "s_896"
        },
        {
            "source": "r_345",
            "target": "s_137"
        },
        {
            "source": "r_346",
            "target": "s_704"
        },
        {
            "source": "r_346",
            "target": "s_817"
        },
        {
            "source": "r_346",
            "target": "s_124"
        },
        {
            "source": "r_347",
            "target": "s_1255"
        },
        {
            "source": "r_347",
            "target": "s_776"
        },
        {
            "source": "r_347",
            "target": "s_643"
        },
        {
            "source": "r_347",
            "target": "s_1461"
        },
        {
            "source": "r_347",
            "target": "s_835"
        },
        {
            "source": "r_347",
            "target": "s_67"
        },
        {
            "source": "r_347",
            "target": "s_708"
        },
        {
            "source": "r_348",
            "target": "s_705"
        },
        {
            "source": "r_348",
            "target": "s_38"
        },
        {
            "source": "r_348",
            "target": "s_766"
        },
        {
            "source": "r_348",
            "target": "s_111"
        },
        {
            "source": "r_348",
            "target": "s_1390"
        },
        {
            "source": "r_349",
            "target": "s_598"
        },
        {
            "source": "r_349",
            "target": "s_1186"
        },
        {
            "source": "r_349",
            "target": "s_519"
        },
        {
            "source": "r_349",
            "target": "s_838"
        },
        {
            "source": "r_349",
            "target": "s_19"
        },
        {
            "source": "r_350",
            "target": "s_1283"
        },
        {
            "source": "r_350",
            "target": "s_112"
        },
        {
            "source": "r_350",
            "target": "s_466"
        },
        {
            "source": "r_350",
            "target": "s_742"
        },
        {
            "source": "r_351",
            "target": "s_1157"
        },
        {
            "source": "r_351",
            "target": "s_966"
        },
        {
            "source": "r_351",
            "target": "s_1350"
        },
        {
            "source": "r_351",
            "target": "s_289"
        },
        {
            "source": "r_351",
            "target": "s_1054"
        },
        {
            "source": "r_351",
            "target": "s_408"
        },
        {
            "source": "r_352",
            "target": "s_1215"
        },
        {
            "source": "r_352",
            "target": "s_1251"
        },
        {
            "source": "r_353",
            "target": "s_284"
        },
        {
            "source": "r_353",
            "target": "s_1401"
        },
        {
            "source": "r_353",
            "target": "s_182"
        },
        {
            "source": "r_353",
            "target": "s_253"
        },
        {
            "source": "r_353",
            "target": "s_734"
        },
        {
            "source": "r_353",
            "target": "s_912"
        },
        {
            "source": "r_354",
            "target": "s_869"
        },
        {
            "source": "r_354",
            "target": "s_1335"
        },
        {
            "source": "r_354",
            "target": "s_213"
        },
        {
            "source": "r_354",
            "target": "s_756"
        },
        {
            "source": "r_354",
            "target": "s_647"
        },
        {
            "source": "r_354",
            "target": "s_1346"
        },
        {
            "source": "r_354",
            "target": "s_211"
        },
        {
            "source": "r_355",
            "target": "s_874"
        },
        {
            "source": "r_355",
            "target": "s_1486"
        },
        {
            "source": "r_355",
            "target": "s_519"
        },
        {
            "source": "r_355",
            "target": "s_443"
        },
        {
            "source": "r_355",
            "target": "s_1227"
        },
        {
            "source": "r_356",
            "target": "s_700"
        },
        {
            "source": "r_356",
            "target": "s_1199"
        },
        {
            "source": "r_357",
            "target": "s_331"
        },
        {
            "source": "r_357",
            "target": "s_724"
        },
        {
            "source": "r_357",
            "target": "s_1058"
        },
        {
            "source": "r_357",
            "target": "s_1206"
        },
        {
            "source": "r_357",
            "target": "s_543"
        },
        {
            "source": "r_357",
            "target": "s_366"
        },
        {
            "source": "r_357",
            "target": "s_285"
        },
        {
            "source": "r_357",
            "target": "s_1214"
        },
        {
            "source": "r_358",
            "target": "s_1468"
        },
        {
            "source": "r_358",
            "target": "s_37"
        },
        {
            "source": "r_358",
            "target": "s_1310"
        },
        {
            "source": "r_358",
            "target": "s_259"
        },
        {
            "source": "r_358",
            "target": "s_1003"
        },
        {
            "source": "r_359",
            "target": "s_592"
        },
        {
            "source": "r_359",
            "target": "s_260"
        },
        {
            "source": "r_359",
            "target": "s_358"
        },
        {
            "source": "r_359",
            "target": "s_944"
        },
        {
            "source": "r_360",
            "target": "s_475"
        },
        {
            "source": "r_360",
            "target": "s_811"
        },
        {
            "source": "r_361",
            "target": "s_773"
        },
        {
            "source": "r_361",
            "target": "s_1381"
        },
        {
            "source": "r_361",
            "target": "s_1476"
        },
        {
            "source": "r_361",
            "target": "s_1003"
        },
        {
            "source": "r_361",
            "target": "s_1220"
        },
        {
            "source": "r_362",
            "target": "s_32"
        },
        {
            "source": "r_362",
            "target": "s_961"
        },
        {
            "source": "r_362",
            "target": "s_929"
        },
        {
            "source": "r_362",
            "target": "s_806"
        },
        {
            "source": "r_362",
            "target": "s_708"
        },
        {
            "source": "r_362",
            "target": "s_35"
        },
        {
            "source": "r_363",
            "target": "s_541"
        },
        {
            "source": "r_363",
            "target": "s_916"
        },
        {
            "source": "r_363",
            "target": "s_611"
        },
        {
            "source": "r_364",
            "target": "s_404"
        },
        {
            "source": "r_364",
            "target": "s_1357"
        },
        {
            "source": "r_364",
            "target": "s_1285"
        },
        {
            "source": "r_364",
            "target": "s_685"
        },
        {
            "source": "r_364",
            "target": "s_547"
        },
        {
            "source": "r_364",
            "target": "s_654"
        },
        {
            "source": "r_364",
            "target": "s_1250"
        },
        {
            "source": "r_365",
            "target": "s_308"
        },
        {
            "source": "r_365",
            "target": "s_1390"
        },
        {
            "source": "r_365",
            "target": "s_578"
        },
        {
            "source": "r_365",
            "target": "s_111"
        },
        {
            "source": "r_366",
            "target": "s_1244"
        },
        {
            "source": "r_366",
            "target": "s_1080"
        },
        {
            "source": "r_366",
            "target": "s_701"
        },
        {
            "source": "r_366",
            "target": "s_108"
        },
        {
            "source": "r_367",
            "target": "s_978"
        },
        {
            "source": "r_367",
            "target": "s_89"
        },
        {
            "source": "r_367",
            "target": "s_1227"
        },
        {
            "source": "r_367",
            "target": "s_496"
        },
        {
            "source": "r_367",
            "target": "s_1362"
        },
        {
            "source": "r_367",
            "target": "s_503"
        },
        {
            "source": "r_367",
            "target": "s_849"
        },
        {
            "source": "r_367",
            "target": "s_519"
        },
        {
            "source": "r_367",
            "target": "s_443"
        },
        {
            "source": "r_368",
            "target": "s_767"
        },
        {
            "source": "r_368",
            "target": "s_202"
        },
        {
            "source": "r_368",
            "target": "s_1188"
        },
        {
            "source": "r_368",
            "target": "s_170"
        },
        {
            "source": "r_368",
            "target": "s_11"
        },
        {
            "source": "r_369",
            "target": "s_231"
        },
        {
            "source": "r_369",
            "target": "s_1347"
        },
        {
            "source": "r_370",
            "target": "s_494"
        },
        {
            "source": "r_370",
            "target": "s_1075"
        },
        {
            "source": "r_370",
            "target": "s_353"
        },
        {
            "source": "r_371",
            "target": "s_1467"
        },
        {
            "source": "r_371",
            "target": "s_152"
        },
        {
            "source": "r_371",
            "target": "s_619"
        },
        {
            "source": "r_371",
            "target": "s_628"
        },
        {
            "source": "r_372",
            "target": "s_110"
        },
        {
            "source": "r_372",
            "target": "s_334"
        },
        {
            "source": "r_372",
            "target": "s_1476"
        },
        {
            "source": "r_372",
            "target": "s_853"
        },
        {
            "source": "r_373",
            "target": "s_620"
        },
        {
            "source": "r_373",
            "target": "s_262"
        },
        {
            "source": "r_373",
            "target": "s_605"
        },
        {
            "source": "r_373",
            "target": "s_572"
        },
        {
            "source": "r_374",
            "target": "s_517"
        },
        {
            "source": "r_374",
            "target": "s_1195"
        },
        {
            "source": "r_374",
            "target": "s_620"
        },
        {
            "source": "r_374",
            "target": "s_605"
        },
        {
            "source": "r_375",
            "target": "s_1221"
        },
        {
            "source": "r_375",
            "target": "s_383"
        },
        {
            "source": "r_375",
            "target": "s_435"
        },
        {
            "source": "r_375",
            "target": "s_299"
        },
        {
            "source": "r_375",
            "target": "s_138"
        },
        {
            "source": "r_376",
            "target": "s_1140"
        },
        {
            "source": "r_376",
            "target": "s_1444"
        },
        {
            "source": "r_376",
            "target": "s_1294"
        },
        {
            "source": "r_376",
            "target": "s_380"
        },
        {
            "source": "r_376",
            "target": "s_349"
        },
        {
            "source": "r_377",
            "target": "s_278"
        },
        {
            "source": "r_377",
            "target": "s_509"
        },
        {
            "source": "r_377",
            "target": "s_161"
        },
        {
            "source": "r_378",
            "target": "s_319"
        },
        {
            "source": "r_378",
            "target": "s_613"
        },
        {
            "source": "r_378",
            "target": "s_1306"
        },
        {
            "source": "r_378",
            "target": "s_1161"
        },
        {
            "source": "r_378",
            "target": "s_1339"
        },
        {
            "source": "r_378",
            "target": "s_1220"
        },
        {
            "source": "r_378",
            "target": "s_1059"
        },
        {
            "source": "r_379",
            "target": "s_243"
        },
        {
            "source": "r_379",
            "target": "s_1"
        },
        {
            "source": "r_379",
            "target": "s_931"
        },
        {
            "source": "r_379",
            "target": "s_723"
        },
        {
            "source": "r_379",
            "target": "s_50"
        },
        {
            "source": "r_379",
            "target": "s_1143"
        },
        {
            "source": "r_379",
            "target": "s_182"
        },
        {
            "source": "r_380",
            "target": "s_413"
        },
        {
            "source": "r_380",
            "target": "s_1199"
        },
        {
            "source": "r_380",
            "target": "s_789"
        },
        {
            "source": "r_380",
            "target": "s_1070"
        },
        {
            "source": "r_380",
            "target": "s_80"
        },
        {
            "source": "r_381",
            "target": "s_1059"
        },
        {
            "source": "r_381",
            "target": "s_131"
        },
        {
            "source": "r_381",
            "target": "s_1450"
        },
        {
            "source": "r_381",
            "target": "s_870"
        },
        {
            "source": "r_381",
            "target": "s_1339"
        },
        {
            "source": "r_381",
            "target": "s_781"
        },
        {
            "source": "r_381",
            "target": "s_1161"
        },
        {
            "source": "r_381",
            "target": "s_1220"
        },
        {
            "source": "r_382",
            "target": "s_594"
        },
        {
            "source": "r_382",
            "target": "s_1046"
        },
        {
            "source": "r_382",
            "target": "s_332"
        },
        {
            "source": "r_382",
            "target": "s_209"
        },
        {
            "source": "r_382",
            "target": "s_2"
        },
        {
            "source": "r_382",
            "target": "s_1354"
        },
        {
            "source": "r_383",
            "target": "s_1488"
        },
        {
            "source": "r_383",
            "target": "s_260"
        },
        {
            "source": "r_384",
            "target": "s_1488"
        },
        {
            "source": "r_384",
            "target": "s_592"
        },
        {
            "source": "r_384",
            "target": "s_260"
        },
        {
            "source": "r_385",
            "target": "s_154"
        },
        {
            "source": "r_385",
            "target": "s_740"
        },
        {
            "source": "r_385",
            "target": "s_535"
        },
        {
            "source": "r_385",
            "target": "s_1220"
        },
        {
            "source": "r_385",
            "target": "s_972"
        },
        {
            "source": "r_386",
            "target": "s_379"
        },
        {
            "source": "r_386",
            "target": "s_1327"
        },
        {
            "source": "r_386",
            "target": "s_999"
        },
        {
            "source": "r_386",
            "target": "s_921"
        },
        {
            "source": "r_387",
            "target": "s_364"
        },
        {
            "source": "r_387",
            "target": "s_303"
        },
        {
            "source": "r_387",
            "target": "s_316"
        },
        {
            "source": "r_387",
            "target": "s_1214"
        },
        {
            "source": "r_388",
            "target": "s_125"
        },
        {
            "source": "r_388",
            "target": "s_37"
        },
        {
            "source": "r_388",
            "target": "s_1341"
        },
        {
            "source": "r_388",
            "target": "s_745"
        },
        {
            "source": "r_389",
            "target": "s_579"
        },
        {
            "source": "r_389",
            "target": "s_83"
        },
        {
            "source": "r_389",
            "target": "s_212"
        },
        {
            "source": "r_389",
            "target": "s_438"
        },
        {
            "source": "r_389",
            "target": "s_1010"
        },
        {
            "source": "r_389",
            "target": "s_297"
        },
        {
            "source": "r_390",
            "target": "s_28"
        },
        {
            "source": "r_390",
            "target": "s_1292"
        },
        {
            "source": "r_390",
            "target": "s_1469"
        },
        {
            "source": "r_390",
            "target": "s_108"
        },
        {
            "source": "r_391",
            "target": "s_1421"
        },
        {
            "source": "r_391",
            "target": "s_978"
        },
        {
            "source": "r_391",
            "target": "s_521"
        },
        {
            "source": "r_391",
            "target": "s_298"
        },
        {
            "source": "r_391",
            "target": "s_1391"
        },
        {
            "source": "r_391",
            "target": "s_1362"
        },
        {
            "source": "r_391",
            "target": "s_519"
        },
        {
            "source": "r_391",
            "target": "s_443"
        },
        {
            "source": "r_392",
            "target": "s_901"
        },
        {
            "source": "r_392",
            "target": "s_815"
        },
        {
            "source": "r_392",
            "target": "s_697"
        },
        {
            "source": "r_393",
            "target": "s_926"
        },
        {
            "source": "r_393",
            "target": "s_790"
        },
        {
            "source": "r_393",
            "target": "s_1097"
        },
        {
            "source": "r_393",
            "target": "s_733"
        },
        {
            "source": "r_393",
            "target": "s_199"
        },
        {
            "source": "r_393",
            "target": "s_1393"
        },
        {
            "source": "r_393",
            "target": "s_1270"
        },
        {
            "source": "r_393",
            "target": "s_299"
        },
        {
            "source": "r_394",
            "target": "s_1258"
        },
        {
            "source": "r_394",
            "target": "s_1320"
        },
        {
            "source": "r_394",
            "target": "s_252"
        },
        {
            "source": "r_395",
            "target": "s_848"
        },
        {
            "source": "r_395",
            "target": "s_464"
        },
        {
            "source": "r_395",
            "target": "s_1469"
        },
        {
            "source": "r_395",
            "target": "s_670"
        },
        {
            "source": "r_396",
            "target": "s_913"
        },
        {
            "source": "r_396",
            "target": "s_402"
        },
        {
            "source": "r_396",
            "target": "s_529"
        },
        {
            "source": "r_397",
            "target": "s_424"
        },
        {
            "source": "r_397",
            "target": "s_1143"
        },
        {
            "source": "r_397",
            "target": "s_1223"
        },
        {
            "source": "r_398",
            "target": "s_925"
        },
        {
            "source": "r_398",
            "target": "s_1144"
        },
        {
            "source": "r_398",
            "target": "s_1378"
        },
        {
            "source": "r_398",
            "target": "s_211"
        },
        {
            "source": "r_399",
            "target": "s_1373"
        },
        {
            "source": "r_399",
            "target": "s_1047"
        },
        {
            "source": "r_399",
            "target": "s_953"
        },
        {
            "source": "r_399",
            "target": "s_695"
        },
        {
            "source": "r_399",
            "target": "s_468"
        },
        {
            "source": "r_399",
            "target": "s_818"
        },
        {
            "source": "r_400",
            "target": "s_851"
        },
        {
            "source": "r_400",
            "target": "s_119"
        },
        {
            "source": "r_400",
            "target": "s_622"
        },
        {
            "source": "r_400",
            "target": "s_537"
        },
        {
            "source": "r_400",
            "target": "s_436"
        },
        {
            "source": "r_401",
            "target": "s_134"
        },
        {
            "source": "r_401",
            "target": "s_665"
        },
        {
            "source": "r_401",
            "target": "s_1093"
        },
        {
            "source": "r_401",
            "target": "s_353"
        },
        {
            "source": "r_401",
            "target": "s_1279"
        },
        {
            "source": "r_402",
            "target": "s_804"
        },
        {
            "source": "r_402",
            "target": "s_914"
        },
        {
            "source": "r_402",
            "target": "s_178"
        },
        {
            "source": "r_402",
            "target": "s_1318"
        },
        {
            "source": "r_402",
            "target": "s_415"
        },
        {
            "source": "r_402",
            "target": "s_732"
        },
        {
            "source": "r_403",
            "target": "s_663"
        },
        {
            "source": "r_403",
            "target": "s_594"
        },
        {
            "source": "r_403",
            "target": "s_209"
        },
        {
            "source": "r_403",
            "target": "s_1227"
        },
        {
            "source": "r_403",
            "target": "s_2"
        },
        {
            "source": "r_403",
            "target": "s_1354"
        },
        {
            "source": "r_404",
            "target": "s_1169"
        },
        {
            "source": "r_404",
            "target": "s_1109"
        },
        {
            "source": "r_404",
            "target": "s_969"
        },
        {
            "source": "r_404",
            "target": "s_1325"
        },
        {
            "source": "r_405",
            "target": "s_686"
        },
        {
            "source": "r_405",
            "target": "s_1083"
        },
        {
            "source": "r_405",
            "target": "s_530"
        },
        {
            "source": "r_405",
            "target": "s_1287"
        },
        {
            "source": "r_405",
            "target": "s_865"
        },
        {
            "source": "r_405",
            "target": "s_1370"
        },
        {
            "source": "r_405",
            "target": "s_539"
        },
        {
            "source": "r_406",
            "target": "s_53"
        },
        {
            "source": "r_406",
            "target": "s_900"
        },
        {
            "source": "r_407",
            "target": "s_589"
        },
        {
            "source": "r_407",
            "target": "s_16"
        },
        {
            "source": "r_407",
            "target": "s_1006"
        },
        {
            "source": "r_407",
            "target": "s_133"
        },
        {
            "source": "r_408",
            "target": "s_334"
        },
        {
            "source": "r_408",
            "target": "s_162"
        },
        {
            "source": "r_408",
            "target": "s_666"
        },
        {
            "source": "r_408",
            "target": "s_853"
        },
        {
            "source": "r_409",
            "target": "s_126"
        },
        {
            "source": "r_409",
            "target": "s_576"
        },
        {
            "source": "r_409",
            "target": "s_1337"
        },
        {
            "source": "r_409",
            "target": "s_455"
        },
        {
            "source": "r_409",
            "target": "s_1242"
        },
        {
            "source": "r_409",
            "target": "s_138"
        },
        {
            "source": "r_410",
            "target": "s_757"
        },
        {
            "source": "r_410",
            "target": "s_21"
        },
        {
            "source": "r_410",
            "target": "s_269"
        },
        {
            "source": "r_410",
            "target": "s_732"
        },
        {
            "source": "r_411",
            "target": "s_21"
        },
        {
            "source": "r_411",
            "target": "s_732"
        },
        {
            "source": "r_412",
            "target": "s_302"
        },
        {
            "source": "r_412",
            "target": "s_1023"
        },
        {
            "source": "r_413",
            "target": "s_293"
        },
        {
            "source": "r_413",
            "target": "s_1048"
        },
        {
            "source": "r_413",
            "target": "s_290"
        },
        {
            "source": "r_413",
            "target": "s_320"
        },
        {
            "source": "r_413",
            "target": "s_1304"
        },
        {
            "source": "r_413",
            "target": "s_237"
        },
        {
            "source": "r_414",
            "target": "s_342"
        },
        {
            "source": "r_414",
            "target": "s_516"
        },
        {
            "source": "r_414",
            "target": "s_216"
        },
        {
            "source": "r_414",
            "target": "s_687"
        },
        {
            "source": "r_415",
            "target": "s_1357"
        },
        {
            "source": "r_415",
            "target": "s_1250"
        },
        {
            "source": "r_415",
            "target": "s_404"
        },
        {
            "source": "r_415",
            "target": "s_562"
        },
        {
            "source": "r_415",
            "target": "s_685"
        },
        {
            "source": "r_415",
            "target": "s_547"
        },
        {
            "source": "r_415",
            "target": "s_1285"
        },
        {
            "source": "r_415",
            "target": "s_279"
        },
        {
            "source": "r_416",
            "target": "s_417"
        },
        {
            "source": "r_416",
            "target": "s_721"
        },
        {
            "source": "r_416",
            "target": "s_1327"
        },
        {
            "source": "r_416",
            "target": "s_915"
        },
        {
            "source": "r_416",
            "target": "s_921"
        },
        {
            "source": "r_417",
            "target": "s_241"
        },
        {
            "source": "r_417",
            "target": "s_1043"
        },
        {
            "source": "r_417",
            "target": "s_978"
        },
        {
            "source": "r_417",
            "target": "s_519"
        },
        {
            "source": "r_417",
            "target": "s_443"
        },
        {
            "source": "r_417",
            "target": "s_1205"
        },
        {
            "source": "r_418",
            "target": "s_218"
        },
        {
            "source": "r_418",
            "target": "s_798"
        },
        {
            "source": "r_418",
            "target": "s_397"
        },
        {
            "source": "r_418",
            "target": "s_324"
        },
        {
            "source": "r_418",
            "target": "s_84"
        },
        {
            "source": "r_419",
            "target": "s_165"
        },
        {
            "source": "r_419",
            "target": "s_1429"
        },
        {
            "source": "r_419",
            "target": "s_749"
        },
        {
            "source": "r_419",
            "target": "s_612"
        },
        {
            "source": "r_419",
            "target": "s_343"
        },
        {
            "source": "r_420",
            "target": "s_1281"
        },
        {
            "source": "r_420",
            "target": "s_900"
        },
        {
            "source": "r_421",
            "target": "s_784"
        },
        {
            "source": "r_421",
            "target": "s_530"
        },
        {
            "source": "r_421",
            "target": "s_727"
        },
        {
            "source": "r_421",
            "target": "s_1232"
        },
        {
            "source": "r_421",
            "target": "s_539"
        },
        {
            "source": "r_422",
            "target": "s_72"
        },
        {
            "source": "r_422",
            "target": "s_900"
        },
        {
            "source": "r_422",
            "target": "s_519"
        },
        {
            "source": "r_422",
            "target": "s_336"
        },
        {
            "source": "r_422",
            "target": "s_876"
        },
        {
            "source": "r_423",
            "target": "s_473"
        },
        {
            "source": "r_423",
            "target": "s_125"
        },
        {
            "source": "r_423",
            "target": "s_774"
        },
        {
            "source": "r_423",
            "target": "s_493"
        },
        {
            "source": "r_423",
            "target": "s_1346"
        },
        {
            "source": "r_423",
            "target": "s_540"
        },
        {
            "source": "r_424",
            "target": "s_81"
        },
        {
            "source": "r_424",
            "target": "s_1392"
        },
        {
            "source": "r_424",
            "target": "s_181"
        },
        {
            "source": "r_424",
            "target": "s_1087"
        },
        {
            "source": "r_424",
            "target": "s_472"
        },
        {
            "source": "r_424",
            "target": "s_337"
        },
        {
            "source": "r_424",
            "target": "s_405"
        },
        {
            "source": "r_425",
            "target": "s_452"
        },
        {
            "source": "r_425",
            "target": "s_1331"
        },
        {
            "source": "r_425",
            "target": "s_722"
        },
        {
            "source": "r_425",
            "target": "s_529"
        },
        {
            "source": "r_425",
            "target": "s_469"
        },
        {
            "source": "r_426",
            "target": "s_257"
        },
        {
            "source": "r_426",
            "target": "s_1106"
        },
        {
            "source": "r_426",
            "target": "s_347"
        },
        {
            "source": "r_426",
            "target": "s_1085"
        },
        {
            "source": "r_426",
            "target": "s_552"
        },
        {
            "source": "r_426",
            "target": "s_836"
        },
        {
            "source": "r_426",
            "target": "s_1293"
        },
        {
            "source": "r_426",
            "target": "s_422"
        },
        {
            "source": "r_426",
            "target": "s_459"
        },
        {
            "source": "r_427",
            "target": "s_954"
        },
        {
            "source": "r_427",
            "target": "s_187"
        },
        {
            "source": "r_427",
            "target": "s_1475"
        },
        {
            "source": "r_427",
            "target": "s_911"
        },
        {
            "source": "r_427",
            "target": "s_1202"
        },
        {
            "source": "r_427",
            "target": "s_247"
        },
        {
            "source": "r_427",
            "target": "s_456"
        },
        {
            "source": "r_427",
            "target": "s_1052"
        },
        {
            "source": "r_427",
            "target": "s_1199"
        },
        {
            "source": "r_428",
            "target": "s_294"
        },
        {
            "source": "r_428",
            "target": "s_815"
        },
        {
            "source": "r_428",
            "target": "s_697"
        },
        {
            "source": "r_429",
            "target": "s_9"
        },
        {
            "source": "r_429",
            "target": "s_1149"
        },
        {
            "source": "r_429",
            "target": "s_317"
        },
        {
            "source": "r_429",
            "target": "s_174"
        },
        {
            "source": "r_430",
            "target": "s_134"
        },
        {
            "source": "r_430",
            "target": "s_1301"
        },
        {
            "source": "r_430",
            "target": "s_353"
        },
        {
            "source": "r_430",
            "target": "s_1121"
        },
        {
            "source": "r_431",
            "target": "s_585"
        },
        {
            "source": "r_431",
            "target": "s_369"
        },
        {
            "source": "r_431",
            "target": "s_228"
        },
        {
            "source": "r_431",
            "target": "s_679"
        },
        {
            "source": "r_432",
            "target": "s_743"
        },
        {
            "source": "r_432",
            "target": "s_708"
        },
        {
            "source": "r_433",
            "target": "s_1048"
        },
        {
            "source": "r_433",
            "target": "s_1060"
        },
        {
            "source": "r_433",
            "target": "s_1257"
        },
        {
            "source": "r_433",
            "target": "s_237"
        },
        {
            "source": "r_433",
            "target": "s_293"
        },
        {
            "source": "r_434",
            "target": "s_1201"
        },
        {
            "source": "r_434",
            "target": "s_1074"
        },
        {
            "source": "r_434",
            "target": "s_985"
        },
        {
            "source": "r_434",
            "target": "s_1216"
        },
        {
            "source": "r_434",
            "target": "s_656"
        },
        {
            "source": "r_434",
            "target": "s_687"
        },
        {
            "source": "r_435",
            "target": "s_695"
        },
        {
            "source": "r_435",
            "target": "s_818"
        },
        {
            "source": "r_435",
            "target": "s_953"
        },
        {
            "source": "r_435",
            "target": "s_1245"
        },
        {
            "source": "r_435",
            "target": "s_367"
        },
        {
            "source": "r_436",
            "target": "s_1375"
        },
        {
            "source": "r_436",
            "target": "s_207"
        },
        {
            "source": "r_436",
            "target": "s_831"
        },
        {
            "source": "r_436",
            "target": "s_1128"
        },
        {
            "source": "r_437",
            "target": "s_1317"
        },
        {
            "source": "r_437",
            "target": "s_1476"
        },
        {
            "source": "r_438",
            "target": "s_940"
        },
        {
            "source": "r_438",
            "target": "s_925"
        },
        {
            "source": "r_438",
            "target": "s_891"
        },
        {
            "source": "r_439",
            "target": "s_1108"
        },
        {
            "source": "r_439",
            "target": "s_1482"
        },
        {
            "source": "r_439",
            "target": "s_411"
        },
        {
            "source": "r_439",
            "target": "s_1115"
        },
        {
            "source": "r_439",
            "target": "s_981"
        },
        {
            "source": "r_439",
            "target": "s_719"
        },
        {
            "source": "r_439",
            "target": "s_655"
        },
        {
            "source": "r_439",
            "target": "s_137"
        },
        {
            "source": "r_440",
            "target": "s_203"
        },
        {
            "source": "r_440",
            "target": "s_108"
        },
        {
            "source": "r_441",
            "target": "s_1278"
        },
        {
            "source": "r_441",
            "target": "s_546"
        },
        {
            "source": "r_441",
            "target": "s_112"
        },
        {
            "source": "r_441",
            "target": "s_691"
        },
        {
            "source": "r_442",
            "target": "s_907"
        },
        {
            "source": "r_442",
            "target": "s_160"
        },
        {
            "source": "r_443",
            "target": "s_808"
        },
        {
            "source": "r_443",
            "target": "s_1225"
        },
        {
            "source": "r_443",
            "target": "s_22"
        },
        {
            "source": "r_443",
            "target": "s_600"
        },
        {
            "source": "r_443",
            "target": "s_619"
        },
        {
            "source": "r_443",
            "target": "s_1467"
        },
        {
            "source": "r_444",
            "target": "s_1485"
        },
        {
            "source": "r_444",
            "target": "s_343"
        },
        {
            "source": "r_445",
            "target": "s_225"
        },
        {
            "source": "r_445",
            "target": "s_917"
        },
        {
            "source": "r_445",
            "target": "s_896"
        },
        {
            "source": "r_445",
            "target": "s_1283"
        },
        {
            "source": "r_445",
            "target": "s_137"
        },
        {
            "source": "r_446",
            "target": "s_1303"
        },
        {
            "source": "r_446",
            "target": "s_718"
        },
        {
            "source": "r_446",
            "target": "s_549"
        },
        {
            "source": "r_446",
            "target": "s_388"
        },
        {
            "source": "r_447",
            "target": "s_251"
        },
        {
            "source": "r_447",
            "target": "s_299"
        },
        {
            "source": "r_448",
            "target": "s_752"
        },
        {
            "source": "r_448",
            "target": "s_1480"
        },
        {
            "source": "r_448",
            "target": "s_977"
        },
        {
            "source": "r_448",
            "target": "s_1226"
        },
        {
            "source": "r_448",
            "target": "s_297"
        },
        {
            "source": "r_449",
            "target": "s_585"
        },
        {
            "source": "r_449",
            "target": "s_369"
        },
        {
            "source": "r_449",
            "target": "s_151"
        },
        {
            "source": "r_449",
            "target": "s_1444"
        },
        {
            "source": "r_449",
            "target": "s_349"
        },
        {
            "source": "r_449",
            "target": "s_679"
        },
        {
            "source": "r_450",
            "target": "s_1317"
        },
        {
            "source": "r_450",
            "target": "s_1476"
        },
        {
            "source": "r_451",
            "target": "s_1291"
        },
        {
            "source": "r_451",
            "target": "s_1101"
        },
        {
            "source": "r_452",
            "target": "s_491"
        },
        {
            "source": "r_452",
            "target": "s_1354"
        },
        {
            "source": "r_452",
            "target": "s_332"
        },
        {
            "source": "r_452",
            "target": "s_209"
        },
        {
            "source": "r_453",
            "target": "s_865"
        },
        {
            "source": "r_453",
            "target": "s_360"
        },
        {
            "source": "r_453",
            "target": "s_727"
        },
        {
            "source": "r_453",
            "target": "s_1231"
        },
        {
            "source": "r_453",
            "target": "s_63"
        },
        {
            "source": "r_454",
            "target": "s_1160"
        },
        {
            "source": "r_454",
            "target": "s_830"
        },
        {
            "source": "r_454",
            "target": "s_1381"
        },
        {
            "source": "r_454",
            "target": "s_773"
        },
        {
            "source": "r_455",
            "target": "s_598"
        },
        {
            "source": "r_455",
            "target": "s_382"
        },
        {
            "source": "r_455",
            "target": "s_519"
        },
        {
            "source": "r_455",
            "target": "s_838"
        },
        {
            "source": "r_455",
            "target": "s_19"
        },
        {
            "source": "r_456",
            "target": "s_471"
        },
        {
            "source": "r_456",
            "target": "s_1432"
        },
        {
            "source": "r_456",
            "target": "s_122"
        },
        {
            "source": "r_456",
            "target": "s_141"
        },
        {
            "source": "r_456",
            "target": "s_847"
        },
        {
            "source": "r_456",
            "target": "s_353"
        },
        {
            "source": "r_457",
            "target": "s_288"
        },
        {
            "source": "r_457",
            "target": "s_763"
        },
        {
            "source": "r_457",
            "target": "s_1412"
        },
        {
            "source": "r_457",
            "target": "s_1447"
        },
        {
            "source": "r_457",
            "target": "s_295"
        },
        {
            "source": "r_458",
            "target": "s_256"
        },
        {
            "source": "r_458",
            "target": "s_881"
        },
        {
            "source": "r_458",
            "target": "s_1283"
        },
        {
            "source": "r_458",
            "target": "s_877"
        },
        {
            "source": "r_458",
            "target": "s_315"
        },
        {
            "source": "r_458",
            "target": "s_581"
        },
        {
            "source": "r_459",
            "target": "s_1219"
        },
        {
            "source": "r_459",
            "target": "s_540"
        },
        {
            "source": "r_459",
            "target": "s_1126"
        },
        {
            "source": "r_459",
            "target": "s_418"
        },
        {
            "source": "r_459",
            "target": "s_341"
        },
        {
            "source": "r_459",
            "target": "s_1346"
        },
        {
            "source": "r_460",
            "target": "s_1284"
        },
        {
            "source": "r_460",
            "target": "s_1255"
        },
        {
            "source": "r_460",
            "target": "s_827"
        },
        {
            "source": "r_460",
            "target": "s_497"
        },
        {
            "source": "r_461",
            "target": "s_450"
        },
        {
            "source": "r_461",
            "target": "s_1125"
        },
        {
            "source": "r_461",
            "target": "s_146"
        },
        {
            "source": "r_461",
            "target": "s_1397"
        },
        {
            "source": "r_461",
            "target": "s_1462"
        },
        {
            "source": "r_461",
            "target": "s_1104"
        },
        {
            "source": "r_461",
            "target": "s_632"
        },
        {
            "source": "r_461",
            "target": "s_879"
        },
        {
            "source": "r_461",
            "target": "s_1069"
        },
        {
            "source": "r_462",
            "target": "s_536"
        },
        {
            "source": "r_462",
            "target": "s_586"
        },
        {
            "source": "r_462",
            "target": "s_79"
        },
        {
            "source": "r_462",
            "target": "s_1453"
        },
        {
            "source": "r_462",
            "target": "s_502"
        },
        {
            "source": "r_463",
            "target": "s_267"
        },
        {
            "source": "r_463",
            "target": "s_937"
        },
        {
            "source": "r_463",
            "target": "s_149"
        },
        {
            "source": "r_463",
            "target": "s_1488"
        },
        {
            "source": "r_463",
            "target": "s_78"
        },
        {
            "source": "r_463",
            "target": "s_592"
        },
        {
            "source": "r_463",
            "target": "s_209"
        },
        {
            "source": "r_464",
            "target": "s_906"
        },
        {
            "source": "r_464",
            "target": "s_975"
        },
        {
            "source": "r_464",
            "target": "s_1148"
        },
        {
            "source": "r_464",
            "target": "s_7"
        },
        {
            "source": "r_465",
            "target": "s_37"
        },
        {
            "source": "r_465",
            "target": "s_545"
        },
        {
            "source": "r_465",
            "target": "s_540"
        },
        {
            "source": "r_465",
            "target": "s_1003"
        },
        {
            "source": "r_465",
            "target": "s_493"
        },
        {
            "source": "r_466",
            "target": "s_1059"
        },
        {
            "source": "r_466",
            "target": "s_741"
        },
        {
            "source": "r_466",
            "target": "s_690"
        },
        {
            "source": "r_466",
            "target": "s_173"
        },
        {
            "source": "r_466",
            "target": "s_1147"
        },
        {
            "source": "r_466",
            "target": "s_1306"
        },
        {
            "source": "r_466",
            "target": "s_1339"
        },
        {
            "source": "r_466",
            "target": "s_589"
        },
        {
            "source": "r_466",
            "target": "s_250"
        },
        {
            "source": "r_466",
            "target": "s_1220"
        },
        {
            "source": "r_467",
            "target": "s_1009"
        },
        {
            "source": "r_467",
            "target": "s_60"
        },
        {
            "source": "r_467",
            "target": "s_1191"
        },
        {
            "source": "r_467",
            "target": "s_711"
        },
        {
            "source": "r_467",
            "target": "s_105"
        },
        {
            "source": "r_467",
            "target": "s_924"
        },
        {
            "source": "r_467",
            "target": "s_137"
        },
        {
            "source": "r_468",
            "target": "s_955"
        },
        {
            "source": "r_468",
            "target": "s_687"
        },
        {
            "source": "r_469",
            "target": "s_1169"
        },
        {
            "source": "r_469",
            "target": "s_349"
        },
        {
            "source": "r_469",
            "target": "s_1100"
        },
        {
            "source": "r_469",
            "target": "s_210"
        },
        {
            "source": "r_469",
            "target": "s_1487"
        },
        {
            "source": "r_469",
            "target": "s_380"
        },
        {
            "source": "r_470",
            "target": "s_1233"
        },
        {
            "source": "r_470",
            "target": "s_260"
        },
        {
            "source": "r_470",
            "target": "s_478"
        },
        {
            "source": "r_470",
            "target": "s_1455"
        },
        {
            "source": "r_470",
            "target": "s_548"
        },
        {
            "source": "r_470",
            "target": "s_90"
        },
        {
            "source": "r_470",
            "target": "s_520"
        },
        {
            "source": "r_470",
            "target": "s_1414"
        },
        {
            "source": "r_470",
            "target": "s_380"
        },
        {
            "source": "r_470",
            "target": "s_642"
        },
        {
            "source": "r_471",
            "target": "s_609"
        },
        {
            "source": "r_471",
            "target": "s_138"
        },
        {
            "source": "r_472",
            "target": "s_791"
        },
        {
            "source": "r_472",
            "target": "s_157"
        },
        {
            "source": "r_472",
            "target": "s_732"
        },
        {
            "source": "r_473",
            "target": "s_582"
        },
        {
            "source": "r_473",
            "target": "s_104"
        },
        {
            "source": "r_473",
            "target": "s_873"
        },
        {
            "source": "r_473",
            "target": "s_13"
        },
        {
            "source": "r_473",
            "target": "s_207"
        },
        {
            "source": "r_474",
            "target": "s_1145"
        },
        {
            "source": "r_474",
            "target": "s_528"
        },
        {
            "source": "r_474",
            "target": "s_614"
        },
        {
            "source": "r_474",
            "target": "s_932"
        },
        {
            "source": "r_474",
            "target": "s_737"
        },
        {
            "source": "r_474",
            "target": "s_81"
        },
        {
            "source": "r_474",
            "target": "s_1335"
        },
        {
            "source": "r_474",
            "target": "s_1087"
        },
        {
            "source": "r_475",
            "target": "s_1417"
        },
        {
            "source": "r_475",
            "target": "s_180"
        },
        {
            "source": "r_475",
            "target": "s_219"
        },
        {
            "source": "r_475",
            "target": "s_377"
        },
        {
            "source": "r_475",
            "target": "s_1045"
        },
        {
            "source": "r_475",
            "target": "s_344"
        },
        {
            "source": "r_475",
            "target": "s_972"
        },
        {
            "source": "r_476",
            "target": "s_1059"
        },
        {
            "source": "r_476",
            "target": "s_179"
        },
        {
            "source": "r_476",
            "target": "s_131"
        },
        {
            "source": "r_476",
            "target": "s_1297"
        },
        {
            "source": "r_476",
            "target": "s_1161"
        },
        {
            "source": "r_476",
            "target": "s_1339"
        },
        {
            "source": "r_476",
            "target": "s_589"
        },
        {
            "source": "r_477",
            "target": "s_506"
        },
        {
            "source": "r_477",
            "target": "s_941"
        },
        {
            "source": "r_477",
            "target": "s_1250"
        },
        {
            "source": "r_477",
            "target": "s_693"
        },
        {
            "source": "r_477",
            "target": "s_43"
        },
        {
            "source": "r_477",
            "target": "s_1464"
        },
        {
            "source": "r_477",
            "target": "s_770"
        },
        {
            "source": "r_477",
            "target": "s_964"
        },
        {
            "source": "r_477",
            "target": "s_64"
        },
        {
            "source": "r_478",
            "target": "s_689"
        },
        {
            "source": "r_478",
            "target": "s_1178"
        },
        {
            "source": "r_478",
            "target": "s_1030"
        },
        {
            "source": "r_478",
            "target": "s_895"
        },
        {
            "source": "r_479",
            "target": "s_501"
        },
        {
            "source": "r_479",
            "target": "s_1266"
        },
        {
            "source": "r_479",
            "target": "s_1442"
        },
        {
            "source": "r_479",
            "target": "s_363"
        },
        {
            "source": "r_479",
            "target": "s_1467"
        },
        {
            "source": "r_480",
            "target": "s_436"
        },
        {
            "source": "r_480",
            "target": "s_622"
        },
        {
            "source": "r_480",
            "target": "s_119"
        },
        {
            "source": "r_480",
            "target": "s_537"
        },
        {
            "source": "r_480",
            "target": "s_851"
        },
        {
            "source": "r_481",
            "target": "s_603"
        },
        {
            "source": "r_481",
            "target": "s_102"
        },
        {
            "source": "r_481",
            "target": "s_1368"
        },
        {
            "source": "r_481",
            "target": "s_590"
        },
        {
            "source": "r_481",
            "target": "s_1346"
        },
        {
            "source": "r_481",
            "target": "s_336"
        },
        {
            "source": "r_482",
            "target": "s_829"
        },
        {
            "source": "r_482",
            "target": "s_589"
        },
        {
            "source": "r_482",
            "target": "s_133"
        },
        {
            "source": "r_483",
            "target": "s_342"
        },
        {
            "source": "r_483",
            "target": "s_1274"
        },
        {
            "source": "r_483",
            "target": "s_687"
        },
        {
            "source": "r_483",
            "target": "s_489"
        },
        {
            "source": "r_483",
            "target": "s_406"
        },
        {
            "source": "r_483",
            "target": "s_919"
        },
        {
            "source": "r_483",
            "target": "s_610"
        },
        {
            "source": "r_483",
            "target": "s_86"
        },
        {
            "source": "r_484",
            "target": "s_333"
        },
        {
            "source": "r_484",
            "target": "s_1299"
        },
        {
            "source": "r_484",
            "target": "s_823"
        },
        {
            "source": "r_484",
            "target": "s_499"
        },
        {
            "source": "r_484",
            "target": "s_300"
        },
        {
            "source": "r_485",
            "target": "s_1173"
        },
        {
            "source": "r_485",
            "target": "s_229"
        },
        {
            "source": "r_485",
            "target": "s_82"
        },
        {
            "source": "r_485",
            "target": "s_1035"
        },
        {
            "source": "r_485",
            "target": "s_809"
        },
        {
            "source": "r_486",
            "target": "s_726"
        },
        {
            "source": "r_486",
            "target": "s_696"
        },
        {
            "source": "r_486",
            "target": "s_1116"
        },
        {
            "source": "r_486",
            "target": "s_1469"
        },
        {
            "source": "r_486",
            "target": "s_1068"
        },
        {
            "source": "r_487",
            "target": "s_1076"
        },
        {
            "source": "r_487",
            "target": "s_540"
        },
        {
            "source": "r_487",
            "target": "s_948"
        },
        {
            "source": "r_487",
            "target": "s_273"
        },
        {
            "source": "r_487",
            "target": "s_523"
        },
        {
            "source": "r_487",
            "target": "s_164"
        },
        {
            "source": "r_487",
            "target": "s_1102"
        },
        {
            "source": "r_487",
            "target": "s_595"
        },
        {
            "source": "r_487",
            "target": "s_1003"
        },
        {
            "source": "r_488",
            "target": "s_245"
        },
        {
            "source": "r_488",
            "target": "s_757"
        },
        {
            "source": "r_488",
            "target": "s_602"
        },
        {
            "source": "r_488",
            "target": "s_732"
        },
        {
            "source": "r_489",
            "target": "s_18"
        },
        {
            "source": "r_489",
            "target": "s_181"
        },
        {
            "source": "r_489",
            "target": "s_1087"
        },
        {
            "source": "r_489",
            "target": "s_49"
        },
        {
            "source": "r_489",
            "target": "s_998"
        },
        {
            "source": "r_490",
            "target": "s_513"
        },
        {
            "source": "r_490",
            "target": "s_1472"
        },
        {
            "source": "r_490",
            "target": "s_235"
        },
        {
            "source": "r_490",
            "target": "s_190"
        },
        {
            "source": "r_490",
            "target": "s_54"
        },
        {
            "source": "r_490",
            "target": "s_171"
        },
        {
            "source": "r_490",
            "target": "s_297"
        },
        {
            "source": "r_491",
            "target": "s_1275"
        },
        {
            "source": "r_491",
            "target": "s_864"
        },
        {
            "source": "r_491",
            "target": "s_334"
        },
        {
            "source": "r_491",
            "target": "s_1469"
        },
        {
            "source": "r_492",
            "target": "s_1419"
        },
        {
            "source": "r_492",
            "target": "s_1425"
        },
        {
            "source": "r_492",
            "target": "s_828"
        },
        {
            "source": "r_492",
            "target": "s_1300"
        },
        {
            "source": "r_492",
            "target": "s_923"
        },
        {
            "source": "r_492",
            "target": "s_1225"
        },
        {
            "source": "r_493",
            "target": "s_183"
        },
        {
            "source": "r_493",
            "target": "s_256"
        },
        {
            "source": "r_493",
            "target": "s_640"
        },
        {
            "source": "r_493",
            "target": "s_1283"
        },
        {
            "source": "r_493",
            "target": "s_47"
        },
        {
            "source": "r_493",
            "target": "s_912"
        },
        {
            "source": "r_494",
            "target": "s_639"
        },
        {
            "source": "r_494",
            "target": "s_355"
        },
        {
            "source": "r_494",
            "target": "s_687"
        },
        {
            "source": "r_495",
            "target": "s_1099"
        },
        {
            "source": "r_495",
            "target": "s_775"
        },
        {
            "source": "r_495",
            "target": "s_335"
        },
        {
            "source": "r_495",
            "target": "s_118"
        },
        {
            "source": "r_496",
            "target": "s_1244"
        },
        {
            "source": "r_496",
            "target": "s_965"
        },
        {
            "source": "r_496",
            "target": "s_976"
        },
        {
            "source": "r_496",
            "target": "s_137"
        },
        {
            "source": "r_497",
            "target": "s_391"
        },
        {
            "source": "r_497",
            "target": "s_900"
        },
        {
            "source": "r_497",
            "target": "s_443"
        },
        {
            "source": "r_497",
            "target": "s_367"
        },
        {
            "source": "r_498",
            "target": "s_626"
        },
        {
            "source": "r_498",
            "target": "s_869"
        },
        {
            "source": "r_498",
            "target": "s_1077"
        },
        {
            "source": "r_498",
            "target": "s_885"
        },
        {
            "source": "r_498",
            "target": "s_647"
        },
        {
            "source": "r_498",
            "target": "s_1346"
        },
        {
            "source": "r_499",
            "target": "s_571"
        },
        {
            "source": "r_499",
            "target": "s_1301"
        },
        {
            "source": "r_499",
            "target": "s_471"
        },
        {
            "source": "r_499",
            "target": "s_1400"
        },
        {
            "source": "r_499",
            "target": "s_353"
        },
        {
            "source": "r_499",
            "target": "s_1121"
        },
        {
            "source": "r_500",
            "target": "s_1123"
        },
        {
            "source": "r_500",
            "target": "s_1250"
        },
        {
            "source": "r_500",
            "target": "s_1090"
        },
        {
            "source": "r_500",
            "target": "s_888"
        },
        {
            "source": "r_500",
            "target": "s_304"
        },
        {
            "source": "r_500",
            "target": "s_921"
        },
        {
            "source": "r_501",
            "target": "s_1136"
        },
        {
            "source": "r_501",
            "target": "s_1037"
        },
        {
            "source": "r_501",
            "target": "s_779"
        },
        {
            "source": "r_501",
            "target": "s_370"
        },
        {
            "source": "r_502",
            "target": "s_1141"
        },
        {
            "source": "r_502",
            "target": "s_15"
        },
        {
            "source": "r_502",
            "target": "s_383"
        },
        {
            "source": "r_502",
            "target": "s_138"
        },
        {
            "source": "r_503",
            "target": "s_910"
        },
        {
            "source": "r_503",
            "target": "s_848"
        },
        {
            "source": "r_503",
            "target": "s_1041"
        },
        {
            "source": "r_503",
            "target": "s_115"
        },
        {
            "source": "r_503",
            "target": "s_321"
        },
        {
            "source": "r_503",
            "target": "s_1460"
        },
        {
            "source": "r_504",
            "target": "s_1322"
        },
        {
            "source": "r_504",
            "target": "s_573"
        },
        {
            "source": "r_504",
            "target": "s_76"
        },
        {
            "source": "r_504",
            "target": "s_1214"
        },
        {
            "source": "r_505",
            "target": "s_83"
        },
        {
            "source": "r_505",
            "target": "s_1472"
        },
        {
            "source": "r_505",
            "target": "s_438"
        },
        {
            "source": "r_505",
            "target": "s_297"
        },
        {
            "source": "r_506",
            "target": "s_266"
        },
        {
            "source": "r_506",
            "target": "s_1044"
        },
        {
            "source": "r_506",
            "target": "s_1395"
        },
        {
            "source": "r_506",
            "target": "s_1343"
        },
        {
            "source": "r_507",
            "target": "s_974"
        },
        {
            "source": "r_507",
            "target": "s_927"
        },
        {
            "source": "r_507",
            "target": "s_407"
        },
        {
            "source": "r_507",
            "target": "s_1311"
        },
        {
            "source": "r_507",
            "target": "s_457"
        },
        {
            "source": "r_508",
            "target": "s_1225"
        },
        {
            "source": "r_508",
            "target": "s_697"
        },
        {
            "source": "r_509",
            "target": "s_1225"
        },
        {
            "source": "r_509",
            "target": "s_697"
        },
        {
            "source": "r_510",
            "target": "s_787"
        },
        {
            "source": "r_510",
            "target": "s_1273"
        },
        {
            "source": "r_510",
            "target": "s_934"
        },
        {
            "source": "r_510",
            "target": "s_799"
        },
        {
            "source": "r_510",
            "target": "s_1059"
        },
        {
            "source": "r_510",
            "target": "s_1182"
        },
        {
            "source": "r_510",
            "target": "s_556"
        },
        {
            "source": "r_510",
            "target": "s_1250"
        },
        {
            "source": "r_511",
            "target": "s_1417"
        },
        {
            "source": "r_511",
            "target": "s_425"
        },
        {
            "source": "r_511",
            "target": "s_88"
        },
        {
            "source": "r_511",
            "target": "s_1206"
        },
        {
            "source": "r_511",
            "target": "s_1214"
        },
        {
            "source": "r_512",
            "target": "s_461"
        },
        {
            "source": "r_512",
            "target": "s_412"
        },
        {
            "source": "r_512",
            "target": "s_221"
        },
        {
            "source": "r_512",
            "target": "s_30"
        },
        {
            "source": "r_513",
            "target": "s_540"
        },
        {
            "source": "r_513",
            "target": "s_493"
        },
        {
            "source": "r_513",
            "target": "s_1346"
        },
        {
            "source": "r_514",
            "target": "s_498"
        },
        {
            "source": "r_514",
            "target": "s_874"
        },
        {
            "source": "r_514",
            "target": "s_900"
        },
        {
            "source": "r_514",
            "target": "s_367"
        },
        {
            "source": "r_515",
            "target": "s_943"
        },
        {
            "source": "r_515",
            "target": "s_433"
        },
        {
            "source": "r_516",
            "target": "s_712"
        },
        {
            "source": "r_516",
            "target": "s_876"
        },
        {
            "source": "r_517",
            "target": "s_1124"
        },
        {
            "source": "r_517",
            "target": "s_467"
        },
        {
            "source": "r_517",
            "target": "s_1226"
        },
        {
            "source": "r_517",
            "target": "s_496"
        },
        {
            "source": "r_517",
            "target": "s_744"
        },
        {
            "source": "r_517",
            "target": "s_875"
        },
        {
            "source": "r_517",
            "target": "s_984"
        },
        {
            "source": "r_518",
            "target": "s_633"
        },
        {
            "source": "r_518",
            "target": "s_725"
        },
        {
            "source": "r_518",
            "target": "s_945"
        },
        {
            "source": "r_518",
            "target": "s_1262"
        },
        {
            "source": "r_518",
            "target": "s_1092"
        },
        {
            "source": "r_518",
            "target": "s_35"
        },
        {
            "source": "r_519",
            "target": "s_1129"
        },
        {
            "source": "r_519",
            "target": "s_1064"
        },
        {
            "source": "r_519",
            "target": "s_841"
        },
        {
            "source": "r_520",
            "target": "s_239"
        },
        {
            "source": "r_520",
            "target": "s_1312"
        },
        {
            "source": "r_520",
            "target": "s_1398"
        },
        {
            "source": "r_520",
            "target": "s_1114"
        },
        {
            "source": "r_520",
            "target": "s_252"
        },
        {
            "source": "r_521",
            "target": "s_1244"
        },
        {
            "source": "r_521",
            "target": "s_453"
        },
        {
            "source": "r_521",
            "target": "s_108"
        },
        {
            "source": "r_522",
            "target": "s_1375"
        },
        {
            "source": "r_522",
            "target": "s_987"
        },
        {
            "source": "r_522",
            "target": "s_801"
        },
        {
            "source": "r_522",
            "target": "s_1441"
        },
        {
            "source": "r_523",
            "target": "s_340"
        },
        {
            "source": "r_523",
            "target": "s_982"
        },
        {
            "source": "r_523",
            "target": "s_345"
        },
        {
            "source": "r_523",
            "target": "s_460"
        },
        {
            "source": "r_523",
            "target": "s_212"
        },
        {
            "source": "r_523",
            "target": "s_297"
        },
        {
            "source": "r_523",
            "target": "s_1001"
        },
        {
            "source": "r_523",
            "target": "s_438"
        },
        {
            "source": "r_523",
            "target": "s_362"
        },
        {
            "source": "r_523",
            "target": "s_959"
        },
        {
            "source": "r_523",
            "target": "s_902"
        },
        {
            "source": "r_523",
            "target": "s_444"
        },
        {
            "source": "r_523",
            "target": "s_606"
        },
        {
            "source": "r_523",
            "target": "s_621"
        },
        {
            "source": "r_523",
            "target": "s_1351"
        },
        {
            "source": "r_523",
            "target": "s_736"
        },
        {
            "source": "r_524",
            "target": "s_608"
        },
        {
            "source": "r_524",
            "target": "s_687"
        },
        {
            "source": "r_524",
            "target": "s_1183"
        },
        {
            "source": "r_525",
            "target": "s_1210"
        },
        {
            "source": "r_525",
            "target": "s_890"
        },
        {
            "source": "r_526",
            "target": "s_1313"
        },
        {
            "source": "r_526",
            "target": "s_1275"
        },
        {
            "source": "r_526",
            "target": "s_1469"
        },
        {
            "source": "r_526",
            "target": "s_268"
        },
        {
            "source": "r_527",
            "target": "s_1246"
        },
        {
            "source": "r_527",
            "target": "s_845"
        },
        {
            "source": "r_527",
            "target": "s_1315"
        },
        {
            "source": "r_527",
            "target": "s_1056"
        },
        {
            "source": "r_528",
            "target": "s_51"
        },
        {
            "source": "r_528",
            "target": "s_3"
        },
        {
            "source": "r_528",
            "target": "s_515"
        },
        {
            "source": "r_528",
            "target": "s_209"
        },
        {
            "source": "r_529",
            "target": "s_1155"
        },
        {
            "source": "r_529",
            "target": "s_1010"
        },
        {
            "source": "r_530",
            "target": "s_678"
        },
        {
            "source": "r_530",
            "target": "s_57"
        },
        {
            "source": "r_530",
            "target": "s_1435"
        },
        {
            "source": "r_530",
            "target": "s_145"
        },
        {
            "source": "r_530",
            "target": "s_181"
        },
        {
            "source": "r_531",
            "target": "s_379"
        },
        {
            "source": "r_531",
            "target": "s_1327"
        },
        {
            "source": "r_531",
            "target": "s_921"
        },
        {
            "source": "r_531",
            "target": "s_71"
        },
        {
            "source": "r_532",
            "target": "s_522"
        },
        {
            "source": "r_532",
            "target": "s_251"
        },
        {
            "source": "r_532",
            "target": "s_481"
        },
        {
            "source": "r_532",
            "target": "s_1406"
        },
        {
            "source": "r_532",
            "target": "s_299"
        },
        {
            "source": "r_532",
            "target": "s_214"
        },
        {
            "source": "r_533",
            "target": "s_23"
        },
        {
            "source": "r_533",
            "target": "s_819"
        },
        {
            "source": "r_533",
            "target": "s_876"
        },
        {
            "source": "r_533",
            "target": "s_733"
        },
        {
            "source": "r_533",
            "target": "s_336"
        },
        {
            "source": "r_534",
            "target": "s_157"
        },
        {
            "source": "r_534",
            "target": "s_553"
        },
        {
            "source": "r_534",
            "target": "s_732"
        },
        {
            "source": "r_534",
            "target": "s_616"
        },
        {
            "source": "r_535",
            "target": "s_1233"
        },
        {
            "source": "r_535",
            "target": "s_868"
        },
        {
            "source": "r_535",
            "target": "s_1411"
        },
        {
            "source": "r_535",
            "target": "s_1414"
        },
        {
            "source": "r_535",
            "target": "s_260"
        },
        {
            "source": "r_536",
            "target": "s_999"
        },
        {
            "source": "r_536",
            "target": "s_1327"
        },
        {
            "source": "r_536",
            "target": "s_417"
        },
        {
            "source": "r_536",
            "target": "s_771"
        },
        {
            "source": "r_536",
            "target": "s_946"
        },
        {
            "source": "r_536",
            "target": "s_921"
        },
        {
            "source": "r_537",
            "target": "s_73"
        },
        {
            "source": "r_537",
            "target": "s_841"
        },
        {
            "source": "r_537",
            "target": "s_928"
        },
        {
            "source": "r_537",
            "target": "s_1027"
        },
        {
            "source": "r_537",
            "target": "s_855"
        },
        {
            "source": "r_537",
            "target": "s_1105"
        },
        {
            "source": "r_538",
            "target": "s_292"
        },
        {
            "source": "r_538",
            "target": "s_1007"
        },
        {
            "source": "r_538",
            "target": "s_1227"
        },
        {
            "source": "r_538",
            "target": "s_367"
        },
        {
            "source": "r_538",
            "target": "s_874"
        },
        {
            "source": "r_539",
            "target": "s_1122"
        },
        {
            "source": "r_539",
            "target": "s_1214"
        },
        {
            "source": "r_539",
            "target": "s_88"
        },
        {
            "source": "r_539",
            "target": "s_671"
        },
        {
            "source": "r_539",
            "target": "s_316"
        },
        {
            "source": "r_539",
            "target": "s_1206"
        },
        {
            "source": "r_539",
            "target": "s_1479"
        },
        {
            "source": "r_539",
            "target": "s_272"
        },
        {
            "source": "r_539",
            "target": "s_1022"
        },
        {
            "source": "r_540",
            "target": "s_816"
        },
        {
            "source": "r_540",
            "target": "s_528"
        },
        {
            "source": "r_540",
            "target": "s_1145"
        },
        {
            "source": "r_540",
            "target": "s_737"
        },
        {
            "source": "r_541",
            "target": "s_615"
        },
        {
            "source": "r_541",
            "target": "s_277"
        },
        {
            "source": "r_541",
            "target": "s_715"
        },
        {
            "source": "r_541",
            "target": "s_189"
        },
        {
            "source": "r_541",
            "target": "s_960"
        },
        {
            "source": "r_541",
            "target": "s_519"
        },
        {
            "source": "r_541",
            "target": "s_818"
        },
        {
            "source": "r_542",
            "target": "s_670"
        },
        {
            "source": "r_542",
            "target": "s_1187"
        },
        {
            "source": "r_542",
            "target": "s_234"
        },
        {
            "source": "r_543",
            "target": "s_477"
        },
        {
            "source": "r_543",
            "target": "s_1260"
        },
        {
            "source": "r_543",
            "target": "s_137"
        },
        {
            "source": "r_544",
            "target": "s_449"
        },
        {
            "source": "r_544",
            "target": "s_443"
        },
        {
            "source": "r_544",
            "target": "s_519"
        },
        {
            "source": "r_544",
            "target": "s_1421"
        },
        {
            "source": "r_545",
            "target": "s_536"
        },
        {
            "source": "r_545",
            "target": "s_1366"
        },
        {
            "source": "r_545",
            "target": "s_502"
        },
        {
            "source": "r_546",
            "target": "s_96"
        },
        {
            "source": "r_546",
            "target": "s_607"
        },
        {
            "source": "r_546",
            "target": "s_1225"
        },
        {
            "source": "r_547",
            "target": "s_785"
        },
        {
            "source": "r_547",
            "target": "s_769"
        },
        {
            "source": "r_547",
            "target": "s_1156"
        },
        {
            "source": "r_547",
            "target": "s_61"
        },
        {
            "source": "r_547",
            "target": "s_878"
        },
        {
            "source": "r_548",
            "target": "s_1059"
        },
        {
            "source": "r_548",
            "target": "s_434"
        },
        {
            "source": "r_548",
            "target": "s_1339"
        },
        {
            "source": "r_548",
            "target": "s_1242"
        },
        {
            "source": "r_548",
            "target": "s_1220"
        },
        {
            "source": "r_548",
            "target": "s_138"
        },
        {
            "source": "r_549",
            "target": "s_201"
        },
        {
            "source": "r_549",
            "target": "s_605"
        },
        {
            "source": "r_550",
            "target": "s_1281"
        },
        {
            "source": "r_550",
            "target": "s_862"
        },
        {
            "source": "r_550",
            "target": "s_900"
        },
        {
            "source": "r_551",
            "target": "s_648"
        },
        {
            "source": "r_551",
            "target": "s_1078"
        },
        {
            "source": "r_552",
            "target": "s_996"
        },
        {
            "source": "r_552",
            "target": "s_550"
        },
        {
            "source": "r_552",
            "target": "s_821"
        },
        {
            "source": "r_552",
            "target": "s_282"
        },
        {
            "source": "r_552",
            "target": "s_890"
        },
        {
            "source": "r_553",
            "target": "s_1024"
        },
        {
            "source": "r_553",
            "target": "s_29"
        },
        {
            "source": "r_553",
            "target": "s_1091"
        },
        {
            "source": "r_553",
            "target": "s_980"
        },
        {
            "source": "r_553",
            "target": "s_380"
        },
        {
            "source": "r_554",
            "target": "s_985"
        },
        {
            "source": "r_554",
            "target": "s_1446"
        },
        {
            "source": "r_554",
            "target": "s_1456"
        },
        {
            "source": "r_554",
            "target": "s_656"
        },
        {
            "source": "r_555",
            "target": "s_702"
        },
        {
            "source": "r_555",
            "target": "s_900"
        },
        {
            "source": "r_556",
            "target": "s_1078"
        },
        {
            "source": "r_556",
            "target": "s_648"
        },
        {
            "source": "r_556",
            "target": "s_1111"
        },
        {
            "source": "r_557",
            "target": "s_1146"
        },
        {
            "source": "r_557",
            "target": "s_1269"
        },
        {
            "source": "r_557",
            "target": "s_1148"
        },
        {
            "source": "r_558",
            "target": "s_1408"
        },
        {
            "source": "r_558",
            "target": "s_897"
        },
        {
            "source": "r_558",
            "target": "s_1049"
        },
        {
            "source": "r_558",
            "target": "s_484"
        },
        {
            "source": "r_558",
            "target": "s_252"
        },
        {
            "source": "r_559",
            "target": "s_1319"
        },
        {
            "source": "r_559",
            "target": "s_1326"
        },
        {
            "source": "r_559",
            "target": "s_1158"
        },
        {
            "source": "r_559",
            "target": "s_310"
        },
        {
            "source": "r_559",
            "target": "s_1012"
        },
        {
            "source": "r_560",
            "target": "s_822"
        },
        {
            "source": "r_560",
            "target": "s_865"
        },
        {
            "source": "r_560",
            "target": "s_1214"
        },
        {
            "source": "r_561",
            "target": "s_1111"
        },
        {
            "source": "r_561",
            "target": "s_1078"
        },
        {
            "source": "r_562",
            "target": "s_414"
        },
        {
            "source": "r_562",
            "target": "s_336"
        },
        {
            "source": "r_562",
            "target": "s_24"
        },
        {
            "source": "r_563",
            "target": "s_792"
        },
        {
            "source": "r_563",
            "target": "s_865"
        },
        {
            "source": "r_563",
            "target": "s_1214"
        },
        {
            "source": "r_564",
            "target": "s_323"
        },
        {
            "source": "r_564",
            "target": "s_953"
        },
        {
            "source": "r_564",
            "target": "s_142"
        },
        {
            "source": "r_564",
            "target": "s_765"
        },
        {
            "source": "r_564",
            "target": "s_753"
        }
    ]
}